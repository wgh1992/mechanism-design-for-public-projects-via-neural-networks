{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1587)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from random import randint\n",
    "import random\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "from pylab import *\n",
    "from scipy.stats import beta\n",
    "import math\n",
    "n = 5\n",
    "epochs = 3\n",
    "supervisionEpochs = 2\n",
    "lr = 0.0001\n",
    "log_interval = 20\n",
    "trainSize = 60000#100000\n",
    "percentage_train_test= 0.25\n",
    "penaltyLambda = 10\n",
    "\n",
    "doublePeakHighMean = 0.9\n",
    "doublePeakLowMean = 0.1\n",
    "doublePeakStd = 0.1\n",
    "\n",
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "\n",
    "def cdf(x, i=None):\n",
    "    return (d1.cdf(x) + d2.cdf(x) - distributionBase) / 2 / distributionRatio\n",
    "\n",
    "# def cdf(x, i=None):\n",
    "#     if x < 0.1:\n",
    "#         return 0\n",
    "#     if x <= 0.2:\n",
    "#         return 0.5 * (x - 0.1) / 0.1\n",
    "#     if x < 0.8:\n",
    "#         return 0.5\n",
    "#     if x < 0.9:\n",
    "#         return 0.5 + 0.5 * (x - 0.8) / 0.1\n",
    "#     return 1\n",
    "\n",
    "\n",
    "print(distributionBase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loc 0.1 scale 0.1\n",
      "loc 0.9 scale 0.1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "signals = np.random.randint(2, size=(trainSize, n))\n",
    "samples1 = np.random.normal(\n",
    "    loc=doublePeakLowMean, scale=doublePeakStd, size=(trainSize, n)\n",
    ")\n",
    "for i in range(trainSize):\n",
    "    for j in range(n):\n",
    "        while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "            samples1[i, j] = np.random.normal(\n",
    "                loc=doublePeakLowMean, scale=doublePeakStd\n",
    "            )\n",
    "samples2 = np.random.normal(\n",
    "    loc=doublePeakHighMean, scale=doublePeakStd, size=(trainSize, n)\n",
    ")\n",
    "for i in range(trainSize):\n",
    "    for j in range(n):\n",
    "        while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "            samples2[i, j] = np.random.normal(\n",
    "                loc=doublePeakHighMean, scale=doublePeakStd\n",
    "            )\n",
    "samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "\n",
    "# tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "plt.hist(samplesJoint,bins=500)\n",
    "plt.show()\n",
    "\n",
    "tp_dataset = TensorDataset(tp_tensor[: int(trainSize * percentage_train_test)])\n",
    "tp_dataset_testing = TensorDataset(tp_tensor[int(trainSize * (1.0-percentage_train_test)) :])\n",
    "tp_dataloader = DataLoader(tp_dataset, batch_size=128, shuffle=True)\n",
    "tp_dataloader_testing = DataLoader(tp_dataset_testing, batch_size=256, shuffle=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        #Initializing hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "         # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (rnn): RNN(5, 100, num_layers=3, batch_first=True)\n",
      "  (fc): Linear(in_features=100, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model with hyperparameters\n",
    "model = Model(input_size=n, output_size=n, hidden_dim=100, n_layers=3)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    if torch.sum(bits).item() == 0:\n",
    "        return torch.ones(n)\n",
    "    bits = bits.type(torch.float32)\n",
    "    negBits = torch.ones(n) - bits\n",
    "    bits=torch.reshape(bits,(1,-1,n))\n",
    "    payments = model(bits)\n",
    "    payments=payments[0].view(n)\n",
    "    payments = payments - 1000 * negBits\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    payments = payments + negBits\n",
    "    return payments\n",
    "\n",
    "\n",
    "def tpToBits(tp, deep , bits=torch.ones(n).type(torch.float32)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = torch.sigmoid((tp-payments+0.0001)/0.000001);\n",
    "    \n",
    "    if torch.allclose(newBits, bits) or deep >= n:\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits(tp,deep+1, newBits)\n",
    "\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp,0))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    return n - torch.sum(tpToBits(tp,0).type(torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
    "    # Creating a multi-dimensional array of zeros with the desired output shape\n",
    "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
    "    \n",
    "    # Replacing the 0 at the relevant character index with a 1 to represent that character\n",
    "    for i in range(batch_size):\n",
    "        for u in range(seq_len):\n",
    "            features[i, u, sequence[i][u]] = 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "tensor(0.5000)\n"
     ]
    }
   ],
   "source": [
    "print(1_000_000)\n",
    "print(cdf(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0.8316, 0.0939, 0.9443, 0.1167, 0.1324]]),)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "torch.Size([1, 1, 5])\n",
      "5\n",
      "tensor([[[ 0.0447,  0.0418,  0.0767, -0.0332, -0.0906,  0.0339, -0.0407,\n",
      "           0.1891, -0.1019,  0.2054, -0.1405, -0.1241, -0.0495,  0.2708,\n",
      "           0.0527, -0.0336,  0.0179,  0.0439,  0.1567,  0.0240, -0.0510,\n",
      "           0.0417, -0.0138, -0.0695,  0.1352,  0.0751, -0.0193, -0.0656,\n",
      "          -0.0404, -0.0500, -0.0028,  0.0511,  0.0381,  0.0463,  0.0178,\n",
      "          -0.0515,  0.0029, -0.0383,  0.0889, -0.0286, -0.0964, -0.1119,\n",
      "          -0.0016, -0.1018, -0.1673, -0.1854,  0.0245, -0.1671, -0.0212,\n",
      "           0.0071, -0.1124,  0.1674,  0.0255, -0.1245,  0.0331, -0.0150,\n",
      "           0.0733, -0.0167, -0.0133, -0.0202, -0.0473, -0.0327,  0.1109,\n",
      "           0.0951,  0.1273, -0.0403,  0.1204, -0.1170, -0.0647, -0.2199,\n",
      "           0.0613, -0.0254, -0.2116,  0.1237,  0.0287,  0.2508,  0.0488,\n",
      "          -0.1412,  0.0359, -0.0304, -0.1608,  0.0374, -0.1349, -0.0033,\n",
      "           0.1328,  0.0405,  0.0217,  0.0312,  0.0787, -0.0012,  0.1362,\n",
      "           0.0214, -0.1171,  0.0790,  0.1462,  0.0450,  0.0790, -0.1348,\n",
      "          -0.0403,  0.2094]],\n",
      "\n",
      "        [[-0.0024, -0.0201,  0.1207, -0.0174, -0.0107,  0.1155,  0.0262,\n",
      "           0.0294,  0.0547,  0.0623,  0.0548,  0.1010, -0.0154, -0.0307,\n",
      "          -0.0387,  0.0579, -0.1653,  0.0035,  0.0291, -0.0709,  0.0215,\n",
      "          -0.1148,  0.1219,  0.1553, -0.1442,  0.0274, -0.0175, -0.1173,\n",
      "           0.0934,  0.0012, -0.0139, -0.1141, -0.2092, -0.0710, -0.0274,\n",
      "           0.1858,  0.1173,  0.1255,  0.2628, -0.0820,  0.0093,  0.0626,\n",
      "          -0.0281,  0.0773, -0.0043, -0.0022,  0.0434,  0.0066, -0.1228,\n",
      "           0.0923, -0.0802,  0.0808, -0.1761, -0.0016, -0.0431,  0.1790,\n",
      "           0.0585, -0.0063,  0.1333,  0.1401, -0.1123, -0.0109, -0.0877,\n",
      "           0.1178,  0.1531, -0.0383,  0.0131,  0.0593,  0.0485, -0.0495,\n",
      "          -0.1684, -0.0740, -0.0481, -0.0035,  0.1063, -0.0517, -0.0234,\n",
      "           0.0764,  0.0835, -0.0457, -0.0208, -0.1165,  0.1079,  0.0461,\n",
      "          -0.0854,  0.0315, -0.0106, -0.1159, -0.1724,  0.0185,  0.0638,\n",
      "          -0.0981,  0.1206, -0.0446,  0.0577,  0.0549,  0.0652, -0.1178,\n",
      "           0.1095, -0.1072]],\n",
      "\n",
      "        [[-0.0829,  0.0683, -0.0735, -0.0457, -0.0246,  0.0991, -0.0566,\n",
      "           0.0396, -0.1552,  0.1170,  0.0816, -0.0607,  0.0092,  0.0960,\n",
      "           0.0430,  0.0832,  0.0187, -0.0397,  0.0485, -0.0006,  0.0760,\n",
      "           0.0764,  0.0351,  0.1592,  0.0138,  0.1245,  0.2193,  0.0676,\n",
      "          -0.0445, -0.0956, -0.1104, -0.0547,  0.0172, -0.0013, -0.1641,\n",
      "          -0.0321, -0.0570, -0.0548,  0.0777,  0.1344,  0.1042, -0.0901,\n",
      "          -0.1371, -0.0514,  0.0388,  0.0927, -0.1449,  0.0833,  0.0037,\n",
      "           0.1008,  0.0946, -0.1320, -0.0414, -0.0620, -0.0160,  0.1060,\n",
      "           0.0241, -0.0825,  0.1407,  0.1712, -0.1557,  0.0834, -0.0438,\n",
      "           0.1153,  0.1680,  0.1454, -0.0104, -0.1128, -0.0394,  0.1509,\n",
      "           0.0040, -0.1249, -0.0998, -0.0928,  0.1297, -0.1735, -0.0547,\n",
      "           0.0824, -0.0125, -0.0186,  0.1491, -0.0414, -0.0826,  0.1077,\n",
      "          -0.0367, -0.1641,  0.0388,  0.1430, -0.0913, -0.0603, -0.1512,\n",
      "           0.0727, -0.1678,  0.1690,  0.1017, -0.1818, -0.0649,  0.0233,\n",
      "           0.0323,  0.2584]]], grad_fn=<StackBackward>)\n",
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "print(tp_dataset_testing[0:1])\n",
    "#inputt= one_hot_encode(tp_dataset_testing, n, 1, 1)\n",
    "print(torch.ones(n))\n",
    "\n",
    "inputxx=torch.from_numpy(np.asarray([t.numpy() for t in tp_dataset_testing[0:1]]))\n",
    "print(inputxx.shape)\n",
    "\n",
    "yy=model(inputxx)\n",
    "print(len(yy[0][0]))\n",
    "print(yy[1])\n",
    "print(yy[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpPrecision = 100\n",
    "# howManyPpl left, money left, yes already\n",
    "dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "# ppl = 0 left\n",
    "for yes in range(n + 1):\n",
    "    for money in range(dpPrecision + 1):\n",
    "        if money == 0:\n",
    "            dp[0, 0, yes] = 0\n",
    "        else:\n",
    "            dp[0, money, yes] = yes# + 1.0\n",
    "for ppl in range(1, n + 1):\n",
    "    for yes in range(n + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            minSoFar = 1_000_000\n",
    "            for offerIndex in range(money + 1):\n",
    "                offer = offerIndex / dpPrecision\n",
    "                res = (1 - cdf(offer)) * dp[\n",
    "                    ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                ] + cdf(offer) * (1 + dp[ppl - 1, money, yes])\n",
    "                if minSoFar > res:\n",
    "                    minSoFar = res\n",
    "                    decision[ppl, money, yes] = offerIndex\n",
    "            dp[ppl, money, yes] = minSoFar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8651759624481201\n"
     ]
    }
   ],
   "source": [
    "print(dp[n, dpPrecision, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.88556217 0.89719416 0.07335954 0.23154794 0.13772879]\n",
      " [0.70228435 0.15693893 0.06616393 0.12891229 0.90710533]\n",
      " [0.12985451 0.92104324 0.18029156 0.96205692 0.0281456 ]\n",
      " ...\n",
      " [0.96822778 0.99124001 0.96101865 0.19451355 0.12722927]\n",
      " [0.64530023 0.20911107 0.01530242 0.0072549  0.02844264]\n",
      " [0.89693434 0.98730135 0.09339012 0.83155798 0.02034373]]\n"
     ]
    }
   ],
   "source": [
    "print(samplesJoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_dp(temp):\n",
    "    #print(temp)\n",
    "    remain=dpPrecision\n",
    "    yes=0;\n",
    "    ans =0;\n",
    "    o_list=[];\n",
    "    remain_list=[];\n",
    "    for ppl in range(n,0,-1):\n",
    "        o=decision[ppl, remain, yes]\n",
    "        #print(o,remain)\n",
    "        o_list.append(o)\n",
    "        remain_list.append(remain);\n",
    "        if(o<temp[n-ppl]):\n",
    "            remain-=int(o);\n",
    "            yes+=1;\n",
    "        elif (remain>0):\n",
    "            ans+=1;\n",
    "    if(remain<=0):\n",
    "        return ans,o_list;\n",
    "    else:\n",
    "        return n,o_list;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9\n"
     ]
    }
   ],
   "source": [
    "ans_list=[];\n",
    "for i in range(100):\n",
    "    temp=samplesJoint[i]*dpPrecision\n",
    "    #print(temp)\n",
    "    ans_list.append(plan_dp(temp)[0]);\n",
    "    #print(\"\\n\",temp)\n",
    "    #print(plan_dp(temp))\n",
    "print(sum(ans_list)/len(ans_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    global  dp,decision\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money, yes]\n",
    "        offer = float(offerIndex) / dpPrecision\n",
    "        if tp[i] >= offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits[i] = 0\n",
    "            payments[i] = 1\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "#     print()\n",
    "#     print(tp)\n",
    "#     print(bits)\n",
    "#     print(payments)\n",
    "#     print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def heuristicSupervisionRule(tp):\n",
    "    global  dp_H,decision_H\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    tempold=-1;\n",
    "    for turns in range(n,0,-1):\n",
    "        money = dpPrecision\n",
    "        j=0\n",
    "        tempo=sum(bits)\n",
    "        #print(\"bits\",tempo)\n",
    "        for i in range(n):\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            \n",
    "            offerIndex = decision_H[tempo ,tempo - i, money]\n",
    "            offer = float(offerIndex) / dpPrecision\n",
    "            while(j<n):\n",
    "                if(bits[j]!=0):\n",
    "                    break;\n",
    "                j+=1;\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            if tp[j] >= offer:\n",
    "                #print(money,j,tp[j],offer)\n",
    "                money -= offerIndex\n",
    "                bits[j] = 1\n",
    "                payments[j] = offer\n",
    "            else:\n",
    "                bits[j] = 0;\n",
    "                payments[j] = 1.0;\n",
    "            j+=1;\n",
    "        #print(\"money\",money)\n",
    "        if(money==0 and tempold==tempo):\n",
    "            break;\n",
    "        tempold=tempo;\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    for k in range(n, -1, -1):\n",
    "        if k == 0:\n",
    "            break\n",
    "        bits = [1 if tp[ii] >= 1.0 / k else 0 for ii in range(n)]\n",
    "        if sum(bits) == k:\n",
    "            break\n",
    "    if k == 0:\n",
    "        payments = [1 for ii in range(n)]\n",
    "    else:\n",
    "        payments = [1.0 / k if bits[ii] == 1 else 1 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    return float(n - torch.sum(costSharingSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def dpDelay(tp):\n",
    "    return float(n - torch.sum(dpSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def heuristicDelay(tp):\n",
    "    return float(n - torch.sum(heuristicSupervisionRule(tp)[0]).item())\n",
    "\n",
    "\n",
    "# templire=0;\n",
    "# num=1\n",
    "# for i in range(num):\n",
    "#     temp=torch.tensor(samplesJoint[i])\n",
    "#     #print(temp)\n",
    "#     #print(temp);\n",
    "#     #print(heuristicSupervisionRule(temp))\n",
    "#     #print(dpSupervisionRule(temp))\n",
    "#     #print(dpDelay(temp))\n",
    "#     res=dpDelay(temp)\n",
    "#     templire+=res\n",
    "#     #print(\"delay\",res)\n",
    "#     #print()\n",
    "# print(templire/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "allBits = [torch.tensor(bits) for bits in itertools.product([0, 1], repeat=n)]\n",
    "\n",
    "runningLossNN = []\n",
    "runningLossCS = []\n",
    "runningLossDP = []\n",
    "\n",
    "\n",
    "def recordAndReport(name, source, loss, n=100):\n",
    "    source.append(loss)\n",
    "    realLength = min(n, len(source))\n",
    "    avgLoss = sum(source[-n:]) / realLength\n",
    "    print(f\"{name} ({realLength}): {avgLoss}\")\n",
    "\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        loss = 0\n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            # print()\n",
    "            # print(\"supervision\")\n",
    "            # print(tp)\n",
    "            # print(bits)\n",
    "            # print()\n",
    "            # print(payments)\n",
    "            # print(bitsToPayments(bits))\n",
    "            # print()\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        # costSharingLoss = 0\n",
    "        # dpLoss = 0\n",
    "        for tp in tp_batch:\n",
    "            # costSharingLoss += costSharingDelay(tp)\n",
    "            # dpLoss += dpDelay(tp)\n",
    "            # print()\n",
    "            # print(\"---\")\n",
    "            # print(tp)\n",
    "            # print(costSharingSupervisionRule(tp))\n",
    "            # print(dpSupervisionRule(tp))\n",
    "            # print(costSharingDelay(tp), dpDelay(tp))\n",
    "            # print()\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                delay1 = tpToTotalDelay(tp1)\n",
    "                delay0 = tpToTotalDelay(tp0)\n",
    "                loss = loss + (1 - cdf(offer)) * delay1 + cdf(offer) * delay0\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        # costSharingLoss /= len(tp_batch)\n",
    "        # dpLoss /= len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "            # recordAndReport(\"NN\", runningLossNN, loss.item())\n",
    "            # recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "            # recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "            # print(dp[n, dpPrecision, 0])\n",
    "            # print(penalty.item())\n",
    "            # print(distributionRatio)\n",
    "            # for i in range(n, 0, -1):\n",
    "            #     print(\n",
    "            #         tpToPayments(\n",
    "            #             torch.tensor([1] * i + [0] * (n - i), dtype=torch.float32)\n",
    "            #         )\n",
    "            #     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8413)\n",
      "Train Epoch: 1 [0/15000 (0%)]\tLoss: 0.000346\n",
      "Train Epoch: 1 [2560/15000 (17%)]\tLoss: 0.000011\n",
      "Train Epoch: 1 [5120/15000 (34%)]\tLoss: 0.000007\n",
      "Train Epoch: 1 [7680/15000 (51%)]\tLoss: 0.000001\n",
      "Train Epoch: 1 [10240/15000 (68%)]\tLoss: 0.000001\n",
      "Train Epoch: 1 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "tensor(0.8413)\n",
      "Train Epoch: 2 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/15000 (0%)]\tLoss: 2.585006\n",
      "Train Epoch: 1 [2560/15000 (17%)]\tLoss: 2.598365\n",
      "Train Epoch: 1 [5120/15000 (34%)]\tLoss: 1.964796\n",
      "Train Epoch: 1 [7680/15000 (51%)]\tLoss: 1.950199\n",
      "Train Epoch: 1 [10240/15000 (68%)]\tLoss: 1.793780\n",
      "Train Epoch: 1 [12800/15000 (85%)]\tLoss: 1.934493\n",
      "NN (1): 1.8125\n",
      "CS (1): 2.3671875\n",
      "DP (1): 1.75\n",
      "1.8651759624481201\n",
      "tensor([0.0400, 0.4567, 0.4251, 0.0472, 0.0310])\n",
      "tensor([0.0486, 0.4640, 0.4302, 0.0571, 1.0000])\n",
      "tensor([0.0655, 0.4859, 0.4486, 1.0000, 1.0000])\n",
      "tensor([0.1401, 0.8599, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "NN (2): 1.8046875\n",
      "CS (2): 2.4296875\n",
      "DP (2): 1.7421875\n",
      "1.8651759624481201\n",
      "tensor([0.0400, 0.4567, 0.4251, 0.0472, 0.0310])\n",
      "tensor([0.0486, 0.4640, 0.4302, 0.0571, 1.0000])\n",
      "tensor([0.0655, 0.4859, 0.4486, 1.0000, 1.0000])\n",
      "tensor([0.1401, 0.8599, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "NN (3): 1.8997395833333333\n",
      "CS (3): 2.4934895833333335\n",
      "DP (3): 1.82421875\n",
      "1.8651759624481201\n",
      "tensor([0.0400, 0.4567, 0.4251, 0.0472, 0.0310])\n",
      "tensor([0.0486, 0.4640, 0.4302, 0.0571, 1.0000])\n",
      "tensor([0.0655, 0.4859, 0.4486, 1.0000, 1.0000])\n",
      "tensor([0.1401, 0.8599, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 2 [0/15000 (0%)]\tLoss: 1.777886\n",
      "Train Epoch: 2 [2560/15000 (17%)]\tLoss: 2.070982\n",
      "Train Epoch: 2 [5120/15000 (34%)]\tLoss: 2.134291\n",
      "Train Epoch: 2 [7680/15000 (51%)]\tLoss: 2.014550\n",
      "Train Epoch: 2 [10240/15000 (68%)]\tLoss: 1.835800\n",
      "Train Epoch: 2 [12800/15000 (85%)]\tLoss: 2.089953\n",
      "NN (4): 1.8759765625\n",
      "CS (4): 2.4619140625\n",
      "DP (4): 1.8056640625\n",
      "1.8651759624481201\n",
      "tensor([0.0413, 0.4371, 0.4424, 0.0423, 0.0369])\n",
      "tensor([0.0500, 0.4486, 0.4498, 0.0516, 1.0000])\n",
      "tensor([0.0674, 0.4685, 0.4642, 1.0000, 1.0000])\n",
      "tensor([0.1548, 0.8452, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "NN (5): 1.86171875\n",
      "CS (5): 2.46796875\n",
      "DP (5): 1.79140625\n",
      "1.8651759624481201\n",
      "tensor([0.0413, 0.4371, 0.4424, 0.0423, 0.0369])\n",
      "tensor([0.0500, 0.4486, 0.4498, 0.0516, 1.0000])\n",
      "tensor([0.0674, 0.4685, 0.4642, 1.0000, 1.0000])\n",
      "tensor([0.1548, 0.8452, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "NN (6): 1.8997395833333333\n",
      "CS (6): 2.4934895833333335\n",
      "DP (6): 1.82421875\n",
      "1.8651759624481201\n",
      "tensor([0.0413, 0.4371, 0.4424, 0.0423, 0.0369])\n",
      "tensor([0.0500, 0.4486, 0.4498, 0.0516, 1.0000])\n",
      "tensor([0.0674, 0.4685, 0.4642, 1.0000, 1.0000])\n",
      "tensor([0.1548, 0.8452, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 3 [0/15000 (0%)]\tLoss: 2.005802\n",
      "Train Epoch: 3 [2560/15000 (17%)]\tLoss: 1.874915\n",
      "Train Epoch: 3 [5120/15000 (34%)]\tLoss: 1.809627\n",
      "Train Epoch: 3 [7680/15000 (51%)]\tLoss: 1.979960\n",
      "Train Epoch: 3 [10240/15000 (68%)]\tLoss: 1.869458\n",
      "Train Epoch: 3 [12800/15000 (85%)]\tLoss: 1.869517\n",
      "NN (7): 1.88671875\n",
      "CS (7): 2.4754464285714284\n",
      "DP (7): 1.8136160714285714\n",
      "1.8651759624481201\n",
      "tensor([0.0322, 0.4773, 0.4104, 0.0415, 0.0386])\n",
      "tensor([0.0405, 0.4872, 0.4208, 0.0514, 1.0000])\n",
      "tensor([0.0547, 0.5067, 0.4386, 1.0000, 1.0000])\n",
      "tensor([0.1261, 0.8739, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "NN (8): 1.873046875\n",
      "CS (8): 2.4775390625\n",
      "DP (8): 1.8037109375\n",
      "1.8651759624481201\n",
      "tensor([0.0322, 0.4773, 0.4104, 0.0415, 0.0386])\n",
      "tensor([0.0405, 0.4872, 0.4208, 0.0514, 1.0000])\n",
      "tensor([0.0547, 0.5067, 0.4386, 1.0000, 1.0000])\n",
      "tensor([0.1261, 0.8739, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "NN (9): 1.8980034722222223\n",
      "CS (9): 2.4934895833333335\n",
      "DP (9): 1.82421875\n",
      "1.8651759624481201\n",
      "tensor([0.0322, 0.4773, 0.4104, 0.0415, 0.0386])\n",
      "tensor([0.0405, 0.4872, 0.4208, 0.0514, 1.0000])\n",
      "tensor([0.0547, 0.5067, 0.4386, 1.0000, 1.0000])\n",
      "tensor([0.1261, 0.8739, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (tp_batch,) in enumerate(tp_dataloader_testing):\n",
    "            costSharingLoss = 0\n",
    "            dpLoss = 0\n",
    "            nnLoss = 0\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            costSharingLoss /= len(tp_batch)\n",
    "            dpLoss /= len(tp_batch)\n",
    "            nnLoss /= len(tp_batch)\n",
    "            if batch_idx % log_interval == 0:\n",
    "                recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "                recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "                recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "                print(dp[n, dpPrecision, 0])\n",
    "                for i in range(n, 0, -1):\n",
    "                    print(\n",
    "                        tpToPayments(\n",
    "                            torch.tensor([1] * i + [0] * (n - i), dtype=torch.float32)\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "\n",
    "for epoch in range(1, supervisionEpochs + 1):\n",
    "    print(distributionRatio)\n",
    "    supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "    # supervisionTrain(epoch, dpSupervisionRule)\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
