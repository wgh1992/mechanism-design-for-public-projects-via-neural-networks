{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1587)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from random import randint\n",
    "import random\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "from pylab import *\n",
    "from scipy.stats import beta\n",
    "import math\n",
    "n = 5\n",
    "epochs = 3\n",
    "supervisionEpochs = 2\n",
    "lr = 0.0001\n",
    "log_interval = 20\n",
    "trainSize = 60000#100000\n",
    "percentage_train_test= 0.25\n",
    "penaltyLambda = 10\n",
    "\n",
    "doublePeakHighMean = 0.9\n",
    "doublePeakLowMean = 0.1\n",
    "doublePeakStd = 0.1\n",
    "\n",
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "\n",
    "def cdf(x, i=None):\n",
    "    return (d1.cdf(x) + d2.cdf(x) - distributionBase) / 2 / distributionRatio\n",
    "\n",
    "# def cdf(x, i=None):\n",
    "#     if x < 0.1:\n",
    "#         return 0\n",
    "#     if x <= 0.2:\n",
    "#         return 0.5 * (x - 0.1) / 0.1\n",
    "#     if x < 0.8:\n",
    "#         return 0.5\n",
    "#     if x < 0.9:\n",
    "#         return 0.5 + 0.5 * (x - 0.8) / 0.1\n",
    "#     return 1\n",
    "\n",
    "\n",
    "print(distributionBase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loc 0.1 scale 0.1\n",
      "loc 0.9 scale 0.1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAATrElEQVR4nO3db4xc133e8e8TUlbc2rDpaiUwJFWqBl2Rimo63TJu3RaKFVSy+oI2EAN0C4cIVNBCpcIBgiKUX9QKCgIuEMdFUckBHQtmitQCUTsVGzlpGSmuayQWQxm0JIpWzVqqtCYhru0mjlNABalfX+wlNFrucu/u/L/7/QCLmTlz78zvkLvP3D333LOpKiRJ3fIT4y5AkjR4hrskdZDhLkkdZLhLUgcZ7pLUQRvHXQDAddddV9u3bx93GZI0VZ566qnvV9XMUs9NRLhv376dkydPjrsMSZoqSf73cs85LCNJHbRiuCf5ySQnknwryekkv9a0P5Dke0lONV939exzf5KzSZ5PcscwOyBJulKbYZlXgfdX1Y+TXAN8PcnvN899pqp+vXfjJLuAfcAtwE8Bf5jkXVV1aZCFS5KWt+KRey34cfPwmubramsW7AUeqapXq+oF4Cywp+9KJUmttRpzT7IhySngAnC8qp5snrovydNJHk6yqWnbArzcs/tc07b4NQ8kOZnk5Pz8fB9dkCQt1ircq+pSVe0GtgJ7kvw08FngncBu4Dzw6WbzLPUSS7zm4aqararZmZklZ/JIktZoVbNlqurPgK8Cd1bVK03ovwZ8jteHXuaAbT27bQXODaBWSVJLbWbLzCR5e3P/zcDPA99Osrlnsw8Bzzb3jwH7klyb5CZgB3BisGVLkq6mzWyZzcCRJBtY+DA4WlW/l+Q/JNnNwpDLi8DHAKrqdJKjwHPAReBeZ8pI0mhlEv5Yx+zsbHmFqqRh237wMV781D8edxkDk+Spqppd6jmvUJWkDjLcJamDDHdJ6iDDXZI6yHCXpA4y3CWpgwx3Seogw12SOshwl6QOMtwlqYMMd0nqIMNdkjrIcJekDjLcJamDDHdJ6iDDXZI6yHCXpA4y3K/iwXueGHcJkrQmhrskdZDhLkkdZLhLUgetGO5JfjLJiSTfSnI6ya817e9IcjzJd5rbTT373J/kbJLnk9wxzA5Ikq7U5sj9VeD9VfVuYDdwZ5L3AgeBx6tqB/B485gku4B9wC3AncBDSTYMo/hJ4slXaTqsl5/VFcO9Fvy4eXhN81XAXuBI034E+GBzfy/wSFW9WlUvAGeBPQOtWpJ0Va3G3JNsSHIKuAAcr6ongRuq6jxAc3t9s/kW4OWe3eeatsWveSDJySQn5+fn++mDJGmRVuFeVZeqajewFdiT5KevsnmWeoklXvNwVc1W1ezMzEy7aiVpEB5427grGLpVzZapqj8DvsrCWPorSTYDNLcXms3mgG09u20FzvVdqSSptTazZWaSvL25/2bg54FvA8eA/c1m+4FHm/vHgH1Jrk1yE7ADODHowiVpGJY74TptJ2I3tthmM3CkmfHyE8DRqvq9JH8CHE1yN/AS8GGAqjqd5CjwHHARuLeqLg2nfEnSUlYM96p6GnjPEu0/AG5fZp9DwKG+qxuj7Qcf41/yZm49civP7H9m3OVI0qp4hepqrIOTMJK6wXBfwq1Hbh13CZLUF8Nd0rqw7EFbR38jN9xbOnPzznGXIGmIuvYzbrgP2PaDj427BEktDSLQJ/VDwXCXpA4y3Afo8pjeg/c84UlZSWNluPfYfvCxqbsKTdLy1nqQ1YWDM8NdUqecuXlnZ2fArIbhvlZ+80id1vub/DROlDDcB2Aa/+Ml9WnCD/AM98awpjNN6jQpSd1muPepCydeJLU04UfrvQx3Seogw12SBmDSplEb7qP4NWuKfpWT9EbTOvRquEvSKk3DRAnDfQ2m4T9W0up1aVqz4S5JHWS4S+q8STvZOQqG+wg5nCNpVFYM9yTbkvxRkjNJTif5eNP+QJLvJTnVfN3Vs8/9Sc4meT7JHcPsgCTpShtbbHMR+JWq+maStwJPJTnePPeZqvr13o2T7AL2AbcAPwX8YZJ3VdWlQRYuSVreikfuVXW+qr7Z3P8L4Ayw5Sq77AUeqapXq+oF4CywZxDFdtF6HAuUBqH3Z2eShjwnZcbNqsbck2wH3gM82TTdl+TpJA8n2dS0bQFe7tltjiU+DJIcSHIyycn5+flVFy5JWl7rcE/yFuBLwC9X1Y+AzwLvBHYD54FPX950id3rioaqw1U1W1WzMzMzqy5ckrS8VuGe5BoWgv13qurLAFX1SlVdqqrXgM/x+tDLHLCtZ/etwLnBlSxJWkmb2TIBPg+cqarf6Gnf3LPZh4Bnm/vHgH1Jrk1yE7ADODG4kiVJK2kzW+Z9wEeBZ5Kcato+AXwkyW4WhlxeBD4GUFWnkxwFnmNhps29zpSRpNFaMdyr6ussPY7+lavscwg41EddkqQ+eIWqpE6blKmJo2a4S1IHretwn6QLH8ALmqQ2bj1y68QejU/SH/ZY1+EuSV1luI/RpB59SFOj909Y+ucs38Bwl6QOMtzHpHdsbpLG6SR1g+EuSUMw7oM2w12SOshwlzQVnICwOusu3M/cvNOz6pI6r9PhPu4xryX5wSJpBDod7pK6xau42zPcJamDDHdJ6tMknuw13CWpg9ZFuDtOJ2mUJiFz1kW4S9J6Y7hLUgcZ7pPGefCSBsBwl6QhGtdffDPcJamDVgz3JNuS/FGSM0lOJ/l40/6OJMeTfKe53dSzz/1JziZ5Pskdw+zAcibhbLUkjUubI/eLwK9U1U7gvcC9SXYBB4HHq2oH8HjzmOa5fcAtwJ3AQ0k2DKN4SZomoxyiWTHcq+p8VX2zuf8XwBlgC7AXONJsdgT4YHN/L/BIVb1aVS8AZ4E9gy68rUm8ckyShm1VY+5JtgPvAZ4Ebqiq87DwAQBc32y2BXi5Z7e5pm3xax1IcjLJyfn5+dVXfhVLBfpErhAp6UrOGBuI1uGe5C3Al4BfrqofXW3TJdrqioaqw1U1W1WzMzMzbcuQJLXQKtyTXMNCsP9OVX25aX4lyebm+c3AhaZ9DtjWs/tW4Nxgyr2KKf+0X3H4aMr7J+mNhj3+3ma2TIDPA2eq6jd6njoG7G/u7wce7Wnfl+TaJDcBO4ATgyt5fRjX3FhpEk39sOoYDs42ttjmfcBHgWeSnGraPgF8Cjia5G7gJeDDAFV1OslR4DkWZtrcW1WXBl65JGlZK4Z7VX2dpcfRAW5fZp9DwKE+6lKPMzfvZOe+cVchjcEDb4MH/nzcVUylTlyh2tkhDMfZJa1RJ8JdkqbGiA7aDPcp5IVZklZiuEtSBxnuktRBhvuUctVLSVfTuXDv7MwZSVqFzoW7JMlwlzQBpn55gQlkuEtSBxnuU8YjHEltGO6SNCbDvCDRcJc0kZz51h/DXZI6yHCXpA4y3CWpgwx3SWPjCqfDY7hLUgcZ7pImhjNkBsdwl6QOMtwljYVXWw/XiuGe5OEkF5I829P2QJLvJTnVfN3V89z9Sc4meT7JHcMqXK9zbXdJi7U5cv8CcOcS7Z+pqt3N11cAkuwC9gG3NPs8lGTDoIqVJLWzYrhX1deAH7Z8vb3AI1X1alW9AJwF9vRRnyRpDfoZc78vydPNsM2mpm0L8HLPNnNN2xWSHEhyMsnJ+fn5PsqQJC221nD/LPBOYDdwHvh0054ltq2lXqCqDlfVbFXNzszMrLGM9W2pE1JOJZMEawz3qnqlqi5V1WvA53h96GUO2Naz6VbgXH8latUeeNu4K5A0ZmsK9ySbex5+CLg8k+YYsC/JtUluAnYAJ/orUZK0WhtX2iDJF4HbgOuSzAGfBG5LspuFIZcXgY8BVNXpJEeB54CLwL1VdWk4pUuSlrNiuFfVR5Zo/vxVtj8EHOqnKElSf7xCdcq5qp6kpRjukkbOq6qHz3CXpA4y3CWNlEOJo2G4d4Vz2zWlHKIZDsNdkjrIcJekDjLcJamDDHdJ6iDDXdJQuVLpeBjuktRBhrskdZDhLkkdZLh3nRc3aYIs9dfDNByGuyR1kOEuSR1kuEtSBxnuktRBhruk4fPE/sgZ7pLUQYa7JHXQiuGe5OEkF5I829P2jiTHk3ynud3U89z9Sc4meT7JHcMqXJK0vDZH7l8A7lzUdhB4vKp2AI83j0myC9gH3NLs81CSDQOrVtL0cJx9rFYM96r6GvDDRc17gSPN/SPAB3vaH6mqV6vqBeAssGdAtUqSWlrrmPsNVXUeoLm9vmnfArzcs91c03aFJAeSnExycn5+fo1l6GpcalXj4vfe+A36hGqWaKulNqyqw1U1W1WzMzMzAy5Di7mmh4bNP3Q9WdYa7q8k2QzQ3F5o2ueAbT3bbQXOrb08DYNBL3XfWsP9GLC/ub8feLSnfV+Sa5PcBOwATvRXoiRptTautEGSLwK3AdclmQM+CXwKOJrkbuAl4MMAVXU6yVHgOeAicG9VXRpS7ZKkZawY7lX1kWWeun2Z7Q8Bh/opSsPz4D1PwN8ddxWShs0rVCUNjOdzJofhLkkdZLivd15FKHWS4S6pb9sPPjbuErSI4S5JHWS4r2NeUSh1l+EuSR1kuK9TjpFK3Wa4S1IHGe6S1EGGuyR1kOEuSR1kuOt1Xq2qlfg9MjUMd0l9cbGwyWS4S1o1A33yGe6S1EGGuyR1kOEuAM7cvHPcJUgaIMN9HXHJAQ2ai89NLsNd0pp4sDDZDHdd4czNO/3BlaZcX+Ge5MUkzyQ5leRk0/aOJMeTfKe53TSYUjVq/sotwAuXptQgjtx/rqp2V9Vs8/gg8HhV7QAebx5LkkZoGMMye4Ejzf0jwAeH8B6SRswZVdOl33Av4L8leSrJgabthqo6D9DcXt/ne0gaEwN9em3sc//3VdW5JNcDx5N8u+2OzYfBAYAbb7yxzzIkSb36OnKvqnPN7QXgd4E9wCtJNgM0txeW2fdwVc1W1ezMzEw/ZWjILs+ccT0RaXqsOdyT/NUkb718H/hHwLPAMWB/s9l+4NF+i5QkrU4/wzI3AL+b5PLr/Meq+oMkfwocTXI38BLw4f7LlCStxprDvaq+C7x7ifYfALf3U5QkqT9eoSpJHWS4S1IHGe5q5fJSBK45I00Hw10S4FTXrjHcdVVL/cC7oFh3uAJodxnuktRBhrukN3jwniccoukAw11aZ5ZaDMwT5t1juKsvrhooTSbDXWvjX+eRJprhLq1Hfjh3nuGugfOE3OQ5c/NOA32dMdwlqYMMd0nqIMNdA+VUOmkyGO4aOsffx8NlItY3w139a07UXe3iGA1X77+9H6YCw12SOslw19A4/j4ky0xp7P33vnzfo/j1y3DXyPTOtV48hOMHgTRYhrvGzvVp+rDoKN5zHLrMcNfEWO7KVsN/gb/daDWGFu5J7kzyfJKzSQ4O633UPdsPPrb+jkAXDVf1LsHb+6G37v5dtGZDCfckG4AHgQ8Au4CPJNk1jPfSFBvgWifjOHE47CPplfrkbzS6mmEdue8BzlbVd6vq/wGPAHuH9F7qqGXDrflQuPXIra1niCw3D3y1HwprCfTF77fSbyYOv2gQUlWDf9HkF4A7q+qfNY8/CvxsVd3Xs80B4EDz8G8Cz6/x7a4Dvt9HudPIPq8P9nl96KfPf72qZpZ6YuPa67mqLNH2hk+RqjoMHO77jZKTVTXb7+tME/u8Ptjn9WFYfR7WsMwcsK3n8Vbg3JDeS5K0yLDC/U+BHUluSvImYB9wbEjvJUlaZCjDMlV1Mcl9wH8FNgAPV9XpYbwXAxjamUL2eX2wz+vDUPo8lBOqkqTx8gpVSeogw12SOmhqwn2l5Qyy4N81zz+d5GfGUecgtejzP236+nSSP07y7nHUOUhtl61I8neSXGquqZhqbfqc5LYkp5KcTvLfR13joLX43n5bkv+S5FtNn39pHHUOSpKHk1xI8uwyzw8+v6pq4r9YOCn7v4C/AbwJ+Bawa9E2dwG/z8Ic+/cCT4677hH0+e8Bm5r7H1gPfe7Z7gngK8AvjLvuEfw/vx14DrixeXz9uOseQZ8/Afyb5v4M8EPgTeOuvY8+/0PgZ4Bnl3l+4Pk1LUfubZYz2Av8di34BvD2JJtHXegArdjnqvrjqvo/zcNvsHA9wTRru2zFvwC+BFwYZXFD0qbP/wT4clW9BFBV097vNn0u4K1JAryFhXC/ONoyB6eqvsZCH5Yz8PyalnDfArzc83iuaVvtNtNktf25m4VP/mm2Yp+TbAE+BPzmCOsapjb/z+8CNiX5apKnkvziyKobjjZ9/vfAThYufnwG+HhVvTaa8sZi4Pk1rOUHBm3F5QxabjNNWvcnyc+xEO5/f6gVDV+bPv9b4Fer6tLCQd3Ua9PnjcDfBm4H3gz8SZJvVNX/HHZxQ9Kmz3cAp4D3A+8Ejif5H1X1o2EXNyYDz69pCfc2yxl0bcmDVv1J8reA3wI+UFU/GFFtw9Kmz7PAI02wXwfcleRiVf3n0ZQ4cG2/t79fVX8J/GWSrwHvBqY13Nv0+ZeAT9XCgPTZJC8ANwMnRlPiyA08v6ZlWKbNcgbHgF9szjq/F/jzqjo/6kIHaMU+J7kR+DLw0Sk+iuu1Yp+r6qaq2l5V24H/BPzzKQ52aPe9/SjwD5JsTPJXgJ8Fzoy4zkFq0+eXWPhNhSQ3sLBy7HdHWuVoDTy/puLIvZZZziDJPc3zv8nCzIm7gLPA/2Xhk39qtezzvwL+GvBQcyR7saZ4Rb2Wfe6UNn2uqjNJ/gB4GngN+K2qWnJK3TRo+f/8r4EvJHmGhSGLX62qqV0KOMkXgduA65LMAZ8EroHh5ZfLD0hSB03LsIwkaRUMd0nqIMNdkjrIcJekDjLcJamDDHdJ6iDDXZI66P8Do2if4l+qiVEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "signals = np.random.randint(2, size=(trainSize, n))\n",
    "samples1 = np.random.normal(\n",
    "    loc=doublePeakLowMean, scale=doublePeakStd, size=(trainSize, n)\n",
    ")\n",
    "for i in range(trainSize):\n",
    "    for j in range(n):\n",
    "        while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "            samples1[i, j] = np.random.normal(\n",
    "                loc=doublePeakLowMean, scale=doublePeakStd\n",
    "            )\n",
    "samples2 = np.random.normal(\n",
    "    loc=doublePeakHighMean, scale=doublePeakStd, size=(trainSize, n)\n",
    ")\n",
    "for i in range(trainSize):\n",
    "    for j in range(n):\n",
    "        while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "            samples2[i, j] = np.random.normal(\n",
    "                loc=doublePeakHighMean, scale=doublePeakStd\n",
    "            )\n",
    "samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "\n",
    "# tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "plt.hist(samplesJoint,bins=500)\n",
    "plt.show()\n",
    "\n",
    "tp_dataset = TensorDataset(tp_tensor[: int(trainSize * percentage_train_test)])\n",
    "tp_dataset_testing = TensorDataset(tp_tensor[int(trainSize * (1.0-percentage_train_test)) :])\n",
    "tp_dataloader = DataLoader(tp_dataset, batch_size=128, shuffle=True)\n",
    "tp_dataloader_testing = DataLoader(tp_dataset_testing, batch_size=256, shuffle=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        #Initializing hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "         # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (rnn): RNN(5, 100, num_layers=3, batch_first=True)\n",
      "  (fc): Linear(in_features=100, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model with hyperparameters\n",
    "model = Model(input_size=n, output_size=n, hidden_dim=100, n_layers=3)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    if torch.sum(bits).item() == 0:\n",
    "        return torch.ones(n)\n",
    "    bits = bits.type(torch.float32)\n",
    "    negBits = torch.ones(n) - bits\n",
    "    bits=torch.reshape(bits,(1,-1,n))\n",
    "    payments = model(bits)\n",
    "    payments=payments[0].view(n)\n",
    "    payments = payments - 1000 * negBits\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    payments = payments + negBits\n",
    "    return payments\n",
    "\n",
    "\n",
    "def tpToBits(tp, deep , bits=torch.ones(n).type(torch.float32)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = torch.sigmoid((tp-payments+0.0001)/0.000001);\n",
    "    \n",
    "    if torch.allclose(newBits, bits) or deep >= n:\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits(tp,deep+1, newBits)\n",
    "\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp,0))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    return n - torch.sum(tpToBits(tp,0).type(torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
    "    # Creating a multi-dimensional array of zeros with the desired output shape\n",
    "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
    "    \n",
    "    # Replacing the 0 at the relevant character index with a 1 to represent that character\n",
    "    for i in range(batch_size):\n",
    "        for u in range(seq_len):\n",
    "            features[i, u, sequence[i][u]] = 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "tensor(0.5000)\n"
     ]
    }
   ],
   "source": [
    "print(1_000_000)\n",
    "print(cdf(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0.1421, 0.2066, 0.0341, 0.9459, 0.1116]]),)\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "torch.Size([1, 1, 5])\n",
      "5\n",
      "tensor([[[ 1.4896e-01,  6.9502e-02, -2.1700e-01, -1.5114e-02,  5.8613e-02,\n",
      "          -2.3798e-01,  4.4906e-02,  8.3861e-02, -7.4258e-02, -1.3460e-01,\n",
      "          -3.8560e-02,  1.0987e-01,  1.0897e-01, -1.3443e-01,  1.8662e-01,\n",
      "           1.7592e-02,  7.8783e-02, -3.6200e-02,  3.4322e-02, -1.0676e-01,\n",
      "           3.9616e-02,  2.1063e-02,  1.9569e-01,  4.9507e-02,  4.7077e-02,\n",
      "           6.3755e-02, -9.9938e-02,  1.0466e-01,  9.1184e-02,  6.1589e-02,\n",
      "           1.2934e-01,  1.1266e-01,  2.1538e-01,  1.1455e-02, -8.2653e-02,\n",
      "          -7.5071e-02,  6.4681e-02, -3.5296e-02,  2.4965e-02,  7.4116e-02,\n",
      "           8.4626e-02,  1.5172e-01,  5.0580e-03,  1.1962e-02,  6.1785e-02,\n",
      "          -1.7411e-01,  8.1585e-02,  1.6666e-01, -7.8319e-02,  6.2164e-02,\n",
      "           5.1378e-03,  1.9078e-02,  5.1654e-02, -8.2448e-02,  8.9181e-02,\n",
      "           7.2085e-02,  7.0195e-02, -6.3818e-02, -1.3981e-01,  3.8385e-02,\n",
      "           1.4468e-01, -2.2687e-01, -5.3195e-02,  3.5480e-02, -1.4650e-01,\n",
      "           3.8121e-02,  1.7693e-01,  9.9541e-03, -2.0078e-01, -1.2162e-01,\n",
      "           1.1149e-01, -1.1772e-01, -1.3066e-01, -8.6180e-02,  1.1550e-01,\n",
      "          -1.8618e-01, -2.0786e-01,  1.1633e-01, -1.5251e-01,  1.4549e-01,\n",
      "           2.1586e-01, -1.3723e-02, -4.7845e-02,  4.6606e-02, -4.2857e-03,\n",
      "           7.1332e-02, -4.8908e-02,  9.9302e-02,  7.1783e-02, -1.2983e-01,\n",
      "           1.0333e-01, -1.7762e-01, -1.7113e-01,  9.6306e-02,  2.4532e-02,\n",
      "           1.8336e-01, -7.1260e-02, -4.0855e-02, -1.4084e-02,  1.8543e-02]],\n",
      "\n",
      "        [[-6.9573e-02, -1.4180e-01, -3.3351e-03,  6.8941e-02, -5.5066e-02,\n",
      "          -4.5364e-02, -7.8740e-02, -1.0001e-01, -3.6900e-02,  9.9158e-02,\n",
      "          -1.0445e-01, -2.4605e-02, -2.3401e-02, -8.9491e-02, -5.1180e-03,\n",
      "           5.8439e-02,  9.3358e-02,  1.8321e-01, -4.2452e-02,  4.6554e-02,\n",
      "           5.1586e-02,  6.4926e-03,  1.1687e-02,  9.6461e-02, -1.1257e-01,\n",
      "          -1.1564e-01,  2.3774e-01, -9.1473e-02, -6.2336e-02,  4.0583e-02,\n",
      "          -5.9417e-02,  3.5460e-02, -2.3066e-02,  7.6482e-02,  8.6415e-02,\n",
      "          -6.7918e-02,  1.1116e-01,  1.3056e-01,  1.1984e-01,  4.5544e-02,\n",
      "           7.5613e-02,  6.5355e-02, -9.1680e-02,  8.1493e-02,  1.2890e-01,\n",
      "           1.8556e-01, -3.1073e-02,  8.2951e-02,  1.4511e-02, -6.5419e-02,\n",
      "          -8.1249e-03,  8.0113e-02,  8.5331e-02,  4.6407e-03, -8.4709e-02,\n",
      "           1.0476e-01,  1.2240e-01, -6.8144e-02,  2.7215e-01,  7.6465e-02,\n",
      "           1.3098e-01,  1.6591e-01,  6.8388e-02,  1.0756e-01,  1.9116e-03,\n",
      "          -1.3158e-01, -2.1677e-01, -6.4014e-02, -1.1220e-01,  6.3110e-02,\n",
      "           2.0273e-02,  4.1810e-02,  4.8998e-02,  8.3108e-02,  4.6313e-02,\n",
      "           8.8138e-03,  8.8558e-02, -4.2573e-02,  1.1943e-01, -1.4704e-02,\n",
      "           1.4398e-02, -1.9572e-03,  7.0465e-02,  3.5746e-02,  1.1426e-01,\n",
      "           5.7874e-02, -5.4321e-02,  1.3788e-02,  4.8805e-02,  6.1291e-02,\n",
      "          -2.2921e-02,  2.0668e-01, -8.6968e-02, -8.5408e-02, -2.3365e-01,\n",
      "           1.1126e-01, -1.0524e-01, -5.2994e-02, -6.4645e-02,  1.2148e-02]],\n",
      "\n",
      "        [[-1.7706e-02, -2.0278e-01, -8.6801e-02, -1.5593e-01,  2.8044e-03,\n",
      "           1.4294e-01, -4.4321e-02,  4.9284e-02, -1.6494e-02,  4.7190e-02,\n",
      "          -1.5638e-01,  4.4192e-03,  6.2074e-02,  1.1463e-01,  1.1349e-01,\n",
      "          -5.2326e-02, -3.9555e-02, -1.6542e-01, -1.3917e-01, -1.3743e-01,\n",
      "           4.2229e-02, -9.5663e-02,  2.6060e-02,  4.6386e-02,  6.5811e-02,\n",
      "           3.0966e-03,  3.0874e-02, -1.0944e-01, -1.5244e-01, -1.2595e-01,\n",
      "           6.0280e-02,  2.6776e-02, -2.3864e-02, -1.4735e-02, -5.8809e-02,\n",
      "           6.8391e-02, -1.0250e-01,  1.1056e-02, -1.2803e-01,  2.5082e-01,\n",
      "          -5.2784e-02,  8.5933e-02, -6.1051e-03, -4.4486e-02, -1.2588e-01,\n",
      "           5.0957e-02,  3.2642e-03,  1.2287e-01,  1.5648e-01, -1.1423e-01,\n",
      "          -4.8269e-02,  6.6795e-02, -4.2185e-02,  1.6038e-02,  4.2441e-02,\n",
      "           4.8666e-02,  5.1178e-02, -2.2194e-02,  2.0849e-02,  6.1457e-02,\n",
      "          -8.9931e-02, -8.2675e-02,  1.8245e-01,  1.3503e-01,  7.3105e-02,\n",
      "           5.0165e-02,  9.7115e-02,  1.2337e-01,  6.6517e-02, -2.3405e-04,\n",
      "          -1.9538e-02,  2.7942e-02,  3.8077e-03,  9.8345e-02, -2.3835e-01,\n",
      "          -1.1881e-01,  1.0076e-01,  1.0250e-01,  3.2802e-02, -1.6650e-02,\n",
      "           2.8218e-02,  1.5192e-02,  5.7420e-03,  6.9746e-02,  7.7193e-03,\n",
      "           1.8844e-01,  2.3145e-01, -1.1777e-01, -2.2749e-03,  1.2462e-02,\n",
      "           4.4816e-02, -2.8025e-02,  2.3858e-02,  9.6158e-02,  1.5912e-02,\n",
      "           7.0023e-02,  3.8522e-02,  1.8721e-02, -1.0536e-01, -1.1808e-02]]],\n",
      "       grad_fn=<StackBackward>)\n",
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "print(tp_dataset_testing[0:1])\n",
    "#inputt= one_hot_encode(tp_dataset_testing, n, 1, 1)\n",
    "print(torch.ones(n))\n",
    "\n",
    "inputxx=torch.from_numpy(np.asarray([t.numpy() for t in tp_dataset_testing[0:1]]))\n",
    "print(inputxx.shape)\n",
    "\n",
    "yy=model(inputxx)\n",
    "print(len(yy[0][0]))\n",
    "print(yy[1])\n",
    "print(yy[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpPrecision = 100\n",
    "# howManyPpl left, money left, yes already\n",
    "dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "# ppl = 0 left\n",
    "for yes in range(n + 1):\n",
    "    for money in range(dpPrecision + 1):\n",
    "        if money == 0:\n",
    "            dp[0, 0, yes] = 0\n",
    "        else:\n",
    "            dp[0, money, yes] = yes# + 1.0\n",
    "for ppl in range(1, n + 1):\n",
    "    for yes in range(n + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            minSoFar = 1_000_000\n",
    "            for offerIndex in range(money + 1):\n",
    "                offer = offerIndex / dpPrecision\n",
    "                res = (1 - cdf(offer)) * dp[\n",
    "                    ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                ] + cdf(offer) * (1 + dp[ppl - 1, money, yes])\n",
    "                if minSoFar > res:\n",
    "                    minSoFar = res\n",
    "                    decision[ppl, money, yes] = offerIndex\n",
    "            dp[ppl, money, yes] = minSoFar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8651759624481201\n"
     ]
    }
   ],
   "source": [
    "print(dp[n, dpPrecision, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.21665868 0.19349191 0.10914864 0.80874103 0.85076692]\n",
      " [0.90753453 0.88523338 0.1782343  0.08691551 0.0738295 ]\n",
      " [0.22135277 0.99675371 0.92616729 0.31032532 0.94036356]\n",
      " ...\n",
      " [0.00411838 0.11478533 0.07912666 0.14453888 0.84158792]\n",
      " [0.86267435 0.13351591 0.96730117 0.10764759 0.0367555 ]\n",
      " [0.91399987 0.19672945 0.19922884 0.87130028 0.05768856]]\n"
     ]
    }
   ],
   "source": [
    "print(samplesJoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_dp(temp):\n",
    "    #print(temp)\n",
    "    remain=dpPrecision\n",
    "    yes=0;\n",
    "    ans =0;\n",
    "    o_list=[];\n",
    "    remain_list=[];\n",
    "    for ppl in range(n,0,-1):\n",
    "        o=decision[ppl, remain, yes]\n",
    "        #print(o,remain)\n",
    "        o_list.append(o)\n",
    "        remain_list.append(remain);\n",
    "        if(o<temp[n-ppl]):\n",
    "            remain-=int(o);\n",
    "            yes+=1;\n",
    "        elif (remain>0):\n",
    "            ans+=1;\n",
    "    if(remain<=0):\n",
    "        return ans,o_list;\n",
    "    else:\n",
    "        return n,o_list;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.73\n"
     ]
    }
   ],
   "source": [
    "ans_list=[];\n",
    "for i in range(100):\n",
    "    temp=samplesJoint[i]*dpPrecision\n",
    "    #print(temp)\n",
    "    ans_list.append(plan_dp(temp)[0]);\n",
    "    #print(\"\\n\",temp)\n",
    "    #print(plan_dp(temp))\n",
    "print(sum(ans_list)/len(ans_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    global  dp,decision\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money, yes]\n",
    "        offer = float(offerIndex) / dpPrecision\n",
    "        if tp[i] >= offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits[i] = 0\n",
    "            payments[i] = 1\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "#     print()\n",
    "#     print(tp)\n",
    "#     print(bits)\n",
    "#     print(payments)\n",
    "#     print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def heuristicSupervisionRule(tp):\n",
    "    global  dp_H,decision_H\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    tempold=-1;\n",
    "    for turns in range(n,0,-1):\n",
    "        money = dpPrecision\n",
    "        j=0\n",
    "        tempo=sum(bits)\n",
    "        #print(\"bits\",tempo)\n",
    "        for i in range(n):\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            \n",
    "            offerIndex = decision_H[tempo ,tempo - i, money]\n",
    "            offer = float(offerIndex) / dpPrecision\n",
    "            while(j<n):\n",
    "                if(bits[j]!=0):\n",
    "                    break;\n",
    "                j+=1;\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            if tp[j] >= offer:\n",
    "                #print(money,j,tp[j],offer)\n",
    "                money -= offerIndex\n",
    "                bits[j] = 1\n",
    "                payments[j] = offer\n",
    "            else:\n",
    "                bits[j] = 0;\n",
    "                payments[j] = 1.0;\n",
    "            j+=1;\n",
    "        #print(\"money\",money)\n",
    "        if(money==0 and tempold==tempo):\n",
    "            break;\n",
    "        tempold=tempo;\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    for k in range(n, -1, -1):\n",
    "        if k == 0:\n",
    "            break\n",
    "        bits = [1 if tp[ii] >= 1.0 / k else 0 for ii in range(n)]\n",
    "        if sum(bits) == k:\n",
    "            break\n",
    "    if k == 0:\n",
    "        payments = [1 for ii in range(n)]\n",
    "    else:\n",
    "        payments = [1.0 / k if bits[ii] == 1 else 1 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    return float(n - torch.sum(costSharingSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def dpDelay(tp):\n",
    "    return float(n - torch.sum(dpSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def heuristicDelay(tp):\n",
    "    return float(n - torch.sum(heuristicSupervisionRule(tp)[0]).item())\n",
    "\n",
    "\n",
    "# templire=0;\n",
    "# num=1\n",
    "# for i in range(num):\n",
    "#     temp=torch.tensor(samplesJoint[i])\n",
    "#     #print(temp)\n",
    "#     #print(temp);\n",
    "#     #print(heuristicSupervisionRule(temp))\n",
    "#     #print(dpSupervisionRule(temp))\n",
    "#     #print(dpDelay(temp))\n",
    "#     res=dpDelay(temp)\n",
    "#     templire+=res\n",
    "#     #print(\"delay\",res)\n",
    "#     #print()\n",
    "# print(templire/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "allBits = [torch.tensor(bits) for bits in itertools.product([0, 1], repeat=n)]\n",
    "\n",
    "runningLossNN = []\n",
    "runningLossCS = []\n",
    "runningLossDP = []\n",
    "\n",
    "\n",
    "def recordAndReport(name, source, loss, n=100):\n",
    "    source.append(loss)\n",
    "    realLength = min(n, len(source))\n",
    "    avgLoss = sum(source[-n:]) / realLength\n",
    "    print(f\"{name} ({realLength}): {avgLoss}\")\n",
    "\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        loss = 0\n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            # print()\n",
    "            # print(\"supervision\")\n",
    "            # print(tp)\n",
    "            # print(bits)\n",
    "            # print()\n",
    "            # print(payments)\n",
    "            # print(bitsToPayments(bits))\n",
    "            # print()\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        # costSharingLoss = 0\n",
    "        # dpLoss = 0\n",
    "        for tp in tp_batch:\n",
    "            # costSharingLoss += costSharingDelay(tp)\n",
    "            # dpLoss += dpDelay(tp)\n",
    "            # print()\n",
    "            # print(\"---\")\n",
    "            # print(tp)\n",
    "            # print(costSharingSupervisionRule(tp))\n",
    "            # print(dpSupervisionRule(tp))\n",
    "            # print(costSharingDelay(tp), dpDelay(tp))\n",
    "            # print()\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                delay1 = tpToTotalDelay(tp1)\n",
    "                delay0 = tpToTotalDelay(tp0)\n",
    "                loss = loss + (1 - cdf(offer)) * delay1 + cdf(offer) * delay0\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        # costSharingLoss /= len(tp_batch)\n",
    "        # dpLoss /= len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "            # recordAndReport(\"NN\", runningLossNN, loss.item())\n",
    "            # recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "            # recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "            # print(dp[n, dpPrecision, 0])\n",
    "            # print(penalty.item())\n",
    "            # print(distributionRatio)\n",
    "            # for i in range(n, 0, -1):\n",
    "            #     print(\n",
    "            #         tpToPayments(\n",
    "            #             torch.tensor([1] * i + [0] * (n - i), dtype=torch.float32)\n",
    "            #         )\n",
    "            #     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8413)\n",
      "Train Epoch: 1 [0/15000 (0%)]\tLoss: 0.000123\n",
      "Train Epoch: 1 [2560/15000 (17%)]\tLoss: 0.000010\n",
      "Train Epoch: 1 [5120/15000 (34%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "tensor(0.8413)\n",
      "Train Epoch: 2 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/15000 (0%)]\tLoss: 2.553363\n",
      "Train Epoch: 1 [2560/15000 (17%)]\tLoss: 2.302655\n",
      "Train Epoch: 1 [5120/15000 (34%)]\tLoss: 2.185313\n",
      "Train Epoch: 1 [7680/15000 (51%)]\tLoss: 2.001864\n",
      "Train Epoch: 1 [10240/15000 (68%)]\tLoss: 1.929018\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (tp_batch,) in enumerate(tp_dataloader_testing):\n",
    "            costSharingLoss = 0\n",
    "            dpLoss = 0\n",
    "            nnLoss = 0\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            costSharingLoss /= len(tp_batch)\n",
    "            dpLoss /= len(tp_batch)\n",
    "            nnLoss /= len(tp_batch)\n",
    "            if batch_idx % log_interval == 0:\n",
    "                recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "                recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "                recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "                print(dp[n, dpPrecision, 0])\n",
    "                for i in range(n, 0, -1):\n",
    "                    print(\n",
    "                        tpToPayments(\n",
    "                            torch.tensor([1] * i + [0] * (n - i), dtype=torch.float32)\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "\n",
    "for epoch in range(1, supervisionEpochs + 1):\n",
    "    print(distributionRatio)\n",
    "    supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "    # supervisionTrain(epoch, dpSupervisionRule)\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
