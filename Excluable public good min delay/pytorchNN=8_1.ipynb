{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40058530825361593\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from random import randint\n",
    "import random\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "from pylab import *\n",
    "from scipy.stats import beta\n",
    "import math\n",
    "n = 8\n",
    "epochs = 1\n",
    "supervisionEpochs = 1\n",
    "lr1 = 0.01\n",
    "lr2 = 0.0005\n",
    "log_interval = 5\n",
    "trainSize = 40000#100000\n",
    "percentage_train_test= 0.5\n",
    "penaltyLambda = 10\n",
    "doublePeakHighMean = 0.85\n",
    "doublePeakLowMean = 0.15\n",
    "doublePeakStd = 0.1\n",
    "uniformlow=0\n",
    "uniformhigh=1.0\n",
    "normalloc = 0.2\n",
    "normalscale = 0.1\n",
    "\n",
    "cauchyloc = 1/n\n",
    "cauchyscalen = 0.004\n",
    "\n",
    "exponentialhigh = 15 #Symbol(\"b\", real=True)\n",
    "exponentiallow  = 15 #Symbol(\"a\", real=True)\n",
    "\n",
    "\n",
    "beta_a = 0.3\n",
    "beta_b  = 0.2\n",
    "kumaraswamy_a = beta_a \n",
    "kumaraswamy_b = (1.0+(beta_a-1.0)*math.pow( (beta_a+beta_b-2.0)/(beta_a-1.0), beta_a) )/beta_a \n",
    "print(kumaraswamy_b)\n",
    "\n",
    "\n",
    "independentnormalloc1=[(float(ii)+1)/(2*n+1) for ii in range(n,0,-1)]\n",
    "independentnormalscale1=[0.05 for ii in range(n)]\n",
    "\n",
    "independentnormalloc2=[(float(ii)+1)/(2*n+1) for ii in range(1,n+1,1)]\n",
    "independentnormalscale2=[0.05 for ii in range(n)]\n",
    "stage=[\"twopeak\"]\n",
    "# \"twopeak\",\"normal\",\"uniform\",\"independent1\",\"independent2\",\"cauchy\",\"beta\",\"U-exponential\",\"arcsine\"\n",
    "order1name=[\"dp\",\"random initializing\",\"costsharing\",\"heuristic\"]\n",
    "# \"costsharing\",\"dp\",\"heuristic\",\"random initializing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio1 = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase1 = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "d3 = D.normal.Normal(normalloc, normalscale)\n",
    "distributionRatio3 = d3.cdf(1) - d3.cdf(0)\n",
    "distributionBase3 = d3.cdf(0)\n",
    "\n",
    "d4 = D.uniform.Uniform(uniformlow,uniformhigh)\n",
    "distributionRatio4 = d4.cdf(1) - d4.cdf(0)\n",
    "distributionBase4 = d4.cdf(0)\n",
    "\n",
    "d5 = [D.normal.Normal(independentnormalloc1[ii], independentnormalscale1[ii]) for ii in range(n)]\n",
    "d6 = [D.normal.Normal(independentnormalloc2[ii], independentnormalscale2[ii]) for ii in range(n)]\n",
    "\n",
    "d7 = D.cauchy.Cauchy(cauchyloc,cauchyscalen)\n",
    "\n",
    "d81 = D.exponential.Exponential(exponentiallow)\n",
    "d82 = D.exponential.Exponential(exponentialhigh)\n",
    "\n",
    "d9 = D.beta.Beta(beta_a,beta_b)\n",
    "#sample_d9=d9.rsample(torch.Size([100000]))\n",
    "\n",
    "#d9_sample=np.linspace(0.0001, 0.9999, 10000)\n",
    "#d9_pdf=torch.exp(d9.log_prob(torch.tensor(d9_sample,dtype=torch.float32)))\n",
    "#d9_delta=d9_sample[1]-d9_sample[0]\n",
    "#d9_sum_pdf=torch.sum(d9_pdf*d9_delta)\n",
    "\n",
    "d10 = D.beta.Beta(0.5,0.5)\n",
    "\n",
    "def cdf(x,y, i=None):\n",
    "    if(y==\"twopeak\"):\n",
    "        return (d1.cdf(x) + d2.cdf(x) - distributionBase1) / 2 / distributionRatio1\n",
    "    elif(y==\"normal\"):\n",
    "        return (d3.cdf(x)-distributionBase3)/distributionRatio3;\n",
    "    elif(y==\"uniform\"):\n",
    "        return (d4.cdf(x)-distributionBase4)/distributionRatio4;\n",
    "    elif(y==\"independent1\"):\n",
    "        return d5[i].cdf(x);\n",
    "    elif(y==\"independent2\"):\n",
    "        return d6[i].cdf(x);\n",
    "    elif(y==\"cauchy\"):\n",
    "        return d7.cdf(x);\n",
    "    elif(y==\"beta\"):\n",
    "#         sum_cdf=0.0;\n",
    "#         if(x<0.0001):\n",
    "#             x=0.00011;\n",
    "#         if(x>0.9999):\n",
    "#             x=0.99989;\n",
    "#         for i in range(len(d9_pdf)):\n",
    "#             if(d9_sample[i]<x):\n",
    "#                 sum_cdf+=d9_pdf[i]*d9_delta;\n",
    "#             else:\n",
    "#                 sum_cdf+=(d9_pdf[i]+d9_pdf[i-1])/ 2 *(x-d9_sample[i-1])\n",
    "#                 break;\n",
    "#         return sum_cdf/d9_sum_pdf\n",
    "#         cdf_v=torch.sum((sample_d9<(x)), dtype=torch.float32)/100000\n",
    "#         return cdf_v\n",
    "#    F(x|a,b)=1–(1–x^a)^b\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(x,kumaraswamy_a),kumaraswamy_b);\n",
    "        except:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(torch.tensor(x,dtype=torch.float32),kumaraswamy_a),kumaraswamy_b);\n",
    "    elif(y==\"arcsine\"):\n",
    "        #\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            res=2.0/math.pi * torch.asin(torch.sqrt(x))\n",
    "            #print(x)\n",
    "            return res# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "        except:\n",
    "            return 2.0/math.pi * torch.asin(torch.sqrt(torch.tensor(x,dtype=torch.float32)))# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "    elif(y==\"U-exponential\"):\n",
    "        return (d81.cdf(x) + (1.0 - d82.cdf(1.0-x)))  / 2 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    if torch.sum(bits).item() == 0:\n",
    "        return torch.ones(n)\n",
    "    bits = bits.type(torch.float32)\n",
    "    negBits = torch.ones(n) - bits\n",
    "    payments = model(bits)\n",
    "    payments = payments - 1000 * negBits\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    payments = payments + negBits\n",
    "    return payments\n",
    "\n",
    "def tpToBits(tp, bits=torch.ones(n).type(torch.uint8)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = (tp >= payments).type(torch.uint8)\n",
    "    if torch.equal(newBits, bits):\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits(tp, newBits)#bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    return n - torch.sum(tpToBits(tp).type(torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    global  dp,decision\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money, yes]\n",
    "        offer = float(offerIndex) / dpPrecision\n",
    "        if tp[i] >= offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits[i] = 0\n",
    "            payments[i] = 1\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "#     print()\n",
    "#     print(tp)\n",
    "#     print(bits)\n",
    "#     print(payments)\n",
    "#     print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def heuristicSupervisionRule(tp):\n",
    "    global  dp_H,decision_H\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    tempold=-1;\n",
    "    for turns in range(n,0,-1):\n",
    "        money = dpPrecision\n",
    "        j=0\n",
    "        tempo=sum(bits)\n",
    "        #print(\"bits\",tempo)\n",
    "        for i in range(n):\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            \n",
    "            offerIndex = decision_H[tempo ,tempo - i, money]\n",
    "            offer = float(offerIndex) / dpPrecision\n",
    "            while(j<n):\n",
    "                if(bits[j]!=0):\n",
    "                    break;\n",
    "                j+=1;\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            if tp[j] >= offer:\n",
    "                #print(money,j,tp[j],offer)\n",
    "                money -= offerIndex\n",
    "                bits[j] = 1\n",
    "                payments[j] = offer\n",
    "            else:\n",
    "                bits[j] = 0;\n",
    "                payments[j] = 1.0;\n",
    "            j+=1;\n",
    "        #print(\"money\",money)\n",
    "        if(money==0 and tempold==tempo):\n",
    "            break;\n",
    "        tempold=tempo;\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    for k in range(n, -1, -1):\n",
    "        if k == 0:\n",
    "            break\n",
    "        bits = [1 if tp[ii] >= 1.0 / k else 0 for ii in range(n)]\n",
    "        if sum(bits) == k:\n",
    "            break\n",
    "    if k == 0:\n",
    "        payments = [1 for ii in range(n)]\n",
    "    else:\n",
    "        payments = [1.0 / k if bits[ii] == 1 else 1 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    return float(n - torch.sum(costSharingSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def dpDelay(tp):\n",
    "    return float(n - torch.sum(dpSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def heuristicDelay(tp):\n",
    "    return float(n - torch.sum(heuristicSupervisionRule(tp)[0]).item())\n",
    "\n",
    "\n",
    "# templire=0;\n",
    "# num=1\n",
    "# for i in range(num):\n",
    "#     temp=torch.tensor(samplesJoint[i])\n",
    "#     #print(temp)\n",
    "#     #print(temp);\n",
    "#     #print(heuristicSupervisionRule(temp))\n",
    "#     #print(dpSupervisionRule(temp))\n",
    "#     #print(dpDelay(temp))\n",
    "#     res=dpDelay(temp)\n",
    "#     templire+=res\n",
    "#     #print(\"Delay\",res)\n",
    "#     #print()\n",
    "# print(templire/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordAndReport(name, source, loss):\n",
    "    source.append(loss)\n",
    "    realLength = len(source)\n",
    "    #print(f\"{name} ({realLength}): {loss}\")\n",
    "    print(name,realLength,\":\" ,loss)\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    \n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda /100000\n",
    "        \n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % (log_interval*5) == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "losslist=[];\n",
    "losslistname=[];\n",
    "losslisttemp=[];\n",
    "def test_batch():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lenLoss= 0\n",
    "        nnLoss = 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "            for tp in tp_batch:\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        nnLoss/=lenLoss\n",
    "    return nnLoss\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                Delay1 = tpToTotalDelay(tp1)\n",
    "                Delay0 = tpToTotalDelay(tp0)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                    loss = loss + (1 - cdf(offer,order)) * Delay1 + cdf(offer,order) * Delay0\n",
    "                else:\n",
    "                    loss = loss + (1 - cdf(offer,order,i)) * Delay1 + cdf(offer,order,i) * Delay0\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "    \n",
    "allBits = [torch.tensor(bits, dtype=torch.int16) for bits in itertools.product([0, 1], repeat=n)]\n",
    "\n",
    "def test():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        costSharingLoss = 0\n",
    "        dpLoss = 0\n",
    "        nnLoss = 0\n",
    "        heuristicLoss=0\n",
    "        lenLoss= 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "                heuristicLoss+= heuristicDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        costSharingLoss /= lenLoss\n",
    "        dpLoss /= lenLoss\n",
    "        nnLoss /= lenLoss\n",
    "        heuristicLoss /= lenLoss\n",
    "        #print(lenLoss)\n",
    "        losslisttemp.append(nnLoss);\n",
    "        recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "        recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "        recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "        recordAndReport(\"heuristic\", runningLossHeuristic, heuristicLoss)\n",
    "        print(\"DP:\",dp[n, dpPrecision,0])\n",
    "        #for i in range(n, 0, -1):\n",
    "        #    print(\"Heuristic:\",i,5*(1-dp_H[i, i, dpPrecision]))\n",
    "        for i in range(n, 0, -1):\n",
    "            print(\n",
    "                    tpToPayments(\n",
    "                            torch.tensor([0 if ii >= i else 1 for ii in range(n)], dtype=torch.float32)\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "dpPrecision = 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def producedata(order):\n",
    "    global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "    dpPrecision = 100 \n",
    "    if(order==\"twopeak\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = np.random.normal(\n",
    "            loc=doublePeakLowMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=doublePeakLowMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samples2 = np.random.normal(\n",
    "            loc=doublePeakHighMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = np.random.normal(\n",
    "                        loc=doublePeakHighMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "    elif(order==\"normal\"):\n",
    "        print(\"loc\",normalloc, \"scale\",normalscale)\n",
    "        samples1 = np.random.normal(\n",
    "            loc=normalloc, scale=normalscale, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=normalloc, scale=normalscale\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"uniform\"):  \n",
    "        print(\"uniformlow\",uniformlow, \"uniformhigh\",uniformhigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        uniformlow, uniformhigh\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent1\"):\n",
    "        print(\"loc\",independentnormalloc1,\"scale\",independentnormalscale1)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent2\"):\n",
    "        print(\"loc\",independentnormalloc2, \"scale\",independentnormalscale2)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"cauchy\"):\n",
    "        print(\"cauchyloc\",cauchyloc, \"cauchyscale\",cauchyscalen)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"beta\"):\n",
    "        print(\"beta_a\",beta_a, \"beta_b\",beta_b)\n",
    "        print(\"kumaraswamy_a\",kumaraswamy_a, \"kumaraswamy_b\",kumaraswamy_b)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"arcsine\"):\n",
    "        print(\"betalow\",0.5, \"betahigh\",0.5)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"U-exponential\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = d81.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d81.rsample(torch.Size([1])).numpy()\n",
    "                    \n",
    "        samples2 = d82.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        \n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = d82.rsample(torch.Size([1])).numpy()\n",
    "        samples2 = 1.0 - samples2\n",
    "        samplesJoint = signals * samples1 - (signals - 1.0) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "        \n",
    "    plt.hist(samplesJoint,bins=500)\n",
    "    plt.show()\n",
    "    \n",
    "    tp_dataset = TensorDataset(tp_tensor[: int(trainSize * percentage_train_test)])\n",
    "    tp_dataset_testing = TensorDataset(tp_tensor[int(trainSize * (1.0-percentage_train_test)) :])\n",
    "    tp_dataloader = DataLoader(tp_dataset, batch_size=128, shuffle=True)\n",
    "    tp_dataloader_testing = DataLoader(tp_dataset_testing, batch_size=256, shuffle=False)\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "    decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for yes in range(n + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp[0, 0, yes] = 0\n",
    "            else:\n",
    "                dp[0, money, yes] = yes# + 1.0\n",
    "    for ppl in range(1,  n + 1):\n",
    "        for yes in range(n + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                minSoFar = 1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1 - cdf(offer,order)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order) * (1 + dp[ppl - 1, money, yes])\n",
    "                    else:\n",
    "                        res = (1 - cdf(offer,order,n-ppl)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order,n-ppl) * (1 + dp[ppl - 1, money, yes])\n",
    "                    if minSoFar > res.item():\n",
    "                        minSoFar = res.item()\n",
    "                        decision[ppl, money, yes] = offerIndex\n",
    "                dp[ppl, money, yes] = minSoFar\n",
    "    \n",
    "    print(\"dp\",dp[n, dpPrecision, 0])\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp_H = np.zeros([n + 1 , n + 1, dpPrecision + 1])\n",
    "    decision_H = np.zeros([n + 1 , n + 1, dpPrecision + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for i in range(1,n+1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp_H[i, 0, 0] = 1\n",
    "            else:\n",
    "                offer = money / dpPrecision\n",
    "                dp_H[i, 0, money] = 0#cdf(offer)# + 1.0\n",
    "    for i in range(1,n+1):\n",
    "        for ppl in range(1, i + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                maxSoFar = -1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1-cdf(offer,order)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    else:\n",
    "                        res = (1-cdf(offer,order,ppl-1)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    if maxSoFar < res.item():\n",
    "                        maxSoFar = res.item()\n",
    "                        decision_H[i, ppl, money] = offerIndex\n",
    "                dp_H[i, ppl, money] = maxSoFar\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loc 0.15 scale 0.1\n",
      "loc 0.85 scale 0.1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dp 1.5344144105911255\n",
      "Supervised Aim: twopeak dp\n",
      "Train Epoch: 1 [0/20000 (0%)]\tLoss: 0.039255\n",
      "Train Epoch: 1 [3200/20000 (16%)]\tLoss: 0.000782\n",
      "Train Epoch: 1 [6400/20000 (32%)]\tLoss: 0.000141\n",
      "Train Epoch: 1 [9600/20000 (48%)]\tLoss: 0.000102\n",
      "Train Epoch: 1 [12800/20000 (64%)]\tLoss: 0.000074\n",
      "Train Epoch: 1 [16000/20000 (80%)]\tLoss: 0.000050\n",
      "Train Epoch: 1 [19200/20000 (96%)]\tLoss: 0.000042\n",
      "NN 1 : tensor(1.5227)\n",
      "CS 1 : 2.1355\n",
      "DP 1 : 1.541\n",
      "heuristic 1 : 1.7159\n",
      "DP: 1.5344144105911255\n",
      "tensor([0.6677, 0.0696, 0.0580, 0.0597, 0.0602, 0.0496, 0.0301, 0.0051])\n",
      "tensor([0.6717, 0.0675, 0.0611, 0.0594, 0.0575, 0.0507, 0.0321, 1.0000])\n",
      "tensor([0.6703, 0.0759, 0.0706, 0.0607, 0.0635, 0.0590, 1.0000, 1.0000])\n",
      "tensor([0.6816, 0.0895, 0.0778, 0.0715, 0.0795, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.7184, 0.1167, 0.0872, 0.0778, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.7694, 0.1372, 0.0933, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.8393, 0.1607, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/20000 (0%)]\tLoss: 1.512520 testing loss: tensor(1.5173)\n",
      "Train Epoch: 1 [640/20000 (3%)]\tLoss: 1.559565 testing loss: tensor(1.5259)\n",
      "Train Epoch: 1 [1280/20000 (6%)]\tLoss: 1.434824 testing loss: tensor(1.5193)\n",
      "Train Epoch: 1 [1920/20000 (10%)]\tLoss: 1.452889 testing loss: tensor(1.5165)\n",
      "Train Epoch: 1 [2560/20000 (13%)]\tLoss: 1.671196 testing loss: tensor(1.5153)\n",
      "Train Epoch: 1 [3200/20000 (16%)]\tLoss: 1.637823 testing loss: tensor(1.5126)\n",
      "Train Epoch: 1 [3840/20000 (19%)]\tLoss: 1.350909 testing loss: tensor(1.5107)\n",
      "Train Epoch: 1 [4480/20000 (22%)]\tLoss: 1.609748 testing loss: tensor(1.5100)\n",
      "Train Epoch: 1 [5120/20000 (25%)]\tLoss: 1.382900 testing loss: tensor(1.5074)\n",
      "Train Epoch: 1 [5760/20000 (29%)]\tLoss: 1.449597 testing loss: tensor(1.5055)\n",
      "Train Epoch: 1 [6400/20000 (32%)]\tLoss: 1.696365 testing loss: tensor(1.5039)\n",
      "Train Epoch: 1 [7040/20000 (35%)]\tLoss: 1.482412 testing loss: tensor(1.5024)\n",
      "Train Epoch: 1 [7680/20000 (38%)]\tLoss: 1.694366 testing loss: tensor(1.5017)\n",
      "Train Epoch: 1 [8320/20000 (41%)]\tLoss: 1.488326 testing loss: tensor(1.4988)\n",
      "Train Epoch: 1 [8960/20000 (45%)]\tLoss: 1.585455 testing loss: tensor(1.4993)\n",
      "Train Epoch: 1 [9600/20000 (48%)]\tLoss: 1.487578 testing loss: tensor(1.4972)\n",
      "Train Epoch: 1 [10240/20000 (51%)]\tLoss: 1.474336 testing loss: tensor(1.4935)\n",
      "Train Epoch: 1 [10880/20000 (54%)]\tLoss: 1.684412 testing loss: tensor(1.4970)\n",
      "Train Epoch: 1 [11520/20000 (57%)]\tLoss: 1.369914 testing loss: tensor(1.4943)\n",
      "Train Epoch: 1 [12160/20000 (61%)]\tLoss: 1.306514 testing loss: tensor(1.4923)\n",
      "Train Epoch: 1 [12800/20000 (64%)]\tLoss: 1.421208 testing loss: tensor(1.4932)\n",
      "Train Epoch: 1 [13440/20000 (67%)]\tLoss: 1.658334 testing loss: tensor(1.4922)\n",
      "Train Epoch: 1 [14080/20000 (70%)]\tLoss: 1.339351 testing loss: tensor(1.4935)\n",
      "Train Epoch: 1 [14720/20000 (73%)]\tLoss: 1.345836 testing loss: tensor(1.4905)\n",
      "Train Epoch: 1 [15360/20000 (76%)]\tLoss: 1.461311 testing loss: tensor(1.4895)\n",
      "Train Epoch: 1 [16000/20000 (80%)]\tLoss: 1.304362 testing loss: tensor(1.4924)\n",
      "Train Epoch: 1 [16640/20000 (83%)]\tLoss: 1.442449 testing loss: tensor(1.4860)\n",
      "Train Epoch: 1 [17280/20000 (86%)]\tLoss: 1.370947 testing loss: tensor(1.4854)\n",
      "Train Epoch: 1 [17920/20000 (89%)]\tLoss: 1.518277 testing loss: tensor(1.4864)\n",
      "Train Epoch: 1 [18560/20000 (92%)]\tLoss: 1.642666 testing loss: tensor(1.4849)\n",
      "Train Epoch: 1 [19200/20000 (96%)]\tLoss: 1.400115 testing loss: tensor(1.4835)\n",
      "Train Epoch: 1 [19840/20000 (99%)]\tLoss: 1.531525 testing loss: tensor(1.4847)\n",
      "penalty: 0.00012881308794021606\n",
      "NN 2 : tensor(1.4843)\n",
      "CS 2 : 2.1355\n",
      "DP 2 : 1.541\n",
      "heuristic 2 : 1.7159\n",
      "DP: 1.5344144105911255\n",
      "tensor([0.6591, 0.0589, 0.0553, 0.0518, 0.0525, 0.0516, 0.0479, 0.0230])\n",
      "tensor([0.6688, 0.0614, 0.0581, 0.0534, 0.0538, 0.0542, 0.0503, 1.0000])\n",
      "tensor([0.6854, 0.0697, 0.0656, 0.0572, 0.0608, 0.0612, 1.0000, 1.0000])\n",
      "tensor([0.7022, 0.0841, 0.0723, 0.0675, 0.0740, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.7474, 0.1008, 0.0790, 0.0728, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.7952, 0.1164, 0.0884, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.8624, 0.1376, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: twopeak random initializing\n",
      "do nothing\n",
      "NN 1 : tensor(2.1841)\n",
      "CS 1 : 2.1355\n",
      "DP 1 : 1.541\n",
      "heuristic 1 : 1.7159\n",
      "DP: 1.5344144105911255\n",
      "tensor([0.0911, 0.1308, 0.1206, 0.1301, 0.1026, 0.1889, 0.0745, 0.1614])\n",
      "tensor([0.1124, 0.1518, 0.1373, 0.1561, 0.1292, 0.2108, 0.1024, 1.0000])\n",
      "tensor([0.1427, 0.1465, 0.1558, 0.1680, 0.1504, 0.2366, 1.0000, 1.0000])\n",
      "tensor([0.1743, 0.1834, 0.2014, 0.2202, 0.2207, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.2317, 0.2558, 0.2741, 0.2384, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.3115, 0.3175, 0.3710, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.4759, 0.5241, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/20000 (0%)]\tLoss: 2.154547 testing loss: tensor(2.1765)\n",
      "Train Epoch: 1 [640/20000 (3%)]\tLoss: 1.795501 testing loss: tensor(2.1451)\n",
      "Train Epoch: 1 [1280/20000 (6%)]\tLoss: 2.106657 testing loss: tensor(2.1418)\n",
      "Train Epoch: 1 [1920/20000 (10%)]\tLoss: 2.308606 testing loss: tensor(2.1462)\n",
      "Train Epoch: 1 [2560/20000 (13%)]\tLoss: 2.246558 testing loss: tensor(2.1474)\n",
      "Train Epoch: 1 [3200/20000 (16%)]\tLoss: 2.072875 testing loss: tensor(2.1426)\n",
      "Train Epoch: 1 [3840/20000 (19%)]\tLoss: 2.175999 testing loss: tensor(2.1377)\n",
      "Train Epoch: 1 [4480/20000 (22%)]\tLoss: 2.274912 testing loss: tensor(2.1332)\n",
      "Train Epoch: 1 [5120/20000 (25%)]\tLoss: 2.127879 testing loss: tensor(2.1309)\n",
      "Train Epoch: 1 [5760/20000 (29%)]\tLoss: 2.086111 testing loss: tensor(2.1310)\n",
      "Train Epoch: 1 [6400/20000 (32%)]\tLoss: 1.995963 testing loss: tensor(2.1290)\n",
      "Train Epoch: 1 [7040/20000 (35%)]\tLoss: 2.288724 testing loss: tensor(2.1284)\n",
      "Train Epoch: 1 [7680/20000 (38%)]\tLoss: 2.192737 testing loss: tensor(2.1268)\n",
      "Train Epoch: 1 [8320/20000 (41%)]\tLoss: 2.211128 testing loss: tensor(2.1268)\n",
      "Train Epoch: 1 [8960/20000 (45%)]\tLoss: 2.049488 testing loss: tensor(2.1281)\n",
      "Train Epoch: 1 [9600/20000 (48%)]\tLoss: 2.147375 testing loss: tensor(2.1215)\n",
      "Train Epoch: 1 [10240/20000 (51%)]\tLoss: 2.039611 testing loss: tensor(2.1163)\n",
      "Train Epoch: 1 [10880/20000 (54%)]\tLoss: 2.077543 testing loss: tensor(2.1179)\n",
      "Train Epoch: 1 [11520/20000 (57%)]\tLoss: 1.950944 testing loss: tensor(2.1110)\n",
      "Train Epoch: 1 [12160/20000 (61%)]\tLoss: 2.280804 testing loss: tensor(2.1051)\n",
      "Train Epoch: 1 [12800/20000 (64%)]\tLoss: 2.044785 testing loss: tensor(2.0974)\n",
      "Train Epoch: 1 [13440/20000 (67%)]\tLoss: 1.980269 testing loss: tensor(2.0912)\n",
      "Train Epoch: 1 [14080/20000 (70%)]\tLoss: 2.065953 testing loss: tensor(2.0754)\n",
      "Train Epoch: 1 [14720/20000 (73%)]\tLoss: 2.039915 testing loss: tensor(2.0728)\n",
      "Train Epoch: 1 [15360/20000 (76%)]\tLoss: 2.023188 testing loss: tensor(2.0553)\n",
      "Train Epoch: 1 [16000/20000 (80%)]\tLoss: 2.314288 testing loss: tensor(2.0478)\n",
      "Train Epoch: 1 [16640/20000 (83%)]\tLoss: 2.191013 testing loss: tensor(2.0367)\n",
      "Train Epoch: 1 [17280/20000 (86%)]\tLoss: 1.887847 testing loss: tensor(2.0053)\n",
      "Train Epoch: 1 [17920/20000 (89%)]\tLoss: 1.784024 testing loss: tensor(1.9564)\n",
      "Train Epoch: 1 [18560/20000 (92%)]\tLoss: 2.076987 testing loss: tensor(1.8832)\n",
      "Train Epoch: 1 [19200/20000 (96%)]\tLoss: 1.924984 testing loss: tensor(1.8273)\n",
      "Train Epoch: 1 [19840/20000 (99%)]\tLoss: 1.794363 testing loss: tensor(1.7770)\n",
      "penalty: 0.03442254662513733\n",
      "NN 2 : tensor(1.7723)\n",
      "CS 2 : 2.1355\n",
      "DP 2 : 1.541\n",
      "heuristic 2 : 1.7159\n",
      "DP: 1.5344144105911255\n",
      "tensor([0.3810, 0.0384, 0.0450, 0.0467, 0.3670, 0.0454, 0.0353, 0.0411])\n",
      "tensor([0.3887, 0.0410, 0.0466, 0.0506, 0.3883, 0.0472, 0.0376, 1.0000])\n",
      "tensor([0.4081, 0.0421, 0.0487, 0.0553, 0.3955, 0.0503, 1.0000, 1.0000])\n",
      "tensor([0.4120, 0.0494, 0.0579, 0.0624, 0.4182, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.7480, 0.0708, 0.0770, 0.1041, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.8278, 0.0846, 0.0876, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.8923, 0.1077, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: twopeak costsharing\n",
      "Train Epoch: 1 [0/20000 (0%)]\tLoss: 0.001270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [3200/20000 (16%)]\tLoss: 0.000018\n",
      "Train Epoch: 1 [6400/20000 (32%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [9600/20000 (48%)]\tLoss: 0.000001\n",
      "Train Epoch: 1 [12800/20000 (64%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [16000/20000 (80%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [19200/20000 (96%)]\tLoss: 0.000000\n",
      "NN 1 : tensor(2.1360)\n",
      "CS 1 : 2.1355\n",
      "DP 1 : 1.541\n",
      "heuristic 1 : 1.7159\n",
      "DP: 1.5344144105911255\n",
      "tensor([0.1248, 0.1251, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1251])\n",
      "tensor([0.1435, 0.1425, 0.1427, 0.1428, 0.1428, 0.1431, 0.1426, 1.0000])\n",
      "tensor([0.1670, 0.1672, 0.1665, 0.1665, 0.1665, 0.1662, 1.0000, 1.0000])\n",
      "tensor([0.1999, 0.1992, 0.2014, 0.1997, 0.1998, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.2511, 0.2512, 0.2490, 0.2488, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.3330, 0.3338, 0.3333, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.4986, 0.5014, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/20000 (0%)]\tLoss: 1.955816 testing loss: tensor(2.1338)\n",
      "Train Epoch: 1 [640/20000 (3%)]\tLoss: 2.114012 testing loss: tensor(2.1351)\n",
      "Train Epoch: 1 [1280/20000 (6%)]\tLoss: 1.873737 testing loss: tensor(2.1334)\n",
      "Train Epoch: 1 [1920/20000 (10%)]\tLoss: 2.204055 testing loss: tensor(2.1355)\n",
      "Train Epoch: 1 [2560/20000 (13%)]\tLoss: 2.282410 testing loss: tensor(2.1349)\n",
      "Train Epoch: 1 [3200/20000 (16%)]\tLoss: 2.121735 testing loss: tensor(2.1344)\n",
      "Train Epoch: 1 [3840/20000 (19%)]\tLoss: 2.292624 testing loss: tensor(2.1320)\n",
      "Train Epoch: 1 [4480/20000 (22%)]\tLoss: 2.127934 testing loss: tensor(2.1333)\n",
      "Train Epoch: 1 [5120/20000 (25%)]\tLoss: 2.027736 testing loss: tensor(2.1330)\n",
      "Train Epoch: 1 [5760/20000 (29%)]\tLoss: 2.219331 testing loss: tensor(2.1342)\n",
      "Train Epoch: 1 [6400/20000 (32%)]\tLoss: 2.352233 testing loss: tensor(2.1349)\n",
      "Train Epoch: 1 [7040/20000 (35%)]\tLoss: 2.131573 testing loss: tensor(2.1367)\n",
      "Train Epoch: 1 [7680/20000 (38%)]\tLoss: 2.262300 testing loss: tensor(2.1360)\n",
      "Train Epoch: 1 [8320/20000 (41%)]\tLoss: 2.040346 testing loss: tensor(2.1369)\n",
      "Train Epoch: 1 [8960/20000 (45%)]\tLoss: 2.137658 testing loss: tensor(2.1360)\n",
      "Train Epoch: 1 [9600/20000 (48%)]\tLoss: 2.126174 testing loss: tensor(2.1361)\n",
      "Train Epoch: 1 [10240/20000 (51%)]\tLoss: 1.941370 testing loss: tensor(2.1354)\n",
      "Train Epoch: 1 [10880/20000 (54%)]\tLoss: 1.976977 testing loss: tensor(2.1347)\n",
      "Train Epoch: 1 [11520/20000 (57%)]\tLoss: 1.893227 testing loss: tensor(2.1358)\n",
      "Train Epoch: 1 [12160/20000 (61%)]\tLoss: 1.901342 testing loss: tensor(2.1354)\n",
      "Train Epoch: 1 [12800/20000 (64%)]\tLoss: 2.092957 testing loss: tensor(2.1354)\n",
      "Train Epoch: 1 [13440/20000 (67%)]\tLoss: 2.058895 testing loss: tensor(2.1362)\n",
      "Train Epoch: 1 [14080/20000 (70%)]\tLoss: 1.899362 testing loss: tensor(2.1372)\n",
      "Train Epoch: 1 [14720/20000 (73%)]\tLoss: 2.136126 testing loss: tensor(2.1367)\n",
      "Train Epoch: 1 [15360/20000 (76%)]\tLoss: 2.142452 testing loss: tensor(2.1369)\n",
      "Train Epoch: 1 [16000/20000 (80%)]\tLoss: 2.145110 testing loss: tensor(2.1361)\n",
      "Train Epoch: 1 [16640/20000 (83%)]\tLoss: 2.088903 testing loss: tensor(2.1330)\n",
      "Train Epoch: 1 [17280/20000 (86%)]\tLoss: 2.181524 testing loss: tensor(2.1352)\n",
      "Train Epoch: 1 [17920/20000 (89%)]\tLoss: 2.319844 testing loss: tensor(2.1335)\n",
      "Train Epoch: 1 [18560/20000 (92%)]\tLoss: 2.172940 testing loss: tensor(2.1337)\n",
      "Train Epoch: 1 [19200/20000 (96%)]\tLoss: 2.232929 testing loss: tensor(2.1340)\n",
      "Train Epoch: 1 [19840/20000 (99%)]\tLoss: 2.084550 testing loss: tensor(2.1367)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(2.1365)\n",
      "CS 2 : 2.1355\n",
      "DP 2 : 1.541\n",
      "heuristic 2 : 1.7159\n",
      "DP: 1.5344144105911255\n",
      "tensor([0.1278, 0.1245, 0.1247, 0.1245, 0.1209, 0.1268, 0.1232, 0.1277])\n",
      "tensor([0.1483, 0.1424, 0.1415, 0.1419, 0.1393, 0.1458, 0.1407, 1.0000])\n",
      "tensor([0.1729, 0.1669, 0.1647, 0.1619, 0.1639, 0.1698, 1.0000, 1.0000])\n",
      "tensor([0.2090, 0.2035, 0.1993, 0.1933, 0.1949, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.2611, 0.2527, 0.2463, 0.2399, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.3422, 0.3350, 0.3229, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.4987, 0.5013, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: twopeak heuristic\n",
      "Train Epoch: 1 [0/20000 (0%)]\tLoss: 0.039753\n",
      "Train Epoch: 1 [3200/20000 (16%)]\tLoss: 0.006311\n",
      "Train Epoch: 1 [6400/20000 (32%)]\tLoss: 0.002013\n",
      "Train Epoch: 1 [9600/20000 (48%)]\tLoss: 0.000522\n",
      "Train Epoch: 1 [12800/20000 (64%)]\tLoss: 0.000459\n",
      "Train Epoch: 1 [16000/20000 (80%)]\tLoss: 0.000274\n",
      "Train Epoch: 1 [19200/20000 (96%)]\tLoss: 0.000202\n",
      "NN 1 : tensor(1.6813)\n",
      "CS 1 : 2.1355\n",
      "DP 1 : 1.541\n",
      "heuristic 1 : 1.7159\n",
      "DP: 1.5344144105911255\n",
      "tensor([4.4241e-02, 4.7050e-02, 5.4857e-02, 6.8906e-01, 5.8501e-02, 6.1549e-02,\n",
      "        4.4119e-02, 6.2547e-04])\n",
      "tensor([0.0336, 0.0320, 0.0382, 0.0558, 0.7197, 0.0223, 0.0983, 1.0000])\n",
      "tensor([0.0384, 0.0407, 0.0328, 0.0928, 0.1330, 0.6624, 1.0000, 1.0000])\n",
      "tensor([0.0390, 0.0367, 0.0413, 0.1524, 0.7307, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.0826, 0.0706, 0.0964, 0.7504, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.1445, 0.1307, 0.7248, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.5043, 0.4957, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/20000 (0%)]\tLoss: 2.021939 testing loss: tensor(1.6856)\n",
      "Train Epoch: 1 [640/20000 (3%)]\tLoss: 2.190806 testing loss: tensor(2.0758)\n",
      "Train Epoch: 1 [1280/20000 (6%)]\tLoss: 2.277651 testing loss: tensor(2.2794)\n",
      "Train Epoch: 1 [1920/20000 (10%)]\tLoss: 2.500561 testing loss: tensor(2.3411)\n",
      "Train Epoch: 1 [2560/20000 (13%)]\tLoss: 2.079213 testing loss: tensor(2.1126)\n",
      "Train Epoch: 1 [3200/20000 (16%)]\tLoss: 1.855121 testing loss: tensor(1.9633)\n",
      "Train Epoch: 1 [3840/20000 (19%)]\tLoss: 1.879892 testing loss: tensor(1.6712)\n",
      "Train Epoch: 1 [4480/20000 (22%)]\tLoss: 1.695371 testing loss: tensor(1.7038)\n",
      "Train Epoch: 1 [5120/20000 (25%)]\tLoss: 1.523898 testing loss: tensor(1.6789)\n",
      "Train Epoch: 1 [5760/20000 (29%)]\tLoss: 1.398068 testing loss: tensor(1.6252)\n",
      "Train Epoch: 1 [6400/20000 (32%)]\tLoss: 1.664711 testing loss: tensor(1.5929)\n",
      "Train Epoch: 1 [7040/20000 (35%)]\tLoss: 1.526549 testing loss: tensor(1.5826)\n",
      "Train Epoch: 1 [7680/20000 (38%)]\tLoss: 1.618543 testing loss: tensor(1.5812)\n",
      "Train Epoch: 1 [8320/20000 (41%)]\tLoss: 1.428057 testing loss: tensor(1.5821)\n",
      "Train Epoch: 1 [8960/20000 (45%)]\tLoss: 1.616165 testing loss: tensor(1.5801)\n",
      "Train Epoch: 1 [9600/20000 (48%)]\tLoss: 1.599587 testing loss: tensor(1.5710)\n",
      "Train Epoch: 1 [10240/20000 (51%)]\tLoss: 1.424659 testing loss: tensor(1.5661)\n",
      "Train Epoch: 1 [10880/20000 (54%)]\tLoss: 1.668491 testing loss: tensor(1.5662)\n",
      "Train Epoch: 1 [11520/20000 (57%)]\tLoss: 1.644165 testing loss: tensor(1.5652)\n",
      "Train Epoch: 1 [12160/20000 (61%)]\tLoss: 1.778934 testing loss: tensor(1.5575)\n",
      "Train Epoch: 1 [12800/20000 (64%)]\tLoss: 1.793246 testing loss: tensor(1.5545)\n",
      "Train Epoch: 1 [13440/20000 (67%)]\tLoss: 1.545287 testing loss: tensor(1.5497)\n",
      "Train Epoch: 1 [14080/20000 (70%)]\tLoss: 1.659735 testing loss: tensor(1.5494)\n",
      "Train Epoch: 1 [14720/20000 (73%)]\tLoss: 1.388863 testing loss: tensor(1.5450)\n",
      "Train Epoch: 1 [15360/20000 (76%)]\tLoss: 1.700826 testing loss: tensor(1.5414)\n",
      "Train Epoch: 1 [16000/20000 (80%)]\tLoss: 1.603057 testing loss: tensor(1.5388)\n",
      "Train Epoch: 1 [16640/20000 (83%)]\tLoss: 1.584728 testing loss: tensor(1.5384)\n",
      "Train Epoch: 1 [17280/20000 (86%)]\tLoss: 1.632286 testing loss: tensor(1.5373)\n",
      "Train Epoch: 1 [17920/20000 (89%)]\tLoss: 1.491012 testing loss: tensor(1.5367)\n",
      "Train Epoch: 1 [18560/20000 (92%)]\tLoss: 1.330161 testing loss: tensor(1.5340)\n",
      "Train Epoch: 1 [19200/20000 (96%)]\tLoss: 1.722225 testing loss: tensor(1.5325)\n",
      "Train Epoch: 1 [19840/20000 (99%)]\tLoss: 1.749999 testing loss: tensor(1.5312)\n",
      "penalty: 0.6383381485939026\n",
      "NN 2 : tensor(1.5321)\n",
      "CS 2 : 2.1355\n",
      "DP 2 : 1.541\n",
      "heuristic 2 : 1.7159\n",
      "DP: 1.5344144105911255\n",
      "tensor([0.0539, 0.0498, 0.0466, 0.0203, 0.0531, 0.0248, 0.0776, 0.6740])\n",
      "tensor([0.0647, 0.0594, 0.0554, 0.0369, 0.0481, 0.0721, 0.6632, 1.0000])\n",
      "tensor([0.0658, 0.0586, 0.0579, 0.0693, 0.1122, 0.6362, 1.0000, 1.0000])\n",
      "tensor([0.0751, 0.0675, 0.0821, 0.1387, 0.6365, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.1249, 0.1203, 0.1805, 0.5744, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2618, 0.2504, 0.4878, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.5024, 0.4976, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.1)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "    \n",
    "for order in stage:\n",
    "    producedata(order)\n",
    "    #running\n",
    "    for order1 in order1name:\n",
    "        print(\"Supervised Aim:\",order,order1)\n",
    "        \n",
    "        # for mapping binary to payments before softmax\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(n, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, n),\n",
    "        )\n",
    "        model.apply(init_weights)\n",
    "        # optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr1)\n",
    "        \n",
    "        runningLossNN = []\n",
    "        runningLossCS = []\n",
    "        runningLossDP = []\n",
    "        runningLossHeuristic = []\n",
    "        #model=torch.load(\"save/pytorchNN=5dp1\");\n",
    "        #model.eval()\n",
    "        ##order1name=[\"costsharing\",\"dp\",\"heuristic\",\"random initializing\"]\n",
    "        for epoch in range(1, supervisionEpochs + 1):\n",
    "#             print(\"distributionRatio\",distributionRatio)\n",
    "            if(order1==\"costsharing\"):\n",
    "                supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "            elif(order1==\"dp\"):\n",
    "                supervisionTrain(epoch, dpSupervisionRule)\n",
    "            elif(order1==\"heuristic\"):\n",
    "                supervisionTrain(epoch, heuristicSupervisionRule)\n",
    "            elif(order1==\"random initializing\"):\n",
    "                print(\"do nothing\");\n",
    "\n",
    "        test()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr2)\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(epoch)\n",
    "            test()\n",
    "        losslistname.append(order+\" \"+order1);\n",
    "        losslist.append(losslisttemp);\n",
    "        losslisttemp=[];\n",
    "        savepath=\"save/pytorchNN=5all-beta\"+order+str(order1)\n",
    "        torch.save(model, savepath);\n",
    "        print(\"end\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colorlist=[\"#D2691E\",'#4169E1',\"#9ACD32\",\"#B22222\",\"#FF00FF\",\"#708090\"]\n",
    "for i in range(len(losslist)):\n",
    "    plt.plot(losslist[i], '.-', color=colorlist[i], alpha=0.8, label=losslistname[i])\n",
    "\n",
    "# 显示标签，如果不加这句，即使加了label='一些数字'的参数，最终还是不会显示标签\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('Trianing Times')\n",
    "plt.ylabel('Delay')\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('pytorchNN=5_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
