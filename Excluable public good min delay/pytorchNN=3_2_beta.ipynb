{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3540388371733616\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from random import randint\n",
    "import random\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "from pylab import *\n",
    "from scipy.stats import beta\n",
    "import math\n",
    "n = 3\n",
    "epochs = 1\n",
    "supervisionEpochs = 1\n",
    "lr1 = 0.005\n",
    "lr2 = 0.0005\n",
    "log_interval = 5\n",
    "trainSize = 50000#100000\n",
    "percentage_train_test= 0.5\n",
    "penaltyLambda = 500\n",
    "doublePeakHighMean = 0.6\n",
    "doublePeakLowMean = 0.2\n",
    "doublePeakStd = 0.1\n",
    "uniformlow=0\n",
    "uniformhigh=1.0\n",
    "normalloc = 0.2\n",
    "normalscale = 0.1\n",
    "\n",
    "cauchyloc = 1/n\n",
    "cauchyscalen = 0.004\n",
    "\n",
    "exponentialhigh = 15 #Symbol(\"b\", real=True)\n",
    "exponentiallow  = 15 #Symbol(\"a\", real=True)\n",
    "\n",
    "\n",
    "beta_a = 0.1\n",
    "beta_b  = 0.1\n",
    "kumaraswamy_a = beta_a \n",
    "kumaraswamy_b = (1.0+(beta_a-1.0)*math.pow( (beta_a+beta_b-2.0)/(beta_a-1.0), beta_a) )/beta_a \n",
    "print(kumaraswamy_b)\n",
    "\n",
    "\n",
    "independentnormalloc1=[(float(ii)+1)/(2*n+1) for ii in range(n,0,-1)]\n",
    "independentnormalscale1=[0.05 for ii in range(n)]\n",
    "\n",
    "independentnormalloc2=[(float(ii)+1)/(2*n+1) for ii in range(1,n+1,1)]\n",
    "independentnormalscale2=[0.05 for ii in range(n)]\n",
    "stage=[\"beta\"]\n",
    "# \"twopeak\",\"normal\",\"uniform\",\"independent1\",\"independent2\",\"cauchy\",\"beta\",\"U-exponential\",\"arcsine\"\n",
    "order1name=[\"dp\",\"random initializing\",\"costsharing\",\"heuristic\"]\n",
    "# \"costsharing\",\"dp\",\"heuristic\",\"random initializing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio1 = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase1 = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "d3 = D.normal.Normal(normalloc, normalscale)\n",
    "distributionRatio3 = d3.cdf(1) - d3.cdf(0)\n",
    "distributionBase3 = d3.cdf(0)\n",
    "\n",
    "d4 = D.uniform.Uniform(uniformlow,uniformhigh)\n",
    "distributionRatio4 = d4.cdf(1) - d4.cdf(0)\n",
    "distributionBase4 = d4.cdf(0)\n",
    "\n",
    "d5 = [D.normal.Normal(independentnormalloc1[ii], independentnormalscale1[ii]) for ii in range(n)]\n",
    "d6 = [D.normal.Normal(independentnormalloc2[ii], independentnormalscale2[ii]) for ii in range(n)]\n",
    "\n",
    "d7 = D.cauchy.Cauchy(cauchyloc,cauchyscalen)\n",
    "\n",
    "d81 = D.exponential.Exponential(exponentiallow)\n",
    "d82 = D.exponential.Exponential(exponentialhigh)\n",
    "\n",
    "d9 = D.beta.Beta(beta_a,beta_b)\n",
    "#sample_d9=d9.rsample(torch.Size([100000]))\n",
    "\n",
    "#d9_sample=np.linspace(0.0001, 0.9999, 10000)\n",
    "#d9_pdf=torch.exp(d9.log_prob(torch.tensor(d9_sample,dtype=torch.float32)))\n",
    "#d9_delta=d9_sample[1]-d9_sample[0]\n",
    "#d9_sum_pdf=torch.sum(d9_pdf*d9_delta)\n",
    "\n",
    "d10 = D.beta.Beta(0.5,0.5)\n",
    "\n",
    "def beta_cdf(x,y, i=None):\n",
    "    return beta.cdf(x, beta_a, beta_b)\n",
    "\n",
    "\n",
    "def cdf(x,y, i=None):\n",
    "    if(y==\"twopeak\"):\n",
    "        return (d1.cdf(x) + d2.cdf(x) - distributionBase1) / 2 / distributionRatio1\n",
    "    elif(y==\"normal\"):\n",
    "        return (d3.cdf(x)-distributionBase3)/distributionRatio3;\n",
    "    elif(y==\"uniform\"):\n",
    "        return (d4.cdf(x)-distributionBase4)/distributionRatio4;\n",
    "    elif(y==\"independent1\"):\n",
    "        return d5[i].cdf(x);\n",
    "    elif(y==\"independent2\"):\n",
    "        return d6[i].cdf(x);\n",
    "    elif(y==\"cauchy\"):\n",
    "        return d7.cdf(x);\n",
    "    elif(y==\"beta\"):\n",
    "#         sum_cdf=0.0;\n",
    "#         if(x<0.0001):\n",
    "#             x=0.00011;\n",
    "#         if(x>0.9999):\n",
    "#             x=0.99989;\n",
    "#         for i in range(len(d9_pdf)):\n",
    "#             if(d9_sample[i]<x):\n",
    "#                 sum_cdf+=d9_pdf[i]*d9_delta;\n",
    "#             else:\n",
    "#                 sum_cdf+=(d9_pdf[i]+d9_pdf[i-1])/ 2 *(x-d9_sample[i-1])\n",
    "#                 break;\n",
    "#         return sum_cdf/d9_sum_pdf\n",
    "#         cdf_v=torch.sum((sample_d9<(x)), dtype=torch.float32)/100000\n",
    "#         return cdf_v\n",
    "#    F(x|a,b)=1–(1–x^a)^b\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(x,kumaraswamy_a),kumaraswamy_b);\n",
    "        except:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(torch.tensor(x,dtype=torch.float32),kumaraswamy_a),kumaraswamy_b);\n",
    "    elif(y==\"arcsine\"):\n",
    "        #\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            res=2.0/math.pi * torch.asin(torch.sqrt(x))\n",
    "            #print(x)\n",
    "            return res# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "        except:\n",
    "            return 2.0/math.pi * torch.asin(torch.sqrt(torch.tensor(x,dtype=torch.float32)))# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "    elif(y==\"U-exponential\"):\n",
    "        return (d81.cdf(x) + (1.0 - d82.cdf(1.0-x)))  / 2 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    if torch.sum(bits).item() == 0:\n",
    "        return torch.ones(n)\n",
    "    bits = bits.type(torch.float32)\n",
    "    negBits = torch.ones(n) - bits\n",
    "    payments = model(bits)\n",
    "    payments = payments - 1000 * negBits\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    payments = payments + negBits\n",
    "    return payments\n",
    "\n",
    "def tpToBits(tp, bits=torch.ones(n).type(torch.uint8)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = (tp >= payments).type(torch.uint8)\n",
    "    if torch.equal(newBits, bits):\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits(tp, newBits)#bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    return n - torch.sum(tpToBits(tp).type(torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    global  dp,decision\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money, yes]\n",
    "        offer = float(offerIndex) / dpPrecision\n",
    "        if tp[i] >= offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits[i] = 0\n",
    "            payments[i] = 1\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "#     print()\n",
    "#     print(tp)\n",
    "#     print(bits)\n",
    "#     print(payments)\n",
    "#     print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def heuristicSupervisionRule(tp):\n",
    "    global  dp_H,decision_H\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    tempold=-1;\n",
    "    for turns in range(n,0,-1):\n",
    "        money = dpPrecision\n",
    "        j=0\n",
    "        tempo=sum(bits)\n",
    "        #print(\"bits\",tempo)\n",
    "        for i in range(n):\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            \n",
    "            offerIndex = decision_H[tempo ,tempo - i, money]\n",
    "            offer = float(offerIndex) / dpPrecision\n",
    "            while(j<n):\n",
    "                if(bits[j]!=0):\n",
    "                    break;\n",
    "                j+=1;\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            if tp[j] >= offer:\n",
    "                #print(money,j,tp[j],offer)\n",
    "                money -= offerIndex\n",
    "                bits[j] = 1\n",
    "                payments[j] = offer\n",
    "            else:\n",
    "                bits[j] = 0;\n",
    "                payments[j] = 1.0;\n",
    "            j+=1;\n",
    "        #print(\"money\",money)\n",
    "        if(money==0 and tempold==tempo):\n",
    "            break;\n",
    "        tempold=tempo;\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    for k in range(n, -1, -1):\n",
    "        if k == 0:\n",
    "            break\n",
    "        bits = [1 if tp[ii] >= 1.0 / k else 0 for ii in range(n)]\n",
    "        if sum(bits) == k:\n",
    "            break\n",
    "    if k == 0:\n",
    "        payments = [1 for ii in range(n)]\n",
    "    else:\n",
    "        payments = [1.0 / k if bits[ii] == 1 else 1 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    return float(n - torch.sum(costSharingSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def dpDelay(tp):\n",
    "    return float(n - torch.sum(dpSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def heuristicDelay(tp):\n",
    "    return float(n - torch.sum(heuristicSupervisionRule(tp)[0]).item())\n",
    "\n",
    "\n",
    "# templire=0;\n",
    "# num=1\n",
    "# for i in range(num):\n",
    "#     temp=torch.tensor(samplesJoint[i])\n",
    "#     #print(temp)\n",
    "#     #print(temp);\n",
    "#     #print(heuristicSupervisionRule(temp))\n",
    "#     #print(dpSupervisionRule(temp))\n",
    "#     #print(dpDelay(temp))\n",
    "#     res=dpDelay(temp)\n",
    "#     templire+=res\n",
    "#     #print(\"Delay\",res)\n",
    "#     #print()\n",
    "# print(templire/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordAndReport(name, source, loss):\n",
    "    source.append(loss)\n",
    "    realLength = len(source)\n",
    "    #print(f\"{name} ({realLength}): {loss}\")\n",
    "    print(name,realLength,\":\" ,loss)\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    \n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda /100000\n",
    "        \n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % (log_interval*5) == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "losslist=[];\n",
    "losslistname=[];\n",
    "losslisttemp=[];\n",
    "def test_batch():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lenLoss= 0\n",
    "        nnLoss = 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "            for tp in tp_batch:\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        nnLoss/=lenLoss\n",
    "    return nnLoss\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                Delay1 = tpToTotalDelay(tp1)\n",
    "                Delay0 = tpToTotalDelay(tp0)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                    loss = loss + (1 - cdf(offer,order)) * Delay1 + cdf(offer,order) * Delay0\n",
    "                else:\n",
    "                    loss = loss + (1 - cdf(offer,order,i)) * Delay1 + cdf(offer,order,i) * Delay0\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "    \n",
    "allBits = [torch.tensor(bits, dtype=torch.int16) for bits in itertools.product([0, 1], repeat=n)]\n",
    "\n",
    "def test():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        costSharingLoss = 0\n",
    "        dpLoss = 0\n",
    "        nnLoss = 0\n",
    "        heuristicLoss=0\n",
    "        lenLoss= 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "                heuristicLoss+= heuristicDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        costSharingLoss /= lenLoss\n",
    "        dpLoss /= lenLoss\n",
    "        nnLoss /= lenLoss\n",
    "        heuristicLoss /= lenLoss\n",
    "        #print(lenLoss)\n",
    "        losslisttemp.append(nnLoss);\n",
    "        recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "        recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "        recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "        recordAndReport(\"heuristic\", runningLossHeuristic, heuristicLoss)\n",
    "        print(\"DP:\",dp[n, dpPrecision,0])\n",
    "        #for i in range(n, 0, -1):\n",
    "        #    print(\"Heuristic:\",i,5*(1-dp_H[i, i, dpPrecision]))\n",
    "        for i in range(n, 0, -1):\n",
    "            print(\n",
    "                    tpToPayments(\n",
    "                            torch.tensor([0 if ii >= i else 1 for ii in range(n)], dtype=torch.float32)\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "dpPrecision = 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def producedata(order):\n",
    "    global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "    dpPrecision = 100 \n",
    "    if(order==\"twopeak\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = np.random.normal(\n",
    "            loc=doublePeakLowMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=doublePeakLowMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samples2 = np.random.normal(\n",
    "            loc=doublePeakHighMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = np.random.normal(\n",
    "                        loc=doublePeakHighMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "    elif(order==\"normal\"):\n",
    "        print(\"loc\",normalloc, \"scale\",normalscale)\n",
    "        samples1 = np.random.normal(\n",
    "            loc=normalloc, scale=normalscale, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=normalloc, scale=normalscale\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"uniform\"):  \n",
    "        print(\"uniformlow\",uniformlow, \"uniformhigh\",uniformhigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        uniformlow, uniformhigh\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent1\"):\n",
    "        print(\"loc\",independentnormalloc1,\"scale\",independentnormalscale1)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent2\"):\n",
    "        print(\"loc\",independentnormalloc2, \"scale\",independentnormalscale2)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"cauchy\"):\n",
    "        print(\"cauchyloc\",cauchyloc, \"cauchyscale\",cauchyscalen)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"beta\"):\n",
    "        print(\"beta_a\",beta_a, \"beta_b\",beta_b)\n",
    "        print(\"kumaraswamy_a\",kumaraswamy_a, \"kumaraswamy_b\",kumaraswamy_b)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"arcsine\"):\n",
    "        print(\"betalow\",0.5, \"betahigh\",0.5)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"U-exponential\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = d81.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d81.rsample(torch.Size([1])).numpy()\n",
    "                    \n",
    "        samples2 = d82.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        \n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = d82.rsample(torch.Size([1])).numpy()\n",
    "        samples2 = 1.0 - samples2\n",
    "        samplesJoint = signals * samples1 - (signals - 1.0) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "        \n",
    "    plt.hist(samplesJoint,bins=500)\n",
    "    plt.show()\n",
    "    \n",
    "    tp_dataset = TensorDataset(tp_tensor[: int(trainSize * percentage_train_test)])\n",
    "    tp_dataset_testing = TensorDataset(tp_tensor[int(trainSize * (1.0-percentage_train_test)) :])\n",
    "    tp_dataloader = DataLoader(tp_dataset, batch_size=128, shuffle=True)\n",
    "    tp_dataloader_testing = DataLoader(tp_dataset_testing, batch_size=256, shuffle=False)\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "    decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for yes in range(n + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp[0, 0, yes] = 0\n",
    "            else:\n",
    "                dp[0, money, yes] = yes# + 1.0\n",
    "    for ppl in range(1,  n + 1):\n",
    "        for yes in range(n + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                minSoFar = 1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order==\"beta\"):\n",
    "                        res = (1 - beta_cdf(offer,order)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + beta_cdf(offer,order) * (1 + dp[ppl - 1, money, yes])\n",
    "#                     if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "#                         res = (1 - cdf(offer,order)) * dp[\n",
    "#                         ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "#                         ] + cdf(offer,order) * (1 + dp[ppl - 1, money, yes])\n",
    "#                     else:\n",
    "#                         res = (1 - cdf(offer,order,n-ppl)) * dp[\n",
    "#                         ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "#                         ] + cdf(offer,order,n-ppl) * (1 + dp[ppl - 1, money, yes])\n",
    "                    if minSoFar > res.item():\n",
    "                        minSoFar = res.item()\n",
    "                        decision[ppl, money, yes] = offerIndex\n",
    "                dp[ppl, money, yes] = minSoFar\n",
    "    \n",
    "    print(\"dp\",dp[n, dpPrecision, 0])\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp_H = np.zeros([n + 1 , n + 1, dpPrecision + 1])\n",
    "    decision_H = np.zeros([n + 1 , n + 1, dpPrecision + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for i in range(1,n+1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp_H[i, 0, 0] = 1\n",
    "            else:\n",
    "                offer = money / dpPrecision\n",
    "                dp_H[i, 0, money] = 0#cdf(offer)# + 1.0\n",
    "    for i in range(1,n+1):\n",
    "        for ppl in range(1, i + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                maxSoFar = -1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1-cdf(offer,order)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    else:\n",
    "                        res = (1-cdf(offer,order,ppl-1)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    if maxSoFar < res.item():\n",
    "                        maxSoFar = res.item()\n",
    "                        decision_H[i, ppl, money] = offerIndex\n",
    "                dp_H[i, ppl, money] = maxSoFar\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_a 0.1 beta_b 0.1\n",
      "kumaraswamy_a 0.1 kumaraswamy_b 0.3540388371733616\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dp 1.7499999999999998\n",
      "Supervised Aim: beta dp\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 0.023932\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 0.000406\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 0.000062\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 0.000026\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 0.000009\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 0.000025\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 0.000012\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 0.000011\n",
      "NN 1 : tensor(1.7685)\n",
      "CS 1 : 1.79112\n",
      "DP 1 : 1.762\n",
      "heuristic 1 : 1.76356\n",
      "DP: 1.7499999999999998\n",
      "tensor([0.4852, 0.5082, 0.0066])\n",
      "tensor([0.4918, 0.5082, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 1.805752 testing loss: tensor(1.7705)\n",
      "Train Epoch: 1 [640/25000 (3%)]\tLoss: 1.940781 testing loss: tensor(1.7679)\n",
      "Train Epoch: 1 [1280/25000 (5%)]\tLoss: 1.893421 testing loss: tensor(1.7679)\n",
      "Train Epoch: 1 [1920/25000 (8%)]\tLoss: 1.844182 testing loss: tensor(1.7667)\n",
      "Train Epoch: 1 [2560/25000 (10%)]\tLoss: 2.019232 testing loss: tensor(1.7664)\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 1.895079 testing loss: tensor(1.7669)\n",
      "Train Epoch: 1 [3840/25000 (15%)]\tLoss: 1.983271 testing loss: tensor(1.7670)\n",
      "Train Epoch: 1 [4480/25000 (18%)]\tLoss: 1.859627 testing loss: tensor(1.7675)\n",
      "Train Epoch: 1 [5120/25000 (20%)]\tLoss: 1.890708 testing loss: tensor(1.7676)\n",
      "Train Epoch: 1 [5760/25000 (23%)]\tLoss: 1.949094 testing loss: tensor(1.7666)\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 1.931056 testing loss: tensor(1.7672)\n",
      "Train Epoch: 1 [7040/25000 (28%)]\tLoss: 1.830256 testing loss: tensor(1.7668)\n",
      "Train Epoch: 1 [7680/25000 (31%)]\tLoss: 1.910652 testing loss: tensor(1.7668)\n",
      "Train Epoch: 1 [8320/25000 (33%)]\tLoss: 1.888834 testing loss: tensor(1.7670)\n",
      "Train Epoch: 1 [8960/25000 (36%)]\tLoss: 1.967924 testing loss: tensor(1.7658)\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 1.860160 testing loss: tensor(1.7651)\n",
      "Train Epoch: 1 [10240/25000 (41%)]\tLoss: 2.085240 testing loss: tensor(1.7660)\n",
      "Train Epoch: 1 [10880/25000 (43%)]\tLoss: 1.898144 testing loss: tensor(1.7653)\n",
      "Train Epoch: 1 [11520/25000 (46%)]\tLoss: 1.994884 testing loss: tensor(1.7660)\n",
      "Train Epoch: 1 [12160/25000 (48%)]\tLoss: 1.898592 testing loss: tensor(1.7666)\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 1.841113 testing loss: tensor(1.7663)\n",
      "Train Epoch: 1 [13440/25000 (54%)]\tLoss: 1.939045 testing loss: tensor(1.7661)\n",
      "Train Epoch: 1 [14080/25000 (56%)]\tLoss: 1.952783 testing loss: tensor(1.7651)\n",
      "Train Epoch: 1 [14720/25000 (59%)]\tLoss: 1.940640 testing loss: tensor(1.7648)\n",
      "Train Epoch: 1 [15360/25000 (61%)]\tLoss: 1.875441 testing loss: tensor(1.7640)\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 1.799644 testing loss: tensor(1.7635)\n",
      "Train Epoch: 1 [16640/25000 (66%)]\tLoss: 1.920213 testing loss: tensor(1.7639)\n",
      "Train Epoch: 1 [17280/25000 (69%)]\tLoss: 1.933835 testing loss: tensor(1.7641)\n",
      "Train Epoch: 1 [17920/25000 (71%)]\tLoss: 1.805722 testing loss: tensor(1.7636)\n",
      "Train Epoch: 1 [18560/25000 (74%)]\tLoss: 2.034458 testing loss: tensor(1.7633)\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 1.894333 testing loss: tensor(1.7639)\n",
      "Train Epoch: 1 [19840/25000 (79%)]\tLoss: 1.900911 testing loss: tensor(1.7639)\n",
      "Train Epoch: 1 [20480/25000 (82%)]\tLoss: 1.863810 testing loss: tensor(1.7642)\n",
      "Train Epoch: 1 [21120/25000 (84%)]\tLoss: 1.930471 testing loss: tensor(1.7640)\n",
      "Train Epoch: 1 [21760/25000 (87%)]\tLoss: 1.829873 testing loss: tensor(1.7628)\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 1.942143 testing loss: tensor(1.7624)\n",
      "Train Epoch: 1 [23040/25000 (92%)]\tLoss: 1.864578 testing loss: tensor(1.7620)\n",
      "Train Epoch: 1 [23680/25000 (94%)]\tLoss: 1.914995 testing loss: tensor(1.7630)\n",
      "Train Epoch: 1 [24320/25000 (97%)]\tLoss: 1.938176 testing loss: tensor(1.7631)\n",
      "Train Epoch: 1 [7800/25000 (99%)]\tLoss: 1.904814 testing loss: tensor(1.7630)\n",
      "penalty: 7.05718994140625e-05\n",
      "NN 2 : tensor(1.7630)\n",
      "CS 2 : 1.79112\n",
      "DP 2 : 1.762\n",
      "heuristic 2 : 1.76356\n",
      "DP: 1.7499999999999998\n",
      "tensor([0.5132, 0.4849, 0.0019])\n",
      "tensor([0.5089, 0.4911, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: beta random initializing\n",
      "do nothing\n",
      "NN 1 : tensor(1.7919)\n",
      "CS 1 : 1.79112\n",
      "DP 1 : 1.762\n",
      "heuristic 1 : 1.76356\n",
      "DP: 1.7499999999999998\n",
      "tensor([0.2651, 0.3064, 0.4284])\n",
      "tensor([0.4580, 0.5420, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 1.847942 testing loss: tensor(1.7912)\n",
      "Train Epoch: 1 [640/25000 (3%)]\tLoss: 1.928286 testing loss: tensor(1.7903)\n",
      "Train Epoch: 1 [1280/25000 (5%)]\tLoss: 1.954993 testing loss: tensor(1.7884)\n",
      "Train Epoch: 1 [1920/25000 (8%)]\tLoss: 1.958137 testing loss: tensor(1.7895)\n",
      "Train Epoch: 1 [2560/25000 (10%)]\tLoss: 1.940899 testing loss: tensor(1.7884)\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 1.880648 testing loss: tensor(1.7871)\n",
      "Train Epoch: 1 [3840/25000 (15%)]\tLoss: 1.939345 testing loss: tensor(1.7877)\n",
      "Train Epoch: 1 [4480/25000 (18%)]\tLoss: 1.924803 testing loss: tensor(1.7868)\n",
      "Train Epoch: 1 [5120/25000 (20%)]\tLoss: 1.999031 testing loss: tensor(1.7876)\n",
      "Train Epoch: 1 [5760/25000 (23%)]\tLoss: 1.971175 testing loss: tensor(1.7890)\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 2.020763 testing loss: tensor(1.7900)\n",
      "Train Epoch: 1 [7040/25000 (28%)]\tLoss: 1.899516 testing loss: tensor(1.7892)\n",
      "Train Epoch: 1 [7680/25000 (31%)]\tLoss: 1.835211 testing loss: tensor(1.7891)\n",
      "Train Epoch: 1 [8320/25000 (33%)]\tLoss: 1.982889 testing loss: tensor(1.7895)\n",
      "Train Epoch: 1 [8960/25000 (36%)]\tLoss: 1.842647 testing loss: tensor(1.7889)\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 1.993818 testing loss: tensor(1.7888)\n",
      "Train Epoch: 1 [10240/25000 (41%)]\tLoss: 1.785225 testing loss: tensor(1.7894)\n",
      "Train Epoch: 1 [10880/25000 (43%)]\tLoss: 1.952922 testing loss: tensor(1.7906)\n",
      "Train Epoch: 1 [11520/25000 (46%)]\tLoss: 1.974989 testing loss: tensor(1.7902)\n",
      "Train Epoch: 1 [12160/25000 (48%)]\tLoss: 1.828467 testing loss: tensor(1.7894)\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 1.873264 testing loss: tensor(1.7898)\n",
      "Train Epoch: 1 [13440/25000 (54%)]\tLoss: 1.858972 testing loss: tensor(1.7894)\n",
      "Train Epoch: 1 [14080/25000 (56%)]\tLoss: 1.799634 testing loss: tensor(1.7909)\n",
      "Train Epoch: 1 [14720/25000 (59%)]\tLoss: 1.924050 testing loss: tensor(1.7904)\n",
      "Train Epoch: 1 [15360/25000 (61%)]\tLoss: 1.905911 testing loss: tensor(1.7890)\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 2.012091 testing loss: tensor(1.7888)\n",
      "Train Epoch: 1 [16640/25000 (66%)]\tLoss: 1.891798 testing loss: tensor(1.7883)\n",
      "Train Epoch: 1 [17280/25000 (69%)]\tLoss: 1.891992 testing loss: tensor(1.7884)\n",
      "Train Epoch: 1 [17920/25000 (71%)]\tLoss: 1.885704 testing loss: tensor(1.7887)\n",
      "Train Epoch: 1 [18560/25000 (74%)]\tLoss: 2.011060 testing loss: tensor(1.7886)\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 1.922342 testing loss: tensor(1.7902)\n",
      "Train Epoch: 1 [19840/25000 (79%)]\tLoss: 1.999129 testing loss: tensor(1.7907)\n",
      "Train Epoch: 1 [20480/25000 (82%)]\tLoss: 1.943083 testing loss: tensor(1.7903)\n",
      "Train Epoch: 1 [21120/25000 (84%)]\tLoss: 1.928047 testing loss: tensor(1.7890)\n",
      "Train Epoch: 1 [21760/25000 (87%)]\tLoss: 2.028922 testing loss: tensor(1.7899)\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 1.990169 testing loss: tensor(1.7901)\n",
      "Train Epoch: 1 [23040/25000 (92%)]\tLoss: 1.907345 testing loss: tensor(1.7905)\n",
      "Train Epoch: 1 [23680/25000 (94%)]\tLoss: 1.903225 testing loss: tensor(1.7903)\n",
      "Train Epoch: 1 [24320/25000 (97%)]\tLoss: 1.872170 testing loss: tensor(1.7908)\n",
      "Train Epoch: 1 [7800/25000 (99%)]\tLoss: 1.836742 testing loss: tensor(1.7914)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.7914)\n",
      "CS 2 : 1.79112\n",
      "DP 2 : 1.762\n",
      "heuristic 2 : 1.76356\n",
      "DP: 1.7499999999999998\n",
      "tensor([0.4021, 0.2238, 0.3741])\n",
      "tensor([0.4968, 0.5032, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: beta costsharing\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 0.001409\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 0.000040\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 0.000003\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 0.000000\n",
      "NN 1 : tensor(1.7911)\n",
      "CS 1 : 1.79112\n",
      "DP 1 : 1.762\n",
      "heuristic 1 : 1.76356\n",
      "DP: 1.7499999999999998\n",
      "tensor([0.3333, 0.3333, 0.3333])\n",
      "tensor([0.5000, 0.5000, 1.0000])\n",
      "tensor([1., 1., 1.])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 1.875171 testing loss: tensor(1.7910)\n",
      "Train Epoch: 1 [640/25000 (3%)]\tLoss: 2.074719 testing loss: tensor(1.7914)\n",
      "Train Epoch: 1 [1280/25000 (5%)]\tLoss: 2.013392 testing loss: tensor(1.7903)\n",
      "Train Epoch: 1 [1920/25000 (8%)]\tLoss: 2.012871 testing loss: tensor(1.7890)\n",
      "Train Epoch: 1 [2560/25000 (10%)]\tLoss: 1.856586 testing loss: tensor(1.7881)\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 1.965839 testing loss: tensor(1.7887)\n",
      "Train Epoch: 1 [3840/25000 (15%)]\tLoss: 1.970367 testing loss: tensor(1.7892)\n",
      "Train Epoch: 1 [4480/25000 (18%)]\tLoss: 1.889280 testing loss: tensor(1.7895)\n",
      "Train Epoch: 1 [5120/25000 (20%)]\tLoss: 1.925077 testing loss: tensor(1.7899)\n",
      "Train Epoch: 1 [5760/25000 (23%)]\tLoss: 1.901805 testing loss: tensor(1.7917)\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 2.025789 testing loss: tensor(1.7912)\n",
      "Train Epoch: 1 [7040/25000 (28%)]\tLoss: 1.924459 testing loss: tensor(1.7916)\n",
      "Train Epoch: 1 [7680/25000 (31%)]\tLoss: 1.970700 testing loss: tensor(1.7906)\n",
      "Train Epoch: 1 [8320/25000 (33%)]\tLoss: 1.989545 testing loss: tensor(1.7896)\n",
      "Train Epoch: 1 [8960/25000 (36%)]\tLoss: 1.954672 testing loss: tensor(1.7895)\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 1.804168 testing loss: tensor(1.7896)\n",
      "Train Epoch: 1 [10240/25000 (41%)]\tLoss: 1.839830 testing loss: tensor(1.7887)\n",
      "Train Epoch: 1 [10880/25000 (43%)]\tLoss: 1.971267 testing loss: tensor(1.7883)\n",
      "Train Epoch: 1 [11520/25000 (46%)]\tLoss: 1.867120 testing loss: tensor(1.7889)\n",
      "Train Epoch: 1 [12160/25000 (48%)]\tLoss: 1.891135 testing loss: tensor(1.7894)\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 1.867778 testing loss: tensor(1.7904)\n",
      "Train Epoch: 1 [13440/25000 (54%)]\tLoss: 1.960562 testing loss: tensor(1.7912)\n",
      "Train Epoch: 1 [14080/25000 (56%)]\tLoss: 1.766050 testing loss: tensor(1.7908)\n",
      "Train Epoch: 1 [14720/25000 (59%)]\tLoss: 1.894455 testing loss: tensor(1.7905)\n",
      "Train Epoch: 1 [15360/25000 (61%)]\tLoss: 1.920995 testing loss: tensor(1.7894)\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 1.821751 testing loss: tensor(1.7893)\n",
      "Train Epoch: 1 [16640/25000 (66%)]\tLoss: 1.923970 testing loss: tensor(1.7892)\n",
      "Train Epoch: 1 [17280/25000 (69%)]\tLoss: 1.896441 testing loss: tensor(1.7893)\n",
      "Train Epoch: 1 [17920/25000 (71%)]\tLoss: 1.988990 testing loss: tensor(1.7891)\n",
      "Train Epoch: 1 [18560/25000 (74%)]\tLoss: 1.891095 testing loss: tensor(1.7906)\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 2.007949 testing loss: tensor(1.7904)\n",
      "Train Epoch: 1 [19840/25000 (79%)]\tLoss: 1.938734 testing loss: tensor(1.7905)\n",
      "Train Epoch: 1 [20480/25000 (82%)]\tLoss: 1.874515 testing loss: tensor(1.7909)\n",
      "Train Epoch: 1 [21120/25000 (84%)]\tLoss: 1.860296 testing loss: tensor(1.7913)\n",
      "Train Epoch: 1 [21760/25000 (87%)]\tLoss: 2.037502 testing loss: tensor(1.7913)\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 1.931602 testing loss: tensor(1.7907)\n",
      "Train Epoch: 1 [23040/25000 (92%)]\tLoss: 1.961886 testing loss: tensor(1.7912)\n",
      "Train Epoch: 1 [23680/25000 (94%)]\tLoss: 1.989553 testing loss: tensor(1.7903)\n",
      "Train Epoch: 1 [24320/25000 (97%)]\tLoss: 2.072242 testing loss: tensor(1.7900)\n",
      "Train Epoch: 1 [7800/25000 (99%)]\tLoss: 1.901281 testing loss: tensor(1.7903)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.7903)\n",
      "CS 2 : 1.79112\n",
      "DP 2 : 1.762\n",
      "heuristic 2 : 1.76356\n",
      "DP: 1.7499999999999998\n",
      "tensor([0.3284, 0.3519, 0.3197])\n",
      "tensor([0.5129, 0.4871, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: beta heuristic\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 0.016438\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 0.000256\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 0.000043\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 0.000009\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 0.000005\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 0.000007\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 0.000004\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 0.000003\n",
      "NN 1 : tensor(1.7688)\n",
      "CS 1 : 1.79112\n",
      "DP 1 : 1.762\n",
      "heuristic 1 : 1.76356\n",
      "DP: 1.7499999999999998\n",
      "tensor([0.0041, 0.5034, 0.4925])\n",
      "tensor([0.5056, 0.4944, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 1.864594 testing loss: tensor(1.7686)\n",
      "Train Epoch: 1 [640/25000 (3%)]\tLoss: 1.897069 testing loss: tensor(1.7687)\n",
      "Train Epoch: 1 [1280/25000 (5%)]\tLoss: 1.838620 testing loss: tensor(1.7686)\n",
      "Train Epoch: 1 [1920/25000 (8%)]\tLoss: 1.924551 testing loss: tensor(1.7684)\n",
      "Train Epoch: 1 [2560/25000 (10%)]\tLoss: 2.039218 testing loss: tensor(1.7656)\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 1.789604 testing loss: tensor(1.7650)\n",
      "Train Epoch: 1 [3840/25000 (15%)]\tLoss: 1.850011 testing loss: tensor(1.7655)\n",
      "Train Epoch: 1 [4480/25000 (18%)]\tLoss: 1.898307 testing loss: tensor(1.7653)\n",
      "Train Epoch: 1 [5120/25000 (20%)]\tLoss: 1.834340 testing loss: tensor(1.7656)\n",
      "Train Epoch: 1 [5760/25000 (23%)]\tLoss: 1.934858 testing loss: tensor(1.7649)\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 2.023918 testing loss: tensor(1.7643)\n",
      "Train Epoch: 1 [7040/25000 (28%)]\tLoss: 2.000730 testing loss: tensor(1.7637)\n",
      "Train Epoch: 1 [7680/25000 (31%)]\tLoss: 1.921797 testing loss: tensor(1.7638)\n",
      "Train Epoch: 1 [8320/25000 (33%)]\tLoss: 1.846902 testing loss: tensor(1.7646)\n",
      "Train Epoch: 1 [8960/25000 (36%)]\tLoss: 1.851895 testing loss: tensor(1.7638)\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 1.964596 testing loss: tensor(1.7635)\n",
      "Train Epoch: 1 [10240/25000 (41%)]\tLoss: 1.854958 testing loss: tensor(1.7632)\n",
      "Train Epoch: 1 [10880/25000 (43%)]\tLoss: 1.910510 testing loss: tensor(1.7632)\n",
      "Train Epoch: 1 [11520/25000 (46%)]\tLoss: 1.843611 testing loss: tensor(1.7634)\n",
      "Train Epoch: 1 [12160/25000 (48%)]\tLoss: 1.865610 testing loss: tensor(1.7634)\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 1.954852 testing loss: tensor(1.7646)\n",
      "Train Epoch: 1 [13440/25000 (54%)]\tLoss: 2.033872 testing loss: tensor(1.7639)\n",
      "Train Epoch: 1 [14080/25000 (56%)]\tLoss: 1.787873 testing loss: tensor(1.7646)\n",
      "Train Epoch: 1 [14720/25000 (59%)]\tLoss: 1.929025 testing loss: tensor(1.7632)\n",
      "Train Epoch: 1 [15360/25000 (61%)]\tLoss: 1.977898 testing loss: tensor(1.7625)\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 1.877521 testing loss: tensor(1.7620)\n",
      "Train Epoch: 1 [16640/25000 (66%)]\tLoss: 1.953360 testing loss: tensor(1.7622)\n",
      "Train Epoch: 1 [17280/25000 (69%)]\tLoss: 1.841853 testing loss: tensor(1.7622)\n",
      "Train Epoch: 1 [17920/25000 (71%)]\tLoss: 1.843230 testing loss: tensor(1.7619)\n",
      "Train Epoch: 1 [18560/25000 (74%)]\tLoss: 1.854415 testing loss: tensor(1.7618)\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 1.883158 testing loss: tensor(1.7621)\n",
      "Train Epoch: 1 [19840/25000 (79%)]\tLoss: 1.928436 testing loss: tensor(1.7629)\n",
      "Train Epoch: 1 [20480/25000 (82%)]\tLoss: 1.911379 testing loss: tensor(1.7632)\n",
      "Train Epoch: 1 [21120/25000 (84%)]\tLoss: 1.925176 testing loss: tensor(1.7629)\n",
      "Train Epoch: 1 [21760/25000 (87%)]\tLoss: 1.854146 testing loss: tensor(1.7629)\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 1.955755 testing loss: tensor(1.7632)\n",
      "Train Epoch: 1 [23040/25000 (92%)]\tLoss: 1.915570 testing loss: tensor(1.7625)\n",
      "Train Epoch: 1 [23680/25000 (94%)]\tLoss: 1.756336 testing loss: tensor(1.7622)\n",
      "Train Epoch: 1 [24320/25000 (97%)]\tLoss: 1.885389 testing loss: tensor(1.7624)\n",
      "Train Epoch: 1 [7800/25000 (99%)]\tLoss: 1.850283 testing loss: tensor(1.7623)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.7623)\n",
      "CS 2 : 1.79112\n",
      "DP 2 : 1.762\n",
      "heuristic 2 : 1.76356\n",
      "DP: 1.7499999999999998\n",
      "tensor([0.0014, 0.4842, 0.5145])\n",
      "tensor([0.4202, 0.5798, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.1)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "    \n",
    "for order in stage:\n",
    "    producedata(order)\n",
    "    #running\n",
    "    for order1 in order1name:\n",
    "        print(\"Supervised Aim:\",order,order1)\n",
    "        \n",
    "        # for mapping binary to payments before softmax\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(n, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, n),\n",
    "        )\n",
    "        model.apply(init_weights)\n",
    "        # optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr1)\n",
    "        \n",
    "        runningLossNN = []\n",
    "        runningLossCS = []\n",
    "        runningLossDP = []\n",
    "        runningLossHeuristic = []\n",
    "        #model=torch.load(\"save/pytorchNN=5dp1\");\n",
    "        #model.eval()\n",
    "        ##order1name=[\"costsharing\",\"dp\",\"heuristic\",\"random initializing\"]\n",
    "        for epoch in range(1, supervisionEpochs + 1):\n",
    "#             print(\"distributionRatio\",distributionRatio)\n",
    "            if(order1==\"costsharing\"):\n",
    "                supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "            elif(order1==\"dp\"):\n",
    "                supervisionTrain(epoch, dpSupervisionRule)\n",
    "            elif(order1==\"heuristic\"):\n",
    "                supervisionTrain(epoch, heuristicSupervisionRule)\n",
    "            elif(order1==\"random initializing\"):\n",
    "                print(\"do nothing\");\n",
    "\n",
    "        test()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr2)\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(epoch)\n",
    "            test()\n",
    "        losslistname.append(order+\" \"+order1);\n",
    "        losslist.append(losslisttemp);\n",
    "        losslisttemp=[];\n",
    "        savepath=\"save/pytorchNN=5all-beta\"+order+str(order1)\n",
    "        torch.save(model, savepath);\n",
    "        print(\"end\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colorlist=[\"#D2691E\",'#4169E1',\"#9ACD32\",\"#B22222\",\"#FF00FF\",\"#708090\"]\n",
    "for i in range(len(losslist)):\n",
    "    plt.plot(losslist[i], '.-', color=colorlist[i], alpha=0.8, label=losslistname[i])\n",
    "\n",
    "# 显示标签，如果不加这句，即使加了label='一些数字'的参数，最终还是不会显示标签\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('Trianing Times')\n",
    "plt.ylabel('Delay')\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('pytorchNN=3_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
