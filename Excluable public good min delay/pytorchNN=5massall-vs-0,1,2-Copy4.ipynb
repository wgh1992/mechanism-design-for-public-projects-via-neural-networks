{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "from pylab import *\n",
    "from scipy.stats import beta\n",
    "import random \n",
    "n = 5\n",
    "epochs = 1\n",
    "supervisionEpochs = 1\n",
    "lr = 0.0005\n",
    "log_interval = 5\n",
    "trainSize = 40000#100000\n",
    "percentage_train_test= 0.75\n",
    "penaltyLambda = 10\n",
    "doublePeakHighMean = 0.85\n",
    "doublePeakLowMean = 0.15\n",
    "doublePeakStd = 0.1\n",
    "uniformlow=0\n",
    "uniformhigh=1.0\n",
    "normalloc = 0.2\n",
    "normalscale = 0.1\n",
    "\n",
    "cauchyloc = 1/n\n",
    "cauchyscalen = 0.004\n",
    "\n",
    "exponentialhigh = 15 #Symbol(\"b\", real=True)\n",
    "exponentiallow  = 15 #Symbol(\"a\", real=True)\n",
    "\n",
    "Sigmoidrate=0.00005\n",
    "Sigmoidbias=0.001\n",
    "\n",
    "\n",
    "\n",
    "independentnormalloc1=[(float(ii)+1)/(2*n+1) for ii in range(n,0,-1)]\n",
    "independentnormalscale1=[0.05 for ii in range(n)]\n",
    "\n",
    "independentnormalloc2=[(float(ii)+1)/(2*n+1) for ii in range(1,n+1,1)]\n",
    "independentnormalscale2=[0.05 for ii in range(n)]\n",
    "stage=[\"twopeak\"]\n",
    "# \"twopeak\",\"normal\",\"uniform\",\"independent1\",\"independent2\",\"cauchy\",\"U-exponential\"\n",
    "order1name=[\"random initializing\"]\n",
    "numberofpeople=['0','1','2']\n",
    "# \"costsharing\",\"dp\",\"heuristic\",\"random initializing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5000)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio1 = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase1 = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "d3 = D.normal.Normal(normalloc, normalscale)\n",
    "distributionRatio3 = d3.cdf(1) - d3.cdf(0)\n",
    "distributionBase3 = d3.cdf(0)\n",
    "\n",
    "d4 = D.uniform.Uniform(uniformlow,uniformhigh)\n",
    "distributionRatio4 = d4.cdf(1) - d4.cdf(0)\n",
    "distributionBase4 = d4.cdf(0)\n",
    "\n",
    "d5 = [D.normal.Normal(independentnormalloc1[ii], independentnormalscale1[ii]) for ii in range(n)]\n",
    "d6 = [D.normal.Normal(independentnormalloc2[ii], independentnormalscale2[ii]) for ii in range(n)]\n",
    "\n",
    "d7 = D.cauchy.Cauchy(cauchyloc,cauchyscalen)\n",
    "\n",
    "d81 = D.exponential.Exponential(exponentiallow)\n",
    "d82 = D.exponential.Exponential(exponentialhigh)\n",
    "\n",
    "#d8 = beta(betahigh,betalow)\n",
    "#d9 = D.beta.Beta(betahigh,betalow)\n",
    "def cdf(x,y, i=None):\n",
    "    if(y==\"twopeak\"):\n",
    "        return (d1.cdf(x) + d2.cdf(x) - distributionBase1) / 2 / distributionRatio1\n",
    "    elif(y==\"normal\"):\n",
    "        return (d3.cdf(x)-distributionBase3)/distributionRatio3;\n",
    "    elif(y==\"uniform\"):\n",
    "        return (d4.cdf(x)-distributionBase4)/distributionRatio4;\n",
    "    elif(y==\"independent1\"):\n",
    "        return d5[i].cdf(x);\n",
    "    elif(y==\"independent2\"):\n",
    "        return d6[i].cdf(x);\n",
    "    elif(y==\"cauchy\"):\n",
    "        return d7.cdf(x);\n",
    "#     elif(y==\"beta\"):\n",
    "#         return torch.tensor(d8.cdf(x));\n",
    "    elif(y==\"U-exponential\"):\n",
    "        return (d81.cdf(x) + (1.0 - d82.cdf(1.0-x)))  / 2 \n",
    "print(cdf(0.5,\"U-exponential\"))\n",
    "\n",
    "print(d81.cdf(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    if torch.sum(bits).item() == 0:\n",
    "        return torch.ones(n)\n",
    "    bits = bits.type(torch.float32)\n",
    "    negBits = torch.ones(n) - bits\n",
    "    payments = model(bits)\n",
    "    payments = payments - 1000 * negBits\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    payments = payments + negBits\n",
    "    return payments\n",
    "\n",
    "def tpToBits(tp, bits=torch.ones(n).type(torch.uint8)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = (tp >= payments).type(torch.uint8)\n",
    "    if torch.equal(newBits, bits):\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits(tp, newBits)#bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    return n - torch.sum(tpToBits(tp).type(torch.float32))\n",
    "\n",
    "def tpToBits0(tp, deep ,bits=torch.ones(n).type(torch.float32)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = torch.sigmoid((tp - payments + Sigmoidbias)/Sigmoidrate)\n",
    "    if torch.allclose(newBits, bits,rtol=0.01,atol=0.01) or deep>=n:\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits0(tp,deep+1 ,newBits)#bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "def tpToTotalDelay0(tp):\n",
    "    return n - torch.sum(tpToBits0(tp,0).type(torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    global  dp,decision\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money, yes]\n",
    "        offer = float(offerIndex) / dpPrecision\n",
    "        if tp[i] >= offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits[i] = 0\n",
    "            payments[i] = 1\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "#     print()\n",
    "#     print(tp)\n",
    "#     print(bits)\n",
    "#     print(payments)\n",
    "#     print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def heuristicSupervisionRule(tp):\n",
    "    global  dp_H,decision_H\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    tempold=-1;\n",
    "    for turns in range(n,0,-1):\n",
    "        money = dpPrecision\n",
    "        j=0\n",
    "        tempo=sum(bits)\n",
    "        #print(\"bits\",tempo)\n",
    "        for i in range(n):\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            \n",
    "            offerIndex = decision_H[tempo ,tempo - i, money]\n",
    "            offer = float(offerIndex) / dpPrecision\n",
    "            while(j<n):\n",
    "                if(bits[j]!=0):\n",
    "                    break;\n",
    "                j+=1;\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            if tp[j] >= offer:\n",
    "                #print(money,j,tp[j],offer)\n",
    "                money -= offerIndex\n",
    "                bits[j] = 1\n",
    "                payments[j] = offer\n",
    "            else:\n",
    "                bits[j] = 0;\n",
    "                payments[j] = 1.0;\n",
    "            j+=1;\n",
    "        #print(\"money\",money)\n",
    "        if(money==0 and tempold==tempo):\n",
    "            break;\n",
    "        tempold=tempo;\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    for k in range(n, -1, -1):\n",
    "        if k == 0:\n",
    "            break\n",
    "        bits = [1 if tp[ii] >= 1.0 / k else 0 for ii in range(n)]\n",
    "        if sum(bits) == k:\n",
    "            break\n",
    "    if k == 0:\n",
    "        payments = [1 for ii in range(n)]\n",
    "    else:\n",
    "        payments = [1.0 / k if bits[ii] == 1 else 1 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    return float(n - torch.sum(costSharingSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def dpDelay(tp):\n",
    "    return float(n - torch.sum(dpSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def heuristicDelay(tp):\n",
    "    return float(n - torch.sum(heuristicSupervisionRule(tp)[0]).item())\n",
    "\n",
    "\n",
    "# templire=0;\n",
    "# num=1\n",
    "# for i in range(num):\n",
    "#     temp=torch.tensor(samplesJoint[i])\n",
    "#     #print(temp)\n",
    "#     #print(temp);\n",
    "#     #print(heuristicSupervisionRule(temp))\n",
    "#     #print(dpSupervisionRule(temp))\n",
    "#     #print(dpDelay(temp))\n",
    "#     res=dpDelay(temp)\n",
    "#     templire+=res\n",
    "#     #print(\"delay\",res)\n",
    "#     #print()\n",
    "# print(templire/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordAndReport(name, source, loss):\n",
    "    source.append(loss)\n",
    "    realLength = len(source)\n",
    "    #print(f\"{name} ({realLength}): {loss}\")\n",
    "    print(name,realLength,\":\" ,loss)\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    \n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda /100000\n",
    "        \n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "losslist=[];\n",
    "losslistname=[];\n",
    "losslisttemp=[];\n",
    "def test_batch():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lenLoss= 0\n",
    "        nnLoss = 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "            for tp in tp_batch:\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        nnLoss/=lenLoss\n",
    "    return nnLoss\n",
    "\n",
    "\n",
    "def train0(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            delay0 = tpToTotalDelay0(tp)\n",
    "            loss = loss + delay0\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "\n",
    "def train1(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                delay1 = tpToTotalDelay(tp1)\n",
    "                delay0 = tpToTotalDelay(tp0)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                    loss = loss + (1 - cdf(offer,order)) * delay1 + cdf(offer,order) * delay0\n",
    "                else:\n",
    "                    loss = loss + (1 - cdf(offer,order,i)) * delay1 + cdf(offer,order,i) * delay0\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "    \n",
    "def train2(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                x1=random.randint(0,n-1);\n",
    "                x2=random.randint(0,n-1);\n",
    "                while(x2==x1):\n",
    "                    x2=random.randint(0,n-1);\n",
    "                    \n",
    "                tp11 = tp.clone()\n",
    "                tp11[x1] = 1\n",
    "                tp11[x2] = 1\n",
    "                tp10 = tp.clone()\n",
    "                tp10[x1] = 1\n",
    "                tp10[x2] = 0\n",
    "                tp01 = tp.clone()\n",
    "                tp01[x1] = 0\n",
    "                tp01[x2] = 1\n",
    "                tp00 = tp.clone()\n",
    "                tp00[x1] = 0\n",
    "                tp00[x2] = 0\n",
    "                \n",
    "                offer1 = tpToPayments(tp11)[x1]\n",
    "                offer2 = tpToPayments(tp11)[x2]\n",
    "                offer3 = tpToPayments(tp10)[x1]\n",
    "                offer4 = tpToPayments(tp01)[x2]\n",
    "                \n",
    "                #print(tp)\n",
    "                #print(offer1,offer2)\n",
    "                \n",
    "                delay11 = tpToTotalDelay(tp11)\n",
    "                delay01 = tpToTotalDelay(tp01)\n",
    "                delay10 = tpToTotalDelay(tp10)\n",
    "                delay00 = tpToTotalDelay(tp00)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "#                     print ((1.0-cdf(offer1,order)) * (1.0-cdf(offer2,order))+                                                   \n",
    "#                            (1.0-cdf(offer3,order)) * cdf(offer2,order)+\n",
    "#                            cdf(offer1,order) * (1.0-cdf(offer4,order))+\n",
    "#                            (cdf(offer3,order) * cdf(offer4,order) -\n",
    "#                                      (cdf(offer3,order)-cdf(offer1,order))*(cdf(offer4,order)-cdf(offer2,order)) )\n",
    "#                           )\n",
    "                    loss = loss +(      (1.0-cdf(offer1,order)) * (1.0-cdf(offer2,order)) * delay11 +\n",
    "                                  (1.0-cdf(offer3,order)) * cdf(offer2,order) * delay10 +\n",
    "                                  cdf(offer1,order) * (1.0-cdf(offer4,order)) * delay01 +\n",
    "                                    (cdf(offer3,order) * cdf(offer4,order) -\n",
    "                                     (cdf(offer3,order)-cdf(offer1,order))*(cdf(offer4,order)-cdf(offer2,order)) )* delay00 \n",
    "                                                       )\n",
    "                else:\n",
    "                    loss = loss +(      (1.0-cdf(offer1,order,x1)) * (1.0-cdf(offer2,order),x2) * delay11 +\n",
    "                                  (1.0-cdf(offer3,order,x1)) * cdf(offer2,order,x2) * delay10 +\n",
    "                                  cdf(offer1,order,x1) * (1.0-cdf(offer4,order,x2)) * delay01 +\n",
    "                                    (cdf(offer3,order,x1) * cdf(offer4,order,x2) -\n",
    "                                     (cdf(offer3,order,x1)-cdf(offer1,order,x1))*(cdf(offer4,order,x2)-cdf(offer2,order,x2)) )* delay00 \n",
    "                                                       )\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "    \n",
    "allBits = [torch.tensor(bits, dtype=torch.int16) for bits in itertools.product([0, 1], repeat=n)]\n",
    "\n",
    "def test():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        costSharingLoss = 0\n",
    "        dpLoss = 0\n",
    "        nnLoss = 0\n",
    "        heuristicLoss=0\n",
    "        lenLoss= 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "                heuristicLoss+= heuristicDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        costSharingLoss /= lenLoss\n",
    "        dpLoss /= lenLoss\n",
    "        nnLoss /= lenLoss\n",
    "        heuristicLoss /= lenLoss\n",
    "        #print(lenLoss)\n",
    "        losslisttemp.append(nnLoss);\n",
    "        recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "        recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "        recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "        recordAndReport(\"heuristic\", runningLossHeuristic, heuristicLoss)\n",
    "        print(\"DP:\",dp[n, dpPrecision,0])\n",
    "        #for i in range(n, 0, -1):\n",
    "        #    print(\"Heuristic:\",i,5*(1-dp_H[i, i, dpPrecision]))\n",
    "        for i in range(n, 0, -1):\n",
    "            print(\n",
    "                    tpToPayments(\n",
    "                            torch.tensor([0 if ii >= i else 1 for ii in range(n)], dtype=torch.float32)\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "dpPrecision = 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def producedata(order):\n",
    "    global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "    dpPrecision = 100 \n",
    "    if(order==\"twopeak\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = np.random.normal(\n",
    "            loc=doublePeakLowMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=doublePeakLowMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samples2 = np.random.normal(\n",
    "            loc=doublePeakHighMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = np.random.normal(\n",
    "                        loc=doublePeakHighMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "    elif(order==\"normal\"):\n",
    "        print(\"loc\",normalloc, \"scale\",normalscale)\n",
    "        samples1 = np.random.normal(\n",
    "            loc=normalloc, scale=normalscale, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=normalloc, scale=normalscale\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"uniform\"):  \n",
    "        print(\"uniformlow\",uniformlow, \"uniformhigh\",uniformhigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        uniformlow, uniformhigh\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent1\"):\n",
    "        print(\"loc\",independentnormalloc1,\"scale\",independentnormalscale1)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent2\"):\n",
    "        print(\"loc\",independentnormalloc2, \"scale\",independentnormalscale2)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"cauchy\"):\n",
    "        print(\"cauchyloc\",cauchyloc, \"cauchyscale\",cauchyscalen)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"beta\"):\n",
    "        print(\"betalow\",betalow, \"betahigh\",betahigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(betahigh,betalow,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(betahigh,betalow,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"U-exponential\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = d81.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d81.rsample(torch.Size([1])).numpy()\n",
    "                    \n",
    "        samples2 = d82.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        \n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = d82.rsample(torch.Size([1])).numpy()\n",
    "        samples2 = 1.0 - samples2\n",
    "        samplesJoint = signals * samples1 - (signals - 1.0) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        plt.hist(samplesJoint,bins=500)\n",
    "        plt.show()\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "        \n",
    "        \n",
    "    tp_dataset = TensorDataset(tp_tensor[: int(trainSize * percentage_train_test)])\n",
    "    tp_dataset_testing = TensorDataset(tp_tensor[int(trainSize * (1.0-percentage_train_test)) :])\n",
    "    tp_dataloader = DataLoader(tp_dataset, batch_size=128, shuffle=True)\n",
    "    tp_dataloader_testing = DataLoader(tp_dataset_testing, batch_size=256, shuffle=False)\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "    decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for yes in range(n + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp[0, 0, yes] = 0\n",
    "            else:\n",
    "                dp[0, money, yes] = yes# + 1.0\n",
    "    for ppl in range(1,  n + 1):\n",
    "        for yes in range(n + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                minSoFar = 1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1 - cdf(offer,order)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order) * (1 + dp[ppl - 1, money, yes])\n",
    "                    else:\n",
    "                        res = (1 - cdf(offer,order,n-ppl)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order,n-ppl) * (1 + dp[ppl - 1, money, yes])\n",
    "                    if minSoFar > res.item():\n",
    "                        minSoFar = res.item()\n",
    "                        decision[ppl, money, yes] = offerIndex\n",
    "                dp[ppl, money, yes] = minSoFar\n",
    "    \n",
    "    print(\"dp\",dp[n, dpPrecision, 0])\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp_H = np.zeros([n + 1 , n + 1, dpPrecision + 1])\n",
    "    decision_H = np.zeros([n + 1 , n + 1, dpPrecision + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for i in range(1,n+1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp_H[i, 0, 0] = 1\n",
    "            else:\n",
    "                offer = money / dpPrecision\n",
    "                dp_H[i, 0, money] = 0#cdf(offer)# + 1.0\n",
    "    for i in range(1,n+1):\n",
    "        for ppl in range(1, i + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                maxSoFar = -1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1-cdf(offer,order)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    else:\n",
    "                        res = (1-cdf(offer,order,ppl-1)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    if maxSoFar < res.item():\n",
    "                        maxSoFar = res.item()\n",
    "                        decision_H[i, ppl, money] = offerIndex\n",
    "                dp_H[i, ppl, money] = maxSoFar\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loc 0.15 scale 0.1\n",
      "loc 0.85 scale 0.1\n",
      "dp 1.7567987442016602\n",
      "Supervised Aim: twopeak random initializing\n",
      "do nothing\n",
      "NN 1 : tensor(2.3333)\n",
      "CS 1 : 2.3533\n",
      "DP 1 : 1.7748333333333333\n",
      "heuristic 1 : 1.6995333333333333\n",
      "DP: 1.7567987442016602\n",
      "tensor([0.2157, 0.1630, 0.1882, 0.2551, 0.1779])\n",
      "tensor([0.2591, 0.1935, 0.2449, 0.3025, 1.0000])\n",
      "tensor([0.3710, 0.2864, 0.3427, 1.0000, 1.0000])\n",
      "tensor([0.5779, 0.4221, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 2.391252 testing loss: tensor(2.3349)\n",
      "Train Epoch: 1 [640/30000 (2%)]\tLoss: 2.484373 testing loss: tensor(2.3273)\n",
      "Train Epoch: 1 [1280/30000 (4%)]\tLoss: 2.562499 testing loss: tensor(2.3173)\n",
      "Train Epoch: 1 [1920/30000 (6%)]\tLoss: 2.328125 testing loss: tensor(2.3073)\n",
      "Train Epoch: 1 [2560/30000 (9%)]\tLoss: 2.164062 testing loss: tensor(2.2998)\n",
      "Train Epoch: 1 [3200/30000 (11%)]\tLoss: 2.671875 testing loss: tensor(2.2905)\n",
      "Train Epoch: 1 [3840/30000 (13%)]\tLoss: 2.687500 testing loss: tensor(2.2825)\n",
      "Train Epoch: 1 [4480/30000 (15%)]\tLoss: 2.437464 testing loss: tensor(2.2770)\n",
      "Train Epoch: 1 [5120/30000 (17%)]\tLoss: 2.281250 testing loss: tensor(2.2702)\n",
      "Train Epoch: 1 [5760/30000 (19%)]\tLoss: 2.468750 testing loss: tensor(2.2663)\n",
      "Train Epoch: 1 [6400/30000 (21%)]\tLoss: 2.210938 testing loss: tensor(2.2596)\n",
      "Train Epoch: 1 [7040/30000 (23%)]\tLoss: 2.257810 testing loss: tensor(2.2548)\n",
      "Train Epoch: 1 [7680/30000 (26%)]\tLoss: 2.289027 testing loss: tensor(2.2459)\n",
      "Train Epoch: 1 [8320/30000 (28%)]\tLoss: 2.289071 testing loss: tensor(2.2284)\n",
      "Train Epoch: 1 [8960/30000 (30%)]\tLoss: 2.210938 testing loss: tensor(2.2086)\n",
      "Train Epoch: 1 [9600/30000 (32%)]\tLoss: 1.914062 testing loss: tensor(2.1927)\n",
      "Train Epoch: 1 [10240/30000 (34%)]\tLoss: 1.968750 testing loss: tensor(2.1825)\n",
      "Train Epoch: 1 [10880/30000 (36%)]\tLoss: 2.414062 testing loss: tensor(2.1616)\n",
      "Train Epoch: 1 [11520/30000 (38%)]\tLoss: 2.015625 testing loss: tensor(2.1368)\n",
      "Train Epoch: 1 [12160/30000 (40%)]\tLoss: 2.281250 testing loss: tensor(2.1206)\n",
      "Train Epoch: 1 [12800/30000 (43%)]\tLoss: 2.281249 testing loss: tensor(2.0899)\n",
      "Train Epoch: 1 [13440/30000 (45%)]\tLoss: 2.039062 testing loss: tensor(2.0677)\n",
      "Train Epoch: 1 [14080/30000 (47%)]\tLoss: 1.882870 testing loss: tensor(2.0460)\n",
      "Train Epoch: 1 [14720/30000 (49%)]\tLoss: 2.007812 testing loss: tensor(2.0301)\n",
      "Train Epoch: 1 [15360/30000 (51%)]\tLoss: 2.109375 testing loss: tensor(2.0108)\n",
      "Train Epoch: 1 [16000/30000 (53%)]\tLoss: 2.125000 testing loss: tensor(1.9932)\n",
      "Train Epoch: 1 [16640/30000 (55%)]\tLoss: 1.937498 testing loss: tensor(1.9618)\n",
      "Train Epoch: 1 [17280/30000 (57%)]\tLoss: 2.025411 testing loss: tensor(1.9235)\n",
      "Train Epoch: 1 [17920/30000 (60%)]\tLoss: 2.063035 testing loss: tensor(1.9376)\n",
      "Train Epoch: 1 [18560/30000 (62%)]\tLoss: 2.031492 testing loss: tensor(1.9530)\n",
      "Train Epoch: 1 [19200/30000 (64%)]\tLoss: 2.054672 testing loss: tensor(1.9694)\n",
      "Train Epoch: 1 [19840/30000 (66%)]\tLoss: 2.164098 testing loss: tensor(1.9755)\n",
      "Train Epoch: 1 [20480/30000 (68%)]\tLoss: 2.015625 testing loss: tensor(1.9785)\n",
      "Train Epoch: 1 [21120/30000 (70%)]\tLoss: 2.031257 testing loss: tensor(1.9777)\n",
      "Train Epoch: 1 [21760/30000 (72%)]\tLoss: 1.812541 testing loss: tensor(1.9783)\n",
      "Train Epoch: 1 [22400/30000 (74%)]\tLoss: 1.968750 testing loss: tensor(1.9784)\n",
      "Train Epoch: 1 [23040/30000 (77%)]\tLoss: 2.070312 testing loss: tensor(1.9785)\n",
      "Train Epoch: 1 [23680/30000 (79%)]\tLoss: 1.546875 testing loss: tensor(1.9843)\n",
      "Train Epoch: 1 [24320/30000 (81%)]\tLoss: 2.140625 testing loss: tensor(1.9881)\n",
      "Train Epoch: 1 [24960/30000 (83%)]\tLoss: 1.898432 testing loss: tensor(1.9904)\n",
      "Train Epoch: 1 [25600/30000 (85%)]\tLoss: 1.687500 testing loss: tensor(1.9965)\n",
      "Train Epoch: 1 [26240/30000 (87%)]\tLoss: 1.789062 testing loss: tensor(1.9980)\n",
      "Train Epoch: 1 [26880/30000 (89%)]\tLoss: 1.656235 testing loss: tensor(1.9983)\n",
      "Train Epoch: 1 [27520/30000 (91%)]\tLoss: 2.054688 testing loss: tensor(2.0021)\n",
      "Train Epoch: 1 [28160/30000 (94%)]\tLoss: 1.875909 testing loss: tensor(1.9883)\n",
      "Train Epoch: 1 [28800/30000 (96%)]\tLoss: 2.078772 testing loss: tensor(1.9821)\n",
      "Train Epoch: 1 [29440/30000 (98%)]\tLoss: 1.992188 testing loss: tensor(1.9671)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.9595)\n",
      "CS 2 : 2.3533\n",
      "DP 2 : 1.7748333333333333\n",
      "heuristic 2 : 1.6995333333333333\n",
      "DP: 1.7567987442016602\n",
      "tensor([0.0179, 0.0652, 0.2054, 0.6770, 0.0344])\n",
      "tensor([0.0287, 0.0747, 0.2110, 0.6856, 1.0000])\n",
      "tensor([0.1078, 0.2640, 0.6281, 1.0000, 1.0000])\n",
      "tensor([0.3120, 0.6880, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "do nothing\n",
      "NN 1 : tensor(2.3342)\n",
      "CS 1 : 2.3533\n",
      "DP 1 : 1.7748333333333333\n",
      "heuristic 1 : 1.6995333333333333\n",
      "DP: 1.7567987442016602\n",
      "tensor([0.1739, 0.2144, 0.1604, 0.2608, 0.1904])\n",
      "tensor([0.2128, 0.2852, 0.2088, 0.2932, 1.0000])\n",
      "tensor([0.2980, 0.4185, 0.2835, 1.0000, 1.0000])\n",
      "tensor([0.4662, 0.5338, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 2.391505 testing loss: tensor(2.3200)\n",
      "Train Epoch: 1 [640/30000 (2%)]\tLoss: 2.103212 testing loss: tensor(2.1962)\n",
      "Train Epoch: 1 [1280/30000 (4%)]\tLoss: 2.032414 testing loss: tensor(1.9839)\n",
      "Train Epoch: 1 [1920/30000 (6%)]\tLoss: 1.979486 testing loss: tensor(1.8387)\n",
      "Train Epoch: 1 [2560/30000 (9%)]\tLoss: 1.858771 testing loss: tensor(1.8268)\n",
      "Train Epoch: 1 [3200/30000 (11%)]\tLoss: 1.935897 testing loss: tensor(1.7676)\n",
      "Train Epoch: 1 [3840/30000 (13%)]\tLoss: 1.853759 testing loss: tensor(1.7620)\n",
      "Train Epoch: 1 [4480/30000 (15%)]\tLoss: 1.839180 testing loss: tensor(1.7313)\n",
      "Train Epoch: 1 [5120/30000 (17%)]\tLoss: 1.792864 testing loss: tensor(1.7134)\n",
      "Train Epoch: 1 [5760/30000 (19%)]\tLoss: 1.743396 testing loss: tensor(1.7002)\n",
      "Train Epoch: 1 [6400/30000 (21%)]\tLoss: 1.634742 testing loss: tensor(1.6896)\n",
      "Train Epoch: 1 [7040/30000 (23%)]\tLoss: 1.786972 testing loss: tensor(1.6828)\n",
      "Train Epoch: 1 [7680/30000 (26%)]\tLoss: 1.537832 testing loss: tensor(1.6740)\n",
      "Train Epoch: 1 [8320/30000 (28%)]\tLoss: 1.477922 testing loss: tensor(1.6650)\n",
      "Train Epoch: 1 [8960/30000 (30%)]\tLoss: 1.796190 testing loss: tensor(1.6582)\n",
      "Train Epoch: 1 [9600/30000 (32%)]\tLoss: 1.539501 testing loss: tensor(1.6486)\n",
      "Train Epoch: 1 [10240/30000 (34%)]\tLoss: 1.481792 testing loss: tensor(1.6422)\n",
      "Train Epoch: 1 [10880/30000 (36%)]\tLoss: 1.586129 testing loss: tensor(1.6374)\n",
      "Train Epoch: 1 [11520/30000 (38%)]\tLoss: 1.577600 testing loss: tensor(1.6381)\n",
      "Train Epoch: 1 [12160/30000 (40%)]\tLoss: 1.698594 testing loss: tensor(1.6360)\n",
      "Train Epoch: 1 [12800/30000 (43%)]\tLoss: 1.727974 testing loss: tensor(1.6374)\n",
      "Train Epoch: 1 [13440/30000 (45%)]\tLoss: 1.760943 testing loss: tensor(1.6355)\n",
      "Train Epoch: 1 [14080/30000 (47%)]\tLoss: 1.630233 testing loss: tensor(1.6372)\n",
      "Train Epoch: 1 [14720/30000 (49%)]\tLoss: 1.751045 testing loss: tensor(1.6349)\n",
      "Train Epoch: 1 [15360/30000 (51%)]\tLoss: 1.520339 testing loss: tensor(1.6387)\n",
      "Train Epoch: 1 [16000/30000 (53%)]\tLoss: 1.354683 testing loss: tensor(1.6368)\n",
      "Train Epoch: 1 [16640/30000 (55%)]\tLoss: 1.501968 testing loss: tensor(1.6344)\n",
      "Train Epoch: 1 [17280/30000 (57%)]\tLoss: 1.360047 testing loss: tensor(1.6354)\n",
      "Train Epoch: 1 [17920/30000 (60%)]\tLoss: 1.715881 testing loss: tensor(1.6365)\n",
      "Train Epoch: 1 [18560/30000 (62%)]\tLoss: 1.583133 testing loss: tensor(1.6335)\n",
      "Train Epoch: 1 [19200/30000 (64%)]\tLoss: 1.627248 testing loss: tensor(1.6360)\n",
      "Train Epoch: 1 [19840/30000 (66%)]\tLoss: 1.568739 testing loss: tensor(1.6359)\n",
      "Train Epoch: 1 [20480/30000 (68%)]\tLoss: 1.520337 testing loss: tensor(1.6347)\n",
      "Train Epoch: 1 [21120/30000 (70%)]\tLoss: 1.621404 testing loss: tensor(1.6317)\n",
      "Train Epoch: 1 [21760/30000 (72%)]\tLoss: 1.739967 testing loss: tensor(1.6331)\n",
      "Train Epoch: 1 [22400/30000 (74%)]\tLoss: 1.411427 testing loss: tensor(1.6361)\n",
      "Train Epoch: 1 [23040/30000 (77%)]\tLoss: 1.616537 testing loss: tensor(1.6339)\n",
      "Train Epoch: 1 [23680/30000 (79%)]\tLoss: 1.519272 testing loss: tensor(1.6318)\n",
      "Train Epoch: 1 [24320/30000 (81%)]\tLoss: 1.661624 testing loss: tensor(1.6332)\n",
      "Train Epoch: 1 [24960/30000 (83%)]\tLoss: 1.562243 testing loss: tensor(1.6318)\n",
      "Train Epoch: 1 [25600/30000 (85%)]\tLoss: 1.555452 testing loss: tensor(1.6337)\n",
      "Train Epoch: 1 [26240/30000 (87%)]\tLoss: 1.621619 testing loss: tensor(1.6332)\n",
      "Train Epoch: 1 [26880/30000 (89%)]\tLoss: 1.721100 testing loss: tensor(1.6337)\n",
      "Train Epoch: 1 [27520/30000 (91%)]\tLoss: 1.715053 testing loss: tensor(1.6332)\n",
      "Train Epoch: 1 [28160/30000 (94%)]\tLoss: 1.699217 testing loss: tensor(1.6315)\n",
      "Train Epoch: 1 [28800/30000 (96%)]\tLoss: 1.700825 testing loss: tensor(1.6356)\n",
      "Train Epoch: 1 [29440/30000 (98%)]\tLoss: 1.479552 testing loss: tensor(1.6336)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.6342)\n",
      "CS 2 : 2.3533\n",
      "DP 2 : 1.7748333333333333\n",
      "heuristic 2 : 1.6995333333333333\n",
      "DP: 1.7567987442016602\n",
      "tensor([0.0808, 0.0819, 0.0838, 0.6724, 0.0811])\n",
      "tensor([0.0937, 0.1100, 0.1078, 0.6885, 1.0000])\n",
      "tensor([0.1419, 0.7168, 0.1414, 1.0000, 1.0000])\n",
      "tensor([0.2733, 0.7267, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "do nothing\n",
      "NN 1 : tensor(2.2984)\n",
      "CS 1 : 2.3533\n",
      "DP 1 : 1.7748333333333333\n",
      "heuristic 1 : 1.6995333333333333\n",
      "DP: 1.7567987442016602\n",
      "tensor([0.2784, 0.2134, 0.1549, 0.2302, 0.1231])\n",
      "tensor([0.3161, 0.2479, 0.1852, 0.2508, 1.0000])\n",
      "tensor([0.4349, 0.2980, 0.2671, 1.0000, 1.0000])\n",
      "tensor([0.5729, 0.4271, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 2.243505 testing loss: tensor(2.2831)\n",
      "Train Epoch: 1 [640/30000 (2%)]\tLoss: 2.273860 testing loss: tensor(2.1490)\n",
      "Train Epoch: 1 [1280/30000 (4%)]\tLoss: 2.085882 testing loss: tensor(1.9478)\n",
      "Train Epoch: 1 [1920/30000 (6%)]\tLoss: 1.788975 testing loss: tensor(1.8881)\n",
      "Train Epoch: 1 [2560/30000 (9%)]\tLoss: 1.811135 testing loss: tensor(1.8811)\n",
      "Train Epoch: 1 [3200/30000 (11%)]\tLoss: 1.862058 testing loss: tensor(1.8280)\n",
      "Train Epoch: 1 [3840/30000 (13%)]\tLoss: 1.782237 testing loss: tensor(1.8143)\n",
      "Train Epoch: 1 [4480/30000 (15%)]\tLoss: 1.712557 testing loss: tensor(1.7781)\n",
      "Train Epoch: 1 [5120/30000 (17%)]\tLoss: 1.810733 testing loss: tensor(1.7663)\n",
      "Train Epoch: 1 [5760/30000 (19%)]\tLoss: 1.690045 testing loss: tensor(1.7328)\n",
      "Train Epoch: 1 [6400/30000 (21%)]\tLoss: 1.643443 testing loss: tensor(1.7075)\n",
      "Train Epoch: 1 [7040/30000 (23%)]\tLoss: 1.692210 testing loss: tensor(1.6937)\n",
      "Train Epoch: 1 [7680/30000 (26%)]\tLoss: 1.862851 testing loss: tensor(1.6763)\n",
      "Train Epoch: 1 [8320/30000 (28%)]\tLoss: 1.726295 testing loss: tensor(1.6654)\n",
      "Train Epoch: 1 [8960/30000 (30%)]\tLoss: 1.567549 testing loss: tensor(1.6641)\n",
      "Train Epoch: 1 [9600/30000 (32%)]\tLoss: 1.784161 testing loss: tensor(1.6581)\n",
      "Train Epoch: 1 [10240/30000 (34%)]\tLoss: 1.621156 testing loss: tensor(1.6528)\n",
      "Train Epoch: 1 [10880/30000 (36%)]\tLoss: 1.611992 testing loss: tensor(1.6498)\n",
      "Train Epoch: 1 [11520/30000 (38%)]\tLoss: 1.598787 testing loss: tensor(1.6480)\n",
      "Train Epoch: 1 [12160/30000 (40%)]\tLoss: 1.585139 testing loss: tensor(1.6446)\n",
      "Train Epoch: 1 [12800/30000 (43%)]\tLoss: 1.619790 testing loss: tensor(1.6424)\n",
      "Train Epoch: 1 [13440/30000 (45%)]\tLoss: 1.669187 testing loss: tensor(1.6396)\n",
      "Train Epoch: 1 [14080/30000 (47%)]\tLoss: 1.611536 testing loss: tensor(1.6393)\n",
      "Train Epoch: 1 [14720/30000 (49%)]\tLoss: 1.696126 testing loss: tensor(1.6383)\n",
      "Train Epoch: 1 [15360/30000 (51%)]\tLoss: 1.625412 testing loss: tensor(1.6387)\n",
      "Train Epoch: 1 [16000/30000 (53%)]\tLoss: 1.670416 testing loss: tensor(1.6362)\n",
      "Train Epoch: 1 [16640/30000 (55%)]\tLoss: 1.645468 testing loss: tensor(1.6376)\n",
      "Train Epoch: 1 [17280/30000 (57%)]\tLoss: 1.671646 testing loss: tensor(1.6369)\n",
      "Train Epoch: 1 [17920/30000 (60%)]\tLoss: 1.587349 testing loss: tensor(1.6367)\n",
      "Train Epoch: 1 [18560/30000 (62%)]\tLoss: 1.763523 testing loss: tensor(1.6398)\n",
      "Train Epoch: 1 [19200/30000 (64%)]\tLoss: 1.590577 testing loss: tensor(1.6389)\n",
      "Train Epoch: 1 [19840/30000 (66%)]\tLoss: 1.604996 testing loss: tensor(1.6380)\n",
      "Train Epoch: 1 [20480/30000 (68%)]\tLoss: 1.539310 testing loss: tensor(1.6376)\n",
      "Train Epoch: 1 [21120/30000 (70%)]\tLoss: 1.506483 testing loss: tensor(1.6369)\n",
      "Train Epoch: 1 [21760/30000 (72%)]\tLoss: 1.601330 testing loss: tensor(1.6378)\n",
      "Train Epoch: 1 [22400/30000 (74%)]\tLoss: 1.706445 testing loss: tensor(1.6347)\n",
      "Train Epoch: 1 [23040/30000 (77%)]\tLoss: 1.505646 testing loss: tensor(1.6380)\n",
      "Train Epoch: 1 [23680/30000 (79%)]\tLoss: 1.623184 testing loss: tensor(1.6358)\n",
      "Train Epoch: 1 [24320/30000 (81%)]\tLoss: 1.779095 testing loss: tensor(1.6373)\n",
      "Train Epoch: 1 [24960/30000 (83%)]\tLoss: 1.471761 testing loss: tensor(1.6367)\n",
      "Train Epoch: 1 [25600/30000 (85%)]\tLoss: 1.760195 testing loss: tensor(1.6360)\n",
      "Train Epoch: 1 [26240/30000 (87%)]\tLoss: 1.521065 testing loss: tensor(1.6365)\n",
      "Train Epoch: 1 [26880/30000 (89%)]\tLoss: 1.589141 testing loss: tensor(1.6380)\n",
      "Train Epoch: 1 [27520/30000 (91%)]\tLoss: 1.674133 testing loss: tensor(1.6374)\n",
      "Train Epoch: 1 [28160/30000 (94%)]\tLoss: 1.547223 testing loss: tensor(1.6404)\n",
      "Train Epoch: 1 [28800/30000 (96%)]\tLoss: 1.623669 testing loss: tensor(1.6363)\n",
      "Train Epoch: 1 [29440/30000 (98%)]\tLoss: 1.592254 testing loss: tensor(1.6360)\n",
      "penalty: 0.002882540225982666\n",
      "NN 2 : tensor(1.6367)\n",
      "CS 2 : 2.3533\n",
      "DP 2 : 1.7748333333333333\n",
      "heuristic 2 : 1.6995333333333333\n",
      "DP: 1.7567987442016602\n",
      "tensor([0.6786, 0.0811, 0.0806, 0.0774, 0.0823])\n",
      "tensor([0.6941, 0.1108, 0.0985, 0.0967, 1.0000])\n",
      "tensor([0.7148, 0.1407, 0.1445, 1.0000, 1.0000])\n",
      "tensor([0.7434, 0.2566, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "    \n",
    "for order in stage:\n",
    "    producedata(order)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "    #running\n",
    "    for order1 in order1name:\n",
    "        print(\"Supervised Aim:\",order,order1)\n",
    "        for trainingnumber in numberofpeople:\n",
    "            # for mapping binary to payments before softmax\n",
    "            model = nn.Sequential(\n",
    "                nn.Linear(n, 100),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(100, 100),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(100, 100),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(100, n),\n",
    "            )\n",
    "            model.apply(init_weights)\n",
    "            # optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "            runningLossNN = []\n",
    "            runningLossCS = []\n",
    "            runningLossDP = []\n",
    "            runningLossHeuristic = []\n",
    "            #model=torch.load(\"save/pytorchNN=5dp1\");\n",
    "            #model.eval()\n",
    "            ##order1name=[\"costsharing\",\"dp\",\"heuristic\",\"random initializing\"]\n",
    "            for epoch in range(1, supervisionEpochs + 1):\n",
    "        #             print(\"distributionRatio\",distributionRatio)\n",
    "                if(order1==\"costsharing\"):\n",
    "                    supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "                elif(order1==\"dp\"):\n",
    "                    supervisionTrain(epoch, dpSupervisionRule)\n",
    "                elif(order1==\"heuristic\"):\n",
    "                    supervisionTrain(epoch, heuristicSupervisionRule)\n",
    "                elif(order1==\"random initializing\"):\n",
    "                    print(\"do nothing\");\n",
    "\n",
    "            test()\n",
    "\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                if(trainingnumber=='0'):\n",
    "                    train0(epoch)\n",
    "                if(trainingnumber=='1'):\n",
    "                    train1(epoch)\n",
    "                if(trainingnumber=='2'):\n",
    "                    train2(epoch)\n",
    "                test()\n",
    "            losslistname.append(order+\" \"+order1+\" choose people:\"+trainingnumber);\n",
    "            losslist.append(losslisttemp);\n",
    "            losslisttemp=[];\n",
    "            savepath=\"save/pytorchNN=5all-phoenix\"+order+str(order1)+trainingnumber\n",
    "            torch.save(model, savepath);\n",
    "            print(\"end\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colorlist=[\"#D2691E\",'#4169E1',\"#9ACD32\",\"#B22222\",\"#FF00FF\",\"#708090\"]\n",
    "for i in range(len(losslist)):\n",
    "    plt.plot(losslist[i], '.-', color=colorlist[i], alpha=0.8, label=losslistname[i])\n",
    "\n",
    "# label=''\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('trianing times')\n",
    "plt.ylabel('delay')\n",
    "plt.title(\"n=\"+str(n))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
