{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40058530825361593\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from random import randint\n",
    "import random\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "from pylab import *\n",
    "from scipy.stats import beta\n",
    "import math\n",
    "n = 10\n",
    "epochs = 1\n",
    "supervisionEpochs = 1\n",
    "lr1 = 0.01\n",
    "lr2 = 0.0005\n",
    "log_interval = 5\n",
    "trainSize = 50000#100000\n",
    "percentage_train_test= 0.6\n",
    "penaltyLambda = 100\n",
    "doublePeakHighMean = 0.85\n",
    "doublePeakLowMean = 0.15\n",
    "doublePeakStd = 0.1\n",
    "uniformlow=0\n",
    "uniformhigh=1.0\n",
    "normalloc = 0.2\n",
    "normalscale = 0.1\n",
    "\n",
    "cauchyloc = 1/n\n",
    "cauchyscalen = 0.004\n",
    "\n",
    "exponentialhigh = 15 #Symbol(\"b\", real=True)\n",
    "exponentiallow  = 15 #Symbol(\"a\", real=True)\n",
    "\n",
    "\n",
    "beta_a = 0.3\n",
    "beta_b  = 0.2\n",
    "kumaraswamy_a = beta_a \n",
    "kumaraswamy_b = (1.0+(beta_a-1.0)*math.pow( (beta_a+beta_b-2.0)/(beta_a-1.0), beta_a) )/beta_a \n",
    "print(kumaraswamy_b)\n",
    "\n",
    "\n",
    "independentnormalloc1=[(float(ii)+1)/(2*n+1) for ii in range(n,0,-1)]\n",
    "independentnormalscale1=[0.05 for ii in range(n)]\n",
    "\n",
    "independentnormalloc2=[(float(ii)+1)/(2*n+1) for ii in range(1,n+1,1)]\n",
    "independentnormalscale2=[0.05 for ii in range(n)]\n",
    "stage=[\"twopeak\"]\n",
    "# \"twopeak\",\"normal\",\"uniform\",\"independent1\",\"independent2\",\"cauchy\",\"beta\",\"U-exponential\",\"arcsine\"\n",
    "order1name=[\"dp\",\"random initializing\",\"costsharing\",\"heuristic\"]\n",
    "# \"costsharing\",\"dp\",\"heuristic\",\"random initializing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio1 = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase1 = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "d3 = D.normal.Normal(normalloc, normalscale)\n",
    "distributionRatio3 = d3.cdf(1) - d3.cdf(0)\n",
    "distributionBase3 = d3.cdf(0)\n",
    "\n",
    "d4 = D.uniform.Uniform(uniformlow,uniformhigh)\n",
    "distributionRatio4 = d4.cdf(1) - d4.cdf(0)\n",
    "distributionBase4 = d4.cdf(0)\n",
    "\n",
    "d5 = [D.normal.Normal(independentnormalloc1[ii], independentnormalscale1[ii]) for ii in range(n)]\n",
    "d6 = [D.normal.Normal(independentnormalloc2[ii], independentnormalscale2[ii]) for ii in range(n)]\n",
    "\n",
    "d7 = D.cauchy.Cauchy(cauchyloc,cauchyscalen)\n",
    "\n",
    "d81 = D.exponential.Exponential(exponentiallow)\n",
    "d82 = D.exponential.Exponential(exponentialhigh)\n",
    "\n",
    "d9 = D.beta.Beta(beta_a,beta_b)\n",
    "#sample_d9=d9.rsample(torch.Size([100000]))\n",
    "\n",
    "#d9_sample=np.linspace(0.0001, 0.9999, 10000)\n",
    "#d9_pdf=torch.exp(d9.log_prob(torch.tensor(d9_sample,dtype=torch.float32)))\n",
    "#d9_delta=d9_sample[1]-d9_sample[0]\n",
    "#d9_sum_pdf=torch.sum(d9_pdf*d9_delta)\n",
    "\n",
    "d10 = D.beta.Beta(0.5,0.5)\n",
    "\n",
    "def cdf(x,y, i=None):\n",
    "    if(y==\"twopeak\"):\n",
    "        return (d1.cdf(x) + d2.cdf(x) - distributionBase1) / 2 / distributionRatio1\n",
    "    elif(y==\"normal\"):\n",
    "        return (d3.cdf(x)-distributionBase3)/distributionRatio3;\n",
    "    elif(y==\"uniform\"):\n",
    "        return (d4.cdf(x)-distributionBase4)/distributionRatio4;\n",
    "    elif(y==\"independent1\"):\n",
    "        return d5[i].cdf(x);\n",
    "    elif(y==\"independent2\"):\n",
    "        return d6[i].cdf(x);\n",
    "    elif(y==\"cauchy\"):\n",
    "        return d7.cdf(x);\n",
    "    elif(y==\"beta\"):\n",
    "#         sum_cdf=0.0;\n",
    "#         if(x<0.0001):\n",
    "#             x=0.00011;\n",
    "#         if(x>0.9999):\n",
    "#             x=0.99989;\n",
    "#         for i in range(len(d9_pdf)):\n",
    "#             if(d9_sample[i]<x):\n",
    "#                 sum_cdf+=d9_pdf[i]*d9_delta;\n",
    "#             else:\n",
    "#                 sum_cdf+=(d9_pdf[i]+d9_pdf[i-1])/ 2 *(x-d9_sample[i-1])\n",
    "#                 break;\n",
    "#         return sum_cdf/d9_sum_pdf\n",
    "#         cdf_v=torch.sum((sample_d9<(x)), dtype=torch.float32)/100000\n",
    "#         return cdf_v\n",
    "#    F(x|a,b)=1–(1–x^a)^b\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(x,kumaraswamy_a),kumaraswamy_b);\n",
    "        except:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(torch.tensor(x,dtype=torch.float32),kumaraswamy_a),kumaraswamy_b);\n",
    "    elif(y==\"arcsine\"):\n",
    "        #\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            res=2.0/math.pi * torch.asin(torch.sqrt(x))\n",
    "            #print(x)\n",
    "            return res# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "        except:\n",
    "            return 2.0/math.pi * torch.asin(torch.sqrt(torch.tensor(x,dtype=torch.float32)))# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "    elif(y==\"U-exponential\"):\n",
    "        return (d81.cdf(x) + (1.0 - d82.cdf(1.0-x)))  / 2 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    if torch.sum(bits).item() == 0:\n",
    "        return torch.ones(n)\n",
    "    bits = bits.type(torch.float32)\n",
    "    negBits = torch.ones(n) - bits\n",
    "    payments = model(bits)\n",
    "    payments = payments - 1000 * negBits\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    payments = payments + negBits\n",
    "    return payments\n",
    "\n",
    "def tpToBits(tp, bits=torch.ones(n).type(torch.uint8)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = (tp >= payments).type(torch.uint8)\n",
    "    if torch.equal(newBits, bits):\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits(tp, newBits)#bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    return n - torch.sum(tpToBits(tp).type(torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    global  dp,decision\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money, yes]\n",
    "        offer = float(offerIndex) / dpPrecision\n",
    "        if tp[i] >= offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits[i] = 0\n",
    "            payments[i] = 1\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "#     print()\n",
    "#     print(tp)\n",
    "#     print(bits)\n",
    "#     print(payments)\n",
    "#     print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def heuristicSupervisionRule(tp):\n",
    "    global  dp_H,decision_H\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    tempold=-1;\n",
    "    for turns in range(n,0,-1):\n",
    "        money = dpPrecision\n",
    "        j=0\n",
    "        tempo=sum(bits)\n",
    "        #print(\"bits\",tempo)\n",
    "        for i in range(n):\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            \n",
    "            offerIndex = decision_H[tempo ,tempo - i, money]\n",
    "            offer = float(offerIndex) / dpPrecision\n",
    "            while(j<n):\n",
    "                if(bits[j]!=0):\n",
    "                    break;\n",
    "                j+=1;\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            if tp[j] >= offer:\n",
    "                #print(money,j,tp[j],offer)\n",
    "                money -= offerIndex\n",
    "                bits[j] = 1\n",
    "                payments[j] = offer\n",
    "            else:\n",
    "                bits[j] = 0;\n",
    "                payments[j] = 1.0;\n",
    "            j+=1;\n",
    "        #print(\"money\",money)\n",
    "        if(money==0 and tempold==tempo):\n",
    "            break;\n",
    "        tempold=tempo;\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    for k in range(n, -1, -1):\n",
    "        if k == 0:\n",
    "            break\n",
    "        bits = [1 if tp[ii] >= 1.0 / k else 0 for ii in range(n)]\n",
    "        if sum(bits) == k:\n",
    "            break\n",
    "    if k == 0:\n",
    "        payments = [1 for ii in range(n)]\n",
    "    else:\n",
    "        payments = [1.0 / k if bits[ii] == 1 else 1 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    return float(n - torch.sum(costSharingSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def dpDelay(tp):\n",
    "    return float(n - torch.sum(dpSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def heuristicDelay(tp):\n",
    "    return float(n - torch.sum(heuristicSupervisionRule(tp)[0]).item())\n",
    "\n",
    "\n",
    "# templire=0;\n",
    "# num=1\n",
    "# for i in range(num):\n",
    "#     temp=torch.tensor(samplesJoint[i])\n",
    "#     #print(temp)\n",
    "#     #print(temp);\n",
    "#     #print(heuristicSupervisionRule(temp))\n",
    "#     #print(dpSupervisionRule(temp))\n",
    "#     #print(dpDelay(temp))\n",
    "#     res=dpDelay(temp)\n",
    "#     templire+=res\n",
    "#     #print(\"Delay\",res)\n",
    "#     #print()\n",
    "# print(templire/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordAndReport(name, source, loss):\n",
    "    source.append(loss)\n",
    "    realLength = len(source)\n",
    "    #print(f\"{name} ({realLength}): {loss}\")\n",
    "    print(name,realLength,\":\" ,loss)\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    \n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda /100000\n",
    "        \n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % (log_interval*5) == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "losslist=[];\n",
    "losslistname=[];\n",
    "losslisttemp=[];\n",
    "def test_batch():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lenLoss= 0\n",
    "        nnLoss = 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "            for tp in tp_batch:\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        nnLoss/=lenLoss\n",
    "    return nnLoss\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                Delay1 = tpToTotalDelay(tp1)\n",
    "                Delay0 = tpToTotalDelay(tp0)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                    loss = loss + (1 - cdf(offer,order)) * Delay1 + cdf(offer,order) * Delay0\n",
    "                else:\n",
    "                    loss = loss + (1 - cdf(offer,order,i)) * Delay1 + cdf(offer,order,i) * Delay0\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "    \n",
    "allBits = [torch.tensor(bits, dtype=torch.int16) for bits in itertools.product([0, 1], repeat=n)]\n",
    "\n",
    "def test():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        costSharingLoss = 0\n",
    "        dpLoss = 0\n",
    "        nnLoss = 0\n",
    "        heuristicLoss=0\n",
    "        lenLoss= 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "                heuristicLoss+= heuristicDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        costSharingLoss /= lenLoss\n",
    "        dpLoss /= lenLoss\n",
    "        nnLoss /= lenLoss\n",
    "        heuristicLoss /= lenLoss\n",
    "        #print(lenLoss)\n",
    "        losslisttemp.append(nnLoss);\n",
    "        recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "        recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "        recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "        recordAndReport(\"heuristic\", runningLossHeuristic, heuristicLoss)\n",
    "        print(\"DP:\",dp[n, dpPrecision,0])\n",
    "        #for i in range(n, 0, -1):\n",
    "        #    print(\"Heuristic:\",i,5*(1-dp_H[i, i, dpPrecision]))\n",
    "        for i in range(n, 0, -1):\n",
    "            print(\n",
    "                    tpToPayments(\n",
    "                            torch.tensor([0 if ii >= i else 1 for ii in range(n)], dtype=torch.float32)\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "dpPrecision = 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def producedata(order):\n",
    "    global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "    dpPrecision = 100 \n",
    "    if(order==\"twopeak\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = np.random.normal(\n",
    "            loc=doublePeakLowMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=doublePeakLowMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samples2 = np.random.normal(\n",
    "            loc=doublePeakHighMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = np.random.normal(\n",
    "                        loc=doublePeakHighMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "    elif(order==\"normal\"):\n",
    "        print(\"loc\",normalloc, \"scale\",normalscale)\n",
    "        samples1 = np.random.normal(\n",
    "            loc=normalloc, scale=normalscale, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=normalloc, scale=normalscale\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"uniform\"):  \n",
    "        print(\"uniformlow\",uniformlow, \"uniformhigh\",uniformhigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        uniformlow, uniformhigh\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent1\"):\n",
    "        print(\"loc\",independentnormalloc1,\"scale\",independentnormalscale1)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent2\"):\n",
    "        print(\"loc\",independentnormalloc2, \"scale\",independentnormalscale2)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"cauchy\"):\n",
    "        print(\"cauchyloc\",cauchyloc, \"cauchyscale\",cauchyscalen)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"beta\"):\n",
    "        print(\"beta_a\",beta_a, \"beta_b\",beta_b)\n",
    "        print(\"kumaraswamy_a\",kumaraswamy_a, \"kumaraswamy_b\",kumaraswamy_b)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"arcsine\"):\n",
    "        print(\"betalow\",0.5, \"betahigh\",0.5)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"U-exponential\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = d81.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d81.rsample(torch.Size([1])).numpy()\n",
    "                    \n",
    "        samples2 = d82.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        \n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = d82.rsample(torch.Size([1])).numpy()\n",
    "        samples2 = 1.0 - samples2\n",
    "        samplesJoint = signals * samples1 - (signals - 1.0) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "        \n",
    "    plt.hist(samplesJoint,bins=500)\n",
    "    plt.show()\n",
    "    \n",
    "    tp_dataset = TensorDataset(tp_tensor[: int(trainSize * percentage_train_test)])\n",
    "    tp_dataset_testing = TensorDataset(tp_tensor[int(trainSize * (1.0-percentage_train_test)) :])\n",
    "    tp_dataloader = DataLoader(tp_dataset, batch_size=128, shuffle=True)\n",
    "    tp_dataloader_testing = DataLoader(tp_dataset_testing, batch_size=256, shuffle=False)\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "    decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for yes in range(n + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp[0, 0, yes] = 0\n",
    "            else:\n",
    "                dp[0, money, yes] = yes# + 1.0\n",
    "    for ppl in range(1,  n + 1):\n",
    "        for yes in range(n + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                minSoFar = 1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1 - cdf(offer,order)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order) * (1 + dp[ppl - 1, money, yes])\n",
    "                    else:\n",
    "                        res = (1 - cdf(offer,order,n-ppl)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order,n-ppl) * (1 + dp[ppl - 1, money, yes])\n",
    "                    if minSoFar > res.item():\n",
    "                        minSoFar = res.item()\n",
    "                        decision[ppl, money, yes] = offerIndex\n",
    "                dp[ppl, money, yes] = minSoFar\n",
    "    \n",
    "    print(\"dp\",dp[n, dpPrecision, 0])\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp_H = np.zeros([n + 1 , n + 1, dpPrecision + 1])\n",
    "    decision_H = np.zeros([n + 1 , n + 1, dpPrecision + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for i in range(1,n+1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp_H[i, 0, 0] = 1\n",
    "            else:\n",
    "                offer = money / dpPrecision\n",
    "                dp_H[i, 0, money] = 0#cdf(offer)# + 1.0\n",
    "    for i in range(1,n+1):\n",
    "        for ppl in range(1, i + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                maxSoFar = -1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1-cdf(offer,order)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    else:\n",
    "                        res = (1-cdf(offer,order,ppl-1)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    if maxSoFar < res.item():\n",
    "                        maxSoFar = res.item()\n",
    "                        decision_H[i, ppl, money] = offerIndex\n",
    "                dp_H[i, ppl, money] = maxSoFar\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loc 0.15 scale 0.1\n",
      "loc 0.85 scale 0.1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dp 1.4498810768127441\n",
      "Supervised Aim: twopeak dp\n",
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 0.027466\n",
      "Train Epoch: 1 [3200/30000 (11%)]\tLoss: 0.001233\n",
      "Train Epoch: 1 [6400/30000 (21%)]\tLoss: 0.000176\n",
      "Train Epoch: 1 [9600/30000 (32%)]\tLoss: 0.000093\n",
      "Train Epoch: 1 [12800/30000 (43%)]\tLoss: 0.000045\n",
      "Train Epoch: 1 [16000/30000 (53%)]\tLoss: 0.000060\n",
      "Train Epoch: 1 [19200/30000 (64%)]\tLoss: 0.000050\n",
      "Train Epoch: 1 [22400/30000 (74%)]\tLoss: 0.000038\n",
      "Train Epoch: 1 [25600/30000 (85%)]\tLoss: 0.000069\n",
      "Train Epoch: 1 [28800/30000 (96%)]\tLoss: 0.000031\n",
      "NN 1 : tensor(1.4413)\n",
      "CS 1 : 1.7277\n",
      "DP 1 : 1.453\n",
      "heuristic 1 : 1.7975\n",
      "DP: 1.4498810768127441\n",
      "tensor([0.6613, 0.0477, 0.0487, 0.0496, 0.0477, 0.0391, 0.0396, 0.0402, 0.0228,\n",
      "        0.0031])\n",
      "tensor([0.6601, 0.0495, 0.0495, 0.0500, 0.0471, 0.0398, 0.0407, 0.0403, 0.0229,\n",
      "        1.0000])\n",
      "tensor([0.6612, 0.0521, 0.0555, 0.0551, 0.0499, 0.0415, 0.0423, 0.0424, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.6671, 0.0574, 0.0611, 0.0614, 0.0587, 0.0475, 0.0468, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.6775, 0.0632, 0.0677, 0.0672, 0.0685, 0.0558, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.7001, 0.0727, 0.0723, 0.0753, 0.0796, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.7358, 0.0922, 0.0840, 0.0881, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.7917, 0.1105, 0.0979, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.8702, 0.1298, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 1.677826 testing loss: tensor(1.4402)\n",
      "Train Epoch: 1 [640/30000 (2%)]\tLoss: 1.303145 testing loss: tensor(1.4455)\n",
      "Train Epoch: 1 [1280/30000 (4%)]\tLoss: 1.168628 testing loss: tensor(1.4427)\n",
      "Train Epoch: 1 [1920/30000 (6%)]\tLoss: 1.604347 testing loss: tensor(1.4397)\n",
      "Train Epoch: 1 [2560/30000 (9%)]\tLoss: 1.234125 testing loss: tensor(1.4402)\n",
      "Train Epoch: 1 [3200/30000 (11%)]\tLoss: 1.768551 testing loss: tensor(1.4390)\n",
      "Train Epoch: 1 [3840/30000 (13%)]\tLoss: 1.290231 testing loss: tensor(1.4375)\n",
      "Train Epoch: 1 [4480/30000 (15%)]\tLoss: 1.675989 testing loss: tensor(1.4363)\n",
      "Train Epoch: 1 [5120/30000 (17%)]\tLoss: 1.312715 testing loss: tensor(1.4369)\n",
      "Train Epoch: 1 [5760/30000 (19%)]\tLoss: 1.593920 testing loss: tensor(1.4361)\n",
      "Train Epoch: 1 [6400/30000 (21%)]\tLoss: 1.494828 testing loss: tensor(1.4344)\n",
      "Train Epoch: 1 [7040/30000 (23%)]\tLoss: 1.467323 testing loss: tensor(1.4345)\n",
      "Train Epoch: 1 [7680/30000 (26%)]\tLoss: 1.204054 testing loss: tensor(1.4340)\n",
      "Train Epoch: 1 [8320/30000 (28%)]\tLoss: 1.286676 testing loss: tensor(1.4343)\n",
      "Train Epoch: 1 [8960/30000 (30%)]\tLoss: 1.208268 testing loss: tensor(1.4328)\n",
      "Train Epoch: 1 [9600/30000 (32%)]\tLoss: 1.393074 testing loss: tensor(1.4327)\n",
      "Train Epoch: 1 [10240/30000 (34%)]\tLoss: 1.685777 testing loss: tensor(1.4340)\n",
      "Train Epoch: 1 [10880/30000 (36%)]\tLoss: 1.554896 testing loss: tensor(1.4349)\n",
      "Train Epoch: 1 [11520/30000 (38%)]\tLoss: 1.191498 testing loss: tensor(1.4327)\n",
      "Train Epoch: 1 [12160/30000 (40%)]\tLoss: 1.473901 testing loss: tensor(1.4326)\n",
      "Train Epoch: 1 [12800/30000 (43%)]\tLoss: 1.403843 testing loss: tensor(1.4324)\n",
      "Train Epoch: 1 [13440/30000 (45%)]\tLoss: 1.390465 testing loss: tensor(1.4318)\n",
      "Train Epoch: 1 [14080/30000 (47%)]\tLoss: 1.451140 testing loss: tensor(1.4303)\n",
      "Train Epoch: 1 [14720/30000 (49%)]\tLoss: 1.181953 testing loss: tensor(1.4308)\n",
      "Train Epoch: 1 [15360/30000 (51%)]\tLoss: 1.462450 testing loss: tensor(1.4303)\n",
      "Train Epoch: 1 [16000/30000 (53%)]\tLoss: 1.474644 testing loss: tensor(1.4325)\n",
      "Train Epoch: 1 [16640/30000 (55%)]\tLoss: 1.735981 testing loss: tensor(1.4323)\n",
      "Train Epoch: 1 [17280/30000 (57%)]\tLoss: 1.291009 testing loss: tensor(1.4307)\n",
      "Train Epoch: 1 [17920/30000 (60%)]\tLoss: 1.516482 testing loss: tensor(1.4312)\n",
      "Train Epoch: 1 [18560/30000 (62%)]\tLoss: 1.640585 testing loss: tensor(1.4309)\n",
      "Train Epoch: 1 [19200/30000 (64%)]\tLoss: 1.618059 testing loss: tensor(1.4311)\n",
      "Train Epoch: 1 [19840/30000 (66%)]\tLoss: 1.411585 testing loss: tensor(1.4313)\n",
      "Train Epoch: 1 [20480/30000 (68%)]\tLoss: 1.481781 testing loss: tensor(1.4299)\n",
      "Train Epoch: 1 [21120/30000 (70%)]\tLoss: 1.282062 testing loss: tensor(1.4295)\n",
      "Train Epoch: 1 [21760/30000 (72%)]\tLoss: 1.581376 testing loss: tensor(1.4296)\n",
      "Train Epoch: 1 [22400/30000 (74%)]\tLoss: 1.455731 testing loss: tensor(1.4300)\n",
      "Train Epoch: 1 [23040/30000 (77%)]\tLoss: 1.505481 testing loss: tensor(1.4286)\n",
      "Train Epoch: 1 [23680/30000 (79%)]\tLoss: 1.548289 testing loss: tensor(1.4285)\n",
      "Train Epoch: 1 [24320/30000 (81%)]\tLoss: 1.387567 testing loss: tensor(1.4298)\n",
      "Train Epoch: 1 [24960/30000 (83%)]\tLoss: 1.362980 testing loss: tensor(1.4272)\n",
      "Train Epoch: 1 [25600/30000 (85%)]\tLoss: 1.497027 testing loss: tensor(1.4267)\n",
      "Train Epoch: 1 [26240/30000 (87%)]\tLoss: 1.392202 testing loss: tensor(1.4263)\n",
      "Train Epoch: 1 [26880/30000 (89%)]\tLoss: 1.233781 testing loss: tensor(1.4271)\n",
      "Train Epoch: 1 [27520/30000 (91%)]\tLoss: 1.588907 testing loss: tensor(1.4265)\n",
      "Train Epoch: 1 [28160/30000 (94%)]\tLoss: 1.426143 testing loss: tensor(1.4272)\n",
      "Train Epoch: 1 [28800/30000 (96%)]\tLoss: 1.511626 testing loss: tensor(1.4258)\n",
      "Train Epoch: 1 [29440/30000 (98%)]\tLoss: 1.217127 testing loss: tensor(1.4261)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.4254)\n",
      "CS 2 : 1.7277\n",
      "DP 2 : 1.453\n",
      "heuristic 2 : 1.7975\n",
      "DP: 1.4498810768127441\n",
      "tensor([0.6506, 0.0479, 0.0413, 0.0415, 0.0432, 0.0411, 0.0410, 0.0442, 0.0377,\n",
      "        0.0116])\n",
      "tensor([0.6542, 0.0500, 0.0421, 0.0424, 0.0436, 0.0421, 0.0422, 0.0448, 0.0387,\n",
      "        1.0000])\n",
      "tensor([0.6690, 0.0528, 0.0485, 0.0480, 0.0469, 0.0438, 0.0439, 0.0472, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.6819, 0.0580, 0.0542, 0.0542, 0.0541, 0.0493, 0.0483, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.6977, 0.0641, 0.0599, 0.0596, 0.0626, 0.0560, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.7205, 0.0742, 0.0671, 0.0672, 0.0710, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.7489, 0.0949, 0.0779, 0.0784, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.7995, 0.1108, 0.0897, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.8726, 0.1274, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: twopeak random initializing\n",
      "do nothing\n",
      "NN 1 : tensor(1.8701)\n",
      "CS 1 : 1.7277\n",
      "DP 1 : 1.453\n",
      "heuristic 1 : 1.7975\n",
      "DP: 1.4498810768127441\n",
      "tensor([0.0802, 0.0874, 0.1495, 0.0567, 0.1442, 0.1315, 0.0604, 0.0846, 0.1169,\n",
      "        0.0885])\n",
      "tensor([0.0773, 0.0990, 0.1494, 0.0662, 0.1830, 0.1337, 0.0631, 0.0972, 0.1311,\n",
      "        1.0000])\n",
      "tensor([0.0915, 0.1223, 0.1644, 0.0772, 0.2131, 0.1426, 0.0745, 0.1144, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.1168, 0.1357, 0.1938, 0.0851, 0.2071, 0.1710, 0.0905, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.1318, 0.1650, 0.2394, 0.0822, 0.2033, 0.1783, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.1390, 0.2223, 0.2865, 0.1227, 0.2295, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.2082, 0.3002, 0.3443, 0.1474, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.2854, 0.3312, 0.3834, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.4381, 0.5619, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 2.344534 testing loss: tensor(1.8505)\n",
      "Train Epoch: 1 [640/30000 (2%)]\tLoss: 1.850586 testing loss: tensor(1.8066)\n",
      "Train Epoch: 1 [1280/30000 (4%)]\tLoss: 1.935571 testing loss: tensor(1.7818)\n",
      "Train Epoch: 1 [1920/30000 (6%)]\tLoss: 1.831160 testing loss: tensor(1.7651)\n",
      "Train Epoch: 1 [2560/30000 (9%)]\tLoss: 1.874553 testing loss: tensor(1.7571)\n",
      "Train Epoch: 1 [3200/30000 (11%)]\tLoss: 1.799788 testing loss: tensor(1.7521)\n",
      "Train Epoch: 1 [3840/30000 (13%)]\tLoss: 1.611910 testing loss: tensor(1.7436)\n",
      "Train Epoch: 1 [4480/30000 (15%)]\tLoss: 1.633920 testing loss: tensor(1.7430)\n",
      "Train Epoch: 1 [5120/30000 (17%)]\tLoss: 1.589018 testing loss: tensor(1.7397)\n",
      "Train Epoch: 1 [5760/30000 (19%)]\tLoss: 1.759141 testing loss: tensor(1.7397)\n",
      "Train Epoch: 1 [6400/30000 (21%)]\tLoss: 1.765628 testing loss: tensor(1.7374)\n",
      "Train Epoch: 1 [7040/30000 (23%)]\tLoss: 1.614497 testing loss: tensor(1.7362)\n",
      "Train Epoch: 1 [7680/30000 (26%)]\tLoss: 1.708822 testing loss: tensor(1.7353)\n",
      "Train Epoch: 1 [8320/30000 (28%)]\tLoss: 1.743472 testing loss: tensor(1.7371)\n",
      "Train Epoch: 1 [8960/30000 (30%)]\tLoss: 1.643713 testing loss: tensor(1.7370)\n",
      "Train Epoch: 1 [9600/30000 (32%)]\tLoss: 1.728024 testing loss: tensor(1.7364)\n",
      "Train Epoch: 1 [10240/30000 (34%)]\tLoss: 1.847192 testing loss: tensor(1.7368)\n",
      "Train Epoch: 1 [10880/30000 (36%)]\tLoss: 1.606425 testing loss: tensor(1.7368)\n",
      "Train Epoch: 1 [11520/30000 (38%)]\tLoss: 1.607958 testing loss: tensor(1.7361)\n",
      "Train Epoch: 1 [12160/30000 (40%)]\tLoss: 1.873516 testing loss: tensor(1.7359)\n",
      "Train Epoch: 1 [12800/30000 (43%)]\tLoss: 1.604660 testing loss: tensor(1.7360)\n",
      "Train Epoch: 1 [13440/30000 (45%)]\tLoss: 1.556574 testing loss: tensor(1.7355)\n",
      "Train Epoch: 1 [14080/30000 (47%)]\tLoss: 1.542890 testing loss: tensor(1.7355)\n",
      "Train Epoch: 1 [14720/30000 (49%)]\tLoss: 1.626798 testing loss: tensor(1.7367)\n",
      "Train Epoch: 1 [15360/30000 (51%)]\tLoss: 1.715162 testing loss: tensor(1.7370)\n",
      "Train Epoch: 1 [16000/30000 (53%)]\tLoss: 1.724281 testing loss: tensor(1.7355)\n",
      "Train Epoch: 1 [16640/30000 (55%)]\tLoss: 1.702515 testing loss: tensor(1.7360)\n",
      "Train Epoch: 1 [17280/30000 (57%)]\tLoss: 2.077810 testing loss: tensor(1.7361)\n",
      "Train Epoch: 1 [17920/30000 (60%)]\tLoss: 1.776191 testing loss: tensor(1.7365)\n",
      "Train Epoch: 1 [18560/30000 (62%)]\tLoss: 1.570105 testing loss: tensor(1.7361)\n",
      "Train Epoch: 1 [19200/30000 (64%)]\tLoss: 1.677595 testing loss: tensor(1.7362)\n",
      "Train Epoch: 1 [19840/30000 (66%)]\tLoss: 1.714837 testing loss: tensor(1.7335)\n",
      "Train Epoch: 1 [20480/30000 (68%)]\tLoss: 1.464010 testing loss: tensor(1.7344)\n",
      "Train Epoch: 1 [21120/30000 (70%)]\tLoss: 1.632696 testing loss: tensor(1.7356)\n",
      "Train Epoch: 1 [21760/30000 (72%)]\tLoss: 1.762288 testing loss: tensor(1.7366)\n",
      "Train Epoch: 1 [22400/30000 (74%)]\tLoss: 1.591170 testing loss: tensor(1.7380)\n",
      "Train Epoch: 1 [23040/30000 (77%)]\tLoss: 1.877207 testing loss: tensor(1.7362)\n",
      "Train Epoch: 1 [23680/30000 (79%)]\tLoss: 1.706371 testing loss: tensor(1.7355)\n",
      "Train Epoch: 1 [24320/30000 (81%)]\tLoss: 1.803427 testing loss: tensor(1.7356)\n",
      "Train Epoch: 1 [24960/30000 (83%)]\tLoss: 1.731234 testing loss: tensor(1.7346)\n",
      "Train Epoch: 1 [25600/30000 (85%)]\tLoss: 1.595140 testing loss: tensor(1.7323)\n",
      "Train Epoch: 1 [26240/30000 (87%)]\tLoss: 1.872201 testing loss: tensor(1.7331)\n",
      "Train Epoch: 1 [26880/30000 (89%)]\tLoss: 1.665708 testing loss: tensor(1.7341)\n",
      "Train Epoch: 1 [27520/30000 (91%)]\tLoss: 1.747752 testing loss: tensor(1.7345)\n",
      "Train Epoch: 1 [28160/30000 (94%)]\tLoss: 1.669628 testing loss: tensor(1.7357)\n",
      "Train Epoch: 1 [28800/30000 (96%)]\tLoss: 1.667785 testing loss: tensor(1.7356)\n",
      "Train Epoch: 1 [29440/30000 (98%)]\tLoss: 1.758861 testing loss: tensor(1.7350)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.7345)\n",
      "CS 2 : 1.7277\n",
      "DP 2 : 1.453\n",
      "heuristic 2 : 1.7975\n",
      "DP: 1.4498810768127441\n",
      "tensor([0.1031, 0.0994, 0.0966, 0.1048, 0.1015, 0.0978, 0.0993, 0.0985, 0.1011,\n",
      "        0.0980])\n",
      "tensor([0.1117, 0.1140, 0.1112, 0.1112, 0.1158, 0.1090, 0.1060, 0.1134, 0.1078,\n",
      "        1.0000])\n",
      "tensor([0.1158, 0.1289, 0.1195, 0.1273, 0.1328, 0.1222, 0.1220, 0.1314, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.1397, 0.1427, 0.1393, 0.1397, 0.1587, 0.1367, 0.1432, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.1563, 0.1728, 0.1676, 0.1595, 0.1938, 0.1500, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.1774, 0.2015, 0.1965, 0.2011, 0.2235, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.2396, 0.2678, 0.2358, 0.2568, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.3490, 0.3279, 0.3231, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.5182, 0.4818, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: twopeak costsharing\n",
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 0.001396\n",
      "Train Epoch: 1 [3200/30000 (11%)]\tLoss: 0.000006\n",
      "Train Epoch: 1 [6400/30000 (21%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [9600/30000 (32%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [12800/30000 (43%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [16000/30000 (53%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [19200/30000 (64%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [22400/30000 (74%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [25600/30000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [28800/30000 (96%)]\tLoss: 0.000000\n",
      "NN 1 : tensor(1.7282)\n",
      "CS 1 : 1.7277\n",
      "DP 1 : 1.453\n",
      "heuristic 1 : 1.7975\n",
      "DP: 1.4498810768127441\n",
      "tensor([0.1002, 0.1000, 0.1001, 0.1000, 0.1000, 0.0999, 0.1000, 0.0999, 0.1000,\n",
      "        0.0999])\n",
      "tensor([0.1113, 0.1111, 0.1110, 0.1111, 0.1114, 0.1112, 0.1108, 0.1109, 0.1110,\n",
      "        1.0000])\n",
      "tensor([0.1246, 0.1250, 0.1248, 0.1253, 0.1251, 0.1251, 0.1252, 0.1249, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.1427, 0.1428, 0.1426, 0.1427, 0.1427, 0.1434, 0.1430, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.1662, 0.1668, 0.1669, 0.1662, 0.1665, 0.1674, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.2012, 0.2005, 0.2000, 0.1989, 0.1995, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.2524, 0.2498, 0.2502, 0.2476, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.3351, 0.3302, 0.3347, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.5008, 0.4992, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 1.912430 testing loss: tensor(1.7284)\n",
      "Train Epoch: 1 [640/30000 (2%)]\tLoss: 1.703020 testing loss: tensor(1.7291)\n",
      "Train Epoch: 1 [1280/30000 (4%)]\tLoss: 1.616703 testing loss: tensor(1.7309)\n",
      "Train Epoch: 1 [1920/30000 (6%)]\tLoss: 1.752651 testing loss: tensor(1.7305)\n",
      "Train Epoch: 1 [2560/30000 (9%)]\tLoss: 1.511446 testing loss: tensor(1.7301)\n",
      "Train Epoch: 1 [3200/30000 (11%)]\tLoss: 1.695900 testing loss: tensor(1.7292)\n",
      "Train Epoch: 1 [3840/30000 (13%)]\tLoss: 1.589482 testing loss: tensor(1.7298)\n",
      "Train Epoch: 1 [4480/30000 (15%)]\tLoss: 1.714671 testing loss: tensor(1.7310)\n",
      "Train Epoch: 1 [5120/30000 (17%)]\tLoss: 1.787616 testing loss: tensor(1.7313)\n",
      "Train Epoch: 1 [5760/30000 (19%)]\tLoss: 1.940073 testing loss: tensor(1.7296)\n",
      "Train Epoch: 1 [6400/30000 (21%)]\tLoss: 1.692167 testing loss: tensor(1.7301)\n",
      "Train Epoch: 1 [7040/30000 (23%)]\tLoss: 1.713691 testing loss: tensor(1.7304)\n",
      "Train Epoch: 1 [7680/30000 (26%)]\tLoss: 1.535062 testing loss: tensor(1.7289)\n",
      "Train Epoch: 1 [8320/30000 (28%)]\tLoss: 1.781957 testing loss: tensor(1.7295)\n",
      "Train Epoch: 1 [8960/30000 (30%)]\tLoss: 1.635348 testing loss: tensor(1.7295)\n",
      "Train Epoch: 1 [9600/30000 (32%)]\tLoss: 1.596064 testing loss: tensor(1.7293)\n",
      "Train Epoch: 1 [10240/30000 (34%)]\tLoss: 1.842580 testing loss: tensor(1.7286)\n",
      "Train Epoch: 1 [10880/30000 (36%)]\tLoss: 1.675715 testing loss: tensor(1.7288)\n",
      "Train Epoch: 1 [11520/30000 (38%)]\tLoss: 1.733344 testing loss: tensor(1.7285)\n",
      "Train Epoch: 1 [12160/30000 (40%)]\tLoss: 1.717569 testing loss: tensor(1.7280)\n",
      "Train Epoch: 1 [12800/30000 (43%)]\tLoss: 2.181540 testing loss: tensor(1.7292)\n",
      "Train Epoch: 1 [13440/30000 (45%)]\tLoss: 1.917857 testing loss: tensor(1.7284)\n",
      "Train Epoch: 1 [14080/30000 (47%)]\tLoss: 1.852625 testing loss: tensor(1.7288)\n",
      "Train Epoch: 1 [14720/30000 (49%)]\tLoss: 1.441855 testing loss: tensor(1.7287)\n",
      "Train Epoch: 1 [15360/30000 (51%)]\tLoss: 1.676789 testing loss: tensor(1.7283)\n",
      "Train Epoch: 1 [16000/30000 (53%)]\tLoss: 1.738620 testing loss: tensor(1.7279)\n",
      "Train Epoch: 1 [16640/30000 (55%)]\tLoss: 1.556755 testing loss: tensor(1.7273)\n",
      "Train Epoch: 1 [17280/30000 (57%)]\tLoss: 1.687950 testing loss: tensor(1.7267)\n",
      "Train Epoch: 1 [17920/30000 (60%)]\tLoss: 1.814300 testing loss: tensor(1.7288)\n",
      "Train Epoch: 1 [18560/30000 (62%)]\tLoss: 1.945071 testing loss: tensor(1.7289)\n",
      "Train Epoch: 1 [19200/30000 (64%)]\tLoss: 1.608763 testing loss: tensor(1.7296)\n",
      "Train Epoch: 1 [19840/30000 (66%)]\tLoss: 1.583343 testing loss: tensor(1.7297)\n",
      "Train Epoch: 1 [20480/30000 (68%)]\tLoss: 1.726455 testing loss: tensor(1.7296)\n",
      "Train Epoch: 1 [21120/30000 (70%)]\tLoss: 1.766469 testing loss: tensor(1.7292)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [21760/30000 (72%)]\tLoss: 1.946152 testing loss: tensor(1.7299)\n",
      "Train Epoch: 1 [22400/30000 (74%)]\tLoss: 1.636405 testing loss: tensor(1.7298)\n",
      "Train Epoch: 1 [23040/30000 (77%)]\tLoss: 1.656751 testing loss: tensor(1.7292)\n",
      "Train Epoch: 1 [23680/30000 (79%)]\tLoss: 1.740047 testing loss: tensor(1.7294)\n",
      "Train Epoch: 1 [24320/30000 (81%)]\tLoss: 1.743302 testing loss: tensor(1.7303)\n",
      "Train Epoch: 1 [24960/30000 (83%)]\tLoss: 1.701697 testing loss: tensor(1.7308)\n",
      "Train Epoch: 1 [25600/30000 (85%)]\tLoss: 1.833775 testing loss: tensor(1.7300)\n",
      "Train Epoch: 1 [26240/30000 (87%)]\tLoss: 1.801817 testing loss: tensor(1.7289)\n",
      "Train Epoch: 1 [26880/30000 (89%)]\tLoss: 1.804552 testing loss: tensor(1.7282)\n",
      "Train Epoch: 1 [27520/30000 (91%)]\tLoss: 1.731236 testing loss: tensor(1.7287)\n",
      "Train Epoch: 1 [28160/30000 (94%)]\tLoss: 1.888692 testing loss: tensor(1.7292)\n",
      "Train Epoch: 1 [28800/30000 (96%)]\tLoss: 1.810160 testing loss: tensor(1.7296)\n",
      "Train Epoch: 1 [29440/30000 (98%)]\tLoss: 1.892976 testing loss: tensor(1.7284)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.7280)\n",
      "CS 2 : 1.7277\n",
      "DP 2 : 1.453\n",
      "heuristic 2 : 1.7975\n",
      "DP: 1.4498810768127441\n",
      "tensor([0.1020, 0.1001, 0.1005, 0.0991, 0.1011, 0.1001, 0.0987, 0.0994, 0.0989,\n",
      "        0.1001])\n",
      "tensor([0.1133, 0.1113, 0.1116, 0.1101, 0.1125, 0.1115, 0.1095, 0.1104, 0.1098,\n",
      "        1.0000])\n",
      "tensor([0.1269, 0.1248, 0.1255, 0.1237, 0.1261, 0.1255, 0.1234, 0.1240, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.1446, 0.1427, 0.1432, 0.1409, 0.1438, 0.1437, 0.1412, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.1681, 0.1664, 0.1672, 0.1635, 0.1674, 0.1673, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.2035, 0.2000, 0.2004, 0.1951, 0.2009, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.2552, 0.2491, 0.2513, 0.2445, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.3387, 0.3270, 0.3344, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.5060, 0.4940, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: twopeak heuristic\n",
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 0.036986\n",
      "Train Epoch: 1 [3200/30000 (11%)]\tLoss: 0.013522\n",
      "Train Epoch: 1 [6400/30000 (21%)]\tLoss: 0.007942\n",
      "Train Epoch: 1 [9600/30000 (32%)]\tLoss: 0.006413\n",
      "Train Epoch: 1 [12800/30000 (43%)]\tLoss: 0.005238\n",
      "Train Epoch: 1 [16000/30000 (53%)]\tLoss: 0.003418\n",
      "Train Epoch: 1 [19200/30000 (64%)]\tLoss: 0.004493\n",
      "Train Epoch: 1 [22400/30000 (74%)]\tLoss: 0.005091\n",
      "Train Epoch: 1 [25600/30000 (85%)]\tLoss: 0.004026\n",
      "Train Epoch: 1 [28800/30000 (96%)]\tLoss: 0.002704\n",
      "NN 1 : tensor(1.5894)\n",
      "CS 1 : 1.7277\n",
      "DP 1 : 1.453\n",
      "heuristic 1 : 1.7975\n",
      "DP: 1.4498810768127441\n",
      "tensor([0.0320, 0.0408, 0.0323, 0.0289, 0.0415, 0.0484, 0.0253, 0.6689, 0.0443,\n",
      "        0.0377])\n",
      "tensor([0.0371, 0.0376, 0.0357, 0.0554, 0.0930, 0.0256, 0.6093, 0.0603, 0.0460,\n",
      "        1.0000])\n",
      "tensor([0.0530, 0.0476, 0.0475, 0.1035, 0.0832, 0.1122, 0.1667, 0.3863, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.0509, 0.0537, 0.0435, 0.0863, 0.5763, 0.0543, 0.1350, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.0795, 0.0763, 0.0536, 0.0845, 0.1769, 0.5292, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.0697, 0.0720, 0.0747, 0.1349, 0.6486, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.0855, 0.0926, 0.2117, 0.6103, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.1785, 0.1682, 0.6533, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.5153, 0.4847, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 12.720317 testing loss: tensor(1.6124)\n",
      "Train Epoch: 1 [640/30000 (2%)]\tLoss: 8.059538 testing loss: tensor(1.8759)\n",
      "Train Epoch: 1 [1280/30000 (4%)]\tLoss: 5.002902 testing loss: tensor(2.0743)\n",
      "Train Epoch: 1 [1920/30000 (6%)]\tLoss: 3.583258 testing loss: tensor(2.1133)\n",
      "Train Epoch: 1 [2560/30000 (9%)]\tLoss: 2.971624 testing loss: tensor(2.0489)\n",
      "Train Epoch: 1 [3200/30000 (11%)]\tLoss: 2.512207 testing loss: tensor(1.9834)\n",
      "Train Epoch: 1 [3840/30000 (13%)]\tLoss: 2.483515 testing loss: tensor(1.9635)\n",
      "Train Epoch: 1 [4480/30000 (15%)]\tLoss: 2.273263 testing loss: tensor(1.9375)\n",
      "Train Epoch: 1 [5120/30000 (17%)]\tLoss: 2.188920 testing loss: tensor(1.9241)\n",
      "Train Epoch: 1 [5760/30000 (19%)]\tLoss: 2.059569 testing loss: tensor(1.9187)\n",
      "Train Epoch: 1 [6400/30000 (21%)]\tLoss: 2.010047 testing loss: tensor(1.9133)\n",
      "Train Epoch: 1 [7040/30000 (23%)]\tLoss: 1.915507 testing loss: tensor(1.9092)\n",
      "Train Epoch: 1 [7680/30000 (26%)]\tLoss: 1.919183 testing loss: tensor(1.9057)\n",
      "Train Epoch: 1 [8320/30000 (28%)]\tLoss: 1.933788 testing loss: tensor(1.9040)\n",
      "Train Epoch: 1 [8960/30000 (30%)]\tLoss: 2.103541 testing loss: tensor(1.9033)\n",
      "Train Epoch: 1 [9600/30000 (32%)]\tLoss: 1.732484 testing loss: tensor(1.9006)\n",
      "Train Epoch: 1 [10240/30000 (34%)]\tLoss: 1.729731 testing loss: tensor(1.9006)\n",
      "Train Epoch: 1 [10880/30000 (36%)]\tLoss: 1.789568 testing loss: tensor(1.8984)\n",
      "Train Epoch: 1 [11520/30000 (38%)]\tLoss: 1.844610 testing loss: tensor(1.8966)\n",
      "Train Epoch: 1 [12160/30000 (40%)]\tLoss: 2.008521 testing loss: tensor(1.8945)\n",
      "Train Epoch: 1 [12800/30000 (43%)]\tLoss: 1.989525 testing loss: tensor(1.8928)\n",
      "Train Epoch: 1 [13440/30000 (45%)]\tLoss: 1.714121 testing loss: tensor(1.8896)\n",
      "Train Epoch: 1 [14080/30000 (47%)]\tLoss: 1.862542 testing loss: tensor(1.8860)\n",
      "Train Epoch: 1 [14720/30000 (49%)]\tLoss: 1.911747 testing loss: tensor(1.8834)\n",
      "Train Epoch: 1 [15360/30000 (51%)]\tLoss: 1.976891 testing loss: tensor(1.8828)\n",
      "Train Epoch: 1 [16000/30000 (53%)]\tLoss: 1.929188 testing loss: tensor(1.8810)\n",
      "Train Epoch: 1 [16640/30000 (55%)]\tLoss: 1.736393 testing loss: tensor(1.8799)\n",
      "Train Epoch: 1 [17280/30000 (57%)]\tLoss: 1.801580 testing loss: tensor(1.8786)\n",
      "Train Epoch: 1 [17920/30000 (60%)]\tLoss: 2.042320 testing loss: tensor(1.8766)\n",
      "Train Epoch: 1 [18560/30000 (62%)]\tLoss: 1.723841 testing loss: tensor(1.8749)\n",
      "Train Epoch: 1 [19200/30000 (64%)]\tLoss: 1.753061 testing loss: tensor(1.8720)\n",
      "Train Epoch: 1 [19840/30000 (66%)]\tLoss: 2.043581 testing loss: tensor(1.8699)\n",
      "Train Epoch: 1 [20480/30000 (68%)]\tLoss: 2.064921 testing loss: tensor(1.8680)\n",
      "Train Epoch: 1 [21120/30000 (70%)]\tLoss: 1.789261 testing loss: tensor(1.8657)\n",
      "Train Epoch: 1 [21760/30000 (72%)]\tLoss: 2.245241 testing loss: tensor(1.8637)\n",
      "Train Epoch: 1 [22400/30000 (74%)]\tLoss: 1.797816 testing loss: tensor(1.8618)\n",
      "Train Epoch: 1 [23040/30000 (77%)]\tLoss: 1.869005 testing loss: tensor(1.8584)\n",
      "Train Epoch: 1 [23680/30000 (79%)]\tLoss: 1.867383 testing loss: tensor(1.8568)\n",
      "Train Epoch: 1 [24320/30000 (81%)]\tLoss: 2.034209 testing loss: tensor(1.8547)\n",
      "Train Epoch: 1 [24960/30000 (83%)]\tLoss: 1.994511 testing loss: tensor(1.8534)\n",
      "Train Epoch: 1 [25600/30000 (85%)]\tLoss: 1.790573 testing loss: tensor(1.8514)\n",
      "Train Epoch: 1 [26240/30000 (87%)]\tLoss: 2.067240 testing loss: tensor(1.8497)\n",
      "Train Epoch: 1 [26880/30000 (89%)]\tLoss: 1.701871 testing loss: tensor(1.8484)\n",
      "Train Epoch: 1 [27520/30000 (91%)]\tLoss: 1.737963 testing loss: tensor(1.8472)\n",
      "Train Epoch: 1 [28160/30000 (94%)]\tLoss: 1.915548 testing loss: tensor(1.8464)\n",
      "Train Epoch: 1 [28800/30000 (96%)]\tLoss: 2.094270 testing loss: tensor(1.8451)\n",
      "Train Epoch: 1 [29440/30000 (98%)]\tLoss: 1.778278 testing loss: tensor(1.8437)\n",
      "penalty: 5.0514936447143555e-05\n",
      "NN 2 : tensor(1.8423)\n",
      "CS 2 : 1.7277\n",
      "DP 2 : 1.453\n",
      "heuristic 2 : 1.7975\n",
      "DP: 1.4498810768127441\n",
      "tensor([0.0718, 0.0719, 0.0701, 0.0994, 0.0947, 0.1045, 0.0988, 0.1451, 0.0807,\n",
      "        0.1631])\n",
      "tensor([0.0800, 0.0821, 0.0771, 0.1045, 0.0975, 0.1045, 0.1159, 0.1499, 0.1885,\n",
      "        1.0000])\n",
      "tensor([0.0865, 0.0943, 0.0892, 0.1148, 0.1116, 0.1221, 0.1233, 0.2582, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.1093, 0.1089, 0.1026, 0.1338, 0.1168, 0.1552, 0.2732, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.1234, 0.1324, 0.1042, 0.1391, 0.1770, 0.3240, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.1606, 0.1705, 0.1544, 0.2062, 0.3084, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.1953, 0.2277, 0.2270, 0.3500, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.3040, 0.3130, 0.3829, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.5140, 0.4860, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.1)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "    \n",
    "for order in stage:\n",
    "    producedata(order)\n",
    "    #running\n",
    "    for order1 in order1name:\n",
    "        print(\"Supervised Aim:\",order,order1)\n",
    "        \n",
    "        # for mapping binary to payments before softmax\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(n, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, n),\n",
    "        )\n",
    "        model.apply(init_weights)\n",
    "        # optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr1)\n",
    "        \n",
    "        runningLossNN = []\n",
    "        runningLossCS = []\n",
    "        runningLossDP = []\n",
    "        runningLossHeuristic = []\n",
    "        #model=torch.load(\"save/pytorchNN=5dp1\");\n",
    "        #model.eval()\n",
    "        ##order1name=[\"costsharing\",\"dp\",\"heuristic\",\"random initializing\"]\n",
    "        for epoch in range(1, supervisionEpochs + 1):\n",
    "#             print(\"distributionRatio\",distributionRatio)\n",
    "            if(order1==\"costsharing\"):\n",
    "                supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "            elif(order1==\"dp\"):\n",
    "                supervisionTrain(epoch, dpSupervisionRule)\n",
    "            elif(order1==\"heuristic\"):\n",
    "                supervisionTrain(epoch, heuristicSupervisionRule)\n",
    "            elif(order1==\"random initializing\"):\n",
    "                print(\"do nothing\");\n",
    "\n",
    "        test()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr2)\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(epoch)\n",
    "            test()\n",
    "        losslistname.append(order+\" \"+order1);\n",
    "        losslist.append(losslisttemp);\n",
    "        losslisttemp=[];\n",
    "        savepath=\"save/pytorchNN=5all-beta\"+order+str(order1)\n",
    "        torch.save(model, savepath);\n",
    "        print(\"end\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colorlist=[\"#D2691E\",'#4169E1',\"#9ACD32\",\"#B22222\",\"#FF00FF\",\"#708090\"]\n",
    "for i in range(len(losslist)):\n",
    "    plt.plot(losslist[i], '.-', color=colorlist[i], alpha=0.8, label=losslistname[i])\n",
    "\n",
    "# 显示标签，如果不加这句，即使加了label='一些数字'的参数，最终还是不会显示标签\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('Trianing Times')\n",
    "plt.ylabel('Delay')\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('pytorchNN=5_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
