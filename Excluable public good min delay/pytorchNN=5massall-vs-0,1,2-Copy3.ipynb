{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "from pylab import *\n",
    "from scipy.stats import beta\n",
    "import random \n",
    "n = 5\n",
    "epochs = 6\n",
    "supervisionEpochs = 1\n",
    "lr = 0.0005\n",
    "log_interval = 10\n",
    "trainSize = 40000#100000\n",
    "percentage_train_test= 0.25\n",
    "penaltyLambda = 10\n",
    "doublePeakHighMean = 0.9\n",
    "doublePeakLowMean = 0.1\n",
    "doublePeakStd = 0.1\n",
    "uniformlow=0\n",
    "uniformhigh=1.0\n",
    "normalloc = 0.2\n",
    "normalscale = 0.1\n",
    "\n",
    "cauchyloc = 1/n\n",
    "cauchyscalen = 0.004\n",
    "\n",
    "exponentialhigh = 15 #Symbol(\"b\", real=True)\n",
    "exponentiallow  = 15 #Symbol(\"a\", real=True)\n",
    "\n",
    "Sigmoidrate=0.00005\n",
    "Sigmoidbias=0.001\n",
    "\n",
    "\n",
    "\n",
    "independentnormalloc1=[(float(ii)+1)/(2*n+1) for ii in range(n,0,-1)]\n",
    "independentnormalscale1=[0.05 for ii in range(n)]\n",
    "\n",
    "independentnormalloc2=[(float(ii)+1)/(2*n+1) for ii in range(1,n+1,1)]\n",
    "independentnormalscale2=[0.05 for ii in range(n)]\n",
    "stage=[\"twopeak\"]\n",
    "# \"twopeak\",\"normal\",\"uniform\",\"independent1\",\"independent2\",\"cauchy\",\"U-exponential\"\n",
    "order1name=[\"costsharing\"]\n",
    "numberofpeople=['0','1','2']\n",
    "# \"costsharing\",\"dp\",\"heuristic\",\"random initializing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5000)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio1 = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase1 = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "d3 = D.normal.Normal(normalloc, normalscale)\n",
    "distributionRatio3 = d3.cdf(1) - d3.cdf(0)\n",
    "distributionBase3 = d3.cdf(0)\n",
    "\n",
    "d4 = D.uniform.Uniform(uniformlow,uniformhigh)\n",
    "distributionRatio4 = d4.cdf(1) - d4.cdf(0)\n",
    "distributionBase4 = d4.cdf(0)\n",
    "\n",
    "d5 = [D.normal.Normal(independentnormalloc1[ii], independentnormalscale1[ii]) for ii in range(n)]\n",
    "d6 = [D.normal.Normal(independentnormalloc2[ii], independentnormalscale2[ii]) for ii in range(n)]\n",
    "\n",
    "d7 = D.cauchy.Cauchy(cauchyloc,cauchyscalen)\n",
    "\n",
    "d81 = D.exponential.Exponential(exponentiallow)\n",
    "d82 = D.exponential.Exponential(exponentialhigh)\n",
    "\n",
    "#d8 = beta(betahigh,betalow)\n",
    "#d9 = D.beta.Beta(betahigh,betalow)\n",
    "def cdf(x,y, i=None):\n",
    "    if(y==\"twopeak\"):\n",
    "        return (d1.cdf(x) + d2.cdf(x) - distributionBase1) / 2 / distributionRatio1\n",
    "    elif(y==\"normal\"):\n",
    "        return (d3.cdf(x)-distributionBase3)/distributionRatio3;\n",
    "    elif(y==\"uniform\"):\n",
    "        return (d4.cdf(x)-distributionBase4)/distributionRatio4;\n",
    "    elif(y==\"independent1\"):\n",
    "        return d5[i].cdf(x);\n",
    "    elif(y==\"independent2\"):\n",
    "        return d6[i].cdf(x);\n",
    "    elif(y==\"cauchy\"):\n",
    "        return d7.cdf(x);\n",
    "#     elif(y==\"beta\"):\n",
    "#         return torch.tensor(d8.cdf(x));\n",
    "    elif(y==\"U-exponential\"):\n",
    "        return (d81.cdf(x) + (1.0 - d82.cdf(1.0-x)))  / 2 \n",
    "print(cdf(0.5,\"U-exponential\"))\n",
    "\n",
    "print(d81.cdf(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    if torch.sum(bits).item() == 0:\n",
    "        return torch.ones(n)\n",
    "    bits = bits.type(torch.float32)\n",
    "    negBits = torch.ones(n) - bits\n",
    "    payments = model(bits)\n",
    "    payments = payments - 1000 * negBits\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    payments = payments + negBits\n",
    "    return payments\n",
    "\n",
    "def tpToBits(tp, bits=torch.ones(n).type(torch.uint8)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = (tp >= payments).type(torch.uint8)\n",
    "    if torch.equal(newBits, bits):\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits(tp, newBits)#bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    return n - torch.sum(tpToBits(tp).type(torch.float32))\n",
    "\n",
    "def tpToBits0(tp, deep ,bits=torch.ones(n).type(torch.float32)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = torch.sigmoid((tp - payments + Sigmoidbias)/Sigmoidrate)\n",
    "    if torch.allclose(newBits, bits,rtol=0.01,atol=0.01) or deep>=n:\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits0(tp,deep+1 ,newBits)#bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "def tpToTotalDelay0(tp):\n",
    "    return n - torch.sum(tpToBits0(tp,0).type(torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    global  dp,decision\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money, yes]\n",
    "        offer = float(offerIndex) / dpPrecision\n",
    "        if tp[i] >= offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits[i] = 0\n",
    "            payments[i] = 1\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "#     print()\n",
    "#     print(tp)\n",
    "#     print(bits)\n",
    "#     print(payments)\n",
    "#     print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def heuristicSupervisionRule(tp):\n",
    "    global  dp_H,decision_H\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    tempold=-1;\n",
    "    for turns in range(n,0,-1):\n",
    "        money = dpPrecision\n",
    "        j=0\n",
    "        tempo=sum(bits)\n",
    "        #print(\"bits\",tempo)\n",
    "        for i in range(n):\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            \n",
    "            offerIndex = decision_H[tempo ,tempo - i, money]\n",
    "            offer = float(offerIndex) / dpPrecision\n",
    "            while(j<n):\n",
    "                if(bits[j]!=0):\n",
    "                    break;\n",
    "                j+=1;\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            if tp[j] >= offer:\n",
    "                #print(money,j,tp[j],offer)\n",
    "                money -= offerIndex\n",
    "                bits[j] = 1\n",
    "                payments[j] = offer\n",
    "            else:\n",
    "                bits[j] = 0;\n",
    "                payments[j] = 1.0;\n",
    "            j+=1;\n",
    "        #print(\"money\",money)\n",
    "        if(money==0 and tempold==tempo):\n",
    "            break;\n",
    "        tempold=tempo;\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    for k in range(n, -1, -1):\n",
    "        if k == 0:\n",
    "            break\n",
    "        bits = [1 if tp[ii] >= 1.0 / k else 0 for ii in range(n)]\n",
    "        if sum(bits) == k:\n",
    "            break\n",
    "    if k == 0:\n",
    "        payments = [1 for ii in range(n)]\n",
    "    else:\n",
    "        payments = [1.0 / k if bits[ii] == 1 else 1 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    return float(n - torch.sum(costSharingSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def dpDelay(tp):\n",
    "    return float(n - torch.sum(dpSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def heuristicDelay(tp):\n",
    "    return float(n - torch.sum(heuristicSupervisionRule(tp)[0]).item())\n",
    "\n",
    "\n",
    "# templire=0;\n",
    "# num=1\n",
    "# for i in range(num):\n",
    "#     temp=torch.tensor(samplesJoint[i])\n",
    "#     #print(temp)\n",
    "#     #print(temp);\n",
    "#     #print(heuristicSupervisionRule(temp))\n",
    "#     #print(dpSupervisionRule(temp))\n",
    "#     #print(dpDelay(temp))\n",
    "#     res=dpDelay(temp)\n",
    "#     templire+=res\n",
    "#     #print(\"delay\",res)\n",
    "#     #print()\n",
    "# print(templire/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordAndReport(name, source, loss):\n",
    "    source.append(loss)\n",
    "    realLength = len(source)\n",
    "    #print(f\"{name} ({realLength}): {loss}\")\n",
    "    print(name,realLength,\":\" ,loss)\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    \n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda /100000\n",
    "        \n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "losslist=[];\n",
    "losslistname=[];\n",
    "losslisttemp=[];\n",
    "def test_batch():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lenLoss= 0\n",
    "        nnLoss = 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "            for tp in tp_batch:\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        nnLoss/=lenLoss\n",
    "    return nnLoss\n",
    "\n",
    "\n",
    "def train0(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            delay0 = tpToTotalDelay0(tp)\n",
    "            loss = loss + delay0\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "\n",
    "def train1(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                delay1 = tpToTotalDelay(tp1)\n",
    "                delay0 = tpToTotalDelay(tp0)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                    loss = loss + (1 - cdf(offer,order)) * delay1 + cdf(offer,order) * delay0\n",
    "                else:\n",
    "                    loss = loss + (1 - cdf(offer,order,i)) * delay1 + cdf(offer,order,i) * delay0\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "    \n",
    "def train2(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                x1=random.randint(0,n-1);\n",
    "                x2=random.randint(0,n-1);\n",
    "                while(x2==x1):\n",
    "                    x2=random.randint(0,n-1);\n",
    "                    \n",
    "                tp11 = tp.clone()\n",
    "                tp11[x1] = 1\n",
    "                tp11[x2] = 1\n",
    "                tp10 = tp.clone()\n",
    "                tp10[x1] = 1\n",
    "                tp10[x2] = 0\n",
    "                tp01 = tp.clone()\n",
    "                tp01[x1] = 0\n",
    "                tp01[x2] = 1\n",
    "                tp00 = tp.clone()\n",
    "                tp00[x1] = 0\n",
    "                tp00[x2] = 0\n",
    "                \n",
    "                offer1 = tpToPayments(tp11)[x1]\n",
    "                offer2 = tpToPayments(tp11)[x2]\n",
    "                offer3 = tpToPayments(tp10)[x1]\n",
    "                offer4 = tpToPayments(tp01)[x2]\n",
    "                \n",
    "                #print(tp)\n",
    "                #print(offer1,offer2)\n",
    "                \n",
    "                delay11 = tpToTotalDelay(tp11)\n",
    "                delay01 = tpToTotalDelay(tp01)\n",
    "                delay10 = tpToTotalDelay(tp10)\n",
    "                delay00 = tpToTotalDelay(tp00)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "#                     print ((1.0-cdf(offer1,order)) * (1.0-cdf(offer2,order))+                                                   \n",
    "#                            (1.0-cdf(offer3,order)) * cdf(offer2,order)+\n",
    "#                            cdf(offer1,order) * (1.0-cdf(offer4,order))+\n",
    "#                            (cdf(offer3,order) * cdf(offer4,order) -\n",
    "#                                      (cdf(offer3,order)-cdf(offer1,order))*(cdf(offer4,order)-cdf(offer2,order)) )\n",
    "#                           )\n",
    "                    loss = loss +(      (1.0-cdf(offer1,order)) * (1.0-cdf(offer2,order)) * delay11 +\n",
    "                                  (1.0-cdf(offer3,order)) * cdf(offer2,order) * delay10 +\n",
    "                                  cdf(offer1,order) * (1.0-cdf(offer4,order)) * delay01 +\n",
    "                                    (cdf(offer3,order) * cdf(offer4,order) -\n",
    "                                     (cdf(offer3,order)-cdf(offer1,order))*(cdf(offer4,order)-cdf(offer2,order)) )* delay00 \n",
    "                                                       )\n",
    "                else:\n",
    "                    loss = loss +(      (1.0-cdf(offer1,order,x1)) * (1.0-cdf(offer2,order),x2) * delay11 +\n",
    "                                  (1.0-cdf(offer3,order,x1)) * cdf(offer2,order,x2) * delay10 +\n",
    "                                  cdf(offer1,order,x1) * (1.0-cdf(offer4,order,x2)) * delay01 +\n",
    "                                    (cdf(offer3,order,x1) * cdf(offer4,order,x2) -\n",
    "                                     (cdf(offer3,order,x1)-cdf(offer1,order,x1))*(cdf(offer4,order,x2)-cdf(offer2,order,x2)) )* delay00 \n",
    "                                                       )\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "    \n",
    "allBits = [torch.tensor(bits, dtype=torch.int16) for bits in itertools.product([0, 1], repeat=n)]\n",
    "\n",
    "def test():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        costSharingLoss = 0\n",
    "        dpLoss = 0\n",
    "        nnLoss = 0\n",
    "        heuristicLoss=0\n",
    "        lenLoss= 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "                heuristicLoss+= heuristicDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        costSharingLoss /= lenLoss\n",
    "        dpLoss /= lenLoss\n",
    "        nnLoss /= lenLoss\n",
    "        heuristicLoss /= lenLoss\n",
    "        #print(lenLoss)\n",
    "        losslisttemp.append(nnLoss);\n",
    "        recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "        recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "        recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "        recordAndReport(\"heuristic\", runningLossHeuristic, heuristicLoss)\n",
    "        print(\"DP:\",dp[n, dpPrecision,0])\n",
    "        #for i in range(n, 0, -1):\n",
    "        #    print(\"Heuristic:\",i,5*(1-dp_H[i, i, dpPrecision]))\n",
    "        for i in range(n, 0, -1):\n",
    "            print(\n",
    "                    tpToPayments(\n",
    "                            torch.tensor([0 if ii >= i else 1 for ii in range(n)], dtype=torch.float32)\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "dpPrecision = 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def producedata(order):\n",
    "    global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "    dpPrecision = 100 \n",
    "    if(order==\"twopeak\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = np.random.normal(\n",
    "            loc=doublePeakLowMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=doublePeakLowMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samples2 = np.random.normal(\n",
    "            loc=doublePeakHighMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = np.random.normal(\n",
    "                        loc=doublePeakHighMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "    elif(order==\"normal\"):\n",
    "        print(\"loc\",normalloc, \"scale\",normalscale)\n",
    "        samples1 = np.random.normal(\n",
    "            loc=normalloc, scale=normalscale, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=normalloc, scale=normalscale\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"uniform\"):  \n",
    "        print(\"uniformlow\",uniformlow, \"uniformhigh\",uniformhigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        uniformlow, uniformhigh\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent1\"):\n",
    "        print(\"loc\",independentnormalloc1,\"scale\",independentnormalscale1)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent2\"):\n",
    "        print(\"loc\",independentnormalloc2, \"scale\",independentnormalscale2)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"cauchy\"):\n",
    "        print(\"cauchyloc\",cauchyloc, \"cauchyscale\",cauchyscalen)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"beta\"):\n",
    "        print(\"betalow\",betalow, \"betahigh\",betahigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(betahigh,betalow,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(betahigh,betalow,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"U-exponential\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = d81.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d81.rsample(torch.Size([1])).numpy()\n",
    "                    \n",
    "        samples2 = d82.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        \n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = d82.rsample(torch.Size([1])).numpy()\n",
    "        samples2 = 1.0 - samples2\n",
    "        samplesJoint = signals * samples1 - (signals - 1.0) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        plt.hist(samplesJoint,bins=500)\n",
    "        plt.show()\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "        \n",
    "        \n",
    "    tp_dataset = TensorDataset(tp_tensor[: int(trainSize * percentage_train_test)])\n",
    "    tp_dataset_testing = TensorDataset(tp_tensor[int(trainSize * (1.0-percentage_train_test)) :])\n",
    "    tp_dataloader = DataLoader(tp_dataset, batch_size=128, shuffle=True)\n",
    "    tp_dataloader_testing = DataLoader(tp_dataset_testing, batch_size=256, shuffle=False)\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "    decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for yes in range(n + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp[0, 0, yes] = 0\n",
    "            else:\n",
    "                dp[0, money, yes] = yes# + 1.0\n",
    "    for ppl in range(1,  n + 1):\n",
    "        for yes in range(n + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                minSoFar = 1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1 - cdf(offer,order)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order) * (1 + dp[ppl - 1, money, yes])\n",
    "                    else:\n",
    "                        res = (1 - cdf(offer,order,n-ppl)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order,n-ppl) * (1 + dp[ppl - 1, money, yes])\n",
    "                    if minSoFar > res.item():\n",
    "                        minSoFar = res.item()\n",
    "                        decision[ppl, money, yes] = offerIndex\n",
    "                dp[ppl, money, yes] = minSoFar\n",
    "    \n",
    "    print(\"dp\",dp[n, dpPrecision, 0])\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp_H = np.zeros([n + 1 , n + 1, dpPrecision + 1])\n",
    "    decision_H = np.zeros([n + 1 , n + 1, dpPrecision + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for i in range(1,n+1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp_H[i, 0, 0] = 1\n",
    "            else:\n",
    "                offer = money / dpPrecision\n",
    "                dp_H[i, 0, money] = 0#cdf(offer)# + 1.0\n",
    "    for i in range(1,n+1):\n",
    "        for ppl in range(1, i + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                maxSoFar = -1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1-cdf(offer,order)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    else:\n",
    "                        res = (1-cdf(offer,order,ppl-1)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    if maxSoFar < res.item():\n",
    "                        maxSoFar = res.item()\n",
    "                        decision_H[i, ppl, money] = offerIndex\n",
    "                dp_H[i, ppl, money] = maxSoFar\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loc 0.1 scale 0.1\n",
      "loc 0.9 scale 0.1\n",
      "dp 1.8651759624481201\n",
      "Supervised Aim: twopeak costsharing\n",
      "Train Epoch: 1 [0/10000 (0%)]\tLoss: 0.000422\n",
      "Train Epoch: 1 [1280/10000 (13%)]\tLoss: 0.000042\n",
      "Train Epoch: 1 [2560/10000 (25%)]\tLoss: 0.000020\n",
      "Train Epoch: 1 [3840/10000 (38%)]\tLoss: 0.000004\n",
      "Train Epoch: 1 [5120/10000 (51%)]\tLoss: 0.000001\n",
      "Train Epoch: 1 [6400/10000 (63%)]\tLoss: 0.000001\n",
      "Train Epoch: 1 [7680/10000 (76%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [8960/10000 (89%)]\tLoss: 0.000000\n",
      "NN 1 : tensor(2.5457)\n",
      "CS 1 : 2.5457\n",
      "DP 1 : 1.8762\n",
      "heuristic 1 : 1.8723\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.2000, 0.2002, 0.1997, 0.2003, 0.1998])\n",
      "tensor([0.2499, 0.2492, 0.2503, 0.2506, 1.0000])\n",
      "tensor([0.3334, 0.3338, 0.3328, 1.0000, 1.0000])\n",
      "tensor([0.5000, 0.5000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/10000 (0%)]\tLoss: 2.632812 testing loss: tensor(2.5457)\n",
      "Train Epoch: 1 [1280/10000 (13%)]\tLoss: 2.757812 testing loss: tensor(2.5411)\n",
      "Train Epoch: 1 [2560/10000 (25%)]\tLoss: 2.468750 testing loss: tensor(2.5372)\n",
      "Train Epoch: 1 [3840/10000 (38%)]\tLoss: 2.421875 testing loss: tensor(2.5336)\n",
      "Train Epoch: 1 [5120/10000 (51%)]\tLoss: 2.515624 testing loss: tensor(2.5268)\n",
      "Train Epoch: 1 [6400/10000 (63%)]\tLoss: 2.515625 testing loss: tensor(2.5183)\n",
      "Train Epoch: 1 [7680/10000 (76%)]\tLoss: 2.617187 testing loss: tensor(2.5118)\n",
      "Train Epoch: 1 [8960/10000 (89%)]\tLoss: 2.554693 testing loss: tensor(2.5109)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(2.4962)\n",
      "CS 2 : 2.5457\n",
      "DP 2 : 1.8762\n",
      "heuristic 2 : 1.8723\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.1097, 0.1839, 0.2399, 0.2782, 0.1883])\n",
      "tensor([0.1510, 0.2439, 0.2800, 0.3251, 1.0000])\n",
      "tensor([0.2295, 0.3578, 0.4127, 1.0000, 1.0000])\n",
      "tensor([0.4193, 0.5807, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 2 [0/10000 (0%)]\tLoss: 2.507812 testing loss: tensor(2.4936)\n",
      "Train Epoch: 2 [1280/10000 (13%)]\tLoss: 2.359401 testing loss: tensor(2.4458)\n",
      "Train Epoch: 2 [2560/10000 (25%)]\tLoss: 2.414062 testing loss: tensor(2.3833)\n",
      "Train Epoch: 2 [3840/10000 (38%)]\tLoss: 2.265625 testing loss: tensor(2.3026)\n",
      "Train Epoch: 2 [5120/10000 (51%)]\tLoss: 1.992188 testing loss: tensor(2.2610)\n",
      "Train Epoch: 2 [6400/10000 (63%)]\tLoss: 2.242183 testing loss: tensor(2.2292)\n",
      "Train Epoch: 2 [7680/10000 (76%)]\tLoss: 1.976562 testing loss: tensor(2.1423)\n",
      "Train Epoch: 2 [8960/10000 (89%)]\tLoss: 1.859399 testing loss: tensor(2.0579)\n",
      "penalty: 0.0\n",
      "NN 3 : tensor(2.0098)\n",
      "CS 3 : 2.5457\n",
      "DP 3 : 1.8762\n",
      "heuristic 3 : 1.8723\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0205, 0.0463, 0.4234, 0.4538, 0.0561])\n",
      "tensor([0.0353, 0.0772, 0.4338, 0.4538, 1.0000])\n",
      "tensor([0.0625, 0.1442, 0.7933, 1.0000, 1.0000])\n",
      "tensor([0.3616, 0.6384, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 3 [0/10000 (0%)]\tLoss: 2.335940 testing loss: tensor(2.0095)\n",
      "Train Epoch: 3 [1280/10000 (13%)]\tLoss: 1.867188 testing loss: tensor(2.0180)\n",
      "Train Epoch: 3 [2560/10000 (25%)]\tLoss: 2.070347 testing loss: tensor(2.0335)\n",
      "Train Epoch: 3 [3840/10000 (38%)]\tLoss: 2.093768 testing loss: tensor(2.0734)\n",
      "Train Epoch: 3 [5120/10000 (51%)]\tLoss: 1.992200 testing loss: tensor(2.0414)\n",
      "Train Epoch: 3 [6400/10000 (63%)]\tLoss: 1.804623 testing loss: tensor(2.0117)\n",
      "Train Epoch: 3 [7680/10000 (76%)]\tLoss: 2.101554 testing loss: tensor(2.0120)\n",
      "Train Epoch: 3 [8960/10000 (89%)]\tLoss: 2.062500 testing loss: tensor(2.0128)\n",
      "penalty: 0.0\n",
      "NN 4 : tensor(2.0073)\n",
      "CS 4 : 2.5457\n",
      "DP 4 : 1.8762\n",
      "heuristic 4 : 1.8723\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0146, 0.0318, 0.4201, 0.4855, 0.0481])\n",
      "tensor([0.0259, 0.0542, 0.4245, 0.4955, 1.0000])\n",
      "tensor([0.0526, 0.1199, 0.8276, 1.0000, 1.0000])\n",
      "tensor([0.3643, 0.6357, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 4 [0/10000 (0%)]\tLoss: 2.070312 testing loss: tensor(2.0067)\n",
      "Train Epoch: 4 [1280/10000 (13%)]\tLoss: 1.890625 testing loss: tensor(2.0079)\n",
      "Train Epoch: 4 [2560/10000 (25%)]\tLoss: 2.093750 testing loss: tensor(2.0182)\n",
      "Train Epoch: 4 [3840/10000 (38%)]\tLoss: 2.031273 testing loss: tensor(2.0887)\n",
      "Train Epoch: 4 [5120/10000 (51%)]\tLoss: 2.148520 testing loss: tensor(2.1771)\n",
      "Train Epoch: 4 [6400/10000 (63%)]\tLoss: 2.179688 testing loss: tensor(2.2656)\n",
      "Train Epoch: 4 [7680/10000 (76%)]\tLoss: 2.328170 testing loss: tensor(2.3509)\n",
      "Train Epoch: 4 [8960/10000 (89%)]\tLoss: 2.679754 testing loss: tensor(2.3772)\n",
      "penalty: 0.00029987096786499023\n",
      "NN 5 : tensor(2.3922)\n",
      "CS 5 : 2.5457\n",
      "DP 5 : 1.8762\n",
      "heuristic 5 : 1.8723\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0058, 0.0123, 0.4394, 0.5255, 0.0170])\n",
      "tensor([0.0105, 0.0212, 0.4420, 0.5263, 1.0000])\n",
      "tensor([0.0265, 0.0596, 0.9139, 1.0000, 1.0000])\n",
      "tensor([0.3692, 0.6308, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 5 [0/10000 (0%)]\tLoss: 2.102965 testing loss: tensor(2.3922)\n",
      "Train Epoch: 5 [1280/10000 (13%)]\tLoss: 1.956229 testing loss: tensor(2.0325)\n",
      "Train Epoch: 5 [2560/10000 (25%)]\tLoss: 1.812585 testing loss: tensor(2.0671)\n",
      "Train Epoch: 5 [3840/10000 (38%)]\tLoss: 2.000000 testing loss: tensor(2.0814)\n",
      "Train Epoch: 5 [5120/10000 (51%)]\tLoss: 1.945312 testing loss: tensor(2.0842)\n",
      "Train Epoch: 5 [6400/10000 (63%)]\tLoss: 2.093750 testing loss: tensor(2.0906)\n",
      "Train Epoch: 5 [7680/10000 (76%)]\tLoss: 2.015625 testing loss: tensor(2.0914)\n",
      "Train Epoch: 5 [8960/10000 (89%)]\tLoss: 1.914062 testing loss: tensor(2.1053)\n",
      "penalty: 0.0\n",
      "NN 6 : tensor(2.1237)\n",
      "CS 6 : 2.5457\n",
      "DP 6 : 1.8762\n",
      "heuristic 6 : 1.8723\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0261, 0.0400, 0.5834, 0.3100, 0.0405])\n",
      "tensor([0.0413, 0.0631, 0.5836, 0.3120, 1.0000])\n",
      "tensor([0.0528, 0.0939, 0.8532, 1.0000, 1.0000])\n",
      "tensor([0.4126, 0.5874, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 6 [0/10000 (0%)]\tLoss: 2.273437 testing loss: tensor(2.1248)\n",
      "Train Epoch: 6 [1280/10000 (13%)]\tLoss: 2.085964 testing loss: tensor(2.1726)\n",
      "Train Epoch: 6 [2560/10000 (25%)]\tLoss: 2.375000 testing loss: tensor(2.1748)\n",
      "Train Epoch: 6 [3840/10000 (38%)]\tLoss: 2.265625 testing loss: tensor(2.1831)\n",
      "Train Epoch: 6 [5120/10000 (51%)]\tLoss: 2.171875 testing loss: tensor(2.1919)\n",
      "Train Epoch: 6 [6400/10000 (63%)]\tLoss: 2.304688 testing loss: tensor(2.1842)\n",
      "Train Epoch: 6 [7680/10000 (76%)]\tLoss: 2.109375 testing loss: tensor(2.1825)\n",
      "Train Epoch: 6 [8960/10000 (89%)]\tLoss: 2.015625 testing loss: tensor(2.1809)\n",
      "penalty: 3.5762786865234375e-06\n",
      "NN 7 : tensor(2.1731)\n",
      "CS 7 : 2.5457\n",
      "DP 7 : 1.8762\n",
      "heuristic 7 : 1.8723\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0135, 0.0194, 0.7498, 0.1915, 0.0259])\n",
      "tensor([0.0217, 0.0316, 0.7519, 0.1947, 1.0000])\n",
      "tensor([0.0246, 0.0422, 0.9331, 1.0000, 1.0000])\n",
      "tensor([0.4143, 0.5857, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Train Epoch: 1 [0/10000 (0%)]\tLoss: 0.000472\n",
      "Train Epoch: 1 [1280/10000 (13%)]\tLoss: 0.000074\n",
      "Train Epoch: 1 [2560/10000 (25%)]\tLoss: 0.000010\n",
      "Train Epoch: 1 [3840/10000 (38%)]\tLoss: 0.000005\n",
      "Train Epoch: 1 [5120/10000 (51%)]\tLoss: 0.000003\n",
      "Train Epoch: 1 [6400/10000 (63%)]\tLoss: 0.000001\n",
      "Train Epoch: 1 [7680/10000 (76%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [8960/10000 (89%)]\tLoss: 0.000000\n",
      "NN 1 : tensor(2.5459)\n",
      "CS 1 : 2.5457\n",
      "DP 1 : 1.8762\n",
      "heuristic 1 : 1.8723\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.1999, 0.2000, 0.1998, 0.2004, 0.1999])\n",
      "tensor([0.2500, 0.2497, 0.2500, 0.2502, 1.0000])\n",
      "tensor([0.3340, 0.3331, 0.3329, 1.0000, 1.0000])\n",
      "tensor([0.5005, 0.4995, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/10000 (0%)]\tLoss: 2.609528 testing loss: tensor(2.5465)\n",
      "Train Epoch: 1 [1280/10000 (13%)]\tLoss: 2.446448 testing loss: tensor(2.4637)\n",
      "Train Epoch: 1 [2560/10000 (25%)]\tLoss: 2.117005 testing loss: tensor(2.0120)\n",
      "Train Epoch: 1 [3840/10000 (38%)]\tLoss: 1.965433 testing loss: tensor(1.9695)\n",
      "Train Epoch: 1 [5120/10000 (51%)]\tLoss: 1.918090 testing loss: tensor(1.9154)\n",
      "Train Epoch: 1 [6400/10000 (63%)]\tLoss: 1.962564 testing loss: tensor(1.8781)\n",
      "Train Epoch: 1 [7680/10000 (76%)]\tLoss: 2.057509 testing loss: tensor(1.8411)\n",
      "Train Epoch: 1 [8960/10000 (89%)]\tLoss: 1.879997 testing loss: tensor(1.8344)\n",
      "penalty: 0.02503267675638199\n",
      "NN 2 : tensor(1.8251)\n",
      "CS 2 : 2.5457\n",
      "DP 2 : 1.8762\n",
      "heuristic 2 : 1.8723\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0974, 0.7497, 0.0462, 0.0500, 0.0566])\n",
      "tensor([0.1095, 0.7541, 0.0668, 0.0696, 1.0000])\n",
      "tensor([0.1346, 0.7740, 0.0915, 1.0000, 1.0000])\n",
      "tensor([0.1776, 0.8224, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 2 [0/10000 (0%)]\tLoss: 1.773472 testing loss: tensor(1.8264)\n",
      "Train Epoch: 2 [1280/10000 (13%)]\tLoss: 1.921283 testing loss: tensor(1.8262)\n",
      "Train Epoch: 2 [2560/10000 (25%)]\tLoss: 1.807109 testing loss: tensor(1.8126)\n",
      "Train Epoch: 2 [3840/10000 (38%)]\tLoss: 1.788125 testing loss: tensor(1.8152)\n",
      "Train Epoch: 2 [5120/10000 (51%)]\tLoss: 1.670302 testing loss: tensor(1.8071)\n",
      "Train Epoch: 2 [6400/10000 (63%)]\tLoss: 2.025425 testing loss: tensor(1.8072)\n",
      "Train Epoch: 2 [7680/10000 (76%)]\tLoss: 1.759621 testing loss: tensor(1.7992)\n",
      "Train Epoch: 2 [8960/10000 (89%)]\tLoss: 1.838402 testing loss: tensor(1.7946)\n",
      "penalty: 0.0010935068130493164\n",
      "NN 3 : tensor(1.7849)\n",
      "CS 3 : 2.5457\n",
      "DP 3 : 1.8762\n",
      "heuristic 3 : 1.8723\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0729, 0.7399, 0.0642, 0.0648, 0.0581])\n",
      "tensor([0.0904, 0.7431, 0.0845, 0.0821, 1.0000])\n",
      "tensor([0.1235, 0.7558, 0.1207, 1.0000, 1.0000])\n",
      "tensor([0.1898, 0.8102, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 3 [0/10000 (0%)]\tLoss: 1.705444 testing loss: tensor(1.7874)\n",
      "Train Epoch: 3 [1280/10000 (13%)]\tLoss: 1.704583 testing loss: tensor(1.7884)\n",
      "Train Epoch: 3 [2560/10000 (25%)]\tLoss: 1.817116 testing loss: tensor(1.7898)\n",
      "Train Epoch: 3 [3840/10000 (38%)]\tLoss: 1.723203 testing loss: tensor(1.7808)\n",
      "Train Epoch: 3 [5120/10000 (51%)]\tLoss: 1.734443 testing loss: tensor(1.7836)\n",
      "Train Epoch: 3 [6400/10000 (63%)]\tLoss: 1.665892 testing loss: tensor(1.7774)\n",
      "Train Epoch: 3 [7680/10000 (76%)]\tLoss: 1.980962 testing loss: tensor(1.7803)\n",
      "Train Epoch: 3 [8960/10000 (89%)]\tLoss: 1.657052 testing loss: tensor(1.7790)\n",
      "penalty: 0.0\n",
      "NN 4 : tensor(1.7849)\n",
      "CS 4 : 2.5457\n",
      "DP 4 : 1.8762\n",
      "heuristic 4 : 1.8723\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0694, 0.7318, 0.0673, 0.0701, 0.0613])\n",
      "tensor([0.0890, 0.7345, 0.0873, 0.0892, 1.0000])\n",
      "tensor([0.1205, 0.7493, 0.1302, 1.0000, 1.0000])\n",
      "tensor([0.2033, 0.7967, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 4 [0/10000 (0%)]\tLoss: 1.695068 testing loss: tensor(1.7843)\n",
      "Train Epoch: 4 [1280/10000 (13%)]\tLoss: 1.740162 testing loss: tensor(1.7806)\n",
      "Train Epoch: 4 [2560/10000 (25%)]\tLoss: 1.978169 testing loss: tensor(1.7849)\n",
      "Train Epoch: 4 [3840/10000 (38%)]\tLoss: 1.928329 testing loss: tensor(1.7834)\n",
      "Train Epoch: 4 [5120/10000 (51%)]\tLoss: 1.828988 testing loss: tensor(1.7841)\n",
      "Train Epoch: 4 [6400/10000 (63%)]\tLoss: 1.677737 testing loss: tensor(1.7807)\n",
      "Train Epoch: 4 [7680/10000 (76%)]\tLoss: 1.934618 testing loss: tensor(1.7856)\n",
      "Train Epoch: 4 [8960/10000 (89%)]\tLoss: 1.736031 testing loss: tensor(1.7823)\n",
      "penalty: 0.0007972121238708496\n",
      "NN 5 : tensor(1.7798)\n",
      "CS 5 : 2.5457\n",
      "DP 5 : 1.8762\n",
      "heuristic 5 : 1.8723\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0699, 0.7332, 0.0676, 0.0667, 0.0625])\n",
      "tensor([0.0883, 0.7388, 0.0872, 0.0857, 1.0000])\n",
      "tensor([0.1185, 0.7515, 0.1299, 1.0000, 1.0000])\n",
      "tensor([0.2061, 0.7939, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 5 [0/10000 (0%)]\tLoss: 1.694953 testing loss: tensor(1.7822)\n",
      "Train Epoch: 5 [1280/10000 (13%)]\tLoss: 1.742678 testing loss: tensor(1.7830)\n",
      "Train Epoch: 5 [2560/10000 (25%)]\tLoss: 1.806788 testing loss: tensor(1.7871)\n",
      "Train Epoch: 5 [3840/10000 (38%)]\tLoss: 1.537708 testing loss: tensor(1.7789)\n",
      "Train Epoch: 5 [5120/10000 (51%)]\tLoss: 1.910730 testing loss: tensor(1.7844)\n",
      "Train Epoch: 5 [6400/10000 (63%)]\tLoss: 1.749059 testing loss: tensor(1.7859)\n",
      "Train Epoch: 5 [7680/10000 (76%)]\tLoss: 1.789782 testing loss: tensor(1.7843)\n",
      "Train Epoch: 5 [8960/10000 (89%)]\tLoss: 1.695539 testing loss: tensor(1.7837)\n",
      "penalty: 0.014127194881439209\n",
      "NN 6 : tensor(1.7814)\n",
      "CS 6 : 2.5457\n",
      "DP 6 : 1.8762\n",
      "heuristic 6 : 1.8723\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0667, 0.7447, 0.0612, 0.0643, 0.0631])\n",
      "tensor([0.0869, 0.7491, 0.0804, 0.0837, 1.0000])\n",
      "tensor([0.1163, 0.7616, 0.1221, 1.0000, 1.0000])\n",
      "tensor([0.2045, 0.7955, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 6 [0/10000 (0%)]\tLoss: 1.749911 testing loss: tensor(1.7820)\n",
      "Train Epoch: 6 [1280/10000 (13%)]\tLoss: 1.945007 testing loss: tensor(1.7875)\n",
      "Train Epoch: 6 [2560/10000 (25%)]\tLoss: 1.884849 testing loss: tensor(1.7846)\n",
      "Train Epoch: 6 [3840/10000 (38%)]\tLoss: 1.843732 testing loss: tensor(1.7826)\n",
      "Train Epoch: 6 [5120/10000 (51%)]\tLoss: 1.863223 testing loss: tensor(1.7816)\n",
      "Train Epoch: 6 [6400/10000 (63%)]\tLoss: 1.860487 testing loss: tensor(1.7794)\n",
      "Train Epoch: 6 [7680/10000 (76%)]\tLoss: 1.772516 testing loss: tensor(1.7852)\n",
      "Train Epoch: 6 [8960/10000 (89%)]\tLoss: 1.791125 testing loss: tensor(1.7799)\n",
      "penalty: 0.0\n",
      "NN 7 : tensor(1.7883)\n",
      "CS 7 : 2.5457\n",
      "DP 7 : 1.8762\n",
      "heuristic 7 : 1.8723\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0650, 0.7450, 0.0637, 0.0647, 0.0616])\n",
      "tensor([0.0841, 0.7529, 0.0805, 0.0824, 1.0000])\n",
      "tensor([0.1156, 0.7639, 0.1205, 1.0000, 1.0000])\n",
      "tensor([0.2110, 0.7890, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Train Epoch: 1 [0/10000 (0%)]\tLoss: 0.000193\n",
      "Train Epoch: 1 [1280/10000 (13%)]\tLoss: 0.000019\n",
      "Train Epoch: 1 [2560/10000 (25%)]\tLoss: 0.000005\n",
      "Train Epoch: 1 [3840/10000 (38%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [5120/10000 (51%)]\tLoss: 0.000001\n",
      "Train Epoch: 1 [6400/10000 (63%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [7680/10000 (76%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [8960/10000 (89%)]\tLoss: 0.000000\n",
      "NN 1 : tensor(2.5455)\n",
      "CS 1 : 2.5457\n",
      "DP 1 : 1.8762\n",
      "heuristic 1 : 1.8723\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.2000, 0.1999, 0.1999, 0.1999, 0.2003])\n",
      "tensor([0.2503, 0.2501, 0.2498, 0.2498, 1.0000])\n",
      "tensor([0.3336, 0.3330, 0.3335, 1.0000, 1.0000])\n",
      "tensor([0.5002, 0.4998, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/10000 (0%)]\tLoss: 2.525450 testing loss: tensor(2.5448)\n",
      "Train Epoch: 1 [1280/10000 (13%)]\tLoss: 2.476301 testing loss: tensor(2.4379)\n",
      "Train Epoch: 1 [2560/10000 (25%)]\tLoss: 2.033122 testing loss: tensor(2.0438)\n",
      "Train Epoch: 1 [3840/10000 (38%)]\tLoss: 1.992034 testing loss: tensor(1.9871)\n",
      "Train Epoch: 1 [5120/10000 (51%)]\tLoss: 1.852301 testing loss: tensor(1.9625)\n",
      "Train Epoch: 1 [6400/10000 (63%)]\tLoss: 1.951235 testing loss: tensor(1.9543)\n",
      "Train Epoch: 1 [7680/10000 (76%)]\tLoss: 1.977045 testing loss: tensor(1.9475)\n",
      "Train Epoch: 1 [8960/10000 (89%)]\tLoss: 1.890651 testing loss: tensor(1.9497)\n",
      "penalty: 0.010762155055999756\n",
      "NN 2 : tensor(1.9366)\n",
      "CS 2 : 2.5457\n",
      "DP 2 : 1.8762\n",
      "heuristic 2 : 1.8723\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0225, 0.0179, 0.3934, 0.5477, 0.0185])\n",
      "tensor([0.0341, 0.0272, 0.3916, 0.5472, 1.0000])\n",
      "tensor([0.1295, 0.1003, 0.7702, 1.0000, 1.0000])\n",
      "tensor([0.5631, 0.4369, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 2 [0/10000 (0%)]\tLoss: 1.981705 testing loss: tensor(1.9352)\n",
      "Train Epoch: 2 [1280/10000 (13%)]\tLoss: 1.807433 testing loss: tensor(1.9337)\n",
      "Train Epoch: 2 [2560/10000 (25%)]\tLoss: 1.930809 testing loss: tensor(1.9321)\n",
      "Train Epoch: 2 [3840/10000 (38%)]\tLoss: 2.065024 testing loss: tensor(1.9280)\n",
      "Train Epoch: 2 [5120/10000 (51%)]\tLoss: 1.815395 testing loss: tensor(1.9066)\n",
      "Train Epoch: 2 [6400/10000 (63%)]\tLoss: 1.948473 testing loss: tensor(1.9073)\n",
      "Train Epoch: 2 [7680/10000 (76%)]\tLoss: 1.952359 testing loss: tensor(1.8929)\n",
      "Train Epoch: 2 [8960/10000 (89%)]\tLoss: 2.052621 testing loss: tensor(1.8793)\n",
      "penalty: 0.020202413201332092\n",
      "NN 3 : tensor(1.8609)\n",
      "CS 3 : 2.5457\n",
      "DP 3 : 1.8762\n",
      "heuristic 3 : 1.8723\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0156, 0.0152, 0.1926, 0.7602, 0.0165])\n",
      "tensor([0.0232, 0.0220, 0.1940, 0.7608, 1.0000])\n",
      "tensor([0.1219, 0.1171, 0.7611, 1.0000, 1.0000])\n",
      "tensor([0.7344, 0.2656, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 3 [0/10000 (0%)]\tLoss: 1.817539 testing loss: tensor(1.8648)\n",
      "Train Epoch: 3 [1280/10000 (13%)]\tLoss: 1.927806 testing loss: tensor(1.8599)\n",
      "Train Epoch: 3 [2560/10000 (25%)]\tLoss: 1.714680 testing loss: tensor(1.8461)\n",
      "Train Epoch: 3 [3840/10000 (38%)]\tLoss: 1.808077 testing loss: tensor(1.8456)\n",
      "Train Epoch: 3 [5120/10000 (51%)]\tLoss: 1.735742 testing loss: tensor(1.8295)\n",
      "Train Epoch: 3 [6400/10000 (63%)]\tLoss: 1.709325 testing loss: tensor(1.8216)\n",
      "Train Epoch: 3 [7680/10000 (76%)]\tLoss: 1.824527 testing loss: tensor(1.8124)\n",
      "Train Epoch: 3 [8960/10000 (89%)]\tLoss: 1.777186 testing loss: tensor(1.8003)\n",
      "penalty: 0.003871619701385498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN 4 : tensor(1.7975)\n",
      "CS 4 : 2.5457\n",
      "DP 4 : 1.8762\n",
      "heuristic 4 : 1.8723\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0286, 0.0580, 0.1035, 0.7472, 0.0627])\n",
      "tensor([0.0404, 0.0672, 0.1302, 0.7623, 1.0000])\n",
      "tensor([0.1165, 0.1350, 0.7485, 1.0000, 1.0000])\n",
      "tensor([0.7750, 0.2250, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 4 [0/10000 (0%)]\tLoss: 1.844510 testing loss: tensor(1.7967)\n",
      "Train Epoch: 4 [1280/10000 (13%)]\tLoss: 1.623065 testing loss: tensor(1.7945)\n",
      "Train Epoch: 4 [2560/10000 (25%)]\tLoss: 1.859325 testing loss: tensor(1.7889)\n",
      "Train Epoch: 4 [3840/10000 (38%)]\tLoss: 1.641940 testing loss: tensor(1.7902)\n",
      "Train Epoch: 4 [5120/10000 (51%)]\tLoss: 1.651636 testing loss: tensor(1.7899)\n",
      "Train Epoch: 4 [6400/10000 (63%)]\tLoss: 1.689195 testing loss: tensor(1.7845)\n",
      "Train Epoch: 4 [7680/10000 (76%)]\tLoss: 1.804716 testing loss: tensor(1.7837)\n",
      "Train Epoch: 4 [8960/10000 (89%)]\tLoss: 1.799233 testing loss: tensor(1.7861)\n",
      "penalty: 0.0\n",
      "NN 5 : tensor(1.7859)\n",
      "CS 5 : 2.5457\n",
      "DP 5 : 1.8762\n",
      "heuristic 5 : 1.8723\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0589, 0.0656, 0.0655, 0.7466, 0.0634])\n",
      "tensor([0.0742, 0.0770, 0.0920, 0.7568, 1.0000])\n",
      "tensor([0.1238, 0.1189, 0.7573, 1.0000, 1.0000])\n",
      "tensor([0.7651, 0.2349, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 5 [0/10000 (0%)]\tLoss: 1.905533 testing loss: tensor(1.7884)\n",
      "Train Epoch: 5 [1280/10000 (13%)]\tLoss: 1.720499 testing loss: tensor(1.7807)\n",
      "Train Epoch: 5 [2560/10000 (25%)]\tLoss: 1.879244 testing loss: tensor(1.7876)\n",
      "Train Epoch: 5 [3840/10000 (38%)]\tLoss: 1.816588 testing loss: tensor(1.7884)\n",
      "Train Epoch: 5 [5120/10000 (51%)]\tLoss: 1.707068 testing loss: tensor(1.7934)\n",
      "Train Epoch: 5 [6400/10000 (63%)]\tLoss: 1.711296 testing loss: tensor(1.7871)\n",
      "Train Epoch: 5 [7680/10000 (76%)]\tLoss: 1.764854 testing loss: tensor(1.7837)\n",
      "Train Epoch: 5 [8960/10000 (89%)]\tLoss: 1.521446 testing loss: tensor(1.7843)\n",
      "penalty: 0.0014649629592895508\n",
      "NN 6 : tensor(1.7850)\n",
      "CS 6 : 2.5457\n",
      "DP 6 : 1.8762\n",
      "heuristic 6 : 1.8723\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0674, 0.0662, 0.0644, 0.7350, 0.0670])\n",
      "tensor([0.0828, 0.0772, 0.0924, 0.7476, 1.0000])\n",
      "tensor([0.1166, 0.1103, 0.7731, 1.0000, 1.0000])\n",
      "tensor([0.7713, 0.2287, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 6 [0/10000 (0%)]\tLoss: 1.674453 testing loss: tensor(1.7874)\n",
      "Train Epoch: 6 [1280/10000 (13%)]\tLoss: 1.824137 testing loss: tensor(1.7806)\n",
      "Train Epoch: 6 [2560/10000 (25%)]\tLoss: 1.602950 testing loss: tensor(1.7959)\n",
      "Train Epoch: 6 [3840/10000 (38%)]\tLoss: 1.809795 testing loss: tensor(1.7818)\n",
      "Train Epoch: 6 [5120/10000 (51%)]\tLoss: 1.873909 testing loss: tensor(1.7837)\n",
      "Train Epoch: 6 [6400/10000 (63%)]\tLoss: 1.844571 testing loss: tensor(1.7887)\n",
      "Train Epoch: 6 [7680/10000 (76%)]\tLoss: 1.828772 testing loss: tensor(1.7848)\n",
      "Train Epoch: 6 [8960/10000 (89%)]\tLoss: 1.630192 testing loss: tensor(1.7814)\n",
      "penalty: 0.0021209716796875\n",
      "NN 7 : tensor(1.7823)\n",
      "CS 7 : 2.5457\n",
      "DP 7 : 1.8762\n",
      "heuristic 7 : 1.8723\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0618, 0.0669, 0.0672, 0.7388, 0.0654])\n",
      "tensor([0.0783, 0.0793, 0.0944, 0.7480, 1.0000])\n",
      "tensor([0.1140, 0.1209, 0.7650, 1.0000, 1.0000])\n",
      "tensor([0.7609, 0.2391, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "    \n",
    "for order in stage:\n",
    "    producedata(order)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "    #running\n",
    "    for order1 in order1name:\n",
    "        print(\"Supervised Aim:\",order,order1)\n",
    "        for trainingnumber in numberofpeople:\n",
    "            # for mapping binary to payments before softmax\n",
    "            model = nn.Sequential(\n",
    "                nn.Linear(n, 100),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(100, 100),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(100, 100),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(100, n),\n",
    "            )\n",
    "            model.apply(init_weights)\n",
    "            # optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "            runningLossNN = []\n",
    "            runningLossCS = []\n",
    "            runningLossDP = []\n",
    "            runningLossHeuristic = []\n",
    "            #model=torch.load(\"save/pytorchNN=5dp1\");\n",
    "            #model.eval()\n",
    "            ##order1name=[\"costsharing\",\"dp\",\"heuristic\",\"random initializing\"]\n",
    "            for epoch in range(1, supervisionEpochs + 1):\n",
    "        #             print(\"distributionRatio\",distributionRatio)\n",
    "                if(order1==\"costsharing\"):\n",
    "                    supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "                elif(order1==\"dp\"):\n",
    "                    supervisionTrain(epoch, dpSupervisionRule)\n",
    "                elif(order1==\"heuristic\"):\n",
    "                    supervisionTrain(epoch, heuristicSupervisionRule)\n",
    "                elif(order1==\"random initializing\"):\n",
    "                    print(\"do nothing\");\n",
    "\n",
    "            test()\n",
    "\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                if(trainingnumber=='0'):\n",
    "                    train0(epoch)\n",
    "                if(trainingnumber=='1'):\n",
    "                    train1(epoch)\n",
    "                if(trainingnumber=='2'):\n",
    "                    train2(epoch)\n",
    "                test()\n",
    "            losslistname.append(order+\" \"+order1+\" choose people:\"+trainingnumber);\n",
    "            losslist.append(losslisttemp);\n",
    "            losslisttemp=[];\n",
    "            savepath=\"save/pytorchNN=5all-phoenix\"+order+str(order1)+trainingnumber\n",
    "            torch.save(model, savepath);\n",
    "            print(\"end\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colorlist=[\"#D2691E\",'#4169E1',\"#9ACD32\",\"#B22222\",\"#FF00FF\",\"#708090\"]\n",
    "for i in range(len(losslist)):\n",
    "    plt.plot(losslist[i], '.-', color=colorlist[i], alpha=0.8, label=losslistname[i])\n",
    "\n",
    "# 显示标签，如果不加这句，即使加了label='一些数字'的参数，最终还是不会显示标签\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('trianing times')\n",
    "plt.ylabel('delay')\n",
    "plt.title(\"n=\"+str(n))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
