{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3540388371733616\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from random import randint\n",
    "import random\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "from pylab import *\n",
    "from scipy.stats import beta\n",
    "import math\n",
    "n = 3\n",
    "epochs = 1\n",
    "supervisionEpochs = 1\n",
    "lr1 = 0.01\n",
    "lr2 = 0.0005\n",
    "log_interval = 10\n",
    "trainSize = 50000#100000\n",
    "percentage_train_test= 0.6\n",
    "penaltyLambda = 500\n",
    "doublePeakHighMean = 0.6\n",
    "doublePeakLowMean = 0.2\n",
    "doublePeakStd = 0.1\n",
    "uniformlow=0\n",
    "uniformhigh=1.0\n",
    "normalloc = 0.2\n",
    "normalscale = 0.1\n",
    "\n",
    "cauchyloc = 1/n\n",
    "cauchyscalen = 0.004\n",
    "\n",
    "exponentialhigh = 15 #Symbol(\"b\", real=True)\n",
    "exponentiallow  = 15 #Symbol(\"a\", real=True)\n",
    "\n",
    "\n",
    "beta_a = 0.1\n",
    "beta_b  = 0.1\n",
    "kumaraswamy_a = beta_a \n",
    "kumaraswamy_b = (1.0+(beta_a-1.0)*math.pow( (beta_a+beta_b-2.0)/(beta_a-1.0), beta_a) )/beta_a \n",
    "print(kumaraswamy_b)\n",
    "\n",
    "\n",
    "independentnormalloc1=[(float(ii)+1)/(2*n+1) for ii in range(n,0,-1)]\n",
    "independentnormalscale1=[0.05 for ii in range(n)]\n",
    "\n",
    "independentnormalloc2=[(float(ii)+1)/(2*n+1) for ii in range(1,n+1,1)]\n",
    "independentnormalscale2=[0.05 for ii in range(n)]\n",
    "stage=[\"beta\"]\n",
    "# \"twopeak\",\"normal\",\"uniform\",\"independent1\",\"independent2\",\"cauchy\",\"beta\",\"U-exponential\",\"arcsine\"\n",
    "order1name=[\"dp\",\"random initializing\",\"costsharing\",\"heuristic\"]\n",
    "# \"costsharing\",\"dp\",\"heuristic\",\"random initializing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio1 = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase1 = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "d3 = D.normal.Normal(normalloc, normalscale)\n",
    "distributionRatio3 = d3.cdf(1) - d3.cdf(0)\n",
    "distributionBase3 = d3.cdf(0)\n",
    "\n",
    "d4 = D.uniform.Uniform(uniformlow,uniformhigh)\n",
    "distributionRatio4 = d4.cdf(1) - d4.cdf(0)\n",
    "distributionBase4 = d4.cdf(0)\n",
    "\n",
    "d5 = [D.normal.Normal(independentnormalloc1[ii], independentnormalscale1[ii]) for ii in range(n)]\n",
    "d6 = [D.normal.Normal(independentnormalloc2[ii], independentnormalscale2[ii]) for ii in range(n)]\n",
    "\n",
    "d7 = D.cauchy.Cauchy(cauchyloc,cauchyscalen)\n",
    "\n",
    "d81 = D.exponential.Exponential(exponentiallow)\n",
    "d82 = D.exponential.Exponential(exponentialhigh)\n",
    "\n",
    "d9 = D.beta.Beta(beta_a,beta_b)\n",
    "#sample_d9=d9.rsample(torch.Size([100000]))\n",
    "\n",
    "#d9_sample=np.linspace(0.0001, 0.9999, 10000)\n",
    "#d9_pdf=torch.exp(d9.log_prob(torch.tensor(d9_sample,dtype=torch.float32)))\n",
    "#d9_delta=d9_sample[1]-d9_sample[0]\n",
    "#d9_sum_pdf=torch.sum(d9_pdf*d9_delta)\n",
    "\n",
    "d10 = D.beta.Beta(0.5,0.5)\n",
    "\n",
    "def beta_cdf(x,y, i=None):\n",
    "    return beta.cdf(x, beta_a, beta_b)\n",
    "\n",
    "\n",
    "def cdf(x,y, i=None):\n",
    "    if(y==\"twopeak\"):\n",
    "        return (d1.cdf(x) + d2.cdf(x) - distributionBase1) / 2 / distributionRatio1\n",
    "    elif(y==\"normal\"):\n",
    "        return (d3.cdf(x)-distributionBase3)/distributionRatio3;\n",
    "    elif(y==\"uniform\"):\n",
    "        return (d4.cdf(x)-distributionBase4)/distributionRatio4;\n",
    "    elif(y==\"independent1\"):\n",
    "        return d5[i].cdf(x);\n",
    "    elif(y==\"independent2\"):\n",
    "        return d6[i].cdf(x);\n",
    "    elif(y==\"cauchy\"):\n",
    "        return d7.cdf(x);\n",
    "    elif(y==\"beta\"):\n",
    "#         sum_cdf=0.0;\n",
    "#         if(x<0.0001):\n",
    "#             x=0.00011;\n",
    "#         if(x>0.9999):\n",
    "#             x=0.99989;\n",
    "#         for i in range(len(d9_pdf)):\n",
    "#             if(d9_sample[i]<x):\n",
    "#                 sum_cdf+=d9_pdf[i]*d9_delta;\n",
    "#             else:\n",
    "#                 sum_cdf+=(d9_pdf[i]+d9_pdf[i-1])/ 2 *(x-d9_sample[i-1])\n",
    "#                 break;\n",
    "#         return sum_cdf/d9_sum_pdf\n",
    "#         cdf_v=torch.sum((sample_d9<(x)), dtype=torch.float32)/100000\n",
    "#         return cdf_v\n",
    "#    F(x|a,b)=1–(1–x^a)^b\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(x,kumaraswamy_a),kumaraswamy_b);\n",
    "        except:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(torch.tensor(x,dtype=torch.float32),kumaraswamy_a),kumaraswamy_b);\n",
    "    elif(y==\"arcsine\"):\n",
    "        #\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            res=2.0/math.pi * torch.asin(torch.sqrt(x))\n",
    "            #print(x)\n",
    "            return res# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "        except:\n",
    "            return 2.0/math.pi * torch.asin(torch.sqrt(torch.tensor(x,dtype=torch.float32)))# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "    elif(y==\"U-exponential\"):\n",
    "        return (d81.cdf(x) + (1.0 - d82.cdf(1.0-x)))  / 2 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    if torch.sum(bits).item() == 0:\n",
    "        return torch.ones(n)\n",
    "    bits = bits.type(torch.float32)\n",
    "    negBits = torch.ones(n) - bits\n",
    "    payments = model(bits)\n",
    "    payments = payments - 1000 * negBits\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    payments = payments + negBits\n",
    "    return payments\n",
    "\n",
    "def tpToBits(tp, bits=torch.ones(n).type(torch.uint8)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = (tp >= payments).type(torch.uint8)\n",
    "    if torch.equal(newBits, bits):\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits(tp, newBits)#bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    return n - torch.sum(tpToBits(tp).type(torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    global  dp,decision\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money, yes]\n",
    "        offer = float(offerIndex) / dpPrecision\n",
    "        if tp[i] >= offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits[i] = 0\n",
    "            payments[i] = 1\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "#     print()\n",
    "#     print(tp)\n",
    "#     print(bits)\n",
    "#     print(payments)\n",
    "#     print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def heuristicSupervisionRule(tp):\n",
    "    global  dp_H,decision_H\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    tempold=-1;\n",
    "    for turns in range(n,0,-1):\n",
    "        money = dpPrecision\n",
    "        j=0\n",
    "        tempo=sum(bits)\n",
    "        #print(\"bits\",tempo)\n",
    "        for i in range(n):\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            \n",
    "            offerIndex = decision_H[tempo ,tempo - i, money]\n",
    "            offer = float(offerIndex) / dpPrecision\n",
    "            while(j<n):\n",
    "                if(bits[j]!=0):\n",
    "                    break;\n",
    "                j+=1;\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            if tp[j] >= offer:\n",
    "                #print(money,j,tp[j],offer)\n",
    "                money -= offerIndex\n",
    "                bits[j] = 1\n",
    "                payments[j] = offer\n",
    "            else:\n",
    "                bits[j] = 0;\n",
    "                payments[j] = 1.0;\n",
    "            j+=1;\n",
    "        #print(\"money\",money)\n",
    "        if(money==0 and tempold==tempo):\n",
    "            break;\n",
    "        tempold=tempo;\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    for k in range(n, -1, -1):\n",
    "        if k == 0:\n",
    "            break\n",
    "        bits = [1 if tp[ii] >= 1.0 / k else 0 for ii in range(n)]\n",
    "        if sum(bits) == k:\n",
    "            break\n",
    "    if k == 0:\n",
    "        payments = [1 for ii in range(n)]\n",
    "    else:\n",
    "        payments = [1.0 / k if bits[ii] == 1 else 1 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    return float(n - torch.sum(costSharingSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def dpDelay(tp):\n",
    "    return float(n - torch.sum(dpSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def heuristicDelay(tp):\n",
    "    return float(n - torch.sum(heuristicSupervisionRule(tp)[0]).item())\n",
    "\n",
    "\n",
    "# templire=0;\n",
    "# num=1\n",
    "# for i in range(num):\n",
    "#     temp=torch.tensor(samplesJoint[i])\n",
    "#     #print(temp)\n",
    "#     #print(temp);\n",
    "#     #print(heuristicSupervisionRule(temp))\n",
    "#     #print(dpSupervisionRule(temp))\n",
    "#     #print(dpDelay(temp))\n",
    "#     res=dpDelay(temp)\n",
    "#     templire+=res\n",
    "#     #print(\"Delay\",res)\n",
    "#     #print()\n",
    "# print(templire/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordAndReport(name, source, loss):\n",
    "    source.append(loss)\n",
    "    realLength = len(source)\n",
    "    #print(f\"{name} ({realLength}): {loss}\")\n",
    "    print(name,realLength,\":\" ,loss)\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    \n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda /100000\n",
    "        \n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % (log_interval*5) == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "losslist=[];\n",
    "losslistname=[];\n",
    "losslisttemp=[];\n",
    "def test_batch():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lenLoss= 0\n",
    "        nnLoss = 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "            for tp in tp_batch:\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        nnLoss/=lenLoss\n",
    "    return nnLoss\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                Delay1 = tpToTotalDelay(tp1)\n",
    "                Delay0 = tpToTotalDelay(tp0)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                    loss = loss + (1 - cdf(offer,order)) * Delay1 + cdf(offer,order) * Delay0\n",
    "                else:\n",
    "                    loss = loss + (1 - cdf(offer,order,i)) * Delay1 + cdf(offer,order,i) * Delay0\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "    \n",
    "allBits = [torch.tensor(bits, dtype=torch.int16) for bits in itertools.product([0, 1], repeat=n)]\n",
    "\n",
    "def test():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        costSharingLoss = 0\n",
    "        dpLoss = 0\n",
    "        nnLoss = 0\n",
    "        heuristicLoss=0\n",
    "        lenLoss= 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "                heuristicLoss+= heuristicDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        costSharingLoss /= lenLoss\n",
    "        dpLoss /= lenLoss\n",
    "        nnLoss /= lenLoss\n",
    "        heuristicLoss /= lenLoss\n",
    "        #print(lenLoss)\n",
    "        losslisttemp.append(nnLoss);\n",
    "        recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "        recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "        recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "        recordAndReport(\"heuristic\", runningLossHeuristic, heuristicLoss)\n",
    "        print(\"DP:\",dp[n, dpPrecision,0])\n",
    "        #for i in range(n, 0, -1):\n",
    "        #    print(\"Heuristic:\",i,5*(1-dp_H[i, i, dpPrecision]))\n",
    "        for i in range(n, 0, -1):\n",
    "            print(\n",
    "                    tpToPayments(\n",
    "                            torch.tensor([0 if ii >= i else 1 for ii in range(n)], dtype=torch.float32)\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "dpPrecision = 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def producedata(order):\n",
    "    global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "    dpPrecision = 100 \n",
    "    if(order==\"twopeak\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = np.random.normal(\n",
    "            loc=doublePeakLowMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=doublePeakLowMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samples2 = np.random.normal(\n",
    "            loc=doublePeakHighMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = np.random.normal(\n",
    "                        loc=doublePeakHighMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "    elif(order==\"normal\"):\n",
    "        print(\"loc\",normalloc, \"scale\",normalscale)\n",
    "        samples1 = np.random.normal(\n",
    "            loc=normalloc, scale=normalscale, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=normalloc, scale=normalscale\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"uniform\"):  \n",
    "        print(\"uniformlow\",uniformlow, \"uniformhigh\",uniformhigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        uniformlow, uniformhigh\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent1\"):\n",
    "        print(\"loc\",independentnormalloc1,\"scale\",independentnormalscale1)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent2\"):\n",
    "        print(\"loc\",independentnormalloc2, \"scale\",independentnormalscale2)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"cauchy\"):\n",
    "        print(\"cauchyloc\",cauchyloc, \"cauchyscale\",cauchyscalen)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"beta\"):\n",
    "        print(\"beta_a\",beta_a, \"beta_b\",beta_b)\n",
    "        print(\"kumaraswamy_a\",kumaraswamy_a, \"kumaraswamy_b\",kumaraswamy_b)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"arcsine\"):\n",
    "        print(\"betalow\",0.5, \"betahigh\",0.5)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"U-exponential\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = d81.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d81.rsample(torch.Size([1])).numpy()\n",
    "                    \n",
    "        samples2 = d82.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        \n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = d82.rsample(torch.Size([1])).numpy()\n",
    "        samples2 = 1.0 - samples2\n",
    "        samplesJoint = signals * samples1 - (signals - 1.0) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "        \n",
    "    plt.hist(samplesJoint,bins=500)\n",
    "    plt.show()\n",
    "    \n",
    "    tp_dataset = TensorDataset(tp_tensor[: int(trainSize * percentage_train_test)])\n",
    "    tp_dataset_testing = TensorDataset(tp_tensor[int(trainSize * (1.0-percentage_train_test)) :])\n",
    "    tp_dataloader = DataLoader(tp_dataset, batch_size=128, shuffle=True)\n",
    "    tp_dataloader_testing = DataLoader(tp_dataset_testing, batch_size=256, shuffle=False)\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "    decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for yes in range(n + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp[0, 0, yes] = 0\n",
    "            else:\n",
    "                dp[0, money, yes] = yes# + 1.0\n",
    "    for ppl in range(1,  n + 1):\n",
    "        for yes in range(n + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                minSoFar = 1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order==\"beta\"):\n",
    "                        res = (1 - beta_cdf(offer,order)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + beta_cdf(offer,order) * (1 + dp[ppl - 1, money, yes])\n",
    "#                     if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "#                         res = (1 - cdf(offer,order)) * dp[\n",
    "#                         ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "#                         ] + cdf(offer,order) * (1 + dp[ppl - 1, money, yes])\n",
    "#                     else:\n",
    "#                         res = (1 - cdf(offer,order,n-ppl)) * dp[\n",
    "#                         ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "#                         ] + cdf(offer,order,n-ppl) * (1 + dp[ppl - 1, money, yes])\n",
    "                    if minSoFar > res.item():\n",
    "                        minSoFar = res.item()\n",
    "                        decision[ppl, money, yes] = offerIndex\n",
    "                dp[ppl, money, yes] = minSoFar\n",
    "    \n",
    "    print(\"dp\",dp[n, dpPrecision, 0])\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp_H = np.zeros([n + 1 , n + 1, dpPrecision + 1])\n",
    "    decision_H = np.zeros([n + 1 , n + 1, dpPrecision + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for i in range(1,n+1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp_H[i, 0, 0] = 1\n",
    "            else:\n",
    "                offer = money / dpPrecision\n",
    "                dp_H[i, 0, money] = 0#cdf(offer)# + 1.0\n",
    "    for i in range(1,n+1):\n",
    "        for ppl in range(1, i + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                maxSoFar = -1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1-cdf(offer,order)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    else:\n",
    "                        res = (1-cdf(offer,order,ppl-1)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    if maxSoFar < res.item():\n",
    "                        maxSoFar = res.item()\n",
    "                        decision_H[i, ppl, money] = offerIndex\n",
    "                dp_H[i, ppl, money] = maxSoFar\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_a 0.1 beta_b 0.1\n",
      "kumaraswamy_a 0.1 kumaraswamy_b 0.3540388371733616\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dp 1.7499999999999998\n",
      "Supervised Aim: beta dp\n",
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 0.010335\n",
      "Train Epoch: 1 [6400/30000 (21%)]\tLoss: 0.000047\n",
      "Train Epoch: 1 [12800/30000 (43%)]\tLoss: 0.000003\n",
      "Train Epoch: 1 [19200/30000 (64%)]\tLoss: 0.001217\n",
      "Train Epoch: 1 [25600/30000 (85%)]\tLoss: 0.000004\n",
      "NN 1 : tensor(1.7555)\n",
      "CS 1 : 1.7827333333333333\n",
      "DP 1 : 1.7541333333333333\n",
      "heuristic 1 : 1.7545\n",
      "DP: 1.7499999999999998\n",
      "tensor([0.4988, 0.4977, 0.0035])\n",
      "tensor([0.5019, 0.4981, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 1.830186 testing loss: tensor(1.7540)\n",
      "Train Epoch: 1 [1280/30000 (4%)]\tLoss: 1.889255 testing loss: tensor(1.7524)\n",
      "Train Epoch: 1 [2560/30000 (9%)]\tLoss: 1.891079 testing loss: tensor(1.7503)\n",
      "Train Epoch: 1 [3840/30000 (13%)]\tLoss: 1.865568 testing loss: tensor(1.7474)\n",
      "Train Epoch: 1 [5120/30000 (17%)]\tLoss: 2.001025 testing loss: tensor(1.7441)\n",
      "Train Epoch: 1 [6400/30000 (21%)]\tLoss: 1.922381 testing loss: tensor(1.7426)\n",
      "Train Epoch: 1 [7680/30000 (26%)]\tLoss: 2.019274 testing loss: tensor(1.7386)\n",
      "Train Epoch: 1 [8960/30000 (30%)]\tLoss: 1.858361 testing loss: tensor(1.7371)\n",
      "Train Epoch: 1 [10240/30000 (34%)]\tLoss: 1.786796 testing loss: tensor(1.7371)\n",
      "Train Epoch: 1 [11520/30000 (38%)]\tLoss: 1.987733 testing loss: tensor(1.7377)\n",
      "Train Epoch: 1 [12800/30000 (43%)]\tLoss: 1.926513 testing loss: tensor(1.7381)\n",
      "Train Epoch: 1 [14080/30000 (47%)]\tLoss: 1.829649 testing loss: tensor(1.7373)\n",
      "Train Epoch: 1 [15360/30000 (51%)]\tLoss: 1.873157 testing loss: tensor(1.7364)\n",
      "Train Epoch: 1 [16640/30000 (55%)]\tLoss: 1.902575 testing loss: tensor(1.7353)\n",
      "Train Epoch: 1 [17920/30000 (60%)]\tLoss: 1.814808 testing loss: tensor(1.7344)\n",
      "Train Epoch: 1 [19200/30000 (64%)]\tLoss: 1.851635 testing loss: tensor(1.7335)\n",
      "Train Epoch: 1 [20480/30000 (68%)]\tLoss: 1.900882 testing loss: tensor(1.7331)\n",
      "Train Epoch: 1 [21760/30000 (72%)]\tLoss: 1.874492 testing loss: tensor(1.7323)\n",
      "Train Epoch: 1 [23040/30000 (77%)]\tLoss: 1.844213 testing loss: tensor(1.7313)\n",
      "Train Epoch: 1 [24320/30000 (81%)]\tLoss: 1.948043 testing loss: tensor(1.7308)\n",
      "Train Epoch: 1 [25600/30000 (85%)]\tLoss: 1.843800 testing loss: tensor(1.7298)\n",
      "Train Epoch: 1 [26880/30000 (89%)]\tLoss: 1.857732 testing loss: tensor(1.7286)\n",
      "Train Epoch: 1 [28160/30000 (94%)]\tLoss: 1.935571 testing loss: tensor(1.7274)\n",
      "Train Epoch: 1 [29440/30000 (98%)]\tLoss: 1.785738 testing loss: tensor(1.7273)\n",
      "penalty: 0.00046196579933166504\n",
      "NN 2 : tensor(1.7269)\n",
      "CS 2 : 1.7827333333333333\n",
      "DP 2 : 1.7541333333333333\n",
      "heuristic 2 : 1.7545\n",
      "DP: 1.7499999999999998\n",
      "tensor([4.9004e-01, 5.0994e-01, 1.3412e-05])\n",
      "tensor([0.4901, 0.5099, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: beta random initializing\n",
      "do nothing\n",
      "NN 1 : tensor(1.7860)\n",
      "CS 1 : 1.7827333333333333\n",
      "DP 1 : 1.7541333333333333\n",
      "heuristic 1 : 1.7545\n",
      "DP: 1.7499999999999998\n",
      "tensor([0.4827, 0.2241, 0.2933])\n",
      "tensor([0.6523, 0.3477, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 2.022383 testing loss: tensor(1.7864)\n",
      "Train Epoch: 1 [1280/30000 (4%)]\tLoss: 1.969813 testing loss: tensor(1.7841)\n",
      "Train Epoch: 1 [2560/30000 (9%)]\tLoss: 1.963588 testing loss: tensor(1.7821)\n",
      "Train Epoch: 1 [3840/30000 (13%)]\tLoss: 1.974108 testing loss: tensor(1.7827)\n",
      "Train Epoch: 1 [5120/30000 (17%)]\tLoss: 1.818127 testing loss: tensor(1.7829)\n",
      "Train Epoch: 1 [6400/30000 (21%)]\tLoss: 1.875899 testing loss: tensor(1.7827)\n",
      "Train Epoch: 1 [7680/30000 (26%)]\tLoss: 1.933100 testing loss: tensor(1.7825)\n",
      "Train Epoch: 1 [8960/30000 (30%)]\tLoss: 1.903967 testing loss: tensor(1.7810)\n",
      "Train Epoch: 1 [10240/30000 (34%)]\tLoss: 1.907568 testing loss: tensor(1.7795)\n",
      "Train Epoch: 1 [11520/30000 (38%)]\tLoss: 1.967114 testing loss: tensor(1.7825)\n",
      "Train Epoch: 1 [12800/30000 (43%)]\tLoss: 1.865031 testing loss: tensor(1.7829)\n",
      "Train Epoch: 1 [14080/30000 (47%)]\tLoss: 1.903989 testing loss: tensor(1.7826)\n",
      "Train Epoch: 1 [15360/30000 (51%)]\tLoss: 1.862653 testing loss: tensor(1.7828)\n",
      "Train Epoch: 1 [16640/30000 (55%)]\tLoss: 1.892848 testing loss: tensor(1.7820)\n",
      "Train Epoch: 1 [17920/30000 (60%)]\tLoss: 1.912680 testing loss: tensor(1.7824)\n",
      "Train Epoch: 1 [19200/30000 (64%)]\tLoss: 1.894078 testing loss: tensor(1.7818)\n",
      "Train Epoch: 1 [20480/30000 (68%)]\tLoss: 1.863107 testing loss: tensor(1.7802)\n",
      "Train Epoch: 1 [21760/30000 (72%)]\tLoss: 1.947206 testing loss: tensor(1.7805)\n",
      "Train Epoch: 1 [23040/30000 (77%)]\tLoss: 1.867506 testing loss: tensor(1.7834)\n",
      "Train Epoch: 1 [24320/30000 (81%)]\tLoss: 2.031459 testing loss: tensor(1.7815)\n",
      "Train Epoch: 1 [25600/30000 (85%)]\tLoss: 2.008722 testing loss: tensor(1.7812)\n",
      "Train Epoch: 1 [26880/30000 (89%)]\tLoss: 1.926144 testing loss: tensor(1.7806)\n",
      "Train Epoch: 1 [28160/30000 (94%)]\tLoss: 1.941920 testing loss: tensor(1.7806)\n",
      "Train Epoch: 1 [29440/30000 (98%)]\tLoss: 1.900353 testing loss: tensor(1.7800)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.7792)\n",
      "CS 2 : 1.7827333333333333\n",
      "DP 2 : 1.7541333333333333\n",
      "heuristic 2 : 1.7545\n",
      "DP: 1.7499999999999998\n",
      "tensor([0.5111, 0.0508, 0.4382])\n",
      "tensor([0.6446, 0.3554, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: beta costsharing\n",
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 0.001361\n",
      "Train Epoch: 1 [6400/30000 (21%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [12800/30000 (43%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [19200/30000 (64%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [25600/30000 (85%)]\tLoss: 0.000000\n",
      "NN 1 : tensor(1.7827)\n",
      "CS 1 : 1.7827333333333333\n",
      "DP 1 : 1.7541333333333333\n",
      "heuristic 1 : 1.7545\n",
      "DP: 1.7499999999999998\n",
      "tensor([0.3333, 0.3333, 0.3333])\n",
      "tensor([0.5000, 0.5000, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 1.962960 testing loss: tensor(1.7825)\n",
      "Train Epoch: 1 [1280/30000 (4%)]\tLoss: 2.028496 testing loss: tensor(1.7819)\n",
      "Train Epoch: 1 [2560/30000 (9%)]\tLoss: 1.957722 testing loss: tensor(1.7823)\n",
      "Train Epoch: 1 [3840/30000 (13%)]\tLoss: 2.061164 testing loss: tensor(1.7820)\n",
      "Train Epoch: 1 [5120/30000 (17%)]\tLoss: 1.973914 testing loss: tensor(1.7827)\n",
      "Train Epoch: 1 [6400/30000 (21%)]\tLoss: 1.959270 testing loss: tensor(1.7828)\n",
      "Train Epoch: 1 [7680/30000 (26%)]\tLoss: 1.856748 testing loss: tensor(1.7827)\n",
      "Train Epoch: 1 [8960/30000 (30%)]\tLoss: 1.934309 testing loss: tensor(1.7826)\n",
      "Train Epoch: 1 [10240/30000 (34%)]\tLoss: 1.998945 testing loss: tensor(1.7827)\n",
      "Train Epoch: 1 [11520/30000 (38%)]\tLoss: 1.908217 testing loss: tensor(1.7826)\n",
      "Train Epoch: 1 [12800/30000 (43%)]\tLoss: 1.994150 testing loss: tensor(1.7828)\n",
      "Train Epoch: 1 [14080/30000 (47%)]\tLoss: 1.959920 testing loss: tensor(1.7827)\n",
      "Train Epoch: 1 [15360/30000 (51%)]\tLoss: 1.924490 testing loss: tensor(1.7829)\n",
      "Train Epoch: 1 [16640/30000 (55%)]\tLoss: 1.924843 testing loss: tensor(1.7829)\n",
      "Train Epoch: 1 [17920/30000 (60%)]\tLoss: 2.045049 testing loss: tensor(1.7832)\n",
      "Train Epoch: 1 [19200/30000 (64%)]\tLoss: 1.785172 testing loss: tensor(1.7834)\n",
      "Train Epoch: 1 [20480/30000 (68%)]\tLoss: 2.004412 testing loss: tensor(1.7833)\n",
      "Train Epoch: 1 [21760/30000 (72%)]\tLoss: 1.817568 testing loss: tensor(1.7828)\n",
      "Train Epoch: 1 [23040/30000 (77%)]\tLoss: 1.905053 testing loss: tensor(1.7829)\n",
      "Train Epoch: 1 [24320/30000 (81%)]\tLoss: 1.905731 testing loss: tensor(1.7832)\n",
      "Train Epoch: 1 [25600/30000 (85%)]\tLoss: 1.827504 testing loss: tensor(1.7831)\n",
      "Train Epoch: 1 [26880/30000 (89%)]\tLoss: 1.968824 testing loss: tensor(1.7830)\n",
      "Train Epoch: 1 [28160/30000 (94%)]\tLoss: 1.968819 testing loss: tensor(1.7828)\n",
      "Train Epoch: 1 [29440/30000 (98%)]\tLoss: 1.890563 testing loss: tensor(1.7831)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.7830)\n",
      "CS 2 : 1.7827333333333333\n",
      "DP 2 : 1.7541333333333333\n",
      "heuristic 2 : 1.7545\n",
      "DP: 1.7499999999999998\n",
      "tensor([0.3408, 0.3195, 0.3398])\n",
      "tensor([0.5058, 0.4942, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: beta heuristic\n",
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 0.028816\n",
      "Train Epoch: 1 [6400/30000 (21%)]\tLoss: 0.000008\n",
      "Train Epoch: 1 [12800/30000 (43%)]\tLoss: 0.000003\n",
      "Train Epoch: 1 [19200/30000 (64%)]\tLoss: 0.000050\n",
      "Train Epoch: 1 [25600/30000 (85%)]\tLoss: 0.000070\n",
      "NN 1 : tensor(1.7594)\n",
      "CS 1 : 1.7827333333333333\n",
      "DP 1 : 1.7541333333333333\n",
      "heuristic 1 : 1.7545\n",
      "DP: 1.7499999999999998\n",
      "tensor([0.0048, 0.5020, 0.4932])\n",
      "tensor([0.5035, 0.4965, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 1.915729 testing loss: tensor(1.7599)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [1280/30000 (4%)]\tLoss: 1.942961 testing loss: tensor(1.7559)\n",
      "Train Epoch: 1 [2560/30000 (9%)]\tLoss: 1.869364 testing loss: tensor(1.7542)\n",
      "Train Epoch: 1 [3840/30000 (13%)]\tLoss: 1.901336 testing loss: tensor(1.7525)\n",
      "Train Epoch: 1 [5120/30000 (17%)]\tLoss: 1.904760 testing loss: tensor(1.7512)\n",
      "Train Epoch: 1 [6400/30000 (21%)]\tLoss: 1.854869 testing loss: tensor(1.7500)\n",
      "Train Epoch: 1 [7680/30000 (26%)]\tLoss: 1.815423 testing loss: tensor(1.7476)\n",
      "Train Epoch: 1 [8960/30000 (30%)]\tLoss: 1.848752 testing loss: tensor(1.7475)\n",
      "Train Epoch: 1 [10240/30000 (34%)]\tLoss: 1.712829 testing loss: tensor(1.7464)\n",
      "Train Epoch: 1 [11520/30000 (38%)]\tLoss: 1.876500 testing loss: tensor(1.7448)\n",
      "Train Epoch: 1 [12800/30000 (43%)]\tLoss: 1.878121 testing loss: tensor(1.7426)\n",
      "Train Epoch: 1 [14080/30000 (47%)]\tLoss: 1.852968 testing loss: tensor(1.7410)\n",
      "Train Epoch: 1 [15360/30000 (51%)]\tLoss: 2.046010 testing loss: tensor(1.7394)\n",
      "Train Epoch: 1 [16640/30000 (55%)]\tLoss: 1.952604 testing loss: tensor(1.7370)\n",
      "Train Epoch: 1 [17920/30000 (60%)]\tLoss: 1.870673 testing loss: tensor(1.7371)\n",
      "Train Epoch: 1 [19200/30000 (64%)]\tLoss: 1.838703 testing loss: tensor(1.7373)\n",
      "Train Epoch: 1 [20480/30000 (68%)]\tLoss: 1.875036 testing loss: tensor(1.7372)\n",
      "Train Epoch: 1 [21760/30000 (72%)]\tLoss: 1.758507 testing loss: tensor(1.7355)\n",
      "Train Epoch: 1 [23040/30000 (77%)]\tLoss: 1.940222 testing loss: tensor(1.7347)\n",
      "Train Epoch: 1 [24320/30000 (81%)]\tLoss: 2.015443 testing loss: tensor(1.7331)\n",
      "Train Epoch: 1 [25600/30000 (85%)]\tLoss: 1.935691 testing loss: tensor(1.7328)\n",
      "Train Epoch: 1 [26880/30000 (89%)]\tLoss: 1.842240 testing loss: tensor(1.7315)\n",
      "Train Epoch: 1 [28160/30000 (94%)]\tLoss: 1.905603 testing loss: tensor(1.7309)\n",
      "Train Epoch: 1 [29440/30000 (98%)]\tLoss: 1.776337 testing loss: tensor(1.7300)\n",
      "penalty: 0.0007536113262176514\n",
      "NN 2 : tensor(1.7295)\n",
      "CS 2 : 1.7827333333333333\n",
      "DP 2 : 1.7541333333333333\n",
      "heuristic 2 : 1.7545\n",
      "DP: 1.7499999999999998\n",
      "tensor([4.2028e-05, 4.9181e-01, 5.0814e-01])\n",
      "tensor([0.4185, 0.5815, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.1)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "    \n",
    "for order in stage:\n",
    "    producedata(order)\n",
    "    #running\n",
    "    for order1 in order1name:\n",
    "        print(\"Supervised Aim:\",order,order1)\n",
    "        \n",
    "        # for mapping binary to payments before softmax\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(n, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, n),\n",
    "        )\n",
    "        model.apply(init_weights)\n",
    "        # optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr1)\n",
    "        \n",
    "        runningLossNN = []\n",
    "        runningLossCS = []\n",
    "        runningLossDP = []\n",
    "        runningLossHeuristic = []\n",
    "        #model=torch.load(\"save/pytorchNN=5dp1\");\n",
    "        #model.eval()\n",
    "        ##order1name=[\"costsharing\",\"dp\",\"heuristic\",\"random initializing\"]\n",
    "        for epoch in range(1, supervisionEpochs + 1):\n",
    "#             print(\"distributionRatio\",distributionRatio)\n",
    "            if(order1==\"costsharing\"):\n",
    "                supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "            elif(order1==\"dp\"):\n",
    "                supervisionTrain(epoch, dpSupervisionRule)\n",
    "            elif(order1==\"heuristic\"):\n",
    "                supervisionTrain(epoch, heuristicSupervisionRule)\n",
    "            elif(order1==\"random initializing\"):\n",
    "                print(\"do nothing\");\n",
    "\n",
    "        test()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr2)\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(epoch)\n",
    "            test()\n",
    "        losslistname.append(order+\" \"+order1);\n",
    "        losslist.append(losslisttemp);\n",
    "        losslisttemp=[];\n",
    "        savepath=\"save/pytorchNN=5all-beta\"+order+str(order1)\n",
    "        torch.save(model, savepath);\n",
    "        print(\"end\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colorlist=[\"#D2691E\",'#4169E1',\"#9ACD32\",\"#B22222\",\"#FF00FF\",\"#708090\"]\n",
    "for i in range(len(losslist)):\n",
    "    plt.plot(losslist[i], '.-', color=colorlist[i], alpha=0.8, label=losslistname[i])\n",
    "\n",
    "# 显示标签，如果不加这句，即使加了label='一些数字'的参数，最终还是不会显示标签\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('Trianing Times')\n",
    "plt.ylabel('Delay')\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('pytorchNN=3_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
