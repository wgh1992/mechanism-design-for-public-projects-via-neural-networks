{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40058530825361593\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from random import randint\n",
    "import random\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "global samplesJoint, tp_dataloader, tp_dataloader_testing, dp, decision, dp_H, decision_H\n",
    "from pylab import *\n",
    "from scipy.stats import beta\n",
    "import math\n",
    "n = 3\n",
    "epochs = 1\n",
    "supervisionEpochs = 1\n",
    "lr1 = 0.01\n",
    "lr2 = 0.0005\n",
    "log_interval = 5\n",
    "trainSize = 50000  #100000\n",
    "percentage_train_test = 0.5\n",
    "penaltyLambda = 10\n",
    "doublePeakHighMean = 0.6\n",
    "doublePeakLowMean = 0.2\n",
    "doublePeakStd = 0.1\n",
    "uniformlow = 0\n",
    "uniformhigh = 1.0\n",
    "normalloc = 0.2\n",
    "normalscale = 0.1\n",
    "\n",
    "cauchyloc = 1 / n\n",
    "cauchyscalen = 0.004\n",
    "\n",
    "exponentialhigh = 15  #Symbol(\"b\", real=True)\n",
    "exponentiallow = 15  #Symbol(\"a\", real=True)\n",
    "\n",
    "beta_a = 0.3\n",
    "beta_b = 0.2\n",
    "kumaraswamy_a = beta_a\n",
    "kumaraswamy_b = (1.0 + (beta_a - 1.0) * math.pow(\n",
    "    (beta_a + beta_b - 2.0) / (beta_a - 1.0), beta_a)) / beta_a\n",
    "print(kumaraswamy_b)\n",
    "\n",
    "independentnormalloc1 = [(float(ii) + 1) / (2 * n + 1)\n",
    "                         for ii in range(n, 0, -1)]\n",
    "independentnormalscale1 = [0.05 for ii in range(n)]\n",
    "\n",
    "independentnormalloc2 = [(float(ii) + 1) / (2 * n + 1)\n",
    "                         for ii in range(1, n + 1, 1)]\n",
    "independentnormalscale2 = [0.05 for ii in range(n)]\n",
    "stage = [\"twopeak\"]\n",
    "# \"twopeak\",\"normal\",\"uniform\",\"independent1\",\"independent2\",\"cauchy\",\"beta\",\"U-exponential\",\"arcsine\"\n",
    "order1name = [\"dp\", \"random initializing\", \"costsharing\", \"heuristic\"]\n",
    "# \"costsharing\",\"dp\",\"heuristic\",\"random initializing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio1 = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase1 = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "d3 = D.normal.Normal(normalloc, normalscale)\n",
    "distributionRatio3 = d3.cdf(1) - d3.cdf(0)\n",
    "distributionBase3 = d3.cdf(0)\n",
    "\n",
    "d4 = D.uniform.Uniform(uniformlow, uniformhigh)\n",
    "distributionRatio4 = d4.cdf(1) - d4.cdf(0)\n",
    "distributionBase4 = d4.cdf(0)\n",
    "\n",
    "d5 = [\n",
    "    D.normal.Normal(independentnormalloc1[ii], independentnormalscale1[ii])\n",
    "    for ii in range(n)\n",
    "]\n",
    "d6 = [\n",
    "    D.normal.Normal(independentnormalloc2[ii], independentnormalscale2[ii])\n",
    "    for ii in range(n)\n",
    "]\n",
    "\n",
    "d7 = D.cauchy.Cauchy(cauchyloc, cauchyscalen)\n",
    "\n",
    "d81 = D.exponential.Exponential(exponentiallow)\n",
    "d82 = D.exponential.Exponential(exponentialhigh)\n",
    "\n",
    "d9 = D.beta.Beta(beta_a, beta_b)\n",
    "#sample_d9=d9.rsample(torch.Size([100000]))\n",
    "\n",
    "#d9_sample=np.linspace(0.0001, 0.9999, 10000)\n",
    "#d9_pdf=torch.exp(d9.log_prob(torch.tensor(d9_sample,dtype=torch.float32)))\n",
    "#d9_delta=d9_sample[1]-d9_sample[0]\n",
    "#d9_sum_pdf=torch.sum(d9_pdf*d9_delta)\n",
    "\n",
    "d10 = D.beta.Beta(0.5, 0.5)\n",
    "\n",
    "\n",
    "def cdf(x, y, i=None):\n",
    "    if (y == \"twopeak\"):\n",
    "        return (d1.cdf(x) + d2.cdf(x) -\n",
    "                distributionBase1) / 2 / distributionRatio1\n",
    "    elif (y == \"normal\"):\n",
    "        return (d3.cdf(x) - distributionBase3) / distributionRatio3\n",
    "    elif (y == \"uniform\"):\n",
    "        return (d4.cdf(x) - distributionBase4) / distributionRatio4\n",
    "    elif (y == \"independent1\"):\n",
    "        return d5[i].cdf(x)\n",
    "    elif (y == \"independent2\"):\n",
    "        return d6[i].cdf(x)\n",
    "    elif (y == \"cauchy\"):\n",
    "        return d7.cdf(x)\n",
    "    elif (y == \"beta\"):\n",
    "        #         sum_cdf=0.0;\n",
    "        #         if(x<0.0001):\n",
    "        #             x=0.00011;\n",
    "        #         if(x>0.9999):\n",
    "        #             x=0.99989;\n",
    "        #         for i in range(len(d9_pdf)):\n",
    "        #             if(d9_sample[i]<x):\n",
    "        #                 sum_cdf+=d9_pdf[i]*d9_delta;\n",
    "        #             else:\n",
    "        #                 sum_cdf+=(d9_pdf[i]+d9_pdf[i-1])/ 2 *(x-d9_sample[i-1])\n",
    "        #                 break;\n",
    "        #         return sum_cdf/d9_sum_pdf\n",
    "        #         cdf_v=torch.sum((sample_d9<(x)), dtype=torch.float32)/100000\n",
    "        #         return cdf_v\n",
    "        #    F(x|a,b)=1–(1–x^a)^b\n",
    "        if (x < 0.0000001):\n",
    "            x = 0.0000001\n",
    "        elif (x > 0.9999999):\n",
    "            x = 0.9999999\n",
    "        try:\n",
    "            return 1.0 - torch.pow(1.0 - torch.pow(x, kumaraswamy_a),\n",
    "                                   kumaraswamy_b)\n",
    "        except:\n",
    "            return 1.0 - torch.pow(\n",
    "                1.0 - torch.pow(torch.tensor(x, dtype=torch.float32),\n",
    "                                kumaraswamy_a), kumaraswamy_b)\n",
    "    elif (y == \"arcsine\"):\n",
    "        #\n",
    "        if (x < 0.0000001):\n",
    "            x = 0.0000001\n",
    "        elif (x > 0.9999999):\n",
    "            x = 0.9999999\n",
    "        try:\n",
    "            res = 2.0 / math.pi * torch.asin(torch.sqrt(x))\n",
    "            #print(x)\n",
    "            return res  # + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "        except:\n",
    "            return 2.0 / math.pi * torch.asin(\n",
    "                torch.sqrt(torch.tensor(\n",
    "                    x, dtype=torch.float32)))  # + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "    elif (y == \"U-exponential\"):\n",
    "        return (d81.cdf(x) + (1.0 - d82.cdf(1.0 - x))) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    if torch.sum(bits).item() == 0:\n",
    "        return torch.ones(n)\n",
    "    bits = bits.type(torch.float32)\n",
    "    negBits = torch.ones(n) - bits\n",
    "    payments = model(bits)\n",
    "    payments = payments - 1000 * negBits\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    payments = payments + negBits\n",
    "    return payments\n",
    "\n",
    "\n",
    "def tpToBits(tp, bits=torch.ones(n).type(torch.uint8)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = (tp >= payments).type(torch.uint8)\n",
    "    if torch.equal(newBits, bits):\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits(tp, newBits)  #bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    return n - torch.sum(tpToBits(tp).type(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    global dp, decision\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money, yes]\n",
    "        offer = float(offerIndex) / dpPrecision\n",
    "        if tp[i] >= offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits[i] = 0\n",
    "            payments[i] = 1\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    #     print()\n",
    "    #     print(tp)\n",
    "    #     print(bits)\n",
    "    #     print(payments)\n",
    "    #     print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def heuristicSupervisionRule(tp):\n",
    "    global dp_H, decision_H\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    tempold = -1\n",
    "    for turns in range(n, 0, -1):\n",
    "        money = dpPrecision\n",
    "        j = 0\n",
    "        tempo = sum(bits)\n",
    "        # print(\"bits\",tempo)\n",
    "        for i in range(n):\n",
    "            if (j >= n):\n",
    "                break\n",
    "\n",
    "            offerIndex = decision_H[tempo, tempo - i, money]\n",
    "            offer = float(offerIndex) / dpPrecision\n",
    "            while (j < n):\n",
    "                if (bits[j] != 0):\n",
    "                    break\n",
    "                j += 1\n",
    "            if (j >= n):\n",
    "                break\n",
    "            if tp[j] >= offer:\n",
    "                # print(money,j,tp[j],offer)\n",
    "                money -= offerIndex\n",
    "                bits[j] = 1\n",
    "                payments[j] = offer\n",
    "            else:\n",
    "                bits[j] = 0\n",
    "                payments[j] = 1.0\n",
    "            j += 1\n",
    "        # print(\"money\",money)\n",
    "        if (money == 0 and tempold == tempo):\n",
    "            break\n",
    "        tempold = tempo\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    for k in range(n, -1, -1):\n",
    "        if k == 0:\n",
    "            break\n",
    "        bits = [1 if tp[ii] >= 1.0 / k else 0 for ii in range(n)]\n",
    "        if sum(bits) == k:\n",
    "            break\n",
    "    if k == 0:\n",
    "        payments = [1 for ii in range(n)]\n",
    "    else:\n",
    "        payments = [1.0 / k if bits[ii] == 1 else 1 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    return float(n - torch.sum(costSharingSupervisionRule(tp)[0]).item())\n",
    "\n",
    "\n",
    "def dpDelay(tp):\n",
    "    return float(n - torch.sum(dpSupervisionRule(tp)[0]).item())\n",
    "\n",
    "\n",
    "def heuristicDelay(tp):\n",
    "    return float(n - torch.sum(heuristicSupervisionRule(tp)[0]).item())\n",
    "\n",
    "\n",
    "# templire=0;\n",
    "# num=1\n",
    "# for i in range(num):\n",
    "#     temp=torch.tensor(samplesJoint[i])\n",
    "#     #print(temp)\n",
    "#     #print(temp);\n",
    "#     #print(heuristicSupervisionRule(temp))\n",
    "#     #print(dpSupervisionRule(temp))\n",
    "#     #print(dpDelay(temp))\n",
    "#     res=dpDelay(temp)\n",
    "#     templire+=res\n",
    "#     #print(\"Delay\",res)\n",
    "#     #print()\n",
    "# print(templire/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordAndReport(name, source, loss):\n",
    "    source.append(loss)\n",
    "    realLength = len(source)\n",
    "    #print(f\"{name} ({realLength}): {loss}\")\n",
    "    print(name, realLength, \":\", loss)\n",
    "\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch, ) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) -\n",
    "                            bitsToPayments(bitsLessOnes)))\n",
    "        loss = penalty * penaltyLambda / 100000\n",
    "\n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % (log_interval * 5) == 0:\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                epoch,\n",
    "                batch_idx * len(tp_batch),\n",
    "                len(tp_dataloader.dataset),\n",
    "                100.0 * batch_idx / len(tp_dataloader),\n",
    "                loss.item(),\n",
    "            ))\n",
    "\n",
    "\n",
    "losslist = []\n",
    "losslistname = []\n",
    "losslisttemp = []\n",
    "\n",
    "\n",
    "def test_batch():\n",
    "    global losslisttemp\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lenLoss = 0\n",
    "        nnLoss = 0\n",
    "        for (tp_batch, ) in tp_dataloader_testing:\n",
    "            for tp in tp_batch:\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            lenLoss += len(tp_batch)\n",
    "        nnLoss /= lenLoss\n",
    "    return nnLoss\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    global losslisttemp\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch, ) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) -\n",
    "                            bitsToPayments(bitsLessOnes)))\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                Delay1 = tpToTotalDelay(tp1)\n",
    "                Delay0 = tpToTotalDelay(tp0)\n",
    "                if (order != \"independent1\" and order != \"independent2\"):\n",
    "                    loss = loss + (1 - cdf(offer, order)) * Delay1 + cdf(\n",
    "                        offer, order) * Delay0\n",
    "                else:\n",
    "                    loss = loss + (1 - cdf(offer, order, i)) * Delay1 + cdf(\n",
    "                        offer, order, i) * Delay0\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp = test_batch()\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ), \"testing loss:\", losstemp)\n",
    "            losslisttemp.append(losstemp)\n",
    "\n",
    "    print(\"penalty:\", penalty.item())\n",
    "\n",
    "\n",
    "allBits = [\n",
    "    torch.tensor(bits, dtype=torch.int16)\n",
    "    for bits in itertools.product([0, 1], repeat=n)\n",
    "]\n",
    "\n",
    "\n",
    "def test():\n",
    "    global losslisttemp\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        costSharingLoss = 0\n",
    "        dpLoss = 0\n",
    "        nnLoss = 0\n",
    "        heuristicLoss = 0\n",
    "        lenLoss = 0\n",
    "        for (tp_batch, ) in tp_dataloader_testing:\n",
    "\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "                heuristicLoss += heuristicDelay(tp)\n",
    "            lenLoss += len(tp_batch)\n",
    "        costSharingLoss /= lenLoss\n",
    "        dpLoss /= lenLoss\n",
    "        nnLoss /= lenLoss\n",
    "        heuristicLoss /= lenLoss\n",
    "        #print(lenLoss)\n",
    "        losslisttemp.append(nnLoss)\n",
    "        recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "        recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "        recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "        recordAndReport(\"heuristic\", runningLossHeuristic, heuristicLoss)\n",
    "        print(\"DP:\", dp[n, dpPrecision, 0])\n",
    "        #for i in range(n, 0, -1):\n",
    "        #    print(\"Heuristic:\",i,5*(1-dp_H[i, i, dpPrecision]))\n",
    "        for i in range(n, 0, -1):\n",
    "            print(\n",
    "                tpToPayments(\n",
    "                    torch.tensor([0 if ii >= i else 1 for ii in range(n)],\n",
    "                                 dtype=torch.float32)))\n",
    "\n",
    "\n",
    "dpPrecision = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def producedata(order):\n",
    "    global samplesJoint, tp_dataloader, tp_dataloader_testing, dp, decision, dp_H, decision_H\n",
    "    dpPrecision = 100\n",
    "    if (order == \"twopeak\"):\n",
    "        print(\"loc\", doublePeakLowMean, \"scale\", doublePeakStd)\n",
    "        print(\"loc\", doublePeakHighMean, \"scale\", doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = np.random.normal(loc=doublePeakLowMean,\n",
    "                                    scale=doublePeakStd,\n",
    "                                    size=(trainSize, n))\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(loc=doublePeakLowMean,\n",
    "                                                      scale=doublePeakStd)\n",
    "        samples2 = np.random.normal(loc=doublePeakHighMean,\n",
    "                                    scale=doublePeakStd,\n",
    "                                    size=(trainSize, n))\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = np.random.normal(loc=doublePeakHighMean,\n",
    "                                                      scale=doublePeakStd)\n",
    "        samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "    elif (order == \"normal\"):\n",
    "        print(\"loc\", normalloc, \"scale\", normalscale)\n",
    "        samples1 = np.random.normal(loc=normalloc,\n",
    "                                    scale=normalscale,\n",
    "                                    size=(trainSize, n))\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(loc=normalloc,\n",
    "                                                      scale=normalscale)\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif (order == \"uniform\"):\n",
    "        print(\"uniformlow\", uniformlow, \"uniformhigh\", uniformhigh)\n",
    "        samples1 = np.random.uniform(uniformlow,\n",
    "                                     uniformhigh,\n",
    "                                     size=(trainSize, n))\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(uniformlow, uniformhigh)\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif (order == \"independent1\"):\n",
    "        print(\"loc\", independentnormalloc1, \"scale\", independentnormalscale1)\n",
    "        samples1 = np.random.uniform(uniformlow,\n",
    "                                     uniformhigh,\n",
    "                                     size=(trainSize, n))\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(independentnormalloc1[j],\n",
    "                                                  independentnormalscale1[j])\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i,\n",
    "                             j] = np.random.normal(independentnormalloc1[j],\n",
    "                                                   independentnormalscale1[j])\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif (order == \"independent2\"):\n",
    "        print(\"loc\", independentnormalloc2, \"scale\", independentnormalscale2)\n",
    "        samples1 = np.random.uniform(uniformlow,\n",
    "                                     uniformhigh,\n",
    "                                     size=(trainSize, n))\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(independentnormalloc2[j],\n",
    "                                                  independentnormalscale2[j])\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i,\n",
    "                             j] = np.random.normal(independentnormalloc2[j],\n",
    "                                                   independentnormalscale2[j])\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif (order == \"cauchy\"):\n",
    "        print(\"cauchyloc\", cauchyloc, \"cauchyscale\", cauchyscalen)\n",
    "        samples1 = np.random.uniform(uniformlow,\n",
    "                                     uniformhigh,\n",
    "                                     size=(trainSize, n))\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif (order == \"beta\"):\n",
    "        print(\"beta_a\", beta_a, \"beta_b\", beta_b)\n",
    "        print(\"kumaraswamy_a\", kumaraswamy_a, \"kumaraswamy_b\", kumaraswamy_b)\n",
    "        samples1 = np.random.uniform(uniformlow,\n",
    "                                     uniformhigh,\n",
    "                                     size=(trainSize, n))\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(beta_a, beta_b, size=1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(beta_a, beta_b, size=1)\n",
    "\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif (order == \"arcsine\"):\n",
    "        print(\"betalow\", 0.5, \"betahigh\", 0.5)\n",
    "        samples1 = np.random.uniform(uniformlow,\n",
    "                                     uniformhigh,\n",
    "                                     size=(trainSize, n))\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(0.5, 0.5, size=1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(0.5, 0.5, size=1)\n",
    "\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif (order == \"U-exponential\"):\n",
    "        print(\"loc\", doublePeakLowMean, \"scale\", doublePeakStd)\n",
    "        print(\"loc\", doublePeakHighMean, \"scale\", doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = d81.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d81.rsample(torch.Size([1])).numpy()\n",
    "\n",
    "        samples2 = d82.rsample(torch.Size([trainSize, n])).numpy()\n",
    "\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = d82.rsample(torch.Size([1])).numpy()\n",
    "        samples2 = 1.0 - samples2\n",
    "        samplesJoint = signals * samples1 - (signals - 1.0) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "\n",
    "    plt.hist(samplesJoint, bins=500)\n",
    "    plt.show()\n",
    "\n",
    "    tp_dataset = TensorDataset(tp_tensor[:int(trainSize *\n",
    "                                              percentage_train_test)])\n",
    "    tp_dataset_testing = TensorDataset(\n",
    "        tp_tensor[int(trainSize * (1.0 - percentage_train_test)):])\n",
    "    tp_dataloader = DataLoader(tp_dataset, batch_size=128, shuffle=True)\n",
    "    tp_dataloader_testing = DataLoader(tp_dataset_testing,\n",
    "                                       batch_size=256,\n",
    "                                       shuffle=False)\n",
    "\n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "    decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for yes in range(n + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp[0, 0, yes] = 0\n",
    "            else:\n",
    "                dp[0, money, yes] = yes  # + 1.0\n",
    "    for ppl in range(1, n + 1):\n",
    "        for yes in range(n + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                minSoFar = 1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if (order != \"independent1\" and order != \"independent2\"):\n",
    "                        res = (1 - cdf(offer, order)) * dp[\n",
    "                            ppl - 1, money - offerIndex,\n",
    "                            min(yes + 1, n)] + cdf(\n",
    "                                offer, order) * (1 + dp[ppl - 1, money, yes])\n",
    "                    else:\n",
    "                        res = (1 - cdf(offer, order, n - ppl)) * dp[\n",
    "                            ppl - 1, money - offerIndex,\n",
    "                            min(yes + 1, n)] + cdf(offer, order, n - ppl) * (\n",
    "                                1 + dp[ppl - 1, money, yes])\n",
    "                    if minSoFar > res.item():\n",
    "                        minSoFar = res.item()\n",
    "                        decision[ppl, money, yes] = offerIndex\n",
    "                dp[ppl, money, yes] = minSoFar\n",
    "\n",
    "    print(\"dp\", dp[n, dpPrecision, 0])\n",
    "\n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp_H = np.zeros([n + 1, n + 1, dpPrecision + 1])\n",
    "    decision_H = np.zeros([n + 1, n + 1, dpPrecision + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for i in range(1, n + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp_H[i, 0, 0] = 1\n",
    "            else:\n",
    "                offer = money / dpPrecision\n",
    "                dp_H[i, 0, money] = 0  #cdf(offer)# + 1.0\n",
    "    for i in range(1, n + 1):\n",
    "        for ppl in range(1, i + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                maxSoFar = -1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if (order != \"independent1\" and order != \"independent2\"):\n",
    "                        res = (1 - cdf(offer, order)\n",
    "                               ) * dp_H[i, ppl - 1, money - offerIndex]\n",
    "                    else:\n",
    "                        res = (1 - cdf(offer, order, ppl - 1)\n",
    "                               ) * dp_H[i, ppl - 1, money - offerIndex]\n",
    "                    if maxSoFar < res.item():\n",
    "                        maxSoFar = res.item()\n",
    "                        decision_H[i, ppl, money] = offerIndex\n",
    "                dp_H[i, ppl, money] = maxSoFar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loc 0.2 scale 0.1\n",
      "loc 0.6 scale 0.1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dp 1.9953912496566772\n",
      "Supervised Aim: twopeak dp\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 0.004981\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 0.000105\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 0.000008\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 0.000001\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 0.000000\n",
      "NN 1 : tensor(1.9948)\n",
      "CS 1 : 2.02076\n",
      "DP 1 : 2.01068\n",
      "heuristic 1 : 2.1538\n",
      "DP: 1.9953912496566772\n",
      "tensor([0.4800, 0.4400, 0.0800])\n",
      "tensor([0.5404, 0.4596, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 2.039596 testing loss: tensor(1.9902)\n",
      "Train Epoch: 1 [640/25000 (3%)]\tLoss: 2.097836 testing loss: tensor(1.9868)\n",
      "Train Epoch: 1 [1280/25000 (5%)]\tLoss: 2.045767 testing loss: tensor(1.9836)\n",
      "Train Epoch: 1 [1920/25000 (8%)]\tLoss: 1.911567 testing loss: tensor(1.9780)\n",
      "Train Epoch: 1 [2560/25000 (10%)]\tLoss: 2.163191 testing loss: tensor(1.9764)\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 1.903298 testing loss: tensor(1.9758)\n",
      "Train Epoch: 1 [3840/25000 (15%)]\tLoss: 2.071319 testing loss: tensor(1.9752)\n",
      "Train Epoch: 1 [4480/25000 (18%)]\tLoss: 2.046610 testing loss: tensor(1.9753)\n",
      "Train Epoch: 1 [5120/25000 (20%)]\tLoss: 2.038426 testing loss: tensor(1.9750)\n",
      "Train Epoch: 1 [5760/25000 (23%)]\tLoss: 1.946736 testing loss: tensor(1.9752)\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 1.986089 testing loss: tensor(1.9772)\n",
      "Train Epoch: 1 [7040/25000 (28%)]\tLoss: 2.028821 testing loss: tensor(1.9760)\n",
      "Train Epoch: 1 [7680/25000 (31%)]\tLoss: 1.977853 testing loss: tensor(1.9750)\n",
      "Train Epoch: 1 [8320/25000 (33%)]\tLoss: 2.042022 testing loss: tensor(1.9760)\n",
      "Train Epoch: 1 [8960/25000 (36%)]\tLoss: 1.964552 testing loss: tensor(1.9753)\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 1.944001 testing loss: tensor(1.9777)\n",
      "Train Epoch: 1 [10240/25000 (41%)]\tLoss: 2.074892 testing loss: tensor(1.9766)\n",
      "Train Epoch: 1 [10880/25000 (43%)]\tLoss: 1.902908 testing loss: tensor(1.9752)\n",
      "Train Epoch: 1 [11520/25000 (46%)]\tLoss: 1.988650 testing loss: tensor(1.9764)\n",
      "Train Epoch: 1 [12160/25000 (48%)]\tLoss: 1.973677 testing loss: tensor(1.9752)\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 2.008055 testing loss: tensor(1.9766)\n",
      "Train Epoch: 1 [13440/25000 (54%)]\tLoss: 2.007730 testing loss: tensor(1.9750)\n",
      "Train Epoch: 1 [14080/25000 (56%)]\tLoss: 2.001295 testing loss: tensor(1.9750)\n",
      "Train Epoch: 1 [14720/25000 (59%)]\tLoss: 1.885240 testing loss: tensor(1.9762)\n",
      "Train Epoch: 1 [15360/25000 (61%)]\tLoss: 1.971892 testing loss: tensor(1.9751)\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 1.938336 testing loss: tensor(1.9758)\n",
      "Train Epoch: 1 [16640/25000 (66%)]\tLoss: 1.737263 testing loss: tensor(1.9742)\n",
      "Train Epoch: 1 [17280/25000 (69%)]\tLoss: 2.047164 testing loss: tensor(1.9762)\n",
      "Train Epoch: 1 [17920/25000 (71%)]\tLoss: 1.886307 testing loss: tensor(1.9754)\n",
      "Train Epoch: 1 [18560/25000 (74%)]\tLoss: 1.904168 testing loss: tensor(1.9747)\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 2.064930 testing loss: tensor(1.9760)\n",
      "Train Epoch: 1 [19840/25000 (79%)]\tLoss: 2.090980 testing loss: tensor(1.9756)\n",
      "Train Epoch: 1 [20480/25000 (82%)]\tLoss: 1.970177 testing loss: tensor(1.9752)\n",
      "Train Epoch: 1 [21120/25000 (84%)]\tLoss: 1.899963 testing loss: tensor(1.9753)\n",
      "Train Epoch: 1 [21760/25000 (87%)]\tLoss: 1.830319 testing loss: tensor(1.9748)\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 1.889873 testing loss: tensor(1.9744)\n",
      "Train Epoch: 1 [23040/25000 (92%)]\tLoss: 1.895654 testing loss: tensor(1.9769)\n",
      "Train Epoch: 1 [23680/25000 (94%)]\tLoss: 2.121154 testing loss: tensor(1.9752)\n",
      "Train Epoch: 1 [24320/25000 (97%)]\tLoss: 1.911679 testing loss: tensor(1.9762)\n",
      "Train Epoch: 1 [7800/25000 (99%)]\tLoss: 1.972601 testing loss: tensor(1.9749)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.9749)\n",
      "CS 2 : 2.02076\n",
      "DP 2 : 2.01068\n",
      "heuristic 2 : 2.1538\n",
      "DP: 1.9953912496566772\n",
      "tensor([0.4334, 0.4303, 0.1363])\n",
      "tensor([0.5022, 0.4978, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: twopeak random initializing\n",
      "do nothing\n",
      "NN 1 : tensor(2.2089)\n",
      "CS 1 : 2.02076\n",
      "DP 1 : 2.01068\n",
      "heuristic 1 : 2.1538\n",
      "DP: 1.9953912496566772\n",
      "tensor([0.1867, 0.5347, 0.2786])\n",
      "tensor([0.3055, 0.6945, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 2.202515 testing loss: tensor(2.1305)\n",
      "Train Epoch: 1 [640/25000 (3%)]\tLoss: 1.885698 testing loss: tensor(2.0362)\n",
      "Train Epoch: 1 [1280/25000 (5%)]\tLoss: 2.043554 testing loss: tensor(2.0372)\n",
      "Train Epoch: 1 [1920/25000 (8%)]\tLoss: 1.971975 testing loss: tensor(2.0225)\n",
      "Train Epoch: 1 [2560/25000 (10%)]\tLoss: 2.057991 testing loss: tensor(2.0151)\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 2.049351 testing loss: tensor(2.0104)\n",
      "Train Epoch: 1 [3840/25000 (15%)]\tLoss: 2.007224 testing loss: tensor(2.0058)\n",
      "Train Epoch: 1 [4480/25000 (18%)]\tLoss: 2.112602 testing loss: tensor(2.0036)\n",
      "Train Epoch: 1 [5120/25000 (20%)]\tLoss: 1.987912 testing loss: tensor(1.9977)\n",
      "Train Epoch: 1 [5760/25000 (23%)]\tLoss: 1.934193 testing loss: tensor(1.9964)\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 2.002672 testing loss: tensor(2.0005)\n",
      "Train Epoch: 1 [7040/25000 (28%)]\tLoss: 1.995710 testing loss: tensor(1.9968)\n",
      "Train Epoch: 1 [7680/25000 (31%)]\tLoss: 1.912506 testing loss: tensor(1.9963)\n",
      "Train Epoch: 1 [8320/25000 (33%)]\tLoss: 1.949923 testing loss: tensor(1.9942)\n",
      "Train Epoch: 1 [8960/25000 (36%)]\tLoss: 1.998184 testing loss: tensor(2.0000)\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 1.937371 testing loss: tensor(1.9974)\n",
      "Train Epoch: 1 [10240/25000 (41%)]\tLoss: 1.907343 testing loss: tensor(1.9974)\n",
      "Train Epoch: 1 [10880/25000 (43%)]\tLoss: 2.050329 testing loss: tensor(1.9985)\n",
      "Train Epoch: 1 [11520/25000 (46%)]\tLoss: 2.095595 testing loss: tensor(1.9938)\n",
      "Train Epoch: 1 [12160/25000 (48%)]\tLoss: 1.918413 testing loss: tensor(1.9961)\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 1.967400 testing loss: tensor(2.0002)\n",
      "Train Epoch: 1 [13440/25000 (54%)]\tLoss: 2.044070 testing loss: tensor(1.9957)\n",
      "Train Epoch: 1 [14080/25000 (56%)]\tLoss: 1.960594 testing loss: tensor(1.9970)\n",
      "Train Epoch: 1 [14720/25000 (59%)]\tLoss: 1.980217 testing loss: tensor(1.9980)\n",
      "Train Epoch: 1 [15360/25000 (61%)]\tLoss: 1.921869 testing loss: tensor(1.9944)\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 2.121022 testing loss: tensor(1.9962)\n",
      "Train Epoch: 1 [16640/25000 (66%)]\tLoss: 2.007428 testing loss: tensor(1.9954)\n",
      "Train Epoch: 1 [17280/25000 (69%)]\tLoss: 2.036427 testing loss: tensor(1.9931)\n",
      "Train Epoch: 1 [17920/25000 (71%)]\tLoss: 2.094182 testing loss: tensor(1.9951)\n",
      "Train Epoch: 1 [18560/25000 (74%)]\tLoss: 2.074858 testing loss: tensor(1.9937)\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 1.984461 testing loss: tensor(1.9987)\n",
      "Train Epoch: 1 [19840/25000 (79%)]\tLoss: 1.949085 testing loss: tensor(1.9958)\n",
      "Train Epoch: 1 [20480/25000 (82%)]\tLoss: 1.946650 testing loss: tensor(1.9961)\n",
      "Train Epoch: 1 [21120/25000 (84%)]\tLoss: 1.975182 testing loss: tensor(1.9926)\n",
      "Train Epoch: 1 [21760/25000 (87%)]\tLoss: 2.090331 testing loss: tensor(1.9908)\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 1.926421 testing loss: tensor(1.9922)\n",
      "Train Epoch: 1 [23040/25000 (92%)]\tLoss: 1.965065 testing loss: tensor(1.9880)\n",
      "Train Epoch: 1 [23680/25000 (94%)]\tLoss: 1.980714 testing loss: tensor(1.9843)\n",
      "Train Epoch: 1 [24320/25000 (97%)]\tLoss: 1.947277 testing loss: tensor(1.9808)\n",
      "Train Epoch: 1 [7800/25000 (99%)]\tLoss: 1.959980 testing loss: tensor(1.9731)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.9731)\n",
      "CS 2 : 2.02076\n",
      "DP 2 : 2.01068\n",
      "heuristic 2 : 2.1538\n",
      "DP: 1.9953912496566772\n",
      "tensor([0.1666, 0.4444, 0.3890])\n",
      "tensor([0.4949, 0.5051, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: twopeak costsharing\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 0.001491\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 0.000016\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 0.000004\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 0.000000\n",
      "NN 1 : tensor(2.0208)\n",
      "CS 1 : 2.02076\n",
      "DP 1 : 2.01068\n",
      "heuristic 1 : 2.1538\n",
      "DP: 1.9953912496566772\n",
      "tensor([0.3333, 0.3333, 0.3333])\n",
      "tensor([0.5000, 0.5000, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 1.919277 testing loss: tensor(2.0208)\n",
      "Train Epoch: 1 [640/25000 (3%)]\tLoss: 2.235397 testing loss: tensor(2.0202)\n",
      "Train Epoch: 1 [1280/25000 (5%)]\tLoss: 2.009732 testing loss: tensor(2.0238)\n",
      "Train Epoch: 1 [1920/25000 (8%)]\tLoss: 2.061781 testing loss: tensor(2.0222)\n",
      "Train Epoch: 1 [2560/25000 (10%)]\tLoss: 2.091539 testing loss: tensor(2.0200)\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 2.086517 testing loss: tensor(2.0205)\n",
      "Train Epoch: 1 [3840/25000 (15%)]\tLoss: 1.956308 testing loss: tensor(2.0208)\n",
      "Train Epoch: 1 [4480/25000 (18%)]\tLoss: 2.100936 testing loss: tensor(2.0204)\n",
      "Train Epoch: 1 [5120/25000 (20%)]\tLoss: 2.081883 testing loss: tensor(2.0206)\n",
      "Train Epoch: 1 [5760/25000 (23%)]\tLoss: 1.999485 testing loss: tensor(2.0198)\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 2.091941 testing loss: tensor(2.0210)\n",
      "Train Epoch: 1 [7040/25000 (28%)]\tLoss: 2.167782 testing loss: tensor(2.0232)\n",
      "Train Epoch: 1 [7680/25000 (31%)]\tLoss: 1.987889 testing loss: tensor(2.0218)\n",
      "Train Epoch: 1 [8320/25000 (33%)]\tLoss: 1.908095 testing loss: tensor(2.0212)\n",
      "Train Epoch: 1 [8960/25000 (36%)]\tLoss: 2.126238 testing loss: tensor(2.0206)\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 2.020357 testing loss: tensor(2.0205)\n",
      "Train Epoch: 1 [10240/25000 (41%)]\tLoss: 1.996247 testing loss: tensor(2.0201)\n",
      "Train Epoch: 1 [10880/25000 (43%)]\tLoss: 1.991564 testing loss: tensor(2.0211)\n",
      "Train Epoch: 1 [11520/25000 (46%)]\tLoss: 1.964985 testing loss: tensor(2.0204)\n",
      "Train Epoch: 1 [12160/25000 (48%)]\tLoss: 1.980420 testing loss: tensor(2.0204)\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 1.937422 testing loss: tensor(2.0205)\n",
      "Train Epoch: 1 [13440/25000 (54%)]\tLoss: 1.921166 testing loss: tensor(2.0236)\n",
      "Train Epoch: 1 [14080/25000 (56%)]\tLoss: 2.038456 testing loss: tensor(2.0210)\n",
      "Train Epoch: 1 [14720/25000 (59%)]\tLoss: 2.007347 testing loss: tensor(2.0199)\n",
      "Train Epoch: 1 [15360/25000 (61%)]\tLoss: 1.907199 testing loss: tensor(2.0184)\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 2.028027 testing loss: tensor(2.0198)\n",
      "Train Epoch: 1 [16640/25000 (66%)]\tLoss: 1.888453 testing loss: tensor(2.0228)\n",
      "Train Epoch: 1 [17280/25000 (69%)]\tLoss: 2.044879 testing loss: tensor(2.0236)\n",
      "Train Epoch: 1 [17920/25000 (71%)]\tLoss: 2.065264 testing loss: tensor(2.0221)\n",
      "Train Epoch: 1 [18560/25000 (74%)]\tLoss: 2.054645 testing loss: tensor(2.0218)\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 2.075785 testing loss: tensor(2.0197)\n",
      "Train Epoch: 1 [19840/25000 (79%)]\tLoss: 2.012264 testing loss: tensor(2.0204)\n",
      "Train Epoch: 1 [20480/25000 (82%)]\tLoss: 1.955347 testing loss: tensor(2.0191)\n",
      "Train Epoch: 1 [21120/25000 (84%)]\tLoss: 1.988799 testing loss: tensor(2.0184)\n",
      "Train Epoch: 1 [21760/25000 (87%)]\tLoss: 2.084766 testing loss: tensor(2.0150)\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 1.916372 testing loss: tensor(2.0098)\n",
      "Train Epoch: 1 [23040/25000 (92%)]\tLoss: 1.984949 testing loss: tensor(2.0079)\n",
      "Train Epoch: 1 [23680/25000 (94%)]\tLoss: 2.071662 testing loss: tensor(2.0030)\n",
      "Train Epoch: 1 [24320/25000 (97%)]\tLoss: 1.999410 testing loss: tensor(1.9988)\n",
      "Train Epoch: 1 [7800/25000 (99%)]\tLoss: 1.921799 testing loss: tensor(1.9919)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.9919)\n",
      "CS 2 : 2.02076\n",
      "DP 2 : 2.01068\n",
      "heuristic 2 : 2.1538\n",
      "DP: 1.9953912496566772\n",
      "tensor([0.4464, 0.2198, 0.3338])\n",
      "tensor([0.5005, 0.4995, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: twopeak heuristic\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 0.004021\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 0.000126\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 0.000003\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 0.000001\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 0.000000\n",
      "NN 1 : tensor(2.1259)\n",
      "CS 1 : 2.02076\n",
      "DP 1 : 2.01068\n",
      "heuristic 1 : 2.1538\n",
      "DP: 1.9953912496566772\n",
      "tensor([0.1112, 0.4388, 0.4500])\n",
      "tensor([0.4976, 0.5024, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 2.079829 testing loss: tensor(2.1189)\n",
      "Train Epoch: 1 [640/25000 (3%)]\tLoss: 2.155754 testing loss: tensor(2.0956)\n",
      "Train Epoch: 1 [1280/25000 (5%)]\tLoss: 2.065892 testing loss: tensor(2.0690)\n",
      "Train Epoch: 1 [1920/25000 (8%)]\tLoss: 2.051300 testing loss: tensor(2.0376)\n",
      "Train Epoch: 1 [2560/25000 (10%)]\tLoss: 2.049181 testing loss: tensor(2.0141)\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 1.927580 testing loss: tensor(2.0008)\n",
      "Train Epoch: 1 [3840/25000 (15%)]\tLoss: 1.956301 testing loss: tensor(1.9932)\n",
      "Train Epoch: 1 [4480/25000 (18%)]\tLoss: 1.951623 testing loss: tensor(1.9904)\n",
      "Train Epoch: 1 [5120/25000 (20%)]\tLoss: 2.030229 testing loss: tensor(1.9844)\n",
      "Train Epoch: 1 [5760/25000 (23%)]\tLoss: 2.055665 testing loss: tensor(1.9783)\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 2.008404 testing loss: tensor(1.9768)\n",
      "Train Epoch: 1 [7040/25000 (28%)]\tLoss: 1.895697 testing loss: tensor(1.9735)\n",
      "Train Epoch: 1 [7680/25000 (31%)]\tLoss: 1.999557 testing loss: tensor(1.9703)\n",
      "Train Epoch: 1 [8320/25000 (33%)]\tLoss: 1.932948 testing loss: tensor(1.9716)\n",
      "Train Epoch: 1 [8960/25000 (36%)]\tLoss: 1.950314 testing loss: tensor(1.9668)\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 1.925262 testing loss: tensor(1.9658)\n",
      "Train Epoch: 1 [10240/25000 (41%)]\tLoss: 1.924690 testing loss: tensor(1.9668)\n",
      "Train Epoch: 1 [10880/25000 (43%)]\tLoss: 1.796170 testing loss: tensor(1.9667)\n",
      "Train Epoch: 1 [11520/25000 (46%)]\tLoss: 1.893883 testing loss: tensor(1.9688)\n",
      "Train Epoch: 1 [12160/25000 (48%)]\tLoss: 1.930627 testing loss: tensor(1.9656)\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 2.030535 testing loss: tensor(1.9708)\n",
      "Train Epoch: 1 [13440/25000 (54%)]\tLoss: 1.963361 testing loss: tensor(1.9666)\n",
      "Train Epoch: 1 [14080/25000 (56%)]\tLoss: 2.030177 testing loss: tensor(1.9703)\n",
      "Train Epoch: 1 [14720/25000 (59%)]\tLoss: 1.930491 testing loss: tensor(1.9684)\n",
      "Train Epoch: 1 [15360/25000 (61%)]\tLoss: 1.942518 testing loss: tensor(1.9664)\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 2.017820 testing loss: tensor(1.9652)\n",
      "Train Epoch: 1 [16640/25000 (66%)]\tLoss: 1.928853 testing loss: tensor(1.9662)\n",
      "Train Epoch: 1 [17280/25000 (69%)]\tLoss: 1.903262 testing loss: tensor(1.9661)\n",
      "Train Epoch: 1 [17920/25000 (71%)]\tLoss: 1.834204 testing loss: tensor(1.9673)\n",
      "Train Epoch: 1 [18560/25000 (74%)]\tLoss: 1.941351 testing loss: tensor(1.9660)\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 1.963066 testing loss: tensor(1.9657)\n",
      "Train Epoch: 1 [19840/25000 (79%)]\tLoss: 1.977481 testing loss: tensor(1.9661)\n",
      "Train Epoch: 1 [20480/25000 (82%)]\tLoss: 2.045202 testing loss: tensor(1.9673)\n",
      "Train Epoch: 1 [21120/25000 (84%)]\tLoss: 1.959993 testing loss: tensor(1.9647)\n",
      "Train Epoch: 1 [21760/25000 (87%)]\tLoss: 1.880448 testing loss: tensor(1.9647)\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 1.833226 testing loss: tensor(1.9652)\n",
      "Train Epoch: 1 [23040/25000 (92%)]\tLoss: 1.908079 testing loss: tensor(1.9655)\n",
      "Train Epoch: 1 [23680/25000 (94%)]\tLoss: 2.124453 testing loss: tensor(1.9657)\n",
      "Train Epoch: 1 [24320/25000 (97%)]\tLoss: 2.000373 testing loss: tensor(1.9663)\n",
      "Train Epoch: 1 [7800/25000 (99%)]\tLoss: 2.042536 testing loss: tensor(1.9660)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.9660)\n",
      "CS 2 : 2.02076\n",
      "DP 2 : 2.01068\n",
      "heuristic 2 : 2.1538\n",
      "DP: 1.9953912496566772\n",
      "tensor([0.1324, 0.4309, 0.4366])\n",
      "tensor([0.4994, 0.5006, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.1)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "\n",
    "\n",
    "for order in stage:\n",
    "    producedata(order)\n",
    "    #running\n",
    "    for order1 in order1name:\n",
    "        print(\"Supervised Aim:\", order, order1)\n",
    "\n",
    "        # for mapping binary to payments before softmax\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(n, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, n),\n",
    "        )\n",
    "        model.apply(init_weights)\n",
    "        # optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr1)\n",
    "\n",
    "        runningLossNN = []\n",
    "        runningLossCS = []\n",
    "        runningLossDP = []\n",
    "        runningLossHeuristic = []\n",
    "        #model=torch.load(\"save/pytorchNN=5dp1\");\n",
    "        #model.eval()\n",
    "        ##order1name=[\"costsharing\",\"dp\",\"heuristic\",\"random initializing\"]\n",
    "        for epoch in range(1, supervisionEpochs + 1):\n",
    "            #             print(\"distributionRatio\",distributionRatio)\n",
    "            if (order1 == \"costsharing\"):\n",
    "                supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "            elif (order1 == \"dp\"):\n",
    "                supervisionTrain(epoch, dpSupervisionRule)\n",
    "            elif (order1 == \"heuristic\"):\n",
    "                supervisionTrain(epoch, heuristicSupervisionRule)\n",
    "            elif (order1 == \"random initializing\"):\n",
    "                print(\"do nothing\")\n",
    "\n",
    "        test()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr2)\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(epoch)\n",
    "            test()\n",
    "        losslistname.append(order + \" \" + order1)\n",
    "        losslist.append(losslisttemp)\n",
    "        losslisttemp = []\n",
    "        savepath = \"save/pytorchNN=5all-beta\" + order + str(order1)\n",
    "        torch.save(model, savepath)\n",
    "        print(\"end\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colorlist = [\"#D2691E\", '#4169E1', \"#9ACD32\", \"#B22222\", \"#FF00FF\", \"#708090\"]\n",
    "for i in range(len(losslist)):\n",
    "    plt.plot(losslist[i],\n",
    "             '.-',\n",
    "             color=colorlist[i],\n",
    "             alpha=0.8,\n",
    "             label=losslistname[i])\n",
    "\n",
    "# 显示标签，如果不加这句，即使加了label='一些数字'的参数，最终还是不会显示标签\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('Trianing Times')\n",
    "plt.ylabel('Delay')\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('pytorchNN=3_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
