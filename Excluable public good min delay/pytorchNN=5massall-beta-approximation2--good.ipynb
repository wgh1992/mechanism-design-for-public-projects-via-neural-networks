{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40058530825361593\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from matplotlib import pylab\n",
    "#from random import randint\n",
    "import random\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "from pylab import *\n",
    "from scipy.stats import beta\n",
    "import math\n",
    "n = 5\n",
    "epochs = 5\n",
    "supervisionEpochs = 10\n",
    "lr = 0.0005\n",
    "log_interval = 20\n",
    "trainSize = 60000#100000\n",
    "percentage_train_test= 0.25\n",
    "penaltyLambda = 10\n",
    "doublePeakHighMean = 0.9\n",
    "doublePeakLowMean = 0.1\n",
    "doublePeakStd = 0.1\n",
    "uniformlow=0\n",
    "uniformhigh=1.0\n",
    "normalloc = 0.2\n",
    "normalscale = 0.1\n",
    "\n",
    "cauchyloc = 1/n\n",
    "cauchyscalen = 0.004\n",
    "\n",
    "exponentialhigh = 15 #Symbol(\"b\", real=True)\n",
    "exponentiallow  = 15 #Symbol(\"a\", real=True)\n",
    "\n",
    "\n",
    "beta_a = 0.3\n",
    "beta_b  = 0.2\n",
    "kumaraswamy_a = beta_a \n",
    "kumaraswamy_b = (1.0+(beta_a-1.0)*math.pow( (beta_a+beta_b-2.0)/(beta_a-1.0), beta_a) )/beta_a \n",
    "print(kumaraswamy_b)\n",
    "\n",
    "\n",
    "independentnormalloc1=[(float(ii)+1)/(2*n+1) for ii in range(n,0,-1)]\n",
    "independentnormalscale1=[0.05 for ii in range(n)]\n",
    "\n",
    "independentnormalloc2=[(float(ii)+1)/(2*n+1) for ii in range(1,n+1,1)]\n",
    "independentnormalscale2=[0.05 for ii in range(n)]\n",
    "stage=[\"beta\"]\n",
    "# \"twopeak\",\"normal\",\"uniform\",\"independent1\",\"independent2\",\"cauchy\",\"beta\",\"U-exponential\",\"arcsine\"\n",
    "order1name=[\"dp\",\"random initializing\",\"costsharing\"]\n",
    "# \"costsharing\",\"dp\",\"heuristic\",\"random initializing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio1 = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase1 = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "d3 = D.normal.Normal(normalloc, normalscale)\n",
    "distributionRatio3 = d3.cdf(1) - d3.cdf(0)\n",
    "distributionBase3 = d3.cdf(0)\n",
    "\n",
    "d4 = D.uniform.Uniform(uniformlow,uniformhigh)\n",
    "distributionRatio4 = d4.cdf(1) - d4.cdf(0)\n",
    "distributionBase4 = d4.cdf(0)\n",
    "\n",
    "d5 = [D.normal.Normal(independentnormalloc1[ii], independentnormalscale1[ii]) for ii in range(n)]\n",
    "d6 = [D.normal.Normal(independentnormalloc2[ii], independentnormalscale2[ii]) for ii in range(n)]\n",
    "\n",
    "d7 = D.cauchy.Cauchy(cauchyloc,cauchyscalen)\n",
    "\n",
    "d81 = D.exponential.Exponential(exponentiallow)\n",
    "d82 = D.exponential.Exponential(exponentialhigh)\n",
    "\n",
    "d9 = D.beta.Beta(beta_a,beta_b)\n",
    "#sample_d9=d9.rsample(torch.Size([100000]))\n",
    "\n",
    "#d9_sample=np.linspace(0.0001, 0.9999, 10000)\n",
    "#d9_pdf=torch.exp(d9.log_prob(torch.tensor(d9_sample,dtype=torch.float32)))\n",
    "#d9_delta=d9_sample[1]-d9_sample[0]\n",
    "#d9_sum_pdf=torch.sum(d9_pdf*d9_delta)\n",
    "\n",
    "d10 = D.beta.Beta(0.5,0.5)\n",
    "\n",
    "def cdf(x,y, i=None):\n",
    "    if(y==\"twopeak\"):\n",
    "        return (d1.cdf(x) + d2.cdf(x) - distributionBase1) / 2 / distributionRatio1\n",
    "    elif(y==\"normal\"):\n",
    "        return (d3.cdf(x)-distributionBase3)/distributionRatio3;\n",
    "    elif(y==\"uniform\"):\n",
    "        return (d4.cdf(x)-distributionBase4)/distributionRatio4;\n",
    "    elif(y==\"independent1\"):\n",
    "        return d5[i].cdf(x);\n",
    "    elif(y==\"independent2\"):\n",
    "        return d6[i].cdf(x);\n",
    "    elif(y==\"cauchy\"):\n",
    "        return d7.cdf(x);\n",
    "    elif(y==\"beta\"):\n",
    "#         sum_cdf=0.0;\n",
    "#         if(x<0.0001):\n",
    "#             x=0.00011;\n",
    "#         if(x>0.9999):\n",
    "#             x=0.99989;\n",
    "#         for i in range(len(d9_pdf)):\n",
    "#             if(d9_sample[i]<x):\n",
    "#                 sum_cdf+=d9_pdf[i]*d9_delta;\n",
    "#             else:\n",
    "#                 sum_cdf+=(d9_pdf[i]+d9_pdf[i-1])/ 2 *(x-d9_sample[i-1])\n",
    "#                 break;\n",
    "#         return sum_cdf/d9_sum_pdf\n",
    "#         cdf_v=torch.sum((sample_d9<(x)), dtype=torch.float32)/100000\n",
    "#         return cdf_v\n",
    "#    F(x|a,b)=1–(1–x^a)^b\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(x,kumaraswamy_a),kumaraswamy_b);\n",
    "        except:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(torch.tensor(x,dtype=torch.float32),kumaraswamy_a),kumaraswamy_b);\n",
    "    elif(y==\"arcsine\"):\n",
    "        #\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            res=2.0/math.pi * torch.asin(torch.sqrt(x))\n",
    "            #print(x)\n",
    "            return res# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "        except:\n",
    "            return 2.0/math.pi * torch.asin(torch.sqrt(torch.tensor(x,dtype=torch.float32)))# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "    elif(y==\"U-exponential\"):\n",
    "        return (d81.cdf(x) + (1.0 - d82.cdf(1.0-x)))  / 2 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    if torch.sum(bits).item() == 0:\n",
    "        return torch.ones(n)\n",
    "    bits = bits.type(torch.float32)\n",
    "    negBits = torch.ones(n) - bits\n",
    "    payments = model(bits)\n",
    "    payments = payments - 1000 * negBits\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    payments = payments + negBits\n",
    "    return payments\n",
    "\n",
    "def tpToBits(tp, bits=torch.ones(n).type(torch.uint8)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = (tp >= payments).type(torch.uint8)\n",
    "    if torch.equal(newBits, bits):\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits(tp, newBits)#bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    return n - torch.sum(tpToBits(tp).type(torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    global  dp,decision\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money, yes]\n",
    "        offer = float(offerIndex) / dpPrecision\n",
    "        if tp[i] >= offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits[i] = 0\n",
    "            payments[i] = 1\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "#     print()\n",
    "#     print(tp)\n",
    "#     print(bits)\n",
    "#     print(payments)\n",
    "#     print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def heuristicSupervisionRule(tp):\n",
    "    global  dp_H,decision_H\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    tempold=-1;\n",
    "    for turns in range(n,0,-1):\n",
    "        money = dpPrecision\n",
    "        j=0\n",
    "        tempo=sum(bits)\n",
    "        #print(\"bits\",tempo)\n",
    "        for i in range(n):\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            \n",
    "            offerIndex = decision_H[tempo ,tempo - i, money]\n",
    "            offer = float(offerIndex) / dpPrecision\n",
    "            while(j<n):\n",
    "                if(bits[j]!=0):\n",
    "                    break;\n",
    "                j+=1;\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            if tp[j] >= offer:\n",
    "                #print(money,j,tp[j],offer)\n",
    "                money -= offerIndex\n",
    "                bits[j] = 1\n",
    "                payments[j] = offer\n",
    "            else:\n",
    "                bits[j] = 0;\n",
    "                payments[j] = 1.0;\n",
    "            j+=1;\n",
    "        #print(\"money\",money)\n",
    "        if(money==0 and tempold==tempo):\n",
    "            break;\n",
    "        tempold=tempo;\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    for k in range(n, -1, -1):\n",
    "        if k == 0:\n",
    "            break\n",
    "        bits = [1 if tp[ii] >= 1.0 / k else 0 for ii in range(n)]\n",
    "        if sum(bits) == k:\n",
    "            break\n",
    "    if k == 0:\n",
    "        payments = [1 for ii in range(n)]\n",
    "    else:\n",
    "        payments = [1.0 / k if bits[ii] == 1 else 1 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    return float(n - torch.sum(costSharingSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def dpDelay(tp):\n",
    "    return float(n - torch.sum(dpSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def heuristicDelay(tp):\n",
    "    return float(n - torch.sum(heuristicSupervisionRule(tp)[0]).item())\n",
    "\n",
    "\n",
    "# templire=0;\n",
    "# num=1\n",
    "# for i in range(num):\n",
    "#     temp=torch.tensor(samplesJoint[i])\n",
    "#     #print(temp)\n",
    "#     #print(temp);\n",
    "#     #print(heuristicSupervisionRule(temp))\n",
    "#     #print(dpSupervisionRule(temp))\n",
    "#     #print(dpDelay(temp))\n",
    "#     res=dpDelay(temp)\n",
    "#     templire+=res\n",
    "#     #print(\"delay\",res)\n",
    "#     #print()\n",
    "# print(templire/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordAndReport(name, source, loss):\n",
    "    source.append(loss)\n",
    "    realLength = len(source)\n",
    "    #print(f\"{name} ({realLength}): {loss}\")\n",
    "    print(name,realLength,\":\" ,loss)\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    \n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda /100000\n",
    "        \n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "losslist=[];\n",
    "losslistname=[];\n",
    "losslisttemp=[];\n",
    "def test_batch():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lenLoss= 0\n",
    "        nnLoss = 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "            for tp in tp_batch:\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        nnLoss/=lenLoss\n",
    "    return nnLoss\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                delay1 = tpToTotalDelay(tp1)\n",
    "                delay0 = tpToTotalDelay(tp0)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                    loss = loss + (1 - cdf(offer,order)) * delay1 + cdf(offer,order) * delay0\n",
    "                else:\n",
    "                    loss = loss + (1 - cdf(offer,order,i)) * delay1 + cdf(offer,order,i) * delay0\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "    \n",
    "allBits = [torch.tensor(bits, dtype=torch.int16) for bits in itertools.product([0, 1], repeat=n)]\n",
    "\n",
    "def test():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        costSharingLoss = 0\n",
    "        dpLoss = 0\n",
    "        nnLoss = 0\n",
    "        heuristicLoss=0\n",
    "        lenLoss= 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "                heuristicLoss+= heuristicDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        costSharingLoss /= lenLoss\n",
    "        dpLoss /= lenLoss\n",
    "        nnLoss /= lenLoss\n",
    "        heuristicLoss /= lenLoss\n",
    "        #print(lenLoss)\n",
    "        losslisttemp.append(nnLoss);\n",
    "        recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "        recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "        recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "        recordAndReport(\"heuristic\", runningLossHeuristic, heuristicLoss)\n",
    "        print(\"DP:\",dp[n, dpPrecision,0])\n",
    "        #for i in range(n, 0, -1):\n",
    "        #    print(\"Heuristic:\",i,5*(1-dp_H[i, i, dpPrecision]))\n",
    "        for i in range(n, 0, -1):\n",
    "            print(\n",
    "                    tpToPayments(\n",
    "                            torch.tensor([0 if ii >= i else 1 for ii in range(n)], dtype=torch.float32)\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "dpPrecision = 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def producedata(order):\n",
    "    global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "    dpPrecision = 100 \n",
    "    if(order==\"twopeak\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = np.random.normal(\n",
    "            loc=doublePeakLowMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=doublePeakLowMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samples2 = np.random.normal(\n",
    "            loc=doublePeakHighMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = np.random.normal(\n",
    "                        loc=doublePeakHighMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "    elif(order==\"normal\"):\n",
    "        print(\"loc\",normalloc, \"scale\",normalscale)\n",
    "        samples1 = np.random.normal(\n",
    "            loc=normalloc, scale=normalscale, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=normalloc, scale=normalscale\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"uniform\"):  \n",
    "        print(\"uniformlow\",uniformlow, \"uniformhigh\",uniformhigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        uniformlow, uniformhigh\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent1\"):\n",
    "        print(\"loc\",independentnormalloc1,\"scale\",independentnormalscale1)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent2\"):\n",
    "        print(\"loc\",independentnormalloc2, \"scale\",independentnormalscale2)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"cauchy\"):\n",
    "        print(\"cauchyloc\",cauchyloc, \"cauchyscale\",cauchyscalen)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"beta\"):\n",
    "        print(\"beta_a\",beta_a, \"beta_b\",beta_b)\n",
    "        print(\"kumaraswamy_a\",kumaraswamy_a, \"kumaraswamy_b\",kumaraswamy_b)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"arcsine\"):\n",
    "        print(\"betalow\",0.5, \"betahigh\",0.5)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"U-exponential\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = d81.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d81.rsample(torch.Size([1])).numpy()\n",
    "                    \n",
    "        samples2 = d82.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        \n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = d82.rsample(torch.Size([1])).numpy()\n",
    "        samples2 = 1.0 - samples2\n",
    "        samplesJoint = signals * samples1 - (signals - 1.0) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "        \n",
    "    plt.hist(samplesJoint,bins=500)\n",
    "    fig = plt.gcf()\n",
    "    plt.show()\n",
    "    savepath=\"beta2 \"+order+str(order)\n",
    "    fig.savefig(savepath+' distribution2.png')\n",
    "    \n",
    "    tp_dataset = TensorDataset(tp_tensor[: int(trainSize * percentage_train_test)])\n",
    "    tp_dataset_testing = TensorDataset(tp_tensor[int(trainSize * (1.0-percentage_train_test)) :])\n",
    "    tp_dataloader = DataLoader(tp_dataset, batch_size=128, shuffle=True)\n",
    "    tp_dataloader_testing = DataLoader(tp_dataset_testing, batch_size=256, shuffle=False)\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "    decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for yes in range(n + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp[0, 0, yes] = 0\n",
    "            else:\n",
    "                dp[0, money, yes] = yes# + 1.0\n",
    "    for ppl in range(1,  n + 1):\n",
    "        for yes in range(n + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                minSoFar = 1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1 - cdf(offer,order)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order) * (1 + dp[ppl - 1, money, yes])\n",
    "                    else:\n",
    "                        res = (1 - cdf(offer,order,n-ppl)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order,n-ppl) * (1 + dp[ppl - 1, money, yes])\n",
    "                    if minSoFar > res.item():\n",
    "                        minSoFar = res.item()\n",
    "                        decision[ppl, money, yes] = offerIndex\n",
    "                dp[ppl, money, yes] = minSoFar\n",
    "    \n",
    "    print(\"dp\",dp[n, dpPrecision, 0])\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp_H = np.zeros([n + 1 , n + 1, dpPrecision + 1])\n",
    "    decision_H = np.zeros([n + 1 , n + 1, dpPrecision + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for i in range(1,n+1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp_H[i, 0, 0] = 1\n",
    "            else:\n",
    "                offer = money / dpPrecision\n",
    "                dp_H[i, 0, money] = 0#cdf(offer)# + 1.0\n",
    "    for i in range(1,n+1):\n",
    "        for ppl in range(1, i + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                maxSoFar = -1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1-cdf(offer,order)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    else:\n",
    "                        res = (1-cdf(offer,order,ppl-1)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    if maxSoFar < res.item():\n",
    "                        maxSoFar = res.item()\n",
    "                        decision_H[i, ppl, money] = offerIndex\n",
    "                dp_H[i, ppl, money] = maxSoFar\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_a 0.3 beta_b 0.2\n",
      "kumaraswamy_a 0.3 kumaraswamy_b 0.40058530825361593\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dp 1.866951823234558\n",
      "Supervised Aim: beta dp\n",
      "Train Epoch: 1 [0/15000 (0%)]\tLoss: 0.032886\n",
      "Train Epoch: 1 [2560/15000 (17%)]\tLoss: 0.013719\n",
      "Train Epoch: 1 [5120/15000 (34%)]\tLoss: 0.008011\n",
      "Train Epoch: 1 [7680/15000 (51%)]\tLoss: 0.003840\n",
      "Train Epoch: 1 [10240/15000 (68%)]\tLoss: 0.002361\n",
      "Train Epoch: 1 [12800/15000 (85%)]\tLoss: 0.001506\n",
      "Train Epoch: 2 [0/15000 (0%)]\tLoss: 0.000705\n",
      "Train Epoch: 2 [2560/15000 (17%)]\tLoss: 0.000426\n",
      "Train Epoch: 2 [5120/15000 (34%)]\tLoss: 0.000258\n",
      "Train Epoch: 2 [7680/15000 (51%)]\tLoss: 0.000210\n",
      "Train Epoch: 2 [10240/15000 (68%)]\tLoss: 0.000148\n",
      "Train Epoch: 2 [12800/15000 (85%)]\tLoss: 0.000114\n",
      "Train Epoch: 3 [0/15000 (0%)]\tLoss: 0.000093\n",
      "Train Epoch: 3 [2560/15000 (17%)]\tLoss: 0.000071\n",
      "Train Epoch: 3 [5120/15000 (34%)]\tLoss: 0.000058\n",
      "Train Epoch: 3 [7680/15000 (51%)]\tLoss: 0.000049\n",
      "Train Epoch: 3 [10240/15000 (68%)]\tLoss: 0.000048\n",
      "Train Epoch: 3 [12800/15000 (85%)]\tLoss: 0.000039\n",
      "Train Epoch: 4 [0/15000 (0%)]\tLoss: 0.000033\n",
      "Train Epoch: 4 [2560/15000 (17%)]\tLoss: 0.000029\n",
      "Train Epoch: 4 [5120/15000 (34%)]\tLoss: 0.000026\n",
      "Train Epoch: 4 [7680/15000 (51%)]\tLoss: 0.000025\n",
      "Train Epoch: 4 [10240/15000 (68%)]\tLoss: 0.000022\n",
      "Train Epoch: 4 [12800/15000 (85%)]\tLoss: 0.000020\n",
      "Train Epoch: 5 [0/15000 (0%)]\tLoss: 0.000018\n",
      "Train Epoch: 5 [2560/15000 (17%)]\tLoss: 0.000016\n",
      "Train Epoch: 5 [5120/15000 (34%)]\tLoss: 0.000017\n",
      "Train Epoch: 5 [7680/15000 (51%)]\tLoss: 0.000013\n",
      "Train Epoch: 5 [10240/15000 (68%)]\tLoss: 0.000012\n",
      "Train Epoch: 5 [12800/15000 (85%)]\tLoss: 0.000012\n",
      "Train Epoch: 6 [0/15000 (0%)]\tLoss: 0.000011\n",
      "Train Epoch: 6 [2560/15000 (17%)]\tLoss: 0.000010\n",
      "Train Epoch: 6 [5120/15000 (34%)]\tLoss: 0.000010\n",
      "Train Epoch: 6 [7680/15000 (51%)]\tLoss: 0.000020\n",
      "Train Epoch: 6 [10240/15000 (68%)]\tLoss: 0.000009\n",
      "Train Epoch: 6 [12800/15000 (85%)]\tLoss: 0.000008\n",
      "Train Epoch: 7 [0/15000 (0%)]\tLoss: 0.000008\n",
      "Train Epoch: 7 [2560/15000 (17%)]\tLoss: 0.000007\n",
      "Train Epoch: 7 [5120/15000 (34%)]\tLoss: 0.000008\n",
      "Train Epoch: 7 [7680/15000 (51%)]\tLoss: 0.000007\n",
      "Train Epoch: 7 [10240/15000 (68%)]\tLoss: 0.000007\n",
      "Train Epoch: 7 [12800/15000 (85%)]\tLoss: 0.000006\n",
      "Train Epoch: 8 [0/15000 (0%)]\tLoss: 0.000009\n",
      "Train Epoch: 8 [2560/15000 (17%)]\tLoss: 0.000008\n",
      "Train Epoch: 8 [5120/15000 (34%)]\tLoss: 0.000010\n",
      "Train Epoch: 8 [7680/15000 (51%)]\tLoss: 0.000008\n",
      "Train Epoch: 8 [10240/15000 (68%)]\tLoss: 0.000014\n",
      "Train Epoch: 8 [12800/15000 (85%)]\tLoss: 0.000012\n",
      "Train Epoch: 9 [0/15000 (0%)]\tLoss: 0.000009\n",
      "Train Epoch: 9 [2560/15000 (17%)]\tLoss: 0.000007\n",
      "Train Epoch: 9 [5120/15000 (34%)]\tLoss: 0.000007\n",
      "Train Epoch: 9 [7680/15000 (51%)]\tLoss: 0.000006\n",
      "Train Epoch: 9 [10240/15000 (68%)]\tLoss: 0.000004\n",
      "Train Epoch: 9 [12800/15000 (85%)]\tLoss: 0.000004\n",
      "Train Epoch: 10 [0/15000 (0%)]\tLoss: 0.000004\n",
      "Train Epoch: 10 [2560/15000 (17%)]\tLoss: 0.000004\n",
      "Train Epoch: 10 [5120/15000 (34%)]\tLoss: 0.000004\n",
      "Train Epoch: 10 [7680/15000 (51%)]\tLoss: 0.000003\n",
      "Train Epoch: 10 [10240/15000 (68%)]\tLoss: 0.000005\n",
      "Train Epoch: 10 [12800/15000 (85%)]\tLoss: 0.000004\n",
      "NN 1 : tensor(1.4191)\n",
      "CS 1 : 1.6291333333333333\n",
      "DP 1 : 1.3145333333333333\n",
      "heuristic 1 : 1.938\n",
      "DP: 1.866951823234558\n",
      "tensor([4.8920e-01, 5.0810e-01, 2.5708e-03, 1.1951e-04, 4.0241e-06])\n",
      "tensor([4.8526e-01, 5.0494e-01, 9.1381e-03, 6.6978e-04, 1.0000e+00])\n",
      "tensor([0.4854, 0.5038, 0.0107, 1.0000, 1.0000])\n",
      "tensor([0.4913, 0.5087, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/15000 (0%)]\tLoss: 1.550066 testing loss: tensor(1.4443)\n",
      "Train Epoch: 1 [2560/15000 (17%)]\tLoss: 1.509365 testing loss: tensor(1.4137)\n",
      "Train Epoch: 1 [5120/15000 (34%)]\tLoss: 1.414410 testing loss: tensor(1.3763)\n",
      "Train Epoch: 1 [7680/15000 (51%)]\tLoss: 1.443841 testing loss: tensor(1.3653)\n",
      "Train Epoch: 1 [10240/15000 (68%)]\tLoss: 1.607709 testing loss: tensor(1.3575)\n",
      "Train Epoch: 1 [12800/15000 (85%)]\tLoss: 1.245962 testing loss: tensor(1.3517)\n",
      "penalty: 0.193210631608963\n",
      "NN 2 : tensor(1.3565)\n",
      "CS 2 : 1.6291333333333333\n",
      "DP 2 : 1.3145333333333333\n",
      "heuristic 2 : 1.938\n",
      "DP: 1.866951823234558\n",
      "tensor([5.2208e-01, 4.7789e-01, 3.0835e-05, 3.1623e-08, 2.8402e-11])\n",
      "tensor([5.1119e-01, 4.8854e-01, 2.7322e-04, 6.1679e-07, 1.0000e+00])\n",
      "tensor([4.9002e-01, 5.0947e-01, 5.1396e-04, 1.0000e+00, 1.0000e+00])\n",
      "tensor([0.5036, 0.4964, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 2 [0/15000 (0%)]\tLoss: 1.200195 testing loss: tensor(1.3605)\n",
      "Train Epoch: 2 [2560/15000 (17%)]\tLoss: 1.323756 testing loss: tensor(1.3466)\n",
      "Train Epoch: 2 [5120/15000 (34%)]\tLoss: 1.549418 testing loss: tensor(1.3496)\n",
      "Train Epoch: 2 [7680/15000 (51%)]\tLoss: 1.515082 testing loss: tensor(1.3432)\n",
      "Train Epoch: 2 [10240/15000 (68%)]\tLoss: 1.294437 testing loss: tensor(1.3401)\n",
      "Train Epoch: 2 [12800/15000 (85%)]\tLoss: 1.415622 testing loss: tensor(1.3371)\n",
      "penalty: 0.11063924431800842\n",
      "NN 3 : tensor(1.3431)\n",
      "CS 3 : 1.6291333333333333\n",
      "DP 3 : 1.3145333333333333\n",
      "heuristic 3 : 1.938\n",
      "DP: 1.866951823234558\n",
      "tensor([5.1760e-01, 4.8239e-01, 1.2043e-05, 1.0016e-08, 1.6909e-12])\n",
      "tensor([5.2099e-01, 4.7888e-01, 1.3036e-04, 2.4798e-07, 1.0000e+00])\n",
      "tensor([5.1763e-01, 4.8209e-01, 2.8150e-04, 1.0000e+00, 1.0000e+00])\n",
      "tensor([0.5356, 0.4644, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 3 [0/15000 (0%)]\tLoss: 1.300607 testing loss: tensor(1.3449)\n",
      "Train Epoch: 3 [2560/15000 (17%)]\tLoss: 1.464955 testing loss: tensor(1.3399)\n",
      "Train Epoch: 3 [5120/15000 (34%)]\tLoss: 1.280314 testing loss: tensor(1.3429)\n",
      "Train Epoch: 3 [7680/15000 (51%)]\tLoss: 1.334162 testing loss: tensor(1.3362)\n",
      "Train Epoch: 3 [10240/15000 (68%)]\tLoss: 1.311219 testing loss: tensor(1.3372)\n",
      "Train Epoch: 3 [12800/15000 (85%)]\tLoss: 1.443287 testing loss: tensor(1.3383)\n",
      "penalty: 0.1042962372303009\n",
      "NN 4 : tensor(1.3429)\n",
      "CS 4 : 1.6291333333333333\n",
      "DP 4 : 1.3145333333333333\n",
      "heuristic 4 : 1.938\n",
      "DP: 1.866951823234558\n",
      "tensor([5.1791e-01, 4.8208e-01, 3.0701e-06, 1.8191e-09, 1.4944e-13])\n",
      "tensor([5.0963e-01, 4.9033e-01, 3.8046e-05, 5.3385e-08, 1.0000e+00])\n",
      "tensor([5.0270e-01, 4.9719e-01, 1.0800e-04, 1.0000e+00, 1.0000e+00])\n",
      "tensor([0.4818, 0.5182, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 4 [0/15000 (0%)]\tLoss: 1.167997 testing loss: tensor(1.3565)\n",
      "Train Epoch: 4 [2560/15000 (17%)]\tLoss: 1.351891 testing loss: tensor(1.3476)\n",
      "Train Epoch: 4 [5120/15000 (34%)]\tLoss: 1.396244 testing loss: tensor(1.3316)\n",
      "Train Epoch: 4 [7680/15000 (51%)]\tLoss: 1.536658 testing loss: tensor(1.3338)\n",
      "Train Epoch: 4 [10240/15000 (68%)]\tLoss: 1.334374 testing loss: tensor(1.3335)\n",
      "Train Epoch: 4 [12800/15000 (85%)]\tLoss: 1.446755 testing loss: tensor(1.3329)\n",
      "penalty: 0.12628939747810364\n",
      "NN 5 : tensor(1.3333)\n",
      "CS 5 : 1.6291333333333333\n",
      "DP 5 : 1.3145333333333333\n",
      "heuristic 5 : 1.938\n",
      "DP: 1.866951823234558\n",
      "tensor([4.9748e-01, 5.0252e-01, 2.2768e-06, 9.4896e-10, 8.2495e-14])\n",
      "tensor([5.0006e-01, 4.9990e-01, 4.0449e-05, 3.7348e-08, 1.0000e+00])\n",
      "tensor([4.9713e-01, 5.0275e-01, 1.1780e-04, 1.0000e+00, 1.0000e+00])\n",
      "tensor([0.5077, 0.4923, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 5 [0/15000 (0%)]\tLoss: 1.337820 testing loss: tensor(1.3319)\n",
      "Train Epoch: 5 [2560/15000 (17%)]\tLoss: 1.282195 testing loss: tensor(1.3288)\n",
      "Train Epoch: 5 [5120/15000 (34%)]\tLoss: 1.516388 testing loss: tensor(1.3320)\n",
      "Train Epoch: 5 [7680/15000 (51%)]\tLoss: 1.287431 testing loss: tensor(1.3325)\n",
      "Train Epoch: 5 [10240/15000 (68%)]\tLoss: 1.445703 testing loss: tensor(1.3282)\n",
      "Train Epoch: 5 [12800/15000 (85%)]\tLoss: 1.352991 testing loss: tensor(1.3292)\n",
      "penalty: 0.13049852848052979\n",
      "NN 6 : tensor(1.3333)\n",
      "CS 6 : 1.6291333333333333\n",
      "DP 6 : 1.3145333333333333\n",
      "heuristic 6 : 1.938\n",
      "DP: 1.866951823234558\n",
      "tensor([4.8486e-01, 5.1514e-01, 9.4241e-07, 4.2619e-10, 1.6011e-14])\n",
      "tensor([4.8742e-01, 5.1256e-01, 1.8280e-05, 1.9189e-08, 1.0000e+00])\n",
      "tensor([4.9049e-01, 5.0946e-01, 5.6322e-05, 1.0000e+00, 1.0000e+00])\n",
      "tensor([0.5050, 0.4950, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: beta random initializing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "NN 1 : tensor(1.6295)\n",
      "CS 1 : 1.6291333333333333\n",
      "DP 1 : 1.3145333333333333\n",
      "heuristic 1 : 1.938\n",
      "DP: 1.866951823234558\n",
      "tensor([0.1634, 0.2597, 0.2321, 0.1691, 0.1758])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1658, 0.2841, 0.3350, 0.2151, 1.0000])\n",
      "tensor([0.2439, 0.3793, 0.3768, 1.0000, 1.0000])\n",
      "tensor([0.4090, 0.5910, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/15000 (0%)]\tLoss: 1.733793 testing loss: tensor(1.6287)\n",
      "Train Epoch: 1 [2560/15000 (17%)]\tLoss: 1.614829 testing loss: tensor(1.6246)\n",
      "Train Epoch: 1 [5120/15000 (34%)]\tLoss: 1.871634 testing loss: tensor(1.6139)\n",
      "Train Epoch: 1 [7680/15000 (51%)]\tLoss: 1.649899 testing loss: tensor(1.5969)\n",
      "Train Epoch: 1 [10240/15000 (68%)]\tLoss: 1.685770 testing loss: tensor(1.5778)\n",
      "Train Epoch: 1 [12800/15000 (85%)]\tLoss: 1.670207 testing loss: tensor(1.5497)\n",
      "penalty: 0.013799130916595459\n",
      "NN 2 : tensor(1.5253)\n",
      "CS 2 : 1.6291333333333333\n",
      "DP 2 : 1.3145333333333333\n",
      "heuristic 2 : 1.938\n",
      "DP: 1.866951823234558\n",
      "tensor([3.0265e-01, 3.1103e-01, 3.1771e-02, 5.2328e-05, 3.5450e-01])\n",
      "tensor([4.1308e-01, 4.6644e-01, 1.1977e-01, 7.1338e-04, 1.0000e+00])\n",
      "tensor([0.4125, 0.4688, 0.1187, 1.0000, 1.0000])\n",
      "tensor([0.4407, 0.5593, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 2 [0/15000 (0%)]\tLoss: 1.697883 testing loss: tensor(1.5245)\n",
      "Train Epoch: 2 [2560/15000 (17%)]\tLoss: 1.561891 testing loss: tensor(1.5074)\n",
      "Train Epoch: 2 [5120/15000 (34%)]\tLoss: 1.579952 testing loss: tensor(1.4811)\n",
      "Train Epoch: 2 [7680/15000 (51%)]\tLoss: 1.497112 testing loss: tensor(1.4579)\n",
      "Train Epoch: 2 [10240/15000 (68%)]\tLoss: 1.635987 testing loss: tensor(1.4458)\n",
      "Train Epoch: 2 [12800/15000 (85%)]\tLoss: 1.619675 testing loss: tensor(1.4281)\n",
      "penalty: 0.08472111821174622\n",
      "NN 3 : tensor(1.4237)\n",
      "CS 3 : 1.6291333333333333\n",
      "DP 3 : 1.3145333333333333\n",
      "heuristic 3 : 1.938\n",
      "DP: 1.866951823234558\n",
      "tensor([3.4909e-01, 3.3104e-01, 1.7140e-05, 7.9692e-11, 3.1986e-01])\n",
      "tensor([4.8971e-01, 5.0871e-01, 1.5833e-03, 1.1315e-07, 1.0000e+00])\n",
      "tensor([0.4860, 0.5114, 0.0026, 1.0000, 1.0000])\n",
      "tensor([0.4886, 0.5114, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 3 [0/15000 (0%)]\tLoss: 1.428330 testing loss: tensor(1.4263)\n",
      "Train Epoch: 3 [2560/15000 (17%)]\tLoss: 1.468318 testing loss: tensor(1.4146)\n",
      "Train Epoch: 3 [5120/15000 (34%)]\tLoss: 1.550004 testing loss: tensor(1.4121)\n",
      "Train Epoch: 3 [7680/15000 (51%)]\tLoss: 1.455755 testing loss: tensor(1.4075)\n",
      "Train Epoch: 3 [10240/15000 (68%)]\tLoss: 1.564151 testing loss: tensor(1.4023)\n",
      "Train Epoch: 3 [12800/15000 (85%)]\tLoss: 1.589674 testing loss: tensor(1.4021)\n",
      "penalty: 0.04510420560836792\n",
      "NN 4 : tensor(1.3987)\n",
      "CS 4 : 1.6291333333333333\n",
      "DP 4 : 1.3145333333333333\n",
      "heuristic 4 : 1.938\n",
      "DP: 1.866951823234558\n",
      "tensor([3.5155e-01, 3.1528e-01, 4.1605e-07, 1.6050e-13, 3.3317e-01])\n",
      "tensor([4.9010e-01, 5.0977e-01, 1.3897e-04, 1.4213e-09, 1.0000e+00])\n",
      "tensor([4.8773e-01, 5.1193e-01, 3.3211e-04, 1.0000e+00, 1.0000e+00])\n",
      "tensor([0.4938, 0.5062, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 4 [0/15000 (0%)]\tLoss: 1.304637 testing loss: tensor(1.3985)\n",
      "Train Epoch: 4 [2560/15000 (17%)]\tLoss: 1.482984 testing loss: tensor(1.3983)\n",
      "Train Epoch: 4 [5120/15000 (34%)]\tLoss: 1.444032 testing loss: tensor(1.3969)\n",
      "Train Epoch: 4 [7680/15000 (51%)]\tLoss: 1.391920 testing loss: tensor(1.3954)\n",
      "Train Epoch: 4 [10240/15000 (68%)]\tLoss: 1.274307 testing loss: tensor(1.3933)\n",
      "Train Epoch: 4 [12800/15000 (85%)]\tLoss: 1.413307 testing loss: tensor(1.3891)\n",
      "penalty: 0.08601406216621399\n",
      "NN 5 : tensor(1.3937)\n",
      "CS 5 : 1.6291333333333333\n",
      "DP 5 : 1.3145333333333333\n",
      "heuristic 5 : 1.938\n",
      "DP: 1.866951823234558\n",
      "tensor([3.2066e-01, 3.6959e-01, 5.9296e-08, 2.8988e-15, 3.0975e-01])\n",
      "tensor([4.5466e-01, 5.4531e-01, 3.2749e-05, 6.2012e-11, 1.0000e+00])\n",
      "tensor([4.5424e-01, 5.4566e-01, 1.0309e-04, 1.0000e+00, 1.0000e+00])\n",
      "tensor([0.4611, 0.5389, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 5 [0/15000 (0%)]\tLoss: 1.450225 testing loss: tensor(1.3920)\n",
      "Train Epoch: 5 [2560/15000 (17%)]\tLoss: 1.572708 testing loss: tensor(1.3908)\n",
      "Train Epoch: 5 [5120/15000 (34%)]\tLoss: 1.514445 testing loss: tensor(1.3891)\n",
      "Train Epoch: 5 [7680/15000 (51%)]\tLoss: 1.495184 testing loss: tensor(1.3903)\n",
      "Train Epoch: 5 [10240/15000 (68%)]\tLoss: 1.565837 testing loss: tensor(1.3899)\n",
      "Train Epoch: 5 [12800/15000 (85%)]\tLoss: 1.331756 testing loss: tensor(1.3883)\n",
      "penalty: 0.03807416558265686\n",
      "NN 6 : tensor(1.3895)\n",
      "CS 6 : 1.6291333333333333\n",
      "DP 6 : 1.3145333333333333\n",
      "heuristic 6 : 1.938\n",
      "DP: 1.866951823234558\n",
      "tensor([2.8858e-01, 3.6684e-01, 1.8714e-08, 1.5759e-16, 3.4458e-01])\n",
      "tensor([4.2759e-01, 5.7240e-01, 1.2309e-05, 6.6712e-12, 1.0000e+00])\n",
      "tensor([4.2882e-01, 5.7113e-01, 4.5112e-05, 1.0000e+00, 1.0000e+00])\n",
      "tensor([0.4328, 0.5672, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: beta costsharing\n",
      "Train Epoch: 1 [0/15000 (0%)]\tLoss: 0.006294\n",
      "Train Epoch: 1 [2560/15000 (17%)]\tLoss: 0.000140\n",
      "Train Epoch: 1 [5120/15000 (34%)]\tLoss: 0.000019\n",
      "Train Epoch: 1 [7680/15000 (51%)]\tLoss: 0.000007\n",
      "Train Epoch: 1 [10240/15000 (68%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "NN 1 : tensor(1.6291)\n",
      "CS 1 : 1.6291333333333333\n",
      "DP 1 : 1.3145333333333333\n",
      "heuristic 1 : 1.938\n",
      "DP: 1.866951823234558\n",
      "tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500, 1.0000])\n",
      "tensor([0.3333, 0.3333, 0.3333, 1.0000, 1.0000])\n",
      "tensor([0.5000, 0.5000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/15000 (0%)]\tLoss: 1.736119 testing loss: tensor(1.6280)\n",
      "Train Epoch: 1 [2560/15000 (17%)]\tLoss: 1.624441 testing loss: tensor(1.6208)\n",
      "Train Epoch: 1 [5120/15000 (34%)]\tLoss: 1.813545 testing loss: tensor(1.5836)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [7680/15000 (51%)]\tLoss: 1.682394 testing loss: tensor(1.5116)\n",
      "Train Epoch: 1 [10240/15000 (68%)]\tLoss: 1.578259 testing loss: tensor(1.4749)\n",
      "Train Epoch: 1 [12800/15000 (85%)]\tLoss: 1.304694 testing loss: tensor(1.4475)\n",
      "penalty: 0.12331235408782959\n",
      "NN 2 : tensor(1.4357)\n",
      "CS 2 : 1.6291333333333333\n",
      "DP 2 : 1.3145333333333333\n",
      "heuristic 2 : 1.938\n",
      "DP: 1.866951823234558\n",
      "tensor([4.2037e-01, 1.0661e-11, 1.9779e-05, 2.4508e-01, 3.3452e-01])\n",
      "tensor([5.8229e-01, 5.8414e-08, 2.0521e-03, 4.1566e-01, 1.0000e+00])\n",
      "tensor([0.6202, 0.0014, 0.3784, 1.0000, 1.0000])\n",
      "tensor([0.6818, 0.3182, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 2 [0/15000 (0%)]\tLoss: 1.577785 testing loss: tensor(1.4322)\n",
      "Train Epoch: 2 [2560/15000 (17%)]\tLoss: 1.662725 testing loss: tensor(1.4199)\n",
      "Train Epoch: 2 [5120/15000 (34%)]\tLoss: 1.570161 testing loss: tensor(1.4205)\n",
      "Train Epoch: 2 [7680/15000 (51%)]\tLoss: 1.468155 testing loss: tensor(1.4133)\n",
      "Train Epoch: 2 [10240/15000 (68%)]\tLoss: 1.579041 testing loss: tensor(1.4043)\n",
      "Train Epoch: 2 [12800/15000 (85%)]\tLoss: 1.625882 testing loss: tensor(1.4096)\n",
      "penalty: 0.11509186029434204\n",
      "NN 3 : tensor(1.4067)\n",
      "CS 3 : 1.6291333333333333\n",
      "DP 3 : 1.3145333333333333\n",
      "heuristic 3 : 1.938\n",
      "DP: 1.866951823234558\n",
      "tensor([2.9515e-01, 6.3326e-16, 5.8334e-08, 3.3303e-01, 3.7181e-01])\n",
      "tensor([4.5341e-01, 4.1870e-11, 3.7313e-05, 5.4656e-01, 1.0000e+00])\n",
      "tensor([5.7363e-01, 2.4536e-04, 4.2613e-01, 1.0000e+00, 1.0000e+00])\n",
      "tensor([0.7097, 0.2903, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 3 [0/15000 (0%)]\tLoss: 1.374917 testing loss: tensor(1.4125)\n",
      "Train Epoch: 3 [2560/15000 (17%)]\tLoss: 1.441971 testing loss: tensor(1.4107)\n",
      "Train Epoch: 3 [5120/15000 (34%)]\tLoss: 1.328480 testing loss: tensor(1.4057)\n",
      "Train Epoch: 3 [7680/15000 (51%)]\tLoss: 1.494124 testing loss: tensor(1.4049)\n",
      "Train Epoch: 3 [10240/15000 (68%)]\tLoss: 1.493602 testing loss: tensor(1.4050)\n",
      "Train Epoch: 3 [12800/15000 (85%)]\tLoss: 1.476917 testing loss: tensor(1.3993)\n",
      "penalty: 0.14859506487846375\n",
      "NN 4 : tensor(1.4009)\n",
      "CS 4 : 1.6291333333333333\n",
      "DP 4 : 1.3145333333333333\n",
      "heuristic 4 : 1.938\n",
      "DP: 1.866951823234558\n",
      "tensor([3.8042e-01, 7.6090e-17, 1.8494e-08, 3.1536e-01, 3.0422e-01])\n",
      "tensor([5.0579e-01, 1.2221e-11, 1.9363e-05, 4.9419e-01, 1.0000e+00])\n",
      "tensor([5.3081e-01, 2.1219e-04, 4.6898e-01, 1.0000e+00, 1.0000e+00])\n",
      "tensor([0.6088, 0.3912, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 4 [0/15000 (0%)]\tLoss: 1.447502 testing loss: tensor(1.4005)\n",
      "Train Epoch: 4 [2560/15000 (17%)]\tLoss: 1.601045 testing loss: tensor(1.4011)\n",
      "Train Epoch: 4 [5120/15000 (34%)]\tLoss: 1.598691 testing loss: tensor(1.3936)\n",
      "Train Epoch: 4 [7680/15000 (51%)]\tLoss: 1.595621 testing loss: tensor(1.3982)\n",
      "Train Epoch: 4 [10240/15000 (68%)]\tLoss: 1.512184 testing loss: tensor(1.3945)\n",
      "Train Epoch: 4 [12800/15000 (85%)]\tLoss: 1.386432 testing loss: tensor(1.3987)\n",
      "penalty: 0.1588774025440216\n",
      "NN 5 : tensor(1.3967)\n",
      "CS 5 : 1.6291333333333333\n",
      "DP 5 : 1.3145333333333333\n",
      "heuristic 5 : 1.938\n",
      "DP: 1.866951823234558\n",
      "tensor([3.5682e-01, 1.5774e-18, 4.1878e-09, 2.8473e-01, 3.5845e-01])\n",
      "tensor([4.9110e-01, 6.3868e-13, 6.1499e-06, 5.0889e-01, 1.0000e+00])\n",
      "tensor([5.8819e-01, 5.5893e-05, 4.1176e-01, 1.0000e+00, 1.0000e+00])\n",
      "tensor([0.6247, 0.3753, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 5 [0/15000 (0%)]\tLoss: 1.612826 testing loss: tensor(1.3985)\n",
      "Train Epoch: 5 [2560/15000 (17%)]\tLoss: 1.521519 testing loss: tensor(1.3986)\n",
      "Train Epoch: 5 [5120/15000 (34%)]\tLoss: 1.534917 testing loss: tensor(1.3966)\n",
      "Train Epoch: 5 [7680/15000 (51%)]\tLoss: 1.416548 testing loss: tensor(1.3988)\n",
      "Train Epoch: 5 [10240/15000 (68%)]\tLoss: 1.397465 testing loss: tensor(1.3937)\n",
      "Train Epoch: 5 [12800/15000 (85%)]\tLoss: 1.492179 testing loss: tensor(1.3975)\n",
      "penalty: 0.09790009260177612\n",
      "NN 6 : tensor(1.3939)\n",
      "CS 6 : 1.6291333333333333\n",
      "DP 6 : 1.3145333333333333\n",
      "heuristic 6 : 1.938\n",
      "DP: 1.866951823234558\n",
      "tensor([3.0347e-01, 1.3758e-19, 1.8435e-09, 3.1407e-01, 3.8247e-01])\n",
      "tensor([4.6569e-01, 1.1033e-13, 3.5788e-06, 5.3430e-01, 1.0000e+00])\n",
      "tensor([5.4013e-01, 3.1557e-05, 4.5984e-01, 1.0000e+00, 1.0000e+00])\n",
      "tensor([0.6117, 0.3883, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.1)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "    \n",
    "for order in stage:\n",
    "    producedata(order)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "    #running\n",
    "    for order1 in order1name:\n",
    "        print(\"Supervised Aim:\",order,order1)\n",
    "        \n",
    "        # for mapping binary to payments before softmax\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(n, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, n),\n",
    "        )\n",
    "        model.apply(init_weights)\n",
    "        # optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        runningLossNN = []\n",
    "        runningLossCS = []\n",
    "        runningLossDP = []\n",
    "        runningLossHeuristic = []\n",
    "        #model=torch.load(\"save/pytorchNN=5dp1\");\n",
    "        #model.eval()\n",
    "        ##order1name=[\"costsharing\",\"dp\",\"heuristic\",\"random initializing\"]\n",
    "        for epoch in range(1, supervisionEpochs + 1):\n",
    "#             print(\"distributionRatio\",distributionRatio)\n",
    "            if(order1==\"costsharing\"):\n",
    "                supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "            elif(order1==\"dp\"):\n",
    "                supervisionTrain(epoch, dpSupervisionRule)\n",
    "            elif(order1==\"heuristic\"):\n",
    "                supervisionTrain(epoch, heuristicSupervisionRule)\n",
    "            elif(order1==\"random initializing\"):\n",
    "                print(\"do nothing\");\n",
    "\n",
    "        test()\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(epoch)\n",
    "            test()\n",
    "        losslistname.append(order+\" \"+order1);\n",
    "        losslist.append(losslisttemp);\n",
    "        losslisttemp=[];\n",
    "        savepath=\"save/pytorchNN=5all-beta\"+order+str(order1)\n",
    "        torch.save(model, savepath);\n",
    "        print(\"end\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colorlist=[\"#D2691E\",'#4169E1',\"#9ACD32\",\"#B22222\",\"#FF00FF\",\"#708090\"]\n",
    "for i in range(len(losslist)):\n",
    "    plt.plot(losslist[i], 'ro-', color=colorlist[i], alpha=0.8, label=losslistname[i])\n",
    "\n",
    "# 显示标签，如果不加这句，即使加了label='一些数字'的参数，最终还是不会显示标签\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('trianing times')\n",
    "plt.ylabel('delay')\n",
    "\n",
    "fig = plt.gcf()\n",
    "plt.show()\n",
    "fig.savefig('result2.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
