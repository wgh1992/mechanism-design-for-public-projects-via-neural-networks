{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "from pylab import *\n",
    "from scipy.stats import beta\n",
    "import random \n",
    "n = 5\n",
    "epochs = 1\n",
    "supervisionEpochs = 3\n",
    "lr = 0.0005\n",
    "log_interval = 10\n",
    "trainSize = 40000#100000\n",
    "percentage_train_test= 0.75\n",
    "penaltyLambda = 10\n",
    "doublePeakHighMean = 0.85\n",
    "doublePeakLowMean = 0.15\n",
    "doublePeakStd = 0.1\n",
    "uniformlow=0\n",
    "uniformhigh=1.0\n",
    "normalloc = 0.2\n",
    "normalscale = 0.1\n",
    "\n",
    "cauchyloc = 1/n\n",
    "cauchyscalen = 0.004\n",
    "\n",
    "exponentialhigh = 15 #Symbol(\"b\", real=True)\n",
    "exponentiallow  = 15 #Symbol(\"a\", real=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "independentnormalloc1=[(float(ii)+1)/(2*n+1) for ii in range(n,0,-1)]\n",
    "independentnormalscale1=[0.05 for ii in range(n)]\n",
    "\n",
    "independentnormalloc2=[(float(ii)+1)/(2*n+1) for ii in range(1,n+1,1)]\n",
    "independentnormalscale2=[0.05 for ii in range(n)]\n",
    "stage=[\"twopeak\"]\n",
    "# \"twopeak\",\"normal\",\"uniform\",\"independent1\",\"independent2\",\"cauchy\",\"U-exponential\"\n",
    "order1name=[\"random initializing\"]\n",
    "numberofpeople=['2','1','0']\n",
    "# \"costsharing\",\"dp\",\"heuristic\",\"random initializing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5000)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio1 = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase1 = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "d3 = D.normal.Normal(normalloc, normalscale)\n",
    "distributionRatio3 = d3.cdf(1) - d3.cdf(0)\n",
    "distributionBase3 = d3.cdf(0)\n",
    "\n",
    "d4 = D.uniform.Uniform(uniformlow,uniformhigh)\n",
    "distributionRatio4 = d4.cdf(1) - d4.cdf(0)\n",
    "distributionBase4 = d4.cdf(0)\n",
    "\n",
    "d5 = [D.normal.Normal(independentnormalloc1[ii], independentnormalscale1[ii]) for ii in range(n)]\n",
    "d6 = [D.normal.Normal(independentnormalloc2[ii], independentnormalscale2[ii]) for ii in range(n)]\n",
    "\n",
    "d7 = D.cauchy.Cauchy(cauchyloc,cauchyscalen)\n",
    "\n",
    "d81 = D.exponential.Exponential(exponentiallow)\n",
    "d82 = D.exponential.Exponential(exponentialhigh)\n",
    "\n",
    "#d8 = beta(betahigh,betalow)\n",
    "#d9 = D.beta.Beta(betahigh,betalow)\n",
    "def cdf(x,y, i=None):\n",
    "    if(y==\"twopeak\"):\n",
    "        return (d1.cdf(x) + d2.cdf(x) - distributionBase1) / 2 / distributionRatio1\n",
    "    elif(y==\"normal\"):\n",
    "        return (d3.cdf(x)-distributionBase3)/distributionRatio3;\n",
    "    elif(y==\"uniform\"):\n",
    "        return (d4.cdf(x)-distributionBase4)/distributionRatio4;\n",
    "    elif(y==\"independent1\"):\n",
    "        return d5[i].cdf(x);\n",
    "    elif(y==\"independent2\"):\n",
    "        return d6[i].cdf(x);\n",
    "    elif(y==\"cauchy\"):\n",
    "        return d7.cdf(x);\n",
    "#     elif(y==\"beta\"):\n",
    "#         return torch.tensor(d8.cdf(x));\n",
    "    elif(y==\"U-exponential\"):\n",
    "        return (d81.cdf(x) + (1.0 - d82.cdf(1.0-x)))  / 2 \n",
    "print(cdf(0.5,\"U-exponential\"))\n",
    "\n",
    "print(d81.cdf(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    if torch.sum(bits).item() == 0:\n",
    "        return torch.ones(n)\n",
    "    bits = bits.type(torch.float32)\n",
    "    negBits = torch.ones(n) - bits\n",
    "    payments = model(bits)\n",
    "    payments = payments - 1000 * negBits\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    payments = payments + negBits\n",
    "    return payments\n",
    "\n",
    "def tpToBits(tp, bits=torch.ones(n).type(torch.uint8)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = (tp >= payments).type(torch.uint8)\n",
    "    if torch.equal(newBits, bits):\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits(tp, newBits)#bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    return n - torch.sum(tpToBits(tp).type(torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    global  dp,decision\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money, yes]\n",
    "        offer = float(offerIndex) / dpPrecision\n",
    "        if tp[i] >= offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits[i] = 0\n",
    "            payments[i] = 1\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "#     print()\n",
    "#     print(tp)\n",
    "#     print(bits)\n",
    "#     print(payments)\n",
    "#     print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def heuristicSupervisionRule(tp):\n",
    "    global  dp_H,decision_H\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    tempold=-1;\n",
    "    for turns in range(n,0,-1):\n",
    "        money = dpPrecision\n",
    "        j=0\n",
    "        tempo=sum(bits)\n",
    "        #print(\"bits\",tempo)\n",
    "        for i in range(n):\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            \n",
    "            offerIndex = decision_H[tempo ,tempo - i, money]\n",
    "            offer = float(offerIndex) / dpPrecision\n",
    "            while(j<n):\n",
    "                if(bits[j]!=0):\n",
    "                    break;\n",
    "                j+=1;\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            if tp[j] >= offer:\n",
    "                #print(money,j,tp[j],offer)\n",
    "                money -= offerIndex\n",
    "                bits[j] = 1\n",
    "                payments[j] = offer\n",
    "            else:\n",
    "                bits[j] = 0;\n",
    "                payments[j] = 1.0;\n",
    "            j+=1;\n",
    "        #print(\"money\",money)\n",
    "        if(money==0 and tempold==tempo):\n",
    "            break;\n",
    "        tempold=tempo;\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    for k in range(n, -1, -1):\n",
    "        if k == 0:\n",
    "            break\n",
    "        bits = [1 if tp[ii] >= 1.0 / k else 0 for ii in range(n)]\n",
    "        if sum(bits) == k:\n",
    "            break\n",
    "    if k == 0:\n",
    "        payments = [1 for ii in range(n)]\n",
    "    else:\n",
    "        payments = [1.0 / k if bits[ii] == 1 else 1 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    return float(n - torch.sum(costSharingSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def dpDelay(tp):\n",
    "    return float(n - torch.sum(dpSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def heuristicDelay(tp):\n",
    "    return float(n - torch.sum(heuristicSupervisionRule(tp)[0]).item())\n",
    "\n",
    "\n",
    "# templire=0;\n",
    "# num=1\n",
    "# for i in range(num):\n",
    "#     temp=torch.tensor(samplesJoint[i])\n",
    "#     #print(temp)\n",
    "#     #print(temp);\n",
    "#     #print(heuristicSupervisionRule(temp))\n",
    "#     #print(dpSupervisionRule(temp))\n",
    "#     #print(dpDelay(temp))\n",
    "#     res=dpDelay(temp)\n",
    "#     templire+=res\n",
    "#     #print(\"delay\",res)\n",
    "#     #print()\n",
    "# print(templire/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordAndReport(name, source, loss):\n",
    "    source.append(loss)\n",
    "    realLength = len(source)\n",
    "    #print(f\"{name} ({realLength}): {loss}\")\n",
    "    print(name,realLength,\":\" ,loss)\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    \n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda /100000\n",
    "        \n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "losslist=[];\n",
    "losslistname=[];\n",
    "losslisttemp=[];\n",
    "def test_batch():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lenLoss= 0\n",
    "        nnLoss = 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "            for tp in tp_batch:\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        nnLoss/=lenLoss\n",
    "    return nnLoss\n",
    "\n",
    "\n",
    "def train0(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            delay1 = tpToTotalDelay(tp)\n",
    "            loss = loss + delay1 \n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "\n",
    "def train1(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                delay1 = tpToTotalDelay(tp1)\n",
    "                delay0 = tpToTotalDelay(tp0)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                    loss = loss + (1 - cdf(offer,order)) * delay1 + cdf(offer,order) * delay0\n",
    "                else:\n",
    "                    loss = loss + (1 - cdf(offer,order,i)) * delay1 + cdf(offer,order,i) * delay0\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "    \n",
    "def train2(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                x1=random.randint(0,n-1);\n",
    "                x2=random.randint(0,n-1);\n",
    "                while(x2==x1):\n",
    "                    x2=random.randint(0,n-1);\n",
    "                    \n",
    "                tp11 = tp.clone()\n",
    "                tp11[x1] = 1\n",
    "                tp11[x2] = 1\n",
    "                tp10 = tp.clone()\n",
    "                tp10[x1] = 1\n",
    "                tp10[x2] = 0\n",
    "                tp01 = tp.clone()\n",
    "                tp01[x1] = 0\n",
    "                tp01[x2] = 1\n",
    "                tp00 = tp.clone()\n",
    "                tp00[x1] = 0\n",
    "                tp00[x2] = 0\n",
    "                \n",
    "                offer1 = tpToPayments(tp11)[x1]\n",
    "                offer2 = tpToPayments(tp11)[x2]\n",
    "                offer3 = tpToPayments(tp10)[x1]\n",
    "                offer4 = tpToPayments(tp01)[x2]\n",
    "                \n",
    "                #print(tp)\n",
    "                #print(offer1,offer2)\n",
    "                \n",
    "                delay11 = tpToTotalDelay(tp11)\n",
    "                delay01 = tpToTotalDelay(tp01)\n",
    "                delay10 = tpToTotalDelay(tp10)\n",
    "                delay00 = tpToTotalDelay(tp00)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "#                     print ((1.0-cdf(offer1,order)) * (1.0-cdf(offer2,order))+                                                   \n",
    "#                            (1.0-cdf(offer3,order)) * cdf(offer2,order)+\n",
    "#                            cdf(offer1,order) * (1.0-cdf(offer4,order))+\n",
    "#                            (cdf(offer3,order) * cdf(offer4,order) -\n",
    "#                                      (cdf(offer3,order)-cdf(offer1,order))*(cdf(offer4,order)-cdf(offer2,order)) )\n",
    "#                           )\n",
    "                    loss = loss +(      (1.0-cdf(offer1,order)) * (1.0-cdf(offer2,order)) * delay11 +\n",
    "                                  (1.0-cdf(offer3,order)) * cdf(offer2,order) * delay10 +\n",
    "                                  cdf(offer1,order) * (1.0-cdf(offer4,order)) * delay01 +\n",
    "                                    (cdf(offer3,order) * cdf(offer4,order) -\n",
    "                                     (cdf(offer3,order)-cdf(offer1,order))*(cdf(offer4,order)-cdf(offer2,order)) )* delay00 \n",
    "                                                       )\n",
    "                else:\n",
    "                    loss = loss +(      (1.0-cdf(offer1,order,x1)) * (1.0-cdf(offer2,order),x2) * delay11 +\n",
    "                                  (1.0-cdf(offer3,order,x1)) * cdf(offer2,order,x2) * delay10 +\n",
    "                                  cdf(offer1,order,x1) * (1.0-cdf(offer4,order,x2)) * delay01 +\n",
    "                                    (cdf(offer3,order,x1) * cdf(offer4,order,x2) -\n",
    "                                     (cdf(offer3,order,x1)-cdf(offer1,order,x1))*(cdf(offer4,order,x2)-cdf(offer2,order,x2)) )* delay00 \n",
    "                                                       )\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "    \n",
    "allBits = [torch.tensor(bits, dtype=torch.int16) for bits in itertools.product([0, 1], repeat=n)]\n",
    "\n",
    "def test():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        costSharingLoss = 0\n",
    "        dpLoss = 0\n",
    "        nnLoss = 0\n",
    "        heuristicLoss=0\n",
    "        lenLoss= 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "                heuristicLoss+= heuristicDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        costSharingLoss /= lenLoss\n",
    "        dpLoss /= lenLoss\n",
    "        nnLoss /= lenLoss\n",
    "        heuristicLoss /= lenLoss\n",
    "        #print(lenLoss)\n",
    "        losslisttemp.append(nnLoss);\n",
    "        recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "        recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "        recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "        recordAndReport(\"heuristic\", runningLossHeuristic, heuristicLoss)\n",
    "        print(\"DP:\",dp[n, dpPrecision,0])\n",
    "        #for i in range(n, 0, -1):\n",
    "        #    print(\"Heuristic:\",i,5*(1-dp_H[i, i, dpPrecision]))\n",
    "        for i in range(n, 0, -1):\n",
    "            print(\n",
    "                    tpToPayments(\n",
    "                            torch.tensor([0 if ii >= i else 1 for ii in range(n)], dtype=torch.float32)\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "dpPrecision = 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def producedata(order):\n",
    "    global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "    dpPrecision = 100 \n",
    "    if(order==\"twopeak\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = np.random.normal(\n",
    "            loc=doublePeakLowMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=doublePeakLowMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samples2 = np.random.normal(\n",
    "            loc=doublePeakHighMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = np.random.normal(\n",
    "                        loc=doublePeakHighMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "    elif(order==\"normal\"):\n",
    "        print(\"loc\",normalloc, \"scale\",normalscale)\n",
    "        samples1 = np.random.normal(\n",
    "            loc=normalloc, scale=normalscale, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=normalloc, scale=normalscale\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"uniform\"):  \n",
    "        print(\"uniformlow\",uniformlow, \"uniformhigh\",uniformhigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        uniformlow, uniformhigh\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent1\"):\n",
    "        print(\"loc\",independentnormalloc1,\"scale\",independentnormalscale1)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent2\"):\n",
    "        print(\"loc\",independentnormalloc2, \"scale\",independentnormalscale2)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"cauchy\"):\n",
    "        print(\"cauchyloc\",cauchyloc, \"cauchyscale\",cauchyscalen)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"beta\"):\n",
    "        print(\"betalow\",betalow, \"betahigh\",betahigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(betahigh,betalow,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(betahigh,betalow,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"U-exponential\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = d81.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d81.rsample(torch.Size([1])).numpy()\n",
    "                    \n",
    "        samples2 = d82.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        \n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = d82.rsample(torch.Size([1])).numpy()\n",
    "        samples2 = 1.0 - samples2\n",
    "        samplesJoint = signals * samples1 - (signals - 1.0) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        plt.hist(samplesJoint,bins=500)\n",
    "        plt.show()\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "        \n",
    "        \n",
    "    tp_dataset = TensorDataset(tp_tensor[: int(trainSize * percentage_train_test)])\n",
    "    tp_dataset_testing = TensorDataset(tp_tensor[int(trainSize * (1.0-percentage_train_test)) :])\n",
    "    tp_dataloader = DataLoader(tp_dataset, batch_size=128, shuffle=True)\n",
    "    tp_dataloader_testing = DataLoader(tp_dataset_testing, batch_size=256, shuffle=False)\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "    decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for yes in range(n + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp[0, 0, yes] = 0\n",
    "            else:\n",
    "                dp[0, money, yes] = yes# + 1.0\n",
    "    for ppl in range(1,  n + 1):\n",
    "        for yes in range(n + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                minSoFar = 1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1 - cdf(offer,order)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order) * (1 + dp[ppl - 1, money, yes])\n",
    "                    else:\n",
    "                        res = (1 - cdf(offer,order,n-ppl)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order,n-ppl) * (1 + dp[ppl - 1, money, yes])\n",
    "                    if minSoFar > res.item():\n",
    "                        minSoFar = res.item()\n",
    "                        decision[ppl, money, yes] = offerIndex\n",
    "                dp[ppl, money, yes] = minSoFar\n",
    "    \n",
    "    print(\"dp\",dp[n, dpPrecision, 0])\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp_H = np.zeros([n + 1 , n + 1, dpPrecision + 1])\n",
    "    decision_H = np.zeros([n + 1 , n + 1, dpPrecision + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for i in range(1,n+1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp_H[i, 0, 0] = 1\n",
    "            else:\n",
    "                offer = money / dpPrecision\n",
    "                dp_H[i, 0, money] = 0#cdf(offer)# + 1.0\n",
    "    for i in range(1,n+1):\n",
    "        for ppl in range(1, i + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                maxSoFar = -1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1-cdf(offer,order)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    else:\n",
    "                        res = (1-cdf(offer,order,ppl-1)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    if maxSoFar < res.item():\n",
    "                        maxSoFar = res.item()\n",
    "                        decision_H[i, ppl, money] = offerIndex\n",
    "                dp_H[i, ppl, money] = maxSoFar\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loc 0.15 scale 0.1\n",
      "loc 0.85 scale 0.1\n",
      "dp 1.7567987442016602\n",
      "Supervised Aim: twopeak random initializing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "NN 1 : tensor(2.3561)\n",
      "CS 1 : 2.3642333333333334\n",
      "DP 1 : 1.7611\n",
      "heuristic 1 : 1.7136333333333333\n",
      "DP: 1.7567987442016602\n",
      "tensor([0.1957, 0.2106, 0.2212, 0.1801, 0.1924])\n",
      "tensor([0.2395, 0.2534, 0.2784, 0.2287, 1.0000])\n",
      "tensor([0.3201, 0.3354, 0.3445, 1.0000, 1.0000])\n",
      "tensor([0.5024, 0.4976, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 2.227800 testing loss: tensor(2.3514)\n",
      "Train Epoch: 1 [1280/30000 (4%)]\tLoss: 2.214441 testing loss: tensor(2.2553)\n",
      "Train Epoch: 1 [2560/30000 (9%)]\tLoss: 2.100234 testing loss: tensor(1.9517)\n",
      "Train Epoch: 1 [3840/30000 (13%)]\tLoss: 1.834521 testing loss: tensor(1.8580)\n",
      "Train Epoch: 1 [5120/30000 (17%)]\tLoss: 1.855092 testing loss: tensor(1.7749)\n",
      "Train Epoch: 1 [6400/30000 (21%)]\tLoss: 1.711617 testing loss: tensor(1.7582)\n",
      "Train Epoch: 1 [7680/30000 (26%)]\tLoss: 1.695716 testing loss: tensor(1.7338)\n",
      "Train Epoch: 1 [8960/30000 (30%)]\tLoss: 1.776946 testing loss: tensor(1.7144)\n",
      "Train Epoch: 1 [10240/30000 (34%)]\tLoss: 1.752406 testing loss: tensor(1.6965)\n",
      "Train Epoch: 1 [11520/30000 (38%)]\tLoss: 1.641348 testing loss: tensor(1.6811)\n",
      "Train Epoch: 1 [12800/30000 (43%)]\tLoss: 1.708922 testing loss: tensor(1.6575)\n",
      "Train Epoch: 1 [14080/30000 (47%)]\tLoss: 1.814461 testing loss: tensor(1.6389)\n",
      "Train Epoch: 1 [15360/30000 (51%)]\tLoss: 1.622533 testing loss: tensor(1.6361)\n",
      "Train Epoch: 1 [16640/30000 (55%)]\tLoss: 1.455841 testing loss: tensor(1.6347)\n",
      "Train Epoch: 1 [17920/30000 (60%)]\tLoss: 1.671259 testing loss: tensor(1.6329)\n",
      "Train Epoch: 1 [19200/30000 (64%)]\tLoss: 1.593772 testing loss: tensor(1.6312)\n",
      "Train Epoch: 1 [20480/30000 (68%)]\tLoss: 1.677505 testing loss: tensor(1.6319)\n",
      "Train Epoch: 1 [21760/30000 (72%)]\tLoss: 1.621888 testing loss: tensor(1.6323)\n",
      "Train Epoch: 1 [23040/30000 (77%)]\tLoss: 1.672713 testing loss: tensor(1.6312)\n",
      "Train Epoch: 1 [24320/30000 (81%)]\tLoss: 1.571234 testing loss: tensor(1.6300)\n",
      "Train Epoch: 1 [25600/30000 (85%)]\tLoss: 1.683920 testing loss: tensor(1.6308)\n",
      "Train Epoch: 1 [26880/30000 (89%)]\tLoss: 1.647681 testing loss: tensor(1.6314)\n",
      "Train Epoch: 1 [28160/30000 (94%)]\tLoss: 1.674925 testing loss: tensor(1.6316)\n",
      "Train Epoch: 1 [29440/30000 (98%)]\tLoss: 1.676944 testing loss: tensor(1.6319)\n",
      "penalty: 0.0009376406669616699\n",
      "NN 2 : tensor(1.6324)\n",
      "CS 2 : 2.3642333333333334\n",
      "DP 2 : 1.7611\n",
      "heuristic 2 : 1.7136333333333333\n",
      "DP: 1.7567987442016602\n",
      "tensor([0.0793, 0.0769, 0.6880, 0.0769, 0.0789])\n",
      "tensor([0.1002, 0.1029, 0.7007, 0.0962, 1.0000])\n",
      "tensor([0.1498, 0.1362, 0.7140, 1.0000, 1.0000])\n",
      "tensor([0.2770, 0.7230, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "NN 1 : tensor(2.3342)\n",
      "CS 1 : 2.3642333333333334\n",
      "DP 1 : 1.7611\n",
      "heuristic 1 : 1.7136333333333333\n",
      "DP: 1.7567987442016602\n",
      "tensor([0.1783, 0.2843, 0.1787, 0.1681, 0.1905])\n",
      "tensor([0.2200, 0.3397, 0.2426, 0.1977, 1.0000])\n",
      "tensor([0.2840, 0.3956, 0.3204, 1.0000, 1.0000])\n",
      "tensor([0.4254, 0.5746, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 2.312644 testing loss: tensor(2.3196)\n",
      "Train Epoch: 1 [1280/30000 (4%)]\tLoss: 2.143773 testing loss: tensor(2.0235)\n",
      "Train Epoch: 1 [2560/30000 (9%)]\tLoss: 1.896914 testing loss: tensor(1.9228)\n",
      "Train Epoch: 1 [3840/30000 (13%)]\tLoss: 1.906080 testing loss: tensor(1.8513)\n",
      "Train Epoch: 1 [5120/30000 (17%)]\tLoss: 1.896352 testing loss: tensor(1.7880)\n",
      "Train Epoch: 1 [6400/30000 (21%)]\tLoss: 1.727388 testing loss: tensor(1.7411)\n",
      "Train Epoch: 1 [7680/30000 (26%)]\tLoss: 1.483646 testing loss: tensor(1.7093)\n",
      "Train Epoch: 1 [8960/30000 (30%)]\tLoss: 1.735974 testing loss: tensor(1.6923)\n",
      "Train Epoch: 1 [10240/30000 (34%)]\tLoss: 1.551308 testing loss: tensor(1.6813)\n",
      "Train Epoch: 1 [11520/30000 (38%)]\tLoss: 1.789880 testing loss: tensor(1.6767)\n",
      "Train Epoch: 1 [12800/30000 (43%)]\tLoss: 1.605254 testing loss: tensor(1.6686)\n",
      "Train Epoch: 1 [14080/30000 (47%)]\tLoss: 1.668991 testing loss: tensor(1.6526)\n",
      "Train Epoch: 1 [15360/30000 (51%)]\tLoss: 1.773404 testing loss: tensor(1.6407)\n",
      "Train Epoch: 1 [16640/30000 (55%)]\tLoss: 1.761891 testing loss: tensor(1.6362)\n",
      "Train Epoch: 1 [17920/30000 (60%)]\tLoss: 1.707984 testing loss: tensor(1.6334)\n",
      "Train Epoch: 1 [19200/30000 (64%)]\tLoss: 1.606874 testing loss: tensor(1.6309)\n",
      "Train Epoch: 1 [20480/30000 (68%)]\tLoss: 1.442325 testing loss: tensor(1.6336)\n",
      "Train Epoch: 1 [21760/30000 (72%)]\tLoss: 1.646421 testing loss: tensor(1.6324)\n",
      "Train Epoch: 1 [23040/30000 (77%)]\tLoss: 1.599311 testing loss: tensor(1.6325)\n",
      "Train Epoch: 1 [24320/30000 (81%)]\tLoss: 1.708719 testing loss: tensor(1.6307)\n",
      "Train Epoch: 1 [25600/30000 (85%)]\tLoss: 1.590021 testing loss: tensor(1.6353)\n",
      "Train Epoch: 1 [26880/30000 (89%)]\tLoss: 1.627525 testing loss: tensor(1.6316)\n",
      "Train Epoch: 1 [28160/30000 (94%)]\tLoss: 1.506389 testing loss: tensor(1.6319)\n",
      "Train Epoch: 1 [29440/30000 (98%)]\tLoss: 1.743366 testing loss: tensor(1.6317)\n",
      "penalty: 0.00043839216232299805\n",
      "NN 2 : tensor(1.6350)\n",
      "CS 2 : 2.3642333333333334\n",
      "DP 2 : 1.7611\n",
      "heuristic 2 : 1.7136333333333333\n",
      "DP: 1.7567987442016602\n",
      "tensor([0.0867, 0.6666, 0.0805, 0.0852, 0.0810])\n",
      "tensor([0.1085, 0.6869, 0.1066, 0.0981, 1.0000])\n",
      "tensor([0.1511, 0.7112, 0.1377, 1.0000, 1.0000])\n",
      "tensor([0.2790, 0.7210, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "NN 1 : tensor(2.3515)\n",
      "CS 1 : 2.3642333333333334\n",
      "DP 1 : 1.7611\n",
      "heuristic 1 : 1.7136333333333333\n",
      "DP: 1.7567987442016602\n",
      "tensor([0.1735, 0.2357, 0.1570, 0.2041, 0.2298])\n",
      "tensor([0.2261, 0.2821, 0.2307, 0.2611, 1.0000])\n",
      "tensor([0.3032, 0.3905, 0.3063, 1.0000, 1.0000])\n",
      "tensor([0.4400, 0.5600, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 2.335938 testing loss: tensor(2.3515)\n",
      "Train Epoch: 1 [1280/30000 (4%)]\tLoss: 2.312500 testing loss: tensor(2.3515)\n",
      "Train Epoch: 1 [2560/30000 (9%)]\tLoss: 2.382812 testing loss: tensor(2.3515)\n",
      "Train Epoch: 1 [3840/30000 (13%)]\tLoss: 2.335938 testing loss: tensor(2.3515)\n",
      "Train Epoch: 1 [5120/30000 (17%)]\tLoss: 2.359375 testing loss: tensor(2.3515)\n",
      "Train Epoch: 1 [6400/30000 (21%)]\tLoss: 2.273438 testing loss: tensor(2.3515)\n",
      "Train Epoch: 1 [7680/30000 (26%)]\tLoss: 2.390625 testing loss: tensor(2.3515)\n",
      "Train Epoch: 1 [8960/30000 (30%)]\tLoss: 2.351562 testing loss: tensor(2.3515)\n",
      "Train Epoch: 1 [10240/30000 (34%)]\tLoss: 2.500000 testing loss: tensor(2.3515)\n",
      "Train Epoch: 1 [11520/30000 (38%)]\tLoss: 2.351562 testing loss: tensor(2.3515)\n",
      "Train Epoch: 1 [12800/30000 (43%)]\tLoss: 2.375000 testing loss: tensor(2.3515)\n",
      "Train Epoch: 1 [14080/30000 (47%)]\tLoss: 2.164062 testing loss: tensor(2.3515)\n",
      "Train Epoch: 1 [15360/30000 (51%)]\tLoss: 2.187500 testing loss: tensor(2.3515)\n",
      "Train Epoch: 1 [16640/30000 (55%)]\tLoss: 2.109375 testing loss: tensor(2.3515)\n",
      "Train Epoch: 1 [17920/30000 (60%)]\tLoss: 2.492188 testing loss: tensor(2.3515)\n",
      "Train Epoch: 1 [19200/30000 (64%)]\tLoss: 2.320312 testing loss: tensor(2.3515)\n",
      "Train Epoch: 1 [20480/30000 (68%)]\tLoss: 2.281250 testing loss: tensor(2.3515)\n",
      "Train Epoch: 1 [21760/30000 (72%)]\tLoss: 2.531250 testing loss: tensor(2.3515)\n",
      "Train Epoch: 1 [23040/30000 (77%)]\tLoss: 2.406250 testing loss: tensor(2.3515)\n",
      "Train Epoch: 1 [24320/30000 (81%)]\tLoss: 2.484375 testing loss: tensor(2.3515)\n",
      "Train Epoch: 1 [25600/30000 (85%)]\tLoss: 2.148438 testing loss: tensor(2.3515)\n",
      "Train Epoch: 1 [26880/30000 (89%)]\tLoss: 2.171875 testing loss: tensor(2.3515)\n",
      "Train Epoch: 1 [28160/30000 (94%)]\tLoss: 2.445312 testing loss: tensor(2.3515)\n",
      "Train Epoch: 1 [29440/30000 (98%)]\tLoss: 2.179688 testing loss: tensor(2.3515)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(2.3515)\n",
      "CS 2 : 2.3642333333333334\n",
      "DP 2 : 1.7611\n",
      "heuristic 2 : 1.7136333333333333\n",
      "DP: 1.7567987442016602\n",
      "tensor([0.1735, 0.2357, 0.1570, 0.2041, 0.2298])\n",
      "tensor([0.2261, 0.2821, 0.2307, 0.2611, 1.0000])\n",
      "tensor([0.3032, 0.3905, 0.3063, 1.0000, 1.0000])\n",
      "tensor([0.4400, 0.5600, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "    \n",
    "for order in stage:\n",
    "    producedata(order)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "    #running\n",
    "    for order1 in order1name:\n",
    "        print(\"Supervised Aim:\",order,order1)\n",
    "        for trainingnumber in numberofpeople:\n",
    "            # for mapping binary to payments before softmax\n",
    "            model = nn.Sequential(\n",
    "                nn.Linear(n, 100),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(100, 100),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(100, 100),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(100, n),\n",
    "            )\n",
    "            model.apply(init_weights)\n",
    "            # optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "            runningLossNN = []\n",
    "            runningLossCS = []\n",
    "            runningLossDP = []\n",
    "            runningLossHeuristic = []\n",
    "            #model=torch.load(\"save/pytorchNN=5dp1\");\n",
    "            #model.eval()\n",
    "            ##order1name=[\"costsharing\",\"dp\",\"heuristic\",\"random initializing\"]\n",
    "            for epoch in range(1, supervisionEpochs + 1):\n",
    "        #             print(\"distributionRatio\",distributionRatio)\n",
    "                if(order1==\"costsharing\"):\n",
    "                    supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "                elif(order1==\"dp\"):\n",
    "                    supervisionTrain(epoch, dpSupervisionRule)\n",
    "                elif(order1==\"heuristic\"):\n",
    "                    supervisionTrain(epoch, heuristicSupervisionRule)\n",
    "                elif(order1==\"random initializing\"):\n",
    "                    print(\"do nothing\");\n",
    "\n",
    "            test()\n",
    "\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                if(trainingnumber=='0'):\n",
    "                    train0(epoch)\n",
    "                if(trainingnumber=='1'):\n",
    "                    train1(epoch)\n",
    "                if(trainingnumber=='2'):\n",
    "                    train2(epoch)\n",
    "                test()\n",
    "            losslistname.append(order+\" \"+order1+\" choose people:\"+trainingnumber);\n",
    "            losslist.append(losslisttemp);\n",
    "            losslisttemp=[];\n",
    "            savepath=\"save/pytorchNN=5all-phoenix\"+order+str(order1)+trainingnumber\n",
    "            torch.save(model, savepath);\n",
    "            print(\"end\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colorlist=[\"#D2691E\",'#4169E1',\"#9ACD32\",\"#B22222\",\"#FF00FF\",\"#708090\"]\n",
    "for i in range(len(losslist)):\n",
    "    plt.plot(losslist[i], '.-', color=colorlist[i], alpha=0.8, label=losslistname[i])\n",
    "\n",
    "# 显示标签，如果不加这句，即使加了label='一些数字'的参数，最终还是不会显示标签\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('trianing times')\n",
    "plt.ylabel('delay')\n",
    "plt.title(\"n=\"+str(n))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
