{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "from pylab import *\n",
    "from scipy.stats import beta\n",
    "import random \n",
    "n = 5\n",
    "epochs = 1\n",
    "supervisionEpochs = 1\n",
    "lr = 0.0001\n",
    "log_interval = 5\n",
    "trainSize = 40000#100000\n",
    "percentage_train_test= 0.75\n",
    "penaltyLambda = 10\n",
    "doublePeakHighMean = 0.85\n",
    "doublePeakLowMean = 0.15\n",
    "doublePeakStd = 0.1\n",
    "uniformlow=0\n",
    "uniformhigh=1.0\n",
    "normalloc = 0.2\n",
    "normalscale = 0.1\n",
    "\n",
    "cauchyloc = 1/n\n",
    "cauchyscalen = 0.004\n",
    "\n",
    "exponentialhigh = 15 #Symbol(\"b\", real=True)\n",
    "exponentiallow  = 15 #Symbol(\"a\", real=True)\n",
    "\n",
    "Sigmoidrate=0.00005\n",
    "Sigmoidbias=0.001\n",
    "\n",
    "\n",
    "\n",
    "independentnormalloc1=[(float(ii)+1)/(2*n+1) for ii in range(n,0,-1)]\n",
    "independentnormalscale1=[0.05 for ii in range(n)]\n",
    "\n",
    "independentnormalloc2=[(float(ii)+1)/(2*n+1) for ii in range(1,n+1,1)]\n",
    "independentnormalscale2=[0.05 for ii in range(n)]\n",
    "stage=[\"twopeak\"]\n",
    "# \"twopeak\",\"normal\",\"uniform\",\"independent1\",\"independent2\",\"cauchy\",\"U-exponential\"\n",
    "order1name=[\"random initializing\"]\n",
    "numberofpeople=['0','1','2']\n",
    "# \"costsharing\",\"dp\",\"heuristic\",\"random initializing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5000)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio1 = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase1 = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "d3 = D.normal.Normal(normalloc, normalscale)\n",
    "distributionRatio3 = d3.cdf(1) - d3.cdf(0)\n",
    "distributionBase3 = d3.cdf(0)\n",
    "\n",
    "d4 = D.uniform.Uniform(uniformlow,uniformhigh)\n",
    "distributionRatio4 = d4.cdf(1) - d4.cdf(0)\n",
    "distributionBase4 = d4.cdf(0)\n",
    "\n",
    "d5 = [D.normal.Normal(independentnormalloc1[ii], independentnormalscale1[ii]) for ii in range(n)]\n",
    "d6 = [D.normal.Normal(independentnormalloc2[ii], independentnormalscale2[ii]) for ii in range(n)]\n",
    "\n",
    "d7 = D.cauchy.Cauchy(cauchyloc,cauchyscalen)\n",
    "\n",
    "d81 = D.exponential.Exponential(exponentiallow)\n",
    "d82 = D.exponential.Exponential(exponentialhigh)\n",
    "\n",
    "#d8 = beta(betahigh,betalow)\n",
    "#d9 = D.beta.Beta(betahigh,betalow)\n",
    "def cdf(x,y, i=None):\n",
    "    if(y==\"twopeak\"):\n",
    "        return (d1.cdf(x) + d2.cdf(x) - distributionBase1) / 2 / distributionRatio1\n",
    "    elif(y==\"normal\"):\n",
    "        return (d3.cdf(x)-distributionBase3)/distributionRatio3;\n",
    "    elif(y==\"uniform\"):\n",
    "        return (d4.cdf(x)-distributionBase4)/distributionRatio4;\n",
    "    elif(y==\"independent1\"):\n",
    "        return d5[i].cdf(x);\n",
    "    elif(y==\"independent2\"):\n",
    "        return d6[i].cdf(x);\n",
    "    elif(y==\"cauchy\"):\n",
    "        return d7.cdf(x);\n",
    "#     elif(y==\"beta\"):\n",
    "#         return torch.tensor(d8.cdf(x));\n",
    "    elif(y==\"U-exponential\"):\n",
    "        return (d81.cdf(x) + (1.0 - d82.cdf(1.0-x)))  / 2 \n",
    "print(cdf(0.5,\"U-exponential\"))\n",
    "\n",
    "print(d81.cdf(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    if torch.sum(bits).item() == 0:\n",
    "        return torch.ones(n)\n",
    "    bits = bits.type(torch.float32)\n",
    "    negBits = torch.ones(n) - bits\n",
    "    payments = model(bits)\n",
    "    payments = payments - 1000 * negBits\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    payments = payments + negBits\n",
    "    return payments\n",
    "\n",
    "def tpToBits(tp, bits=torch.ones(n).type(torch.uint8)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = (tp >= payments).type(torch.uint8)\n",
    "    if torch.equal(newBits, bits):\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits(tp, newBits)#bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    return n - torch.sum(tpToBits(tp).type(torch.float32))\n",
    "\n",
    "def tpToBits0(tp, deep ,bits=torch.ones(n).type(torch.float32)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = torch.sigmoid((tp - payments + Sigmoidbias)/Sigmoidrate)\n",
    "    if torch.allclose(newBits, bits,rtol=0.01,atol=0.01) or deep>=n:\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits0(tp,deep+1 ,newBits)#bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "def tpToTotalDelay0(tp):\n",
    "    return n - torch.sum(tpToBits0(tp,0).type(torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    global  dp,decision\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money, yes]\n",
    "        offer = float(offerIndex) / dpPrecision\n",
    "        if tp[i] >= offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits[i] = 0\n",
    "            payments[i] = 1\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "#     print()\n",
    "#     print(tp)\n",
    "#     print(bits)\n",
    "#     print(payments)\n",
    "#     print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def heuristicSupervisionRule(tp):\n",
    "    global  dp_H,decision_H\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    tempold=-1;\n",
    "    for turns in range(n,0,-1):\n",
    "        money = dpPrecision\n",
    "        j=0\n",
    "        tempo=sum(bits)\n",
    "        #print(\"bits\",tempo)\n",
    "        for i in range(n):\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            \n",
    "            offerIndex = decision_H[tempo ,tempo - i, money]\n",
    "            offer = float(offerIndex) / dpPrecision\n",
    "            while(j<n):\n",
    "                if(bits[j]!=0):\n",
    "                    break;\n",
    "                j+=1;\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            if tp[j] >= offer:\n",
    "                #print(money,j,tp[j],offer)\n",
    "                money -= offerIndex\n",
    "                bits[j] = 1\n",
    "                payments[j] = offer\n",
    "            else:\n",
    "                bits[j] = 0;\n",
    "                payments[j] = 1.0;\n",
    "            j+=1;\n",
    "        #print(\"money\",money)\n",
    "        if(money==0 and tempold==tempo):\n",
    "            break;\n",
    "        tempold=tempo;\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    for k in range(n, -1, -1):\n",
    "        if k == 0:\n",
    "            break\n",
    "        bits = [1 if tp[ii] >= 1.0 / k else 0 for ii in range(n)]\n",
    "        if sum(bits) == k:\n",
    "            break\n",
    "    if k == 0:\n",
    "        payments = [1 for ii in range(n)]\n",
    "    else:\n",
    "        payments = [1.0 / k if bits[ii] == 1 else 1 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    return float(n - torch.sum(costSharingSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def dpDelay(tp):\n",
    "    return float(n - torch.sum(dpSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def heuristicDelay(tp):\n",
    "    return float(n - torch.sum(heuristicSupervisionRule(tp)[0]).item())\n",
    "\n",
    "\n",
    "# templire=0;\n",
    "# num=1\n",
    "# for i in range(num):\n",
    "#     temp=torch.tensor(samplesJoint[i])\n",
    "#     #print(temp)\n",
    "#     #print(temp);\n",
    "#     #print(heuristicSupervisionRule(temp))\n",
    "#     #print(dpSupervisionRule(temp))\n",
    "#     #print(dpDelay(temp))\n",
    "#     res=dpDelay(temp)\n",
    "#     templire+=res\n",
    "#     #print(\"delay\",res)\n",
    "#     #print()\n",
    "# print(templire/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordAndReport(name, source, loss):\n",
    "    source.append(loss)\n",
    "    realLength = len(source)\n",
    "    #print(f\"{name} ({realLength}): {loss}\")\n",
    "    print(name,realLength,\":\" ,loss)\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    \n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda /100000\n",
    "        \n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "losslist=[];\n",
    "losslistname=[];\n",
    "losslisttemp=[];\n",
    "def test_batch():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lenLoss= 0\n",
    "        nnLoss = 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "            for tp in tp_batch:\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        nnLoss/=lenLoss\n",
    "    return nnLoss\n",
    "\n",
    "\n",
    "def train0(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            delay0 = tpToTotalDelay0(tp)\n",
    "            loss = loss + delay0\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "\n",
    "def train1(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                delay1 = tpToTotalDelay(tp1)\n",
    "                delay0 = tpToTotalDelay(tp0)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                    loss = loss + (1 - cdf(offer,order)) * delay1 + cdf(offer,order) * delay0\n",
    "                else:\n",
    "                    loss = loss + (1 - cdf(offer,order,i)) * delay1 + cdf(offer,order,i) * delay0\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "    \n",
    "def train2(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                x1=random.randint(0,n-1);\n",
    "                x2=random.randint(0,n-1);\n",
    "                while(x2==x1):\n",
    "                    x2=random.randint(0,n-1);\n",
    "                    \n",
    "                tp11 = tp.clone()\n",
    "                tp11[x1] = 1\n",
    "                tp11[x2] = 1\n",
    "                tp10 = tp.clone()\n",
    "                tp10[x1] = 1\n",
    "                tp10[x2] = 0\n",
    "                tp01 = tp.clone()\n",
    "                tp01[x1] = 0\n",
    "                tp01[x2] = 1\n",
    "                tp00 = tp.clone()\n",
    "                tp00[x1] = 0\n",
    "                tp00[x2] = 0\n",
    "                \n",
    "                offer1 = tpToPayments(tp11)[x1]\n",
    "                offer2 = tpToPayments(tp11)[x2]\n",
    "                offer3 = tpToPayments(tp10)[x1]\n",
    "                offer4 = tpToPayments(tp01)[x2]\n",
    "                \n",
    "                #print(tp)\n",
    "                #print(offer1,offer2)\n",
    "                \n",
    "                delay11 = tpToTotalDelay(tp11)\n",
    "                delay01 = tpToTotalDelay(tp01)\n",
    "                delay10 = tpToTotalDelay(tp10)\n",
    "                delay00 = tpToTotalDelay(tp00)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "#                     print ((1.0-cdf(offer1,order)) * (1.0-cdf(offer2,order))+                                                   \n",
    "#                            (1.0-cdf(offer3,order)) * cdf(offer2,order)+\n",
    "#                            cdf(offer1,order) * (1.0-cdf(offer4,order))+\n",
    "#                            (cdf(offer3,order) * cdf(offer4,order) -\n",
    "#                                      (cdf(offer3,order)-cdf(offer1,order))*(cdf(offer4,order)-cdf(offer2,order)) )\n",
    "#                           )\n",
    "                    loss = loss +(      (1.0-cdf(offer1,order)) * (1.0-cdf(offer2,order)) * delay11 +\n",
    "                                  (1.0-cdf(offer3,order)) * cdf(offer2,order) * delay10 +\n",
    "                                  cdf(offer1,order) * (1.0-cdf(offer4,order)) * delay01 +\n",
    "                                    (cdf(offer3,order) * cdf(offer4,order) -\n",
    "                                     (cdf(offer3,order)-cdf(offer1,order))*(cdf(offer4,order)-cdf(offer2,order)) )* delay00 \n",
    "                                                       )\n",
    "                else:\n",
    "                    loss = loss +(      (1.0-cdf(offer1,order,x1)) * (1.0-cdf(offer2,order),x2) * delay11 +\n",
    "                                  (1.0-cdf(offer3,order,x1)) * cdf(offer2,order,x2) * delay10 +\n",
    "                                  cdf(offer1,order,x1) * (1.0-cdf(offer4,order,x2)) * delay01 +\n",
    "                                    (cdf(offer3,order,x1) * cdf(offer4,order,x2) -\n",
    "                                     (cdf(offer3,order,x1)-cdf(offer1,order,x1))*(cdf(offer4,order,x2)-cdf(offer2,order,x2)) )* delay00 \n",
    "                                                       )\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "    \n",
    "allBits = [torch.tensor(bits, dtype=torch.int16) for bits in itertools.product([0, 1], repeat=n)]\n",
    "\n",
    "def test():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        costSharingLoss = 0\n",
    "        dpLoss = 0\n",
    "        nnLoss = 0\n",
    "        heuristicLoss=0\n",
    "        lenLoss= 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "                heuristicLoss+= heuristicDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        costSharingLoss /= lenLoss\n",
    "        dpLoss /= lenLoss\n",
    "        nnLoss /= lenLoss\n",
    "        heuristicLoss /= lenLoss\n",
    "        #print(lenLoss)\n",
    "        losslisttemp.append(nnLoss);\n",
    "        recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "        recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "        recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "        recordAndReport(\"heuristic\", runningLossHeuristic, heuristicLoss)\n",
    "        print(\"DP:\",dp[n, dpPrecision,0])\n",
    "        #for i in range(n, 0, -1):\n",
    "        #    print(\"Heuristic:\",i,5*(1-dp_H[i, i, dpPrecision]))\n",
    "        for i in range(n, 0, -1):\n",
    "            print(\n",
    "                    tpToPayments(\n",
    "                            torch.tensor([0 if ii >= i else 1 for ii in range(n)], dtype=torch.float32)\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "dpPrecision = 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def producedata(order):\n",
    "    global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "    dpPrecision = 100 \n",
    "    if(order==\"twopeak\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = np.random.normal(\n",
    "            loc=doublePeakLowMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=doublePeakLowMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samples2 = np.random.normal(\n",
    "            loc=doublePeakHighMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = np.random.normal(\n",
    "                        loc=doublePeakHighMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "    elif(order==\"normal\"):\n",
    "        print(\"loc\",normalloc, \"scale\",normalscale)\n",
    "        samples1 = np.random.normal(\n",
    "            loc=normalloc, scale=normalscale, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=normalloc, scale=normalscale\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"uniform\"):  \n",
    "        print(\"uniformlow\",uniformlow, \"uniformhigh\",uniformhigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        uniformlow, uniformhigh\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent1\"):\n",
    "        print(\"loc\",independentnormalloc1,\"scale\",independentnormalscale1)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent2\"):\n",
    "        print(\"loc\",independentnormalloc2, \"scale\",independentnormalscale2)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"cauchy\"):\n",
    "        print(\"cauchyloc\",cauchyloc, \"cauchyscale\",cauchyscalen)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"beta\"):\n",
    "        print(\"betalow\",betalow, \"betahigh\",betahigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(betahigh,betalow,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(betahigh,betalow,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"U-exponential\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = d81.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d81.rsample(torch.Size([1])).numpy()\n",
    "                    \n",
    "        samples2 = d82.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        \n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = d82.rsample(torch.Size([1])).numpy()\n",
    "        samples2 = 1.0 - samples2\n",
    "        samplesJoint = signals * samples1 - (signals - 1.0) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        plt.hist(samplesJoint,bins=500)\n",
    "        plt.show()\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "        \n",
    "        \n",
    "    tp_dataset = TensorDataset(tp_tensor[: int(trainSize * percentage_train_test)])\n",
    "    tp_dataset_testing = TensorDataset(tp_tensor[int(trainSize * (1.0-percentage_train_test)) :])\n",
    "    tp_dataloader = DataLoader(tp_dataset, batch_size=128, shuffle=True)\n",
    "    tp_dataloader_testing = DataLoader(tp_dataset_testing, batch_size=256, shuffle=False)\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "    decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for yes in range(n + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp[0, 0, yes] = 0\n",
    "            else:\n",
    "                dp[0, money, yes] = yes# + 1.0\n",
    "    for ppl in range(1,  n + 1):\n",
    "        for yes in range(n + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                minSoFar = 1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1 - cdf(offer,order)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order) * (1 + dp[ppl - 1, money, yes])\n",
    "                    else:\n",
    "                        res = (1 - cdf(offer,order,n-ppl)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order,n-ppl) * (1 + dp[ppl - 1, money, yes])\n",
    "                    if minSoFar > res.item():\n",
    "                        minSoFar = res.item()\n",
    "                        decision[ppl, money, yes] = offerIndex\n",
    "                dp[ppl, money, yes] = minSoFar\n",
    "    \n",
    "    print(\"dp\",dp[n, dpPrecision, 0])\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp_H = np.zeros([n + 1 , n + 1, dpPrecision + 1])\n",
    "    decision_H = np.zeros([n + 1 , n + 1, dpPrecision + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for i in range(1,n+1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp_H[i, 0, 0] = 1\n",
    "            else:\n",
    "                offer = money / dpPrecision\n",
    "                dp_H[i, 0, money] = 0#cdf(offer)# + 1.0\n",
    "    for i in range(1,n+1):\n",
    "        for ppl in range(1, i + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                maxSoFar = -1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1-cdf(offer,order)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    else:\n",
    "                        res = (1-cdf(offer,order,ppl-1)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    if maxSoFar < res.item():\n",
    "                        maxSoFar = res.item()\n",
    "                        decision_H[i, ppl, money] = offerIndex\n",
    "                dp_H[i, ppl, money] = maxSoFar\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loc 0.15 scale 0.1\n",
      "loc 0.85 scale 0.1\n",
      "dp 1.7567987442016602\n",
      "Supervised Aim: twopeak random initializing\n",
      "do nothing\n",
      "NN 1 : tensor(2.3405)\n",
      "CS 1 : 2.3521\n",
      "DP 1 : 1.7483\n",
      "heuristic 1 : 1.7004666666666666\n",
      "DP: 1.7567987442016602\n",
      "tensor([0.2044, 0.1921, 0.2417, 0.1861, 0.1758])\n",
      "tensor([0.2504, 0.2319, 0.2852, 0.2325, 1.0000])\n",
      "tensor([0.3413, 0.2844, 0.3744, 1.0000, 1.0000])\n",
      "tensor([0.5020, 0.4980, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 2.390625 testing loss: tensor(2.3410)\n",
      "Train Epoch: 1 [640/30000 (2%)]\tLoss: 2.156372 testing loss: tensor(2.3401)\n",
      "Train Epoch: 1 [1280/30000 (4%)]\tLoss: 2.242185 testing loss: tensor(2.3402)\n",
      "Train Epoch: 1 [1920/30000 (6%)]\tLoss: 2.164062 testing loss: tensor(2.3412)\n",
      "Train Epoch: 1 [2560/30000 (9%)]\tLoss: 2.367188 testing loss: tensor(2.3399)\n",
      "Train Epoch: 1 [3200/30000 (11%)]\tLoss: 2.398437 testing loss: tensor(2.3387)\n",
      "Train Epoch: 1 [3840/30000 (13%)]\tLoss: 2.335938 testing loss: tensor(2.3390)\n",
      "Train Epoch: 1 [4480/30000 (15%)]\tLoss: 2.382812 testing loss: tensor(2.3395)\n",
      "Train Epoch: 1 [5120/30000 (17%)]\tLoss: 2.359375 testing loss: tensor(2.3399)\n",
      "Train Epoch: 1 [5760/30000 (19%)]\tLoss: 2.328125 testing loss: tensor(2.3393)\n",
      "Train Epoch: 1 [6400/30000 (21%)]\tLoss: 2.265625 testing loss: tensor(2.3383)\n",
      "Train Epoch: 1 [7040/30000 (23%)]\tLoss: 2.414017 testing loss: tensor(2.3377)\n",
      "Train Epoch: 1 [7680/30000 (26%)]\tLoss: 2.640551 testing loss: tensor(2.3355)\n",
      "Train Epoch: 1 [8320/30000 (28%)]\tLoss: 2.218750 testing loss: tensor(2.3344)\n",
      "Train Epoch: 1 [8960/30000 (30%)]\tLoss: 2.328125 testing loss: tensor(2.3349)\n",
      "Train Epoch: 1 [9600/30000 (32%)]\tLoss: 2.393634 testing loss: tensor(2.3341)\n",
      "Train Epoch: 1 [10240/30000 (34%)]\tLoss: 2.023438 testing loss: tensor(2.3329)\n",
      "Train Epoch: 1 [10880/30000 (36%)]\tLoss: 2.437452 testing loss: tensor(2.3323)\n",
      "Train Epoch: 1 [11520/30000 (38%)]\tLoss: 2.499999 testing loss: tensor(2.3315)\n",
      "Train Epoch: 1 [12160/30000 (40%)]\tLoss: 2.046541 testing loss: tensor(2.3306)\n",
      "Train Epoch: 1 [12800/30000 (43%)]\tLoss: 2.429688 testing loss: tensor(2.3307)\n",
      "Train Epoch: 1 [13440/30000 (45%)]\tLoss: 2.242188 testing loss: tensor(2.3314)\n",
      "Train Epoch: 1 [14080/30000 (47%)]\tLoss: 2.351562 testing loss: tensor(2.3311)\n",
      "Train Epoch: 1 [14720/30000 (49%)]\tLoss: 2.507812 testing loss: tensor(2.3312)\n",
      "Train Epoch: 1 [15360/30000 (51%)]\tLoss: 2.429688 testing loss: tensor(2.3314)\n",
      "Train Epoch: 1 [16000/30000 (53%)]\tLoss: 2.249981 testing loss: tensor(2.3310)\n",
      "Train Epoch: 1 [16640/30000 (55%)]\tLoss: 2.453125 testing loss: tensor(2.3301)\n",
      "Train Epoch: 1 [17280/30000 (57%)]\tLoss: 2.289062 testing loss: tensor(2.3303)\n",
      "Train Epoch: 1 [17920/30000 (60%)]\tLoss: 2.374736 testing loss: tensor(2.3304)\n",
      "Train Epoch: 1 [18560/30000 (62%)]\tLoss: 2.187492 testing loss: tensor(2.3303)\n",
      "Train Epoch: 1 [19200/30000 (64%)]\tLoss: 2.235658 testing loss: tensor(2.3300)\n",
      "Train Epoch: 1 [19840/30000 (66%)]\tLoss: 2.156250 testing loss: tensor(2.3289)\n",
      "Train Epoch: 1 [20480/30000 (68%)]\tLoss: 2.445312 testing loss: tensor(2.3279)\n",
      "Train Epoch: 1 [21120/30000 (70%)]\tLoss: 2.437500 testing loss: tensor(2.3271)\n",
      "Train Epoch: 1 [21760/30000 (72%)]\tLoss: 1.968761 testing loss: tensor(2.3266)\n",
      "Train Epoch: 1 [22400/30000 (74%)]\tLoss: 2.359377 testing loss: tensor(2.3265)\n",
      "Train Epoch: 1 [23040/30000 (77%)]\tLoss: 2.312499 testing loss: tensor(2.3267)\n",
      "Train Epoch: 1 [23680/30000 (79%)]\tLoss: 2.359375 testing loss: tensor(2.3270)\n",
      "Train Epoch: 1 [24320/30000 (81%)]\tLoss: 1.960938 testing loss: tensor(2.3271)\n",
      "Train Epoch: 1 [24960/30000 (83%)]\tLoss: 2.375062 testing loss: tensor(2.3271)\n",
      "Train Epoch: 1 [25600/30000 (85%)]\tLoss: 2.312500 testing loss: tensor(2.3274)\n",
      "Train Epoch: 1 [26240/30000 (87%)]\tLoss: 2.484374 testing loss: tensor(2.3274)\n",
      "Train Epoch: 1 [26880/30000 (89%)]\tLoss: 2.179678 testing loss: tensor(2.3266)\n",
      "Train Epoch: 1 [27520/30000 (91%)]\tLoss: 2.062500 testing loss: tensor(2.3264)\n",
      "Train Epoch: 1 [28160/30000 (94%)]\tLoss: 2.273436 testing loss: tensor(2.3265)\n",
      "Train Epoch: 1 [28800/30000 (96%)]\tLoss: 2.281250 testing loss: tensor(2.3268)\n",
      "Train Epoch: 1 [29440/30000 (98%)]\tLoss: 2.296875 testing loss: tensor(2.3265)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(2.3265)\n",
      "CS 2 : 2.3521\n",
      "DP 2 : 1.7483\n",
      "heuristic 2 : 1.7004666666666666\n",
      "DP: 1.7567987442016602\n",
      "tensor([0.2283, 0.1912, 0.2508, 0.1569, 0.1727])\n",
      "tensor([0.2789, 0.2305, 0.2893, 0.2013, 1.0000])\n",
      "tensor([0.3637, 0.2715, 0.3648, 1.0000, 1.0000])\n",
      "tensor([0.5231, 0.4769, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "do nothing\n",
      "NN 1 : tensor(2.3500)\n",
      "CS 1 : 2.3521\n",
      "DP 1 : 1.7483\n",
      "heuristic 1 : 1.7004666666666666\n",
      "DP: 1.7567987442016602\n",
      "tensor([0.2019, 0.1813, 0.2222, 0.2074, 0.1872])\n",
      "tensor([0.2363, 0.2273, 0.2896, 0.2468, 1.0000])\n",
      "tensor([0.3042, 0.3270, 0.3688, 1.0000, 1.0000])\n",
      "tensor([0.4802, 0.5198, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 2.240982 testing loss: tensor(2.3494)\n",
      "Train Epoch: 1 [640/30000 (2%)]\tLoss: 2.143789 testing loss: tensor(2.3480)\n",
      "Train Epoch: 1 [1280/30000 (4%)]\tLoss: 2.442425 testing loss: tensor(2.3442)\n",
      "Train Epoch: 1 [1920/30000 (6%)]\tLoss: 2.205914 testing loss: tensor(2.3395)\n",
      "Train Epoch: 1 [2560/30000 (9%)]\tLoss: 2.386226 testing loss: tensor(2.3327)\n",
      "Train Epoch: 1 [3200/30000 (11%)]\tLoss: 2.350121 testing loss: tensor(2.3230)\n",
      "Train Epoch: 1 [3840/30000 (13%)]\tLoss: 2.300558 testing loss: tensor(2.3076)\n",
      "Train Epoch: 1 [4480/30000 (15%)]\tLoss: 2.282934 testing loss: tensor(2.2870)\n",
      "Train Epoch: 1 [5120/30000 (17%)]\tLoss: 2.170489 testing loss: tensor(2.2634)\n",
      "Train Epoch: 1 [5760/30000 (19%)]\tLoss: 2.178703 testing loss: tensor(2.2322)\n",
      "Train Epoch: 1 [6400/30000 (21%)]\tLoss: 2.125226 testing loss: tensor(2.1951)\n",
      "Train Epoch: 1 [7040/30000 (23%)]\tLoss: 2.188131 testing loss: tensor(2.1493)\n",
      "Train Epoch: 1 [7680/30000 (26%)]\tLoss: 2.034834 testing loss: tensor(2.0926)\n",
      "Train Epoch: 1 [8320/30000 (28%)]\tLoss: 2.073010 testing loss: tensor(2.0312)\n",
      "Train Epoch: 1 [8960/30000 (30%)]\tLoss: 2.005855 testing loss: tensor(1.9724)\n",
      "Train Epoch: 1 [9600/30000 (32%)]\tLoss: 2.181864 testing loss: tensor(1.9188)\n",
      "Train Epoch: 1 [10240/30000 (34%)]\tLoss: 1.832316 testing loss: tensor(1.8789)\n",
      "Train Epoch: 1 [10880/30000 (36%)]\tLoss: 1.891736 testing loss: tensor(1.8525)\n",
      "Train Epoch: 1 [11520/30000 (38%)]\tLoss: 1.747019 testing loss: tensor(1.8420)\n",
      "Train Epoch: 1 [12160/30000 (40%)]\tLoss: 1.758579 testing loss: tensor(1.8360)\n",
      "Train Epoch: 1 [12800/30000 (43%)]\tLoss: 1.635286 testing loss: tensor(1.8246)\n",
      "Train Epoch: 1 [13440/30000 (45%)]\tLoss: 1.973225 testing loss: tensor(1.8113)\n",
      "Train Epoch: 1 [14080/30000 (47%)]\tLoss: 1.711461 testing loss: tensor(1.7947)\n",
      "Train Epoch: 1 [14720/30000 (49%)]\tLoss: 1.711321 testing loss: tensor(1.7849)\n",
      "Train Epoch: 1 [15360/30000 (51%)]\tLoss: 1.723364 testing loss: tensor(1.7701)\n",
      "Train Epoch: 1 [16000/30000 (53%)]\tLoss: 1.958169 testing loss: tensor(1.7634)\n",
      "Train Epoch: 1 [16640/30000 (55%)]\tLoss: 1.736136 testing loss: tensor(1.7539)\n",
      "Train Epoch: 1 [17280/30000 (57%)]\tLoss: 1.751784 testing loss: tensor(1.7470)\n",
      "Train Epoch: 1 [17920/30000 (60%)]\tLoss: 1.938052 testing loss: tensor(1.7393)\n",
      "Train Epoch: 1 [18560/30000 (62%)]\tLoss: 1.666672 testing loss: tensor(1.7330)\n",
      "Train Epoch: 1 [19200/30000 (64%)]\tLoss: 1.709387 testing loss: tensor(1.7279)\n",
      "Train Epoch: 1 [19840/30000 (66%)]\tLoss: 1.810576 testing loss: tensor(1.7222)\n",
      "Train Epoch: 1 [20480/30000 (68%)]\tLoss: 1.753560 testing loss: tensor(1.7167)\n",
      "Train Epoch: 1 [21120/30000 (70%)]\tLoss: 1.636351 testing loss: tensor(1.7110)\n",
      "Train Epoch: 1 [21760/30000 (72%)]\tLoss: 1.736097 testing loss: tensor(1.7078)\n",
      "Train Epoch: 1 [22400/30000 (74%)]\tLoss: 1.579233 testing loss: tensor(1.7063)\n",
      "Train Epoch: 1 [23040/30000 (77%)]\tLoss: 1.693955 testing loss: tensor(1.7027)\n",
      "Train Epoch: 1 [23680/30000 (79%)]\tLoss: 1.591717 testing loss: tensor(1.7033)\n",
      "Train Epoch: 1 [24320/30000 (81%)]\tLoss: 1.699751 testing loss: tensor(1.7015)\n",
      "Train Epoch: 1 [24960/30000 (83%)]\tLoss: 1.755818 testing loss: tensor(1.6998)\n",
      "Train Epoch: 1 [25600/30000 (85%)]\tLoss: 1.753574 testing loss: tensor(1.6985)\n",
      "Train Epoch: 1 [26240/30000 (87%)]\tLoss: 1.759391 testing loss: tensor(1.6962)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [26880/30000 (89%)]\tLoss: 1.677203 testing loss: tensor(1.6955)\n",
      "Train Epoch: 1 [27520/30000 (91%)]\tLoss: 1.849812 testing loss: tensor(1.6936)\n",
      "Train Epoch: 1 [28160/30000 (94%)]\tLoss: 1.642730 testing loss: tensor(1.6926)\n",
      "Train Epoch: 1 [28800/30000 (96%)]\tLoss: 1.897741 testing loss: tensor(1.6903)\n",
      "Train Epoch: 1 [29440/30000 (98%)]\tLoss: 1.685393 testing loss: tensor(1.6892)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.6885)\n",
      "CS 2 : 2.3521\n",
      "DP 2 : 1.7483\n",
      "heuristic 2 : 1.7004666666666666\n",
      "DP: 1.7567987442016602\n",
      "tensor([0.1147, 0.0656, 0.6811, 0.0702, 0.0684])\n",
      "tensor([0.1345, 0.0824, 0.6927, 0.0904, 1.0000])\n",
      "tensor([0.1523, 0.1215, 0.7261, 1.0000, 1.0000])\n",
      "tensor([0.7273, 0.2727, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "do nothing\n",
      "NN 1 : tensor(2.3454)\n",
      "CS 1 : 2.3521\n",
      "DP 1 : 1.7483\n",
      "heuristic 1 : 1.7004666666666666\n",
      "DP: 1.7567987442016602\n",
      "tensor([0.2196, 0.1900, 0.2074, 0.1977, 0.1853])\n",
      "tensor([0.2722, 0.2266, 0.2496, 0.2516, 1.0000])\n",
      "tensor([0.3491, 0.3238, 0.3271, 1.0000, 1.0000])\n",
      "tensor([0.5203, 0.4797, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 2.392541 testing loss: tensor(2.3456)\n",
      "Train Epoch: 1 [640/30000 (2%)]\tLoss: 2.394346 testing loss: tensor(2.3437)\n",
      "Train Epoch: 1 [1280/30000 (4%)]\tLoss: 2.522617 testing loss: tensor(2.3387)\n",
      "Train Epoch: 1 [1920/30000 (6%)]\tLoss: 2.374379 testing loss: tensor(2.3321)\n",
      "Train Epoch: 1 [2560/30000 (9%)]\tLoss: 2.405646 testing loss: tensor(2.3279)\n",
      "Train Epoch: 1 [3200/30000 (11%)]\tLoss: 2.448440 testing loss: tensor(2.3226)\n",
      "Train Epoch: 1 [3840/30000 (13%)]\tLoss: 2.391207 testing loss: tensor(2.3156)\n",
      "Train Epoch: 1 [4480/30000 (15%)]\tLoss: 2.226773 testing loss: tensor(2.3051)\n",
      "Train Epoch: 1 [5120/30000 (17%)]\tLoss: 2.257620 testing loss: tensor(2.2925)\n",
      "Train Epoch: 1 [5760/30000 (19%)]\tLoss: 2.328935 testing loss: tensor(2.2748)\n",
      "Train Epoch: 1 [6400/30000 (21%)]\tLoss: 2.243315 testing loss: tensor(2.2537)\n",
      "Train Epoch: 1 [7040/30000 (23%)]\tLoss: 2.153852 testing loss: tensor(2.2306)\n",
      "Train Epoch: 1 [7680/30000 (26%)]\tLoss: 2.105986 testing loss: tensor(2.2005)\n",
      "Train Epoch: 1 [8320/30000 (28%)]\tLoss: 2.114409 testing loss: tensor(2.1566)\n",
      "Train Epoch: 1 [8960/30000 (30%)]\tLoss: 2.152928 testing loss: tensor(2.1028)\n",
      "Train Epoch: 1 [9600/30000 (32%)]\tLoss: 2.102752 testing loss: tensor(2.0392)\n",
      "Train Epoch: 1 [10240/30000 (34%)]\tLoss: 1.966665 testing loss: tensor(1.9640)\n",
      "Train Epoch: 1 [10880/30000 (36%)]\tLoss: 1.820372 testing loss: tensor(1.8930)\n",
      "Train Epoch: 1 [11520/30000 (38%)]\tLoss: 1.785599 testing loss: tensor(1.8512)\n",
      "Train Epoch: 1 [12160/30000 (40%)]\tLoss: 1.730954 testing loss: tensor(1.8455)\n",
      "Train Epoch: 1 [12800/30000 (43%)]\tLoss: 1.805662 testing loss: tensor(1.8466)\n",
      "Train Epoch: 1 [13440/30000 (45%)]\tLoss: 1.695841 testing loss: tensor(1.8364)\n",
      "Train Epoch: 1 [14080/30000 (47%)]\tLoss: 1.980189 testing loss: tensor(1.8302)\n",
      "Train Epoch: 1 [14720/30000 (49%)]\tLoss: 1.757271 testing loss: tensor(1.8239)\n",
      "Train Epoch: 1 [15360/30000 (51%)]\tLoss: 1.755068 testing loss: tensor(1.8186)\n",
      "Train Epoch: 1 [16000/30000 (53%)]\tLoss: 1.838093 testing loss: tensor(1.8117)\n",
      "Train Epoch: 1 [16640/30000 (55%)]\tLoss: 1.907934 testing loss: tensor(1.8055)\n",
      "Train Epoch: 1 [17280/30000 (57%)]\tLoss: 1.886764 testing loss: tensor(1.7984)\n",
      "Train Epoch: 1 [17920/30000 (60%)]\tLoss: 1.686544 testing loss: tensor(1.7878)\n",
      "Train Epoch: 1 [18560/30000 (62%)]\tLoss: 1.788321 testing loss: tensor(1.7811)\n",
      "Train Epoch: 1 [19200/30000 (64%)]\tLoss: 1.692545 testing loss: tensor(1.7744)\n",
      "Train Epoch: 1 [19840/30000 (66%)]\tLoss: 1.715512 testing loss: tensor(1.7648)\n",
      "Train Epoch: 1 [20480/30000 (68%)]\tLoss: 1.754945 testing loss: tensor(1.7578)\n",
      "Train Epoch: 1 [21120/30000 (70%)]\tLoss: 1.713732 testing loss: tensor(1.7478)\n",
      "Train Epoch: 1 [21760/30000 (72%)]\tLoss: 1.673938 testing loss: tensor(1.7389)\n",
      "Train Epoch: 1 [22400/30000 (74%)]\tLoss: 1.974956 testing loss: tensor(1.7312)\n",
      "Train Epoch: 1 [23040/30000 (77%)]\tLoss: 1.664948 testing loss: tensor(1.7249)\n",
      "Train Epoch: 1 [23680/30000 (79%)]\tLoss: 1.783340 testing loss: tensor(1.7186)\n",
      "Train Epoch: 1 [24320/30000 (81%)]\tLoss: 1.791382 testing loss: tensor(1.7128)\n",
      "Train Epoch: 1 [24960/30000 (83%)]\tLoss: 1.550263 testing loss: tensor(1.7096)\n",
      "Train Epoch: 1 [25600/30000 (85%)]\tLoss: 1.741101 testing loss: tensor(1.7057)\n",
      "Train Epoch: 1 [26240/30000 (87%)]\tLoss: 1.689199 testing loss: tensor(1.7013)\n",
      "Train Epoch: 1 [26880/30000 (89%)]\tLoss: 1.615215 testing loss: tensor(1.7006)\n",
      "Train Epoch: 1 [27520/30000 (91%)]\tLoss: 1.644460 testing loss: tensor(1.6964)\n",
      "Train Epoch: 1 [28160/30000 (94%)]\tLoss: 1.535752 testing loss: tensor(1.6936)\n",
      "Train Epoch: 1 [28800/30000 (96%)]\tLoss: 1.703645 testing loss: tensor(1.6904)\n",
      "Train Epoch: 1 [29440/30000 (98%)]\tLoss: 1.654624 testing loss: tensor(1.6879)\n",
      "penalty: 0.007886350154876709\n",
      "NN 2 : tensor(1.6862)\n",
      "CS 2 : 2.3521\n",
      "DP 2 : 1.7483\n",
      "heuristic 2 : 1.7004666666666666\n",
      "DP: 1.7567987442016602\n",
      "tensor([0.1273, 0.0563, 0.6841, 0.0692, 0.0631])\n",
      "tensor([0.1469, 0.0749, 0.6866, 0.0916, 1.0000])\n",
      "tensor([0.1623, 0.1065, 0.7312, 1.0000, 1.0000])\n",
      "tensor([0.7491, 0.2509, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "    \n",
    "for order in stage:\n",
    "    producedata(order)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "    #running\n",
    "    for order1 in order1name:\n",
    "        print(\"Supervised Aim:\",order,order1)\n",
    "        for trainingnumber in numberofpeople:\n",
    "            # for mapping binary to payments before softmax\n",
    "            model = nn.Sequential(\n",
    "                nn.Linear(n, 100),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(100, 100),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(100, 100),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(100, n),\n",
    "            )\n",
    "            model.apply(init_weights)\n",
    "            # optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "            runningLossNN = []\n",
    "            runningLossCS = []\n",
    "            runningLossDP = []\n",
    "            runningLossHeuristic = []\n",
    "            #model=torch.load(\"save/pytorchNN=5dp1\");\n",
    "            #model.eval()\n",
    "            ##order1name=[\"costsharing\",\"dp\",\"heuristic\",\"random initializing\"]\n",
    "            for epoch in range(1, supervisionEpochs + 1):\n",
    "        #             print(\"distributionRatio\",distributionRatio)\n",
    "                if(order1==\"costsharing\"):\n",
    "                    supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "                elif(order1==\"dp\"):\n",
    "                    supervisionTrain(epoch, dpSupervisionRule)\n",
    "                elif(order1==\"heuristic\"):\n",
    "                    supervisionTrain(epoch, heuristicSupervisionRule)\n",
    "                elif(order1==\"random initializing\"):\n",
    "                    print(\"do nothing\");\n",
    "\n",
    "            test()\n",
    "\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                if(trainingnumber=='0'):\n",
    "                    train0(epoch)\n",
    "                if(trainingnumber=='1'):\n",
    "                    train1(epoch)\n",
    "                if(trainingnumber=='2'):\n",
    "                    train2(epoch)\n",
    "                test()\n",
    "            losslistname.append(order+\" \"+order1+\" choose people:\"+trainingnumber);\n",
    "            losslist.append(losslisttemp);\n",
    "            losslisttemp=[];\n",
    "            savepath=\"save/pytorchNN=5all-phoenix\"+order+str(order1)+trainingnumber\n",
    "            torch.save(model, savepath);\n",
    "            print(\"end\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colorlist=[\"#D2691E\",'#4169E1',\"#9ACD32\",\"#B22222\",\"#FF00FF\",\"#708090\"]\n",
    "for i in range(len(losslist)):\n",
    "    plt.plot(losslist[i], '.-', color=colorlist[i], alpha=0.8, label=losslistname[i])\n",
    "\n",
    "# 显示标签，如果不加这句，即使加了label='一些数字'的参数，最终还是不会显示标签\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('trianing times')\n",
    "plt.ylabel('delay')\n",
    "plt.title(\"n=\"+str(n))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
