{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40058530825361593\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from random import randint\n",
    "import random\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "from pylab import *\n",
    "from scipy.stats import beta\n",
    "import math\n",
    "n = 3\n",
    "epochs = 1\n",
    "supervisionEpochs = 1\n",
    "lr1 = 0.01\n",
    "lr2 = 0.0001\n",
    "log_interval = 5\n",
    "trainSize = 40000#100000\n",
    "percentage_train_test= 0.5\n",
    "penaltyLambda = 10\n",
    "doublePeakHighMean = 0.6\n",
    "doublePeakLowMean = 0.2\n",
    "doublePeakStd = 0.1\n",
    "uniformlow=0\n",
    "uniformhigh=1.0\n",
    "normalloc = 0.4\n",
    "normalscale = 0.1\n",
    "\n",
    "cauchyloc = 1/n\n",
    "cauchyscalen = 0.004\n",
    "\n",
    "exponentialhigh = 15 #Symbol(\"b\", real=True)\n",
    "exponentiallow  = 15 #Symbol(\"a\", real=True)\n",
    "\n",
    "\n",
    "beta_a = 0.3\n",
    "beta_b  = 0.2\n",
    "kumaraswamy_a = beta_a \n",
    "kumaraswamy_b = (1.0+(beta_a-1.0)*math.pow( (beta_a+beta_b-2.0)/(beta_a-1.0), beta_a) )/beta_a \n",
    "print(kumaraswamy_b)\n",
    "\n",
    "\n",
    "independentnormalloc1=[(float(ii)+1)/(2*n+1) for ii in range(n,0,-1)]\n",
    "independentnormalscale1=[0.05 for ii in range(n)]\n",
    "\n",
    "independentnormalloc2=[(float(ii)+1)/(2*n+1) for ii in range(1,n+1,1)]\n",
    "independentnormalscale2=[0.05 for ii in range(n)]\n",
    "stage=[\"normal\"]\n",
    "# \"twopeak\",\"normal\",\"uniform\",\"independent1\",\"independent2\",\"cauchy\",\"beta\",\"U-exponential\",\"arcsine\"\n",
    "order1name=[\"dp\",\"random initializing\",\"costsharing\",\"heuristic\"]\n",
    "# \"costsharing\",\"dp\",\"heuristic\",\"random initializing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio1 = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase1 = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "d3 = D.normal.Normal(normalloc, normalscale)\n",
    "distributionRatio3 = d3.cdf(1) - d3.cdf(0)\n",
    "distributionBase3 = d3.cdf(0)\n",
    "\n",
    "d4 = D.uniform.Uniform(uniformlow,uniformhigh)\n",
    "distributionRatio4 = d4.cdf(1) - d4.cdf(0)\n",
    "distributionBase4 = d4.cdf(0)\n",
    "\n",
    "d5 = [D.normal.Normal(independentnormalloc1[ii], independentnormalscale1[ii]) for ii in range(n)]\n",
    "d6 = [D.normal.Normal(independentnormalloc2[ii], independentnormalscale2[ii]) for ii in range(n)]\n",
    "\n",
    "d7 = D.cauchy.Cauchy(cauchyloc,cauchyscalen)\n",
    "\n",
    "d81 = D.exponential.Exponential(exponentiallow)\n",
    "d82 = D.exponential.Exponential(exponentialhigh)\n",
    "\n",
    "d9 = D.beta.Beta(beta_a,beta_b)\n",
    "#sample_d9=d9.rsample(torch.Size([100000]))\n",
    "\n",
    "#d9_sample=np.linspace(0.0001, 0.9999, 10000)\n",
    "#d9_pdf=torch.exp(d9.log_prob(torch.tensor(d9_sample,dtype=torch.float32)))\n",
    "#d9_delta=d9_sample[1]-d9_sample[0]\n",
    "#d9_sum_pdf=torch.sum(d9_pdf*d9_delta)\n",
    "\n",
    "d10 = D.beta.Beta(0.5,0.5)\n",
    "\n",
    "def cdf(x,y, i=None):\n",
    "    if(y==\"twopeak\"):\n",
    "        return (d1.cdf(x) + d2.cdf(x) - distributionBase1) / 2 / distributionRatio1\n",
    "    elif(y==\"normal\"):\n",
    "        return (d3.cdf(x)-distributionBase3)/distributionRatio3;\n",
    "    elif(y==\"uniform\"):\n",
    "        return (d4.cdf(x)-distributionBase4)/distributionRatio4;\n",
    "    elif(y==\"independent1\"):\n",
    "        return d5[i].cdf(x);\n",
    "    elif(y==\"independent2\"):\n",
    "        return d6[i].cdf(x);\n",
    "    elif(y==\"cauchy\"):\n",
    "        return d7.cdf(x);\n",
    "    elif(y==\"beta\"):\n",
    "#         sum_cdf=0.0;\n",
    "#         if(x<0.0001):\n",
    "#             x=0.00011;\n",
    "#         if(x>0.9999):\n",
    "#             x=0.99989;\n",
    "#         for i in range(len(d9_pdf)):\n",
    "#             if(d9_sample[i]<x):\n",
    "#                 sum_cdf+=d9_pdf[i]*d9_delta;\n",
    "#             else:\n",
    "#                 sum_cdf+=(d9_pdf[i]+d9_pdf[i-1])/ 2 *(x-d9_sample[i-1])\n",
    "#                 break;\n",
    "#         return sum_cdf/d9_sum_pdf\n",
    "#         cdf_v=torch.sum((sample_d9<(x)), dtype=torch.float32)/100000\n",
    "#         return cdf_v\n",
    "#    F(x|a,b)=1–(1–x^a)^b\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(x,kumaraswamy_a),kumaraswamy_b);\n",
    "        except:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(torch.tensor(x,dtype=torch.float32),kumaraswamy_a),kumaraswamy_b);\n",
    "    elif(y==\"arcsine\"):\n",
    "        #\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            res=2.0/math.pi * torch.asin(torch.sqrt(x))\n",
    "            #print(x)\n",
    "            return res# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "        except:\n",
    "            return 2.0/math.pi * torch.asin(torch.sqrt(torch.tensor(x,dtype=torch.float32)))# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "    elif(y==\"U-exponential\"):\n",
    "        return (d81.cdf(x) + (1.0 - d82.cdf(1.0-x)))  / 2 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    if torch.sum(bits).item() == 0:\n",
    "        return torch.ones(n)\n",
    "    bits = bits.type(torch.float32)\n",
    "    negBits = torch.ones(n) - bits\n",
    "    payments = model(bits)\n",
    "    payments = payments - 1000 * negBits\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    payments = payments + negBits\n",
    "    return payments\n",
    "\n",
    "def tpToBits(tp, bits=torch.ones(n).type(torch.uint8)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = (tp >= payments).type(torch.uint8)\n",
    "    if torch.equal(newBits, bits):\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits(tp, newBits)#bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    return n - torch.sum(tpToBits(tp).type(torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    global  dp,decision\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money, yes]\n",
    "        offer = float(offerIndex) / dpPrecision\n",
    "        if tp[i] >= offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits[i] = 0\n",
    "            payments[i] = 1\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "#     print()\n",
    "#     print(tp)\n",
    "#     print(bits)\n",
    "#     print(payments)\n",
    "#     print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def heuristicSupervisionRule(tp):\n",
    "    global  dp_H,decision_H\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    tempold=-1;\n",
    "    for turns in range(n,0,-1):\n",
    "        money = dpPrecision\n",
    "        j=0\n",
    "        tempo=sum(bits)\n",
    "        #print(\"bits\",tempo)\n",
    "        for i in range(n):\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            \n",
    "            offerIndex = decision_H[tempo ,tempo - i, money]\n",
    "            offer = float(offerIndex) / dpPrecision\n",
    "            while(j<n):\n",
    "                if(bits[j]!=0):\n",
    "                    break;\n",
    "                j+=1;\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            if tp[j] >= offer:\n",
    "                #print(money,j,tp[j],offer)\n",
    "                money -= offerIndex\n",
    "                bits[j] = 1\n",
    "                payments[j] = offer\n",
    "            else:\n",
    "                bits[j] = 0;\n",
    "                payments[j] = 1.0;\n",
    "            j+=1;\n",
    "        #print(\"money\",money)\n",
    "        if(money==0 and tempold==tempo):\n",
    "            break;\n",
    "        tempold=tempo;\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    for k in range(n, -1, -1):\n",
    "        if k == 0:\n",
    "            break\n",
    "        bits = [1 if tp[ii] >= 1.0 / k else 0 for ii in range(n)]\n",
    "        if sum(bits) == k:\n",
    "            break\n",
    "    if k == 0:\n",
    "        payments = [1 for ii in range(n)]\n",
    "    else:\n",
    "        payments = [1.0 / k if bits[ii] == 1 else 1 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    return float(n - torch.sum(costSharingSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def dpDelay(tp):\n",
    "    return float(n - torch.sum(dpSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def heuristicDelay(tp):\n",
    "    return float(n - torch.sum(heuristicSupervisionRule(tp)[0]).item())\n",
    "\n",
    "\n",
    "# templire=0;\n",
    "# num=1\n",
    "# for i in range(num):\n",
    "#     temp=torch.tensor(samplesJoint[i])\n",
    "#     #print(temp)\n",
    "#     #print(temp);\n",
    "#     #print(heuristicSupervisionRule(temp))\n",
    "#     #print(dpSupervisionRule(temp))\n",
    "#     #print(dpDelay(temp))\n",
    "#     res=dpDelay(temp)\n",
    "#     templire+=res\n",
    "#     #print(\"Delay\",res)\n",
    "#     #print()\n",
    "# print(templire/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordAndReport(name, source, loss):\n",
    "    source.append(loss)\n",
    "    realLength = len(source)\n",
    "    #print(f\"{name} ({realLength}): {loss}\")\n",
    "    print(name,realLength,\":\" ,loss)\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    \n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda /100000\n",
    "        \n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % (log_interval*5) == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "losslist=[];\n",
    "losslistname=[];\n",
    "losslisttemp=[];\n",
    "def test_batch():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lenLoss= 0\n",
    "        nnLoss = 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "            for tp in tp_batch:\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        nnLoss/=lenLoss\n",
    "    return nnLoss\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                Delay1 = tpToTotalDelay(tp1)\n",
    "                Delay0 = tpToTotalDelay(tp0)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                    loss = loss + (1 - cdf(offer,order)) * Delay1 + cdf(offer,order) * Delay0\n",
    "                else:\n",
    "                    loss = loss + (1 - cdf(offer,order,i)) * Delay1 + cdf(offer,order,i) * Delay0\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "    \n",
    "allBits = [torch.tensor(bits, dtype=torch.int16) for bits in itertools.product([0, 1], repeat=n)]\n",
    "\n",
    "def test():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        costSharingLoss = 0\n",
    "        dpLoss = 0\n",
    "        nnLoss = 0\n",
    "        heuristicLoss=0\n",
    "        lenLoss= 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "                heuristicLoss+= heuristicDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        costSharingLoss /= lenLoss\n",
    "        dpLoss /= lenLoss\n",
    "        nnLoss /= lenLoss\n",
    "        heuristicLoss /= lenLoss\n",
    "        #print(lenLoss)\n",
    "        losslisttemp.append(nnLoss);\n",
    "        recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "        recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "        recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "        recordAndReport(\"heuristic\", runningLossHeuristic, heuristicLoss)\n",
    "        print(\"DP:\",dp[n, dpPrecision,0])\n",
    "        #for i in range(n, 0, -1):\n",
    "        #    print(\"Heuristic:\",i,5*(1-dp_H[i, i, dpPrecision]))\n",
    "        for i in range(n, 0, -1):\n",
    "            print(\n",
    "                    tpToPayments(\n",
    "                            torch.tensor([0 if ii >= i else 1 for ii in range(n)], dtype=torch.float32)\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "dpPrecision = 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def producedata(order):\n",
    "    global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "    dpPrecision = 100 \n",
    "    if(order==\"twopeak\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = np.random.normal(\n",
    "            loc=doublePeakLowMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=doublePeakLowMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samples2 = np.random.normal(\n",
    "            loc=doublePeakHighMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = np.random.normal(\n",
    "                        loc=doublePeakHighMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "    elif(order==\"normal\"):\n",
    "        print(\"loc\",normalloc, \"scale\",normalscale)\n",
    "        samples1 = np.random.normal(\n",
    "            loc=normalloc, scale=normalscale, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=normalloc, scale=normalscale\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"uniform\"):  \n",
    "        print(\"uniformlow\",uniformlow, \"uniformhigh\",uniformhigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        uniformlow, uniformhigh\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent1\"):\n",
    "        print(\"loc\",independentnormalloc1,\"scale\",independentnormalscale1)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent2\"):\n",
    "        print(\"loc\",independentnormalloc2, \"scale\",independentnormalscale2)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"cauchy\"):\n",
    "        print(\"cauchyloc\",cauchyloc, \"cauchyscale\",cauchyscalen)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"beta\"):\n",
    "        print(\"beta_a\",beta_a, \"beta_b\",beta_b)\n",
    "        print(\"kumaraswamy_a\",kumaraswamy_a, \"kumaraswamy_b\",kumaraswamy_b)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"arcsine\"):\n",
    "        print(\"betalow\",0.5, \"betahigh\",0.5)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"U-exponential\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = d81.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d81.rsample(torch.Size([1])).numpy()\n",
    "                    \n",
    "        samples2 = d82.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        \n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = d82.rsample(torch.Size([1])).numpy()\n",
    "        samples2 = 1.0 - samples2\n",
    "        samplesJoint = signals * samples1 - (signals - 1.0) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "        \n",
    "    plt.hist(samplesJoint,bins=500)\n",
    "    plt.show()\n",
    "    \n",
    "    tp_dataset = TensorDataset(tp_tensor[: int(trainSize * percentage_train_test)])\n",
    "    tp_dataset_testing = TensorDataset(tp_tensor[int(trainSize * (1.0-percentage_train_test)) :])\n",
    "    tp_dataloader = DataLoader(tp_dataset, batch_size=128, shuffle=True)\n",
    "    tp_dataloader_testing = DataLoader(tp_dataset_testing, batch_size=256, shuffle=False)\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "    decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for yes in range(n + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp[0, 0, yes] = 0\n",
    "            else:\n",
    "                dp[0, money, yes] = yes# + 1.0\n",
    "    for ppl in range(1,  n + 1):\n",
    "        for yes in range(n + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                minSoFar = 1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1 - cdf(offer,order)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order) * (1 + dp[ppl - 1, money, yes])\n",
    "                    else:\n",
    "                        res = (1 - cdf(offer,order,n-ppl)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order,n-ppl) * (1 + dp[ppl - 1, money, yes])\n",
    "                    if minSoFar > res.item():\n",
    "                        minSoFar = res.item()\n",
    "                        decision[ppl, money, yes] = offerIndex\n",
    "                dp[ppl, money, yes] = minSoFar\n",
    "    \n",
    "    print(\"dp\",dp[n, dpPrecision, 0])\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp_H = np.zeros([n + 1 , n + 1, dpPrecision + 1])\n",
    "    decision_H = np.zeros([n + 1 , n + 1, dpPrecision + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for i in range(1,n+1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp_H[i, 0, 0] = 1\n",
    "            else:\n",
    "                offer = money / dpPrecision\n",
    "                dp_H[i, 0, money] = 0#cdf(offer)# + 1.0\n",
    "    for i in range(1,n+1):\n",
    "        for ppl in range(1, i + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                maxSoFar = -1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1-cdf(offer,order)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    else:\n",
    "                        res = (1-cdf(offer,order,ppl-1)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    if maxSoFar < res.item():\n",
    "                        maxSoFar = res.item()\n",
    "                        decision_H[i, ppl, money] = offerIndex\n",
    "                dp_H[i, ppl, money] = maxSoFar\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loc 0.4 scale 0.1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dp 1.733353853225708\n",
      "Supervised Aim: normal dp\n",
      "Train Epoch: 1 [0/20000 (0%)]\tLoss: 0.001734\n",
      "Train Epoch: 1 [3200/20000 (16%)]\tLoss: 0.000006\n",
      "Train Epoch: 1 [6400/20000 (32%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [9600/20000 (48%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [12800/20000 (64%)]\tLoss: 0.000144\n",
      "Train Epoch: 1 [16000/20000 (80%)]\tLoss: 0.000005\n",
      "Train Epoch: 1 [19200/20000 (96%)]\tLoss: 0.000000\n",
      "NN 1 : tensor(1.7312)\n",
      "CS 1 : 1.722\n",
      "DP 1 : 1.74435\n",
      "heuristic 1 : 1.73395\n",
      "DP: 1.733353853225708\n",
      "tensor([0.3397, 0.3301, 0.3301])\n",
      "tensor([0.5057, 0.4943, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 1 [0/20000 (0%)]\tLoss: 1.728006 testing loss: tensor(1.7284)\n",
      "Train Epoch: 1 [640/20000 (3%)]\tLoss: 1.778383 testing loss: tensor(1.7292)\n",
      "Train Epoch: 1 [1280/20000 (6%)]\tLoss: 1.694255 testing loss: tensor(1.7309)\n",
      "Train Epoch: 1 [1920/20000 (10%)]\tLoss: 1.840725 testing loss: tensor(1.7308)\n",
      "Train Epoch: 1 [2560/20000 (13%)]\tLoss: 1.808261 testing loss: tensor(1.7320)\n",
      "Train Epoch: 1 [3200/20000 (16%)]\tLoss: 1.740843 testing loss: tensor(1.7306)\n",
      "Train Epoch: 1 [3840/20000 (19%)]\tLoss: 1.773464 testing loss: tensor(1.7315)\n",
      "Train Epoch: 1 [4480/20000 (22%)]\tLoss: 1.735722 testing loss: tensor(1.7281)\n",
      "Train Epoch: 1 [5120/20000 (25%)]\tLoss: 1.588457 testing loss: tensor(1.7295)\n",
      "Train Epoch: 1 [5760/20000 (29%)]\tLoss: 1.775792 testing loss: tensor(1.7299)\n",
      "Train Epoch: 1 [6400/20000 (32%)]\tLoss: 1.674612 testing loss: tensor(1.7295)\n",
      "Train Epoch: 1 [7040/20000 (35%)]\tLoss: 1.814998 testing loss: tensor(1.7277)\n",
      "Train Epoch: 1 [7680/20000 (38%)]\tLoss: 1.724281 testing loss: tensor(1.7276)\n",
      "Train Epoch: 1 [8320/20000 (41%)]\tLoss: 1.675788 testing loss: tensor(1.7255)\n",
      "Train Epoch: 1 [8960/20000 (45%)]\tLoss: 1.697482 testing loss: tensor(1.7250)\n",
      "Train Epoch: 1 [9600/20000 (48%)]\tLoss: 1.726241 testing loss: tensor(1.7249)\n",
      "Train Epoch: 1 [10240/20000 (51%)]\tLoss: 1.613261 testing loss: tensor(1.7252)\n",
      "Train Epoch: 1 [10880/20000 (54%)]\tLoss: 1.753656 testing loss: tensor(1.7241)\n",
      "Train Epoch: 1 [11520/20000 (57%)]\tLoss: 1.794752 testing loss: tensor(1.7230)\n",
      "Train Epoch: 1 [12160/20000 (61%)]\tLoss: 1.687072 testing loss: tensor(1.7225)\n",
      "Train Epoch: 1 [12800/20000 (64%)]\tLoss: 1.790639 testing loss: tensor(1.7249)\n",
      "Train Epoch: 1 [13440/20000 (67%)]\tLoss: 1.741877 testing loss: tensor(1.7227)\n",
      "Train Epoch: 1 [14080/20000 (70%)]\tLoss: 1.750257 testing loss: tensor(1.7227)\n",
      "Train Epoch: 1 [14720/20000 (73%)]\tLoss: 1.787158 testing loss: tensor(1.7230)\n",
      "Train Epoch: 1 [15360/20000 (76%)]\tLoss: 1.773362 testing loss: tensor(1.7229)\n",
      "Train Epoch: 1 [16000/20000 (80%)]\tLoss: 1.649365 testing loss: tensor(1.7217)\n",
      "Train Epoch: 1 [16640/20000 (83%)]\tLoss: 1.668967 testing loss: tensor(1.7205)\n",
      "Train Epoch: 1 [17280/20000 (86%)]\tLoss: 1.747905 testing loss: tensor(1.7206)\n",
      "Train Epoch: 1 [17920/20000 (89%)]\tLoss: 1.671560 testing loss: tensor(1.7215)\n",
      "Train Epoch: 1 [18560/20000 (92%)]\tLoss: 1.700004 testing loss: tensor(1.7204)\n",
      "Train Epoch: 1 [19200/20000 (96%)]\tLoss: 1.722492 testing loss: tensor(1.7195)\n",
      "Train Epoch: 1 [19840/20000 (99%)]\tLoss: 1.643327 testing loss: tensor(1.7226)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.7226)\n",
      "CS 2 : 1.722\n",
      "DP 2 : 1.74435\n",
      "heuristic 2 : 1.73395\n",
      "DP: 1.733353853225708\n",
      "tensor([0.3342, 0.3336, 0.3323])\n",
      "tensor([0.4991, 0.5009, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: normal random initializing\n",
      "do nothing\n",
      "NN 1 : tensor(2.0665)\n",
      "CS 1 : 1.722\n",
      "DP 1 : 1.74435\n",
      "heuristic 1 : 1.73395\n",
      "DP: 1.733353853225708\n",
      "tensor([0.3257, 0.2555, 0.4189])\n",
      "tensor([0.5957, 0.4043, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 1 [0/20000 (0%)]\tLoss: 2.223575 testing loss: tensor(2.0185)\n",
      "Train Epoch: 1 [640/20000 (3%)]\tLoss: 1.979964 testing loss: tensor(1.8537)\n",
      "Train Epoch: 1 [1280/20000 (6%)]\tLoss: 1.761709 testing loss: tensor(1.7641)\n",
      "Train Epoch: 1 [1920/20000 (10%)]\tLoss: 1.814332 testing loss: tensor(1.7256)\n",
      "Train Epoch: 1 [2560/20000 (13%)]\tLoss: 1.675870 testing loss: tensor(1.7341)\n",
      "Train Epoch: 1 [3200/20000 (16%)]\tLoss: 1.708525 testing loss: tensor(1.7385)\n",
      "Train Epoch: 1 [3840/20000 (19%)]\tLoss: 1.781554 testing loss: tensor(1.7324)\n",
      "Train Epoch: 1 [4480/20000 (22%)]\tLoss: 1.588725 testing loss: tensor(1.7217)\n",
      "Train Epoch: 1 [5120/20000 (25%)]\tLoss: 1.818074 testing loss: tensor(1.7226)\n",
      "Train Epoch: 1 [5760/20000 (29%)]\tLoss: 1.780290 testing loss: tensor(1.7249)\n",
      "Train Epoch: 1 [6400/20000 (32%)]\tLoss: 1.664540 testing loss: tensor(1.7250)\n",
      "Train Epoch: 1 [7040/20000 (35%)]\tLoss: 1.788546 testing loss: tensor(1.7217)\n",
      "Train Epoch: 1 [7680/20000 (38%)]\tLoss: 1.723523 testing loss: tensor(1.7216)\n",
      "Train Epoch: 1 [8320/20000 (41%)]\tLoss: 1.743904 testing loss: tensor(1.7212)\n",
      "Train Epoch: 1 [8960/20000 (45%)]\tLoss: 1.663044 testing loss: tensor(1.7224)\n",
      "Train Epoch: 1 [9600/20000 (48%)]\tLoss: 1.764586 testing loss: tensor(1.7228)\n",
      "Train Epoch: 1 [10240/20000 (51%)]\tLoss: 1.671127 testing loss: tensor(1.7228)\n",
      "Train Epoch: 1 [10880/20000 (54%)]\tLoss: 1.619092 testing loss: tensor(1.7221)\n",
      "Train Epoch: 1 [11520/20000 (57%)]\tLoss: 1.761848 testing loss: tensor(1.7205)\n",
      "Train Epoch: 1 [12160/20000 (61%)]\tLoss: 1.765681 testing loss: tensor(1.7203)\n",
      "Train Epoch: 1 [12800/20000 (64%)]\tLoss: 1.627198 testing loss: tensor(1.7201)\n",
      "Train Epoch: 1 [13440/20000 (67%)]\tLoss: 1.641574 testing loss: tensor(1.7208)\n",
      "Train Epoch: 1 [14080/20000 (70%)]\tLoss: 1.833865 testing loss: tensor(1.7232)\n",
      "Train Epoch: 1 [14720/20000 (73%)]\tLoss: 1.569017 testing loss: tensor(1.7235)\n",
      "Train Epoch: 1 [15360/20000 (76%)]\tLoss: 1.551284 testing loss: tensor(1.7237)\n",
      "Train Epoch: 1 [16000/20000 (80%)]\tLoss: 1.855972 testing loss: tensor(1.7224)\n",
      "Train Epoch: 1 [16640/20000 (83%)]\tLoss: 1.792917 testing loss: tensor(1.7235)\n",
      "Train Epoch: 1 [17280/20000 (86%)]\tLoss: 1.806992 testing loss: tensor(1.7219)\n",
      "Train Epoch: 1 [17920/20000 (89%)]\tLoss: 1.732334 testing loss: tensor(1.7225)\n",
      "Train Epoch: 1 [18560/20000 (92%)]\tLoss: 1.684727 testing loss: tensor(1.7243)\n",
      "Train Epoch: 1 [19200/20000 (96%)]\tLoss: 1.705803 testing loss: tensor(1.7250)\n",
      "Train Epoch: 1 [19840/20000 (99%)]\tLoss: 1.742535 testing loss: tensor(1.7207)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.7196)\n",
      "CS 2 : 1.722\n",
      "DP 2 : 1.74435\n",
      "heuristic 2 : 1.73395\n",
      "DP: 1.733353853225708\n",
      "tensor([0.3368, 0.3322, 0.3311])\n",
      "tensor([0.5056, 0.4944, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: normal costsharing\n",
      "Train Epoch: 1 [0/20000 (0%)]\tLoss: 0.002386\n",
      "Train Epoch: 1 [3200/20000 (16%)]\tLoss: 0.000011\n",
      "Train Epoch: 1 [6400/20000 (32%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [9600/20000 (48%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [12800/20000 (64%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [16000/20000 (80%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [19200/20000 (96%)]\tLoss: 0.000000\n",
      "NN 1 : tensor(1.7219)\n",
      "CS 1 : 1.722\n",
      "DP 1 : 1.74435\n",
      "heuristic 1 : 1.73395\n",
      "DP: 1.733353853225708\n",
      "tensor([0.3333, 0.3333, 0.3333])\n",
      "tensor([0.4998, 0.5002, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 1 [0/20000 (0%)]\tLoss: 1.697374 testing loss: tensor(1.7211)\n",
      "Train Epoch: 1 [640/20000 (3%)]\tLoss: 1.758698 testing loss: tensor(1.7205)\n",
      "Train Epoch: 1 [1280/20000 (6%)]\tLoss: 1.790871 testing loss: tensor(1.7213)\n",
      "Train Epoch: 1 [1920/20000 (10%)]\tLoss: 1.637411 testing loss: tensor(1.7228)\n",
      "Train Epoch: 1 [2560/20000 (13%)]\tLoss: 1.847894 testing loss: tensor(1.7210)\n",
      "Train Epoch: 1 [3200/20000 (16%)]\tLoss: 1.723864 testing loss: tensor(1.7214)\n",
      "Train Epoch: 1 [3840/20000 (19%)]\tLoss: 1.814157 testing loss: tensor(1.7201)\n",
      "Train Epoch: 1 [4480/20000 (22%)]\tLoss: 1.524391 testing loss: tensor(1.7214)\n",
      "Train Epoch: 1 [5120/20000 (25%)]\tLoss: 1.704135 testing loss: tensor(1.7207)\n",
      "Train Epoch: 1 [5760/20000 (29%)]\tLoss: 1.711004 testing loss: tensor(1.7210)\n",
      "Train Epoch: 1 [6400/20000 (32%)]\tLoss: 1.712844 testing loss: tensor(1.7200)\n",
      "Train Epoch: 1 [7040/20000 (35%)]\tLoss: 1.631717 testing loss: tensor(1.7222)\n",
      "Train Epoch: 1 [7680/20000 (38%)]\tLoss: 1.614377 testing loss: tensor(1.7227)\n",
      "Train Epoch: 1 [8320/20000 (41%)]\tLoss: 1.878539 testing loss: tensor(1.7215)\n",
      "Train Epoch: 1 [8960/20000 (45%)]\tLoss: 1.611701 testing loss: tensor(1.7211)\n",
      "Train Epoch: 1 [9600/20000 (48%)]\tLoss: 1.669141 testing loss: tensor(1.7227)\n",
      "Train Epoch: 1 [10240/20000 (51%)]\tLoss: 1.728014 testing loss: tensor(1.7212)\n",
      "Train Epoch: 1 [10880/20000 (54%)]\tLoss: 1.515273 testing loss: tensor(1.7213)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [11520/20000 (57%)]\tLoss: 1.729301 testing loss: tensor(1.7212)\n",
      "Train Epoch: 1 [12160/20000 (61%)]\tLoss: 1.775426 testing loss: tensor(1.7217)\n",
      "Train Epoch: 1 [12800/20000 (64%)]\tLoss: 1.659770 testing loss: tensor(1.7205)\n",
      "Train Epoch: 1 [13440/20000 (67%)]\tLoss: 1.599893 testing loss: tensor(1.7203)\n",
      "Train Epoch: 1 [14080/20000 (70%)]\tLoss: 1.720734 testing loss: tensor(1.7196)\n",
      "Train Epoch: 1 [14720/20000 (73%)]\tLoss: 1.696695 testing loss: tensor(1.7212)\n",
      "Train Epoch: 1 [15360/20000 (76%)]\tLoss: 1.734840 testing loss: tensor(1.7206)\n",
      "Train Epoch: 1 [16000/20000 (80%)]\tLoss: 1.671872 testing loss: tensor(1.7212)\n",
      "Train Epoch: 1 [16640/20000 (83%)]\tLoss: 1.848352 testing loss: tensor(1.7213)\n",
      "Train Epoch: 1 [17280/20000 (86%)]\tLoss: 1.648517 testing loss: tensor(1.7218)\n",
      "Train Epoch: 1 [17920/20000 (89%)]\tLoss: 1.631878 testing loss: tensor(1.7217)\n",
      "Train Epoch: 1 [18560/20000 (92%)]\tLoss: 1.695461 testing loss: tensor(1.7224)\n",
      "Train Epoch: 1 [19200/20000 (96%)]\tLoss: 1.704813 testing loss: tensor(1.7220)\n",
      "Train Epoch: 1 [19840/20000 (99%)]\tLoss: 1.581841 testing loss: tensor(1.7212)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.7213)\n",
      "CS 2 : 1.722\n",
      "DP 2 : 1.74435\n",
      "heuristic 2 : 1.73395\n",
      "DP: 1.733353853225708\n",
      "tensor([0.3352, 0.3322, 0.3327])\n",
      "tensor([0.5019, 0.4981, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: normal heuristic\n",
      "Train Epoch: 1 [0/20000 (0%)]\tLoss: 0.000741\n",
      "Train Epoch: 1 [3200/20000 (16%)]\tLoss: 0.000038\n",
      "Train Epoch: 1 [6400/20000 (32%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [9600/20000 (48%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [12800/20000 (64%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [16000/20000 (80%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [19200/20000 (96%)]\tLoss: 0.000000\n",
      "NN 1 : tensor(1.7207)\n",
      "CS 1 : 1.722\n",
      "DP 1 : 1.74435\n",
      "heuristic 1 : 1.73395\n",
      "DP: 1.733353853225708\n",
      "tensor([0.3400, 0.3300, 0.3300])\n",
      "tensor([0.5000, 0.5000, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 1 [0/20000 (0%)]\tLoss: 1.647622 testing loss: tensor(1.7175)\n",
      "Train Epoch: 1 [640/20000 (3%)]\tLoss: 1.703345 testing loss: tensor(1.7192)\n",
      "Train Epoch: 1 [1280/20000 (6%)]\tLoss: 1.644853 testing loss: tensor(1.7205)\n",
      "Train Epoch: 1 [1920/20000 (10%)]\tLoss: 1.648472 testing loss: tensor(1.7212)\n",
      "Train Epoch: 1 [2560/20000 (13%)]\tLoss: 1.578971 testing loss: tensor(1.7205)\n",
      "Train Epoch: 1 [3200/20000 (16%)]\tLoss: 1.612358 testing loss: tensor(1.7197)\n",
      "Train Epoch: 1 [3840/20000 (19%)]\tLoss: 1.762383 testing loss: tensor(1.7203)\n",
      "Train Epoch: 1 [4480/20000 (22%)]\tLoss: 1.759148 testing loss: tensor(1.7219)\n",
      "Train Epoch: 1 [5120/20000 (25%)]\tLoss: 1.609903 testing loss: tensor(1.7212)\n",
      "Train Epoch: 1 [5760/20000 (29%)]\tLoss: 1.717216 testing loss: tensor(1.7196)\n",
      "Train Epoch: 1 [6400/20000 (32%)]\tLoss: 1.672049 testing loss: tensor(1.7212)\n",
      "Train Epoch: 1 [7040/20000 (35%)]\tLoss: 1.853016 testing loss: tensor(1.7203)\n",
      "Train Epoch: 1 [7680/20000 (38%)]\tLoss: 1.630329 testing loss: tensor(1.7198)\n",
      "Train Epoch: 1 [8320/20000 (41%)]\tLoss: 1.871966 testing loss: tensor(1.7201)\n",
      "Train Epoch: 1 [8960/20000 (45%)]\tLoss: 1.649259 testing loss: tensor(1.7205)\n",
      "Train Epoch: 1 [9600/20000 (48%)]\tLoss: 1.793733 testing loss: tensor(1.7212)\n",
      "Train Epoch: 1 [10240/20000 (51%)]\tLoss: 1.744856 testing loss: tensor(1.7204)\n",
      "Train Epoch: 1 [10880/20000 (54%)]\tLoss: 1.695872 testing loss: tensor(1.7205)\n",
      "Train Epoch: 1 [11520/20000 (57%)]\tLoss: 1.736378 testing loss: tensor(1.7213)\n",
      "Train Epoch: 1 [12160/20000 (61%)]\tLoss: 1.749006 testing loss: tensor(1.7203)\n",
      "Train Epoch: 1 [12800/20000 (64%)]\tLoss: 1.758205 testing loss: tensor(1.7206)\n",
      "Train Epoch: 1 [13440/20000 (67%)]\tLoss: 1.771982 testing loss: tensor(1.7207)\n",
      "Train Epoch: 1 [14080/20000 (70%)]\tLoss: 1.631642 testing loss: tensor(1.7206)\n",
      "Train Epoch: 1 [14720/20000 (73%)]\tLoss: 1.936943 testing loss: tensor(1.7213)\n",
      "Train Epoch: 1 [15360/20000 (76%)]\tLoss: 1.656545 testing loss: tensor(1.7207)\n",
      "Train Epoch: 1 [16000/20000 (80%)]\tLoss: 1.733093 testing loss: tensor(1.7186)\n",
      "Train Epoch: 1 [16640/20000 (83%)]\tLoss: 1.652256 testing loss: tensor(1.7196)\n",
      "Train Epoch: 1 [17280/20000 (86%)]\tLoss: 1.696356 testing loss: tensor(1.7211)\n",
      "Train Epoch: 1 [17920/20000 (89%)]\tLoss: 1.628936 testing loss: tensor(1.7213)\n",
      "Train Epoch: 1 [18560/20000 (92%)]\tLoss: 1.639268 testing loss: tensor(1.7208)\n",
      "Train Epoch: 1 [19200/20000 (96%)]\tLoss: 1.702989 testing loss: tensor(1.7206)\n",
      "Train Epoch: 1 [19840/20000 (99%)]\tLoss: 1.564268 testing loss: tensor(1.7214)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.7210)\n",
      "CS 2 : 1.722\n",
      "DP 2 : 1.74435\n",
      "heuristic 2 : 1.73395\n",
      "DP: 1.733353853225708\n",
      "tensor([0.3342, 0.3312, 0.3346])\n",
      "tensor([0.4982, 0.5018, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.1)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "    \n",
    "for order in stage:\n",
    "    producedata(order)\n",
    "    #running\n",
    "    for order1 in order1name:\n",
    "        print(\"Supervised Aim:\",order,order1)\n",
    "        \n",
    "        # for mapping binary to payments before softmax\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(n, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, n),\n",
    "        )\n",
    "        model.apply(init_weights)\n",
    "        # optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr1)\n",
    "        \n",
    "        runningLossNN = []\n",
    "        runningLossCS = []\n",
    "        runningLossDP = []\n",
    "        runningLossHeuristic = []\n",
    "        #model=torch.load(\"save/pytorchNN=5dp1\");\n",
    "        #model.eval()\n",
    "        ##order1name=[\"costsharing\",\"dp\",\"heuristic\",\"random initializing\"]\n",
    "        for epoch in range(1, supervisionEpochs + 1):\n",
    "#             print(\"distributionRatio\",distributionRatio)\n",
    "            if(order1==\"costsharing\"):\n",
    "                supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "            elif(order1==\"dp\"):\n",
    "                supervisionTrain(epoch, dpSupervisionRule)\n",
    "            elif(order1==\"heuristic\"):\n",
    "                supervisionTrain(epoch, heuristicSupervisionRule)\n",
    "            elif(order1==\"random initializing\"):\n",
    "                print(\"do nothing\");\n",
    "\n",
    "        test()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr2)\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(epoch)\n",
    "            test()\n",
    "        losslistname.append(order+\" \"+order1);\n",
    "        losslist.append(losslisttemp);\n",
    "        losslisttemp=[];\n",
    "        savepath=\"save/pytorchNN=5all-beta\"+order+str(order1)\n",
    "        torch.save(model, savepath);\n",
    "        print(\"end\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colorlist=[\"#D2691E\",'#4169E1',\"#9ACD32\",\"#B22222\",\"#FF00FF\",\"#708090\"]\n",
    "for i in range(len(losslist)):\n",
    "    plt.plot(losslist[i], '.-', color=colorlist[i], alpha=0.8, label=losslistname[i])\n",
    "\n",
    "# 显示标签，如果不加这句，即使加了label='一些数字'的参数，最终还是不会显示标签\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('Trianing Times')\n",
    "plt.ylabel('Delay')\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('pytorchNN=3_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
