{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3540388371733616\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from random import randint\n",
    "import random\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "from pylab import *\n",
    "from scipy.stats import beta\n",
    "import math\n",
    "n = 3\n",
    "epochs = 3\n",
    "supervisionEpochs = 5\n",
    "lr = 0.0005\n",
    "log_interval = 20\n",
    "trainSize = 60000#100000\n",
    "percentage_train_test= 0.25\n",
    "penaltyLambda = 10\n",
    "doublePeakHighMean = 0.9\n",
    "doublePeakLowMean = 0.1\n",
    "doublePeakStd = 0.1\n",
    "uniformlow=0\n",
    "uniformhigh=1.0\n",
    "normalloc = 0.2\n",
    "normalscale = 0.1\n",
    "\n",
    "cauchyloc = 1/n\n",
    "cauchyscalen = 0.004\n",
    "\n",
    "exponentialhigh = 15 #Symbol(\"b\", real=True)\n",
    "exponentiallow  = 15 #Symbol(\"a\", real=True)\n",
    "\n",
    "\n",
    "beta_a = 0.1\n",
    "beta_b  = 0.1\n",
    "kumaraswamy_a = beta_a \n",
    "kumaraswamy_b = (1.0+(beta_a-1.0)*math.pow( (beta_a+beta_b-2.0)/(beta_a-1.0), beta_a) )/beta_a \n",
    "print(kumaraswamy_b)\n",
    "\n",
    "independentnormalloc1=[(float(ii)+1)/(2*n+1) for ii in range(n,0,-1)]\n",
    "independentnormalscale1=[0.05 for ii in range(n)]\n",
    "\n",
    "independentnormalloc2=[(float(ii)+1)/(2*n+1) for ii in range(1,n+1,1)]\n",
    "independentnormalscale2=[0.05 for ii in range(n)]\n",
    "stage=[\"beta\"]\n",
    "# \"twopeak\",\"normal\",\"uniform\",\"independent1\",\"independent2\",\"cauchy\",\"beta\",\"U-exponential\",\"arcsine\"\n",
    "order1name=[\"dp\",\"random initializing\",\"costsharing\"]\n",
    "#order1name=[\"random initializing1\",\"random initializing2\",\"random initializing3\"]\n",
    "# \"costsharing\",\"dp\",\"heuristic\",\"random initializing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio1 = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase1 = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "d3 = D.normal.Normal(normalloc, normalscale)\n",
    "distributionRatio3 = d3.cdf(1) - d3.cdf(0)\n",
    "distributionBase3 = d3.cdf(0)\n",
    "\n",
    "d4 = D.uniform.Uniform(uniformlow,uniformhigh)\n",
    "distributionRatio4 = d4.cdf(1) - d4.cdf(0)\n",
    "distributionBase4 = d4.cdf(0)\n",
    "\n",
    "d5 = [D.normal.Normal(independentnormalloc1[ii], independentnormalscale1[ii]) for ii in range(n)]\n",
    "d6 = [D.normal.Normal(independentnormalloc2[ii], independentnormalscale2[ii]) for ii in range(n)]\n",
    "\n",
    "d7 = D.cauchy.Cauchy(cauchyloc,cauchyscalen)\n",
    "\n",
    "d81 = D.exponential.Exponential(exponentiallow)\n",
    "d82 = D.exponential.Exponential(exponentialhigh)\n",
    "\n",
    "d9 = D.beta.Beta(beta_a,beta_b)\n",
    "#sample_d9=d9.rsample(torch.Size([100000]))\n",
    "\n",
    "#d9_sample=np.linspace(0.0001, 0.9999, 10000)\n",
    "#d9_pdf=torch.exp(d9.log_prob(torch.tensor(d9_sample,dtype=torch.float32)))\n",
    "#d9_delta=d9_sample[1]-d9_sample[0]\n",
    "#d9_sum_pdf=torch.sum(d9_pdf*d9_delta)\n",
    "\n",
    "d10 = D.beta.Beta(0.5,0.5)\n",
    "\n",
    "def cdf(x,y, i=None):\n",
    "    if(y==\"twopeak\"):\n",
    "        return (d1.cdf(x) + d2.cdf(x) - distributionBase1) / 2 / distributionRatio1\n",
    "    elif(y==\"normal\"):\n",
    "        return (d3.cdf(x)-distributionBase3)/distributionRatio3;\n",
    "    elif(y==\"uniform\"):\n",
    "        return (d4.cdf(x)-distributionBase4)/distributionRatio4;\n",
    "    elif(y==\"independent1\"):\n",
    "        return d5[i].cdf(x);\n",
    "    elif(y==\"independent2\"):\n",
    "        return d6[i].cdf(x);\n",
    "    elif(y==\"cauchy\"):\n",
    "        return d7.cdf(x);\n",
    "    elif(y==\"beta\"):\n",
    "#         sum_cdf=0.0;\n",
    "#         if(x<0.0001):\n",
    "#             x=0.00011;\n",
    "#         if(x>0.9999):\n",
    "#             x=0.99989;\n",
    "#         for i in range(len(d9_pdf)):\n",
    "#             if(d9_sample[i]<x):\n",
    "#                 sum_cdf+=d9_pdf[i]*d9_delta;\n",
    "#             else:\n",
    "#                 sum_cdf+=(d9_pdf[i]+d9_pdf[i-1])/ 2 *(x-d9_sample[i-1])\n",
    "#                 break;\n",
    "#         return sum_cdf/d9_sum_pdf\n",
    "#         cdf_v=torch.sum((sample_d9<(x)), dtype=torch.float32)/100000\n",
    "#         return cdf_v\n",
    "#    F(x|a,b)=1–(1–x^a)^b\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(x,kumaraswamy_a),kumaraswamy_b);\n",
    "        except:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(torch.tensor(x,dtype=torch.float32),kumaraswamy_a),kumaraswamy_b);\n",
    "    elif(y==\"arcsine\"):\n",
    "        #\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            res=2.0/math.pi * torch.asin(torch.sqrt(x))\n",
    "            #print(x)\n",
    "            return res# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "        except:\n",
    "            return 2.0/math.pi * torch.asin(torch.sqrt(torch.tensor(x,dtype=torch.float32)))# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "    elif(y==\"U-exponential\"):\n",
    "        return (d81.cdf(x) + (1.0 - d82.cdf(1.0-x)))  / 2 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        #Initializing hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "         # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"beta_a\",beta_a, \"beta_b\",beta_b)\n",
    "# print(\"kumaraswamy_a\",kumaraswamy_a, \"kumaraswamy_b\",kumaraswamy_b)\n",
    "# samples1 = np.random.uniform(\n",
    "#     uniformlow, uniformhigh, size=(trainSize, n)\n",
    "# )\n",
    "# for i in range(trainSize):\n",
    "#     for j in range(n):\n",
    "#         samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "#         while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "#             samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "\n",
    "# samplesJoint = samples1\n",
    "# tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "\n",
    "# dpPrecision=100\n",
    "# # howManyPpl left, money left, yes already\n",
    "# dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "# decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "# # ppl = 0 left\n",
    "# for yes in range(n + 1):\n",
    "#     for money in range(dpPrecision + 1):\n",
    "#         if money == 0:\n",
    "#             dp[0, 0, yes] = 0\n",
    "#         else:\n",
    "#             dp[0, money, yes] = yes#+ 1.0\n",
    "# order=\"beta\"\n",
    "# for ppl in range(1,  n + 1):\n",
    "#     for yes in range(n + 1):\n",
    "#         for money in range(dpPrecision + 1):\n",
    "#             minSoFar = 1000000\n",
    "#             for offerIndex in range(money + 1):\n",
    "#                 offer = float(offerIndex) / dpPrecision\n",
    "#                 if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "#                     res = (1 - cdf(offer,order)) * dp[\n",
    "#                     ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "#                     ] + cdf(offer,order) * (1.0 + dp[ppl - 1, money, yes])\n",
    "#                 else:\n",
    "#                     res = (1 - cdf(offer,order,n-ppl)) * dp[\n",
    "#                     ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "#                     ] + cdf(offer,order,n-ppl) * (1.0 + dp[ppl - 1, money, yes])\n",
    "#                 if minSoFar > res.item():\n",
    "#                     minSoFar = res.item()\n",
    "#                     decision[ppl, money, yes] = offerIndex\n",
    "#             dp[ppl, money, yes] = minSoFar\n",
    "\n",
    "# print(\"dp\",dp[n, dpPrecision, 0])\n",
    "\n",
    "# def plan_dp(temp):\n",
    "#     #print(temp)\n",
    "#     remain=dpPrecision\n",
    "#     yes=0;\n",
    "#     ans =0;\n",
    "#     o_list=[];\n",
    "#     remain_list=[];\n",
    "#     for ppl in range(n,0,-1):\n",
    "#         o=decision[ppl, remain, yes]\n",
    "#         #print(o,remain)\n",
    "#         o_list.append(o)\n",
    "#         remain_list.append(remain);\n",
    "#         if(o<temp[n-ppl]):\n",
    "#             remain-=int(o);\n",
    "#             yes+=1;\n",
    "#         elif (remain>0):\n",
    "#             ans+=1;\n",
    "#     if(remain<=0):\n",
    "#         return ans,o_list;\n",
    "#     else:\n",
    "#         return n,o_list;\n",
    "# ans_list=[];\n",
    "# for i in range(10000):\n",
    "#     temp=samplesJoint[i]*dpPrecision\n",
    "#     #print(temp)\n",
    "#     ans_list.append(plan_dp(temp)[0]);\n",
    "#     #print(\"\\n\",temp)\n",
    "#     #print(plan_dp(temp))\n",
    "# print(sum(ans_list)/len(ans_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    if torch.sum(bits).item() == 0:\n",
    "        return torch.ones(n)\n",
    "    bits = bits.type(torch.float32)\n",
    "    negBits = torch.ones(n) - bits\n",
    "    bits=torch.reshape(bits,(1,-1,n))\n",
    "    payments = model(bits)\n",
    "    payments=payments[0].view(n)\n",
    "    payments = payments - 1000 * negBits\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    payments = payments + negBits\n",
    "    return payments\n",
    "\n",
    "def tpToBits(tp, bits=torch.ones(n).type(torch.uint8)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = (tp >= payments).type(torch.uint8)\n",
    "    if torch.equal(newBits, bits):\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits(tp, newBits)#bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    return n - torch.sum(tpToBits(tp).type(torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    global  dp,decision\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money, yes]\n",
    "        offer = float(offerIndex) / dpPrecision\n",
    "        if tp[i] >= offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits[i] = 0\n",
    "            payments[i] = 1\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "#     print()\n",
    "#     print(tp)\n",
    "#     print(bits)\n",
    "#     print(payments)\n",
    "#     print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def heuristicSupervisionRule(tp):\n",
    "    global  dp_H,decision_H\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    tempold=-1;\n",
    "    for turns in range(n,0,-1):\n",
    "        money = dpPrecision\n",
    "        j=0\n",
    "        tempo=sum(bits)\n",
    "        #print(\"bits\",tempo)\n",
    "        for i in range(n):\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            \n",
    "            offerIndex = decision_H[tempo ,tempo - i, money]\n",
    "            offer = float(offerIndex) / dpPrecision\n",
    "            while(j<n):\n",
    "                if(bits[j]!=0):\n",
    "                    break;\n",
    "                j+=1;\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            if tp[j] >= offer:\n",
    "                #print(money,j,tp[j],offer)\n",
    "                money -= offerIndex\n",
    "                bits[j] = 1\n",
    "                payments[j] = offer\n",
    "            else:\n",
    "                bits[j] = 0;\n",
    "                payments[j] = 1.0;\n",
    "            j+=1;\n",
    "        #print(\"money\",money)\n",
    "        if(money==0 and tempold==tempo):\n",
    "            break;\n",
    "        tempold=tempo;\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    for k in range(n, -1, -1):\n",
    "        if k == 0:\n",
    "            break\n",
    "        bits = [1 if tp[ii] >= 1.0 / k else 0 for ii in range(n)]\n",
    "        if sum(bits) == k:\n",
    "            break\n",
    "    if k == 0:\n",
    "        payments = [1 for ii in range(n)]\n",
    "    else:\n",
    "        payments = [1.0 / k if bits[ii] == 1 else 1 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    return float(n - torch.sum(costSharingSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def dpDelay(tp):\n",
    "    return float(n - torch.sum(dpSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def heuristicDelay(tp):\n",
    "    return float(n - torch.sum(heuristicSupervisionRule(tp)[0]).item())\n",
    "\n",
    "\n",
    "# templire=0;\n",
    "# num=1\n",
    "# for i in range(num):\n",
    "#     temp=torch.tensor(samplesJoint[i])\n",
    "#     #print(temp)\n",
    "#     #print(temp);\n",
    "#     #print(heuristicSupervisionRule(temp))\n",
    "#     #print(dpSupervisionRule(temp))\n",
    "#     #print(dpDelay(temp))\n",
    "#     res=dpDelay(temp)\n",
    "#     templire+=res\n",
    "#     #print(\"delay\",res)\n",
    "#     #print()\n",
    "# print(templire/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordAndReport(name, source, loss):\n",
    "    source.append(loss)\n",
    "    realLength = len(source)\n",
    "    #print(f\"{name} ({realLength}): {loss}\")\n",
    "    print(name,realLength,\":\" ,loss)\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    \n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda /100000\n",
    "        \n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "losslist=[];\n",
    "losslistname=[];\n",
    "losslisttemp=[];\n",
    "def test_batch():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lenLoss= 0\n",
    "        nnLoss = 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "            for tp in tp_batch:\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        nnLoss/=lenLoss\n",
    "    return nnLoss\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                delay1 = tpToTotalDelay(tp1)\n",
    "                delay0 = tpToTotalDelay(tp0)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                    loss = loss + (1 - cdf(offer,order)) * delay1 + cdf(offer,order) * delay0\n",
    "                else:\n",
    "                    loss = loss + (1 - cdf(offer,order,i)) * delay1 + cdf(offer,order,i) * delay0\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "    \n",
    "allBits = [torch.tensor(bits, dtype=torch.int16) for bits in itertools.product([0, 1], repeat=n)]\n",
    "\n",
    "def test():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        costSharingLoss = 0\n",
    "        dpLoss = 0\n",
    "        nnLoss = 0\n",
    "        heuristicLoss=0\n",
    "        lenLoss= 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "                heuristicLoss+= heuristicDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        costSharingLoss /= lenLoss\n",
    "        dpLoss /= lenLoss\n",
    "        nnLoss /= lenLoss\n",
    "        heuristicLoss /= lenLoss\n",
    "        #print(lenLoss)\n",
    "        losslisttemp.append(nnLoss);\n",
    "        recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "        recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "        recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "        recordAndReport(\"heuristic\", runningLossHeuristic, heuristicLoss)\n",
    "        print(\"DP:\",dp[n, dpPrecision,0])\n",
    "        #for i in range(n, 0, -1):\n",
    "        #    print(\"Heuristic:\",i,5*(1-dp_H[i, i, dpPrecision]))\n",
    "        for i in range(n, 0, -1):\n",
    "            print(\n",
    "                    tpToPayments(\n",
    "                            torch.tensor([0 if ii >= i else 1 for ii in range(n)], dtype=torch.float32)\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "dpPrecision = 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def producedata(order):\n",
    "    global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "    dpPrecision = 100 \n",
    "    if(order==\"twopeak\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = np.random.normal(\n",
    "            loc=doublePeakLowMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=doublePeakLowMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samples2 = np.random.normal(\n",
    "            loc=doublePeakHighMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = np.random.normal(\n",
    "                        loc=doublePeakHighMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "    elif(order==\"normal\"):\n",
    "        print(\"loc\",normalloc, \"scale\",normalscale)\n",
    "        samples1 = np.random.normal(\n",
    "            loc=normalloc, scale=normalscale, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=normalloc, scale=normalscale\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"uniform\"):  \n",
    "        print(\"uniformlow\",uniformlow, \"uniformhigh\",uniformhigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        uniformlow, uniformhigh\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent1\"):\n",
    "        print(\"loc\",independentnormalloc1,\"scale\",independentnormalscale1)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent2\"):\n",
    "        print(\"loc\",independentnormalloc2, \"scale\",independentnormalscale2)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"cauchy\"):\n",
    "        print(\"cauchyloc\",cauchyloc, \"cauchyscale\",cauchyscalen)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"beta\"):\n",
    "        print(\"beta_a\",beta_a, \"beta_b\",beta_b)\n",
    "        print(\"kumaraswamy_a\",kumaraswamy_a, \"kumaraswamy_b\",kumaraswamy_b)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"arcsine\"):\n",
    "        print(\"betalow\",0.5, \"betahigh\",0.5)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"U-exponential\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = d81.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d81.rsample(torch.Size([1])).numpy()\n",
    "                    \n",
    "        samples2 = d82.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        \n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = d82.rsample(torch.Size([1])).numpy()\n",
    "        samples2 = 1.0 - samples2\n",
    "        samplesJoint = signals * samples1 - (signals - 1.0) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "        \n",
    "    plt.hist(samplesJoint,bins=500)\n",
    "    plt.show()\n",
    "    \n",
    "    tp_dataset = TensorDataset(tp_tensor[: int(trainSize * percentage_train_test)])\n",
    "    tp_dataset_testing = TensorDataset(tp_tensor[int(trainSize * (1.0-percentage_train_test)) :])\n",
    "    tp_dataloader = DataLoader(tp_dataset, batch_size=128, shuffle=True)\n",
    "    tp_dataloader_testing = DataLoader(tp_dataset_testing, batch_size=256, shuffle=False)\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "    decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for yes in range(n + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp[0, 0, yes] = 0\n",
    "            else:\n",
    "                dp[0, money, yes] = yes# + 1.0\n",
    "    for ppl in range(1,  n + 1):\n",
    "        for yes in range(n + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                minSoFar = 1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1 - cdf(offer,order)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order) * (1 + dp[ppl - 1, money, yes])\n",
    "                    else:\n",
    "                        res = (1 - cdf(offer,order,n-ppl)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order,n-ppl) * (1 + dp[ppl - 1, money, yes])\n",
    "                    if minSoFar > res.item():\n",
    "                        minSoFar = res.item()\n",
    "                        decision[ppl, money, yes] = offerIndex\n",
    "                dp[ppl, money, yes] = minSoFar\n",
    "    \n",
    "    print(\"dp\",dp[n, dpPrecision, 0])\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp_H = np.zeros([n + 1 , n + 1, dpPrecision + 1])\n",
    "    decision_H = np.zeros([n + 1 , n + 1, dpPrecision + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for i in range(1,n+1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp_H[i, 0, 0] = 1\n",
    "            else:\n",
    "                offer = money / dpPrecision\n",
    "                dp_H[i, 0, money] = 0#cdf(offer)# + 1.0\n",
    "    for i in range(1,n+1):\n",
    "        for ppl in range(1, i + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                maxSoFar = -1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1-cdf(offer,order)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    else:\n",
    "                        res = (1-cdf(offer,order,ppl-1)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    if maxSoFar < res.item():\n",
    "                        maxSoFar = res.item()\n",
    "                        decision_H[i, ppl, money] = offerIndex\n",
    "                dp_H[i, ppl, money] = maxSoFar\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_a 0.1 beta_b 0.1\n",
      "kumaraswamy_a 0.1 kumaraswamy_b 0.3540388371733616\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dp 2.2055420875549316\n",
      "Supervised Aim: beta dp\n",
      "Model(\n",
      "  (rnn): RNN(3, 100, num_layers=3, batch_first=True)\n",
      "  (fc): Linear(in_features=100, out_features=3, bias=True)\n",
      ")\n",
      "Train Epoch: 1 [0/15000 (0%)]\tLoss: 0.014855\n",
      "Train Epoch: 1 [2560/15000 (17%)]\tLoss: 0.008458\n",
      "Train Epoch: 1 [5120/15000 (34%)]\tLoss: 0.005334\n",
      "Train Epoch: 1 [7680/15000 (51%)]\tLoss: 0.001347\n",
      "Train Epoch: 1 [10240/15000 (68%)]\tLoss: 0.000217\n",
      "Train Epoch: 1 [12800/15000 (85%)]\tLoss: 0.000062\n",
      "Train Epoch: 2 [0/15000 (0%)]\tLoss: 0.000037\n",
      "Train Epoch: 2 [2560/15000 (17%)]\tLoss: 0.000034\n",
      "Train Epoch: 2 [5120/15000 (34%)]\tLoss: 0.000019\n",
      "Train Epoch: 2 [7680/15000 (51%)]\tLoss: 0.000016\n",
      "Train Epoch: 2 [10240/15000 (68%)]\tLoss: 0.000017\n",
      "Train Epoch: 2 [12800/15000 (85%)]\tLoss: 0.000011\n",
      "Train Epoch: 3 [0/15000 (0%)]\tLoss: 0.000008\n",
      "Train Epoch: 3 [2560/15000 (17%)]\tLoss: 0.000007\n",
      "Train Epoch: 3 [5120/15000 (34%)]\tLoss: 0.000009\n",
      "Train Epoch: 3 [7680/15000 (51%)]\tLoss: 0.000007\n",
      "Train Epoch: 3 [10240/15000 (68%)]\tLoss: 0.000005\n",
      "Train Epoch: 3 [12800/15000 (85%)]\tLoss: 0.000006\n",
      "Train Epoch: 4 [0/15000 (0%)]\tLoss: 0.000005\n",
      "Train Epoch: 4 [2560/15000 (17%)]\tLoss: 0.000006\n",
      "Train Epoch: 4 [5120/15000 (34%)]\tLoss: 0.000004\n",
      "Train Epoch: 4 [7680/15000 (51%)]\tLoss: 0.000003\n",
      "Train Epoch: 4 [10240/15000 (68%)]\tLoss: 0.000002\n",
      "Train Epoch: 4 [12800/15000 (85%)]\tLoss: 0.000003\n",
      "Train Epoch: 5 [0/15000 (0%)]\tLoss: 0.000003\n",
      "Train Epoch: 5 [2560/15000 (17%)]\tLoss: 0.000003\n",
      "Train Epoch: 5 [5120/15000 (34%)]\tLoss: 0.000002\n",
      "Train Epoch: 5 [7680/15000 (51%)]\tLoss: 0.000002\n",
      "Train Epoch: 5 [10240/15000 (68%)]\tLoss: 0.000002\n",
      "Train Epoch: 5 [12800/15000 (85%)]\tLoss: 0.000002\n",
      "NN 1 : tensor(1.7545)\n",
      "CS 1 : 1.7832666666666668\n",
      "DP 1 : 1.7485333333333333\n",
      "heuristic 1 : 1.7528666666666666\n",
      "DP: 2.2055420875549316\n",
      "tensor([0.4984, 0.4977, 0.0039])\n",
      "tensor([0.4996, 0.5004, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 1 [0/15000 (0%)]\tLoss: 1.846344 testing loss: tensor(1.7595)\n",
      "Train Epoch: 1 [2560/15000 (17%)]\tLoss: 2.042503 testing loss: tensor(1.7341)\n",
      "Train Epoch: 1 [5120/15000 (34%)]\tLoss: 1.987232 testing loss: tensor(1.7406)\n",
      "Train Epoch: 1 [7680/15000 (51%)]\tLoss: 1.970999 testing loss: tensor(1.7145)\n",
      "Train Epoch: 1 [10240/15000 (68%)]\tLoss: 1.932295 testing loss: tensor(1.7140)\n",
      "Train Epoch: 1 [12800/15000 (85%)]\tLoss: 1.949321 testing loss: tensor(1.7091)\n",
      "penalty: 0.0011032521724700928\n",
      "NN 2 : tensor(1.7105)\n",
      "CS 2 : 1.7832666666666668\n",
      "DP 2 : 1.7485333333333333\n",
      "heuristic 2 : 1.7528666666666666\n",
      "DP: 2.2055420875549316\n",
      "tensor([3.5286e-01, 6.4714e-01, 9.9068e-08])\n",
      "tensor([0.3527, 0.6473, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 2 [0/15000 (0%)]\tLoss: 1.875384 testing loss: tensor(1.7103)\n",
      "Train Epoch: 2 [2560/15000 (17%)]\tLoss: 1.761493 testing loss: tensor(1.7115)\n",
      "Train Epoch: 2 [5120/15000 (34%)]\tLoss: 1.877298 testing loss: tensor(1.7081)\n",
      "Train Epoch: 2 [7680/15000 (51%)]\tLoss: 1.889271 testing loss: tensor(1.7051)\n",
      "Train Epoch: 2 [10240/15000 (68%)]\tLoss: 1.918013 testing loss: tensor(1.7093)\n",
      "Train Epoch: 2 [12800/15000 (85%)]\tLoss: 1.908189 testing loss: tensor(1.7187)\n",
      "penalty: 0.00522187352180481\n",
      "NN 3 : tensor(1.7046)\n",
      "CS 3 : 1.7832666666666668\n",
      "DP 3 : 1.7485333333333333\n",
      "heuristic 3 : 1.7528666666666666\n",
      "DP: 2.2055420875549316\n",
      "tensor([4.8196e-01, 5.1804e-01, 7.3993e-08])\n",
      "tensor([0.4835, 0.5165, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 3 [0/15000 (0%)]\tLoss: 1.914662 testing loss: tensor(1.7088)\n",
      "Train Epoch: 3 [2560/15000 (17%)]\tLoss: 1.876741 testing loss: tensor(1.7193)\n",
      "Train Epoch: 3 [5120/15000 (34%)]\tLoss: 1.919127 testing loss: tensor(1.7078)\n",
      "Train Epoch: 3 [7680/15000 (51%)]\tLoss: 1.955397 testing loss: tensor(1.7066)\n",
      "Train Epoch: 3 [10240/15000 (68%)]\tLoss: 1.930845 testing loss: tensor(1.7069)\n",
      "Train Epoch: 3 [12800/15000 (85%)]\tLoss: 1.789680 testing loss: tensor(1.7071)\n",
      "penalty: 0.027040183544158936\n",
      "NN 4 : tensor(1.7074)\n",
      "CS 4 : 1.7832666666666668\n",
      "DP 4 : 1.7485333333333333\n",
      "heuristic 4 : 1.7528666666666666\n",
      "DP: 2.2055420875549316\n",
      "tensor([4.5788e-01, 5.4212e-01, 9.6210e-08])\n",
      "tensor([0.4563, 0.5437, 1.0000])\n",
      "tensor([1., 1., 1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ComputerSoftwares\\Anaconda\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Model. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n",
      "\n",
      "Supervised Aim: beta random initializing\n",
      "Model(\n",
      "  (rnn): RNN(3, 100, num_layers=3, batch_first=True)\n",
      "  (fc): Linear(in_features=100, out_features=3, bias=True)\n",
      ")\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "NN 1 : tensor(1.7839)\n",
      "CS 1 : 1.7832666666666668\n",
      "DP 1 : 1.7485333333333333\n",
      "heuristic 1 : 1.7528666666666666\n",
      "DP: 2.2055420875549316\n",
      "tensor([0.3676, 0.2871, 0.3453])\n",
      "tensor([0.5590, 0.4410, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 1 [0/15000 (0%)]\tLoss: 1.919767 testing loss: tensor(1.7841)\n",
      "Train Epoch: 1 [2560/15000 (17%)]\tLoss: 1.904220 testing loss: tensor(1.7849)\n",
      "Train Epoch: 1 [5120/15000 (34%)]\tLoss: 1.945303 testing loss: tensor(1.7832)\n",
      "Train Epoch: 1 [7680/15000 (51%)]\tLoss: 1.863404 testing loss: tensor(1.7815)\n",
      "Train Epoch: 1 [10240/15000 (68%)]\tLoss: 1.956165 testing loss: tensor(1.7825)\n",
      "Train Epoch: 1 [12800/15000 (85%)]\tLoss: 1.883449 testing loss: tensor(1.7835)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.7842)\n",
      "CS 2 : 1.7832666666666668\n",
      "DP 2 : 1.7485333333333333\n",
      "heuristic 2 : 1.7528666666666666\n",
      "DP: 2.2055420875549316\n",
      "tensor([0.3309, 0.3144, 0.3548])\n",
      "tensor([0.5079, 0.4921, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 2 [0/15000 (0%)]\tLoss: 1.966816 testing loss: tensor(1.7843)\n",
      "Train Epoch: 2 [2560/15000 (17%)]\tLoss: 1.976017 testing loss: tensor(1.7831)\n",
      "Train Epoch: 2 [5120/15000 (34%)]\tLoss: 1.758256 testing loss: tensor(1.7845)\n",
      "Train Epoch: 2 [7680/15000 (51%)]\tLoss: 1.889271 testing loss: tensor(1.7845)\n",
      "Train Epoch: 2 [10240/15000 (68%)]\tLoss: 1.828169 testing loss: tensor(1.7833)\n",
      "Train Epoch: 2 [12800/15000 (85%)]\tLoss: 1.903864 testing loss: tensor(1.7828)\n",
      "penalty: 0.0\n",
      "NN 3 : tensor(1.7831)\n",
      "CS 3 : 1.7832666666666668\n",
      "DP 3 : 1.7485333333333333\n",
      "heuristic 3 : 1.7528666666666666\n",
      "DP: 2.2055420875549316\n",
      "tensor([0.3471, 0.3105, 0.3424])\n",
      "tensor([0.5202, 0.4798, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 3 [0/15000 (0%)]\tLoss: 1.798617 testing loss: tensor(1.7830)\n",
      "Train Epoch: 3 [2560/15000 (17%)]\tLoss: 1.892143 testing loss: tensor(1.7828)\n",
      "Train Epoch: 3 [5120/15000 (34%)]\tLoss: 1.869641 testing loss: tensor(1.7843)\n",
      "Train Epoch: 3 [7680/15000 (51%)]\tLoss: 1.931095 testing loss: tensor(1.7841)\n",
      "Train Epoch: 3 [10240/15000 (68%)]\tLoss: 1.840378 testing loss: tensor(1.7835)\n",
      "Train Epoch: 3 [12800/15000 (85%)]\tLoss: 1.978570 testing loss: tensor(1.7834)\n",
      "penalty: 0.0\n",
      "NN 4 : tensor(1.7828)\n",
      "CS 4 : 1.7832666666666668\n",
      "DP 4 : 1.7485333333333333\n",
      "heuristic 4 : 1.7528666666666666\n",
      "DP: 2.2055420875549316\n",
      "tensor([0.3444, 0.3251, 0.3305])\n",
      "tensor([0.5066, 0.4934, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: beta costsharing\n",
      "Model(\n",
      "  (rnn): RNN(3, 100, num_layers=3, batch_first=True)\n",
      "  (fc): Linear(in_features=100, out_features=3, bias=True)\n",
      ")\n",
      "Train Epoch: 1 [0/15000 (0%)]\tLoss: 0.000115\n",
      "Train Epoch: 1 [2560/15000 (17%)]\tLoss: 0.000009\n",
      "Train Epoch: 1 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "NN 1 : tensor(1.7833)\n",
      "CS 1 : 1.7832666666666668\n",
      "DP 1 : 1.7485333333333333\n",
      "heuristic 1 : 1.7528666666666666\n",
      "DP: 2.2055420875549316\n",
      "tensor([0.3333, 0.3333, 0.3333])\n",
      "tensor([0.5000, 0.5000, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 1 [0/15000 (0%)]\tLoss: 1.925129 testing loss: tensor(1.7835)\n",
      "Train Epoch: 1 [2560/15000 (17%)]\tLoss: 1.892345 testing loss: tensor(1.7887)\n",
      "Train Epoch: 1 [5120/15000 (34%)]\tLoss: 2.005526 testing loss: tensor(1.7842)\n",
      "Train Epoch: 1 [7680/15000 (51%)]\tLoss: 1.884455 testing loss: tensor(1.7850)\n",
      "Train Epoch: 1 [10240/15000 (68%)]\tLoss: 1.882238 testing loss: tensor(1.7822)\n",
      "Train Epoch: 1 [12800/15000 (85%)]\tLoss: 2.082712 testing loss: tensor(1.7843)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.7836)\n",
      "CS 2 : 1.7832666666666668\n",
      "DP 2 : 1.7485333333333333\n",
      "heuristic 2 : 1.7528666666666666\n",
      "DP: 2.2055420875549316\n",
      "tensor([0.3097, 0.3394, 0.3509])\n",
      "tensor([0.4832, 0.5168, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 2 [0/15000 (0%)]\tLoss: 1.832082 testing loss: tensor(1.7840)\n",
      "Train Epoch: 2 [2560/15000 (17%)]\tLoss: 2.000626 testing loss: tensor(1.7809)\n",
      "Train Epoch: 2 [5120/15000 (34%)]\tLoss: 1.897084 testing loss: tensor(1.7830)\n",
      "Train Epoch: 2 [7680/15000 (51%)]\tLoss: 1.983237 testing loss: tensor(1.7826)\n",
      "Train Epoch: 2 [10240/15000 (68%)]\tLoss: 1.937912 testing loss: tensor(1.7832)\n",
      "Train Epoch: 2 [12800/15000 (85%)]\tLoss: 2.028256 testing loss: tensor(1.7827)\n",
      "penalty: 0.0\n",
      "NN 3 : tensor(1.7828)\n",
      "CS 3 : 1.7832666666666668\n",
      "DP 3 : 1.7485333333333333\n",
      "heuristic 3 : 1.7528666666666666\n",
      "DP: 2.2055420875549316\n",
      "tensor([0.3290, 0.3447, 0.3263])\n",
      "tensor([0.4855, 0.5145, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 3 [0/15000 (0%)]\tLoss: 1.931268 testing loss: tensor(1.7829)\n",
      "Train Epoch: 3 [2560/15000 (17%)]\tLoss: 1.899366 testing loss: tensor(1.7838)\n",
      "Train Epoch: 3 [5120/15000 (34%)]\tLoss: 1.875469 testing loss: tensor(1.7833)\n",
      "Train Epoch: 3 [7680/15000 (51%)]\tLoss: 1.942699 testing loss: tensor(1.7835)\n",
      "Train Epoch: 3 [10240/15000 (68%)]\tLoss: 1.844046 testing loss: tensor(1.7835)\n",
      "Train Epoch: 3 [12800/15000 (85%)]\tLoss: 1.905882 testing loss: tensor(1.7829)\n",
      "penalty: 0.0\n",
      "NN 4 : tensor(1.7827)\n",
      "CS 4 : 1.7832666666666668\n",
      "DP 4 : 1.7485333333333333\n",
      "heuristic 4 : 1.7528666666666666\n",
      "DP: 2.2055420875549316\n",
      "tensor([0.3399, 0.3316, 0.3285])\n",
      "tensor([0.5043, 0.4957, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "        \n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.1)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "        \n",
    "def init_weights_xavier_uniform(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.1)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "        \n",
    "def init_weights_xavier_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.1)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "        \n",
    "        \n",
    "def init_weights_kaiming_uniform(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.kaiming_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.1)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "        \n",
    "        \n",
    "def init_weights_kaiming_normal_(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.1)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "\n",
    "\n",
    "        \n",
    "for order in stage:\n",
    "    producedata(order)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "    #running\n",
    "    for order1 in order1name:\n",
    "        print(\"Supervised Aim:\",order,order1)\n",
    "        \n",
    "        # Instantiate the model with hyperparameters\n",
    "        model = Model(input_size=n, output_size=n, hidden_dim=100, n_layers=3)\n",
    "        print(model)\n",
    "        # optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        runningLossNN = []\n",
    "        runningLossCS = []\n",
    "        runningLossDP = []\n",
    "        runningLossHeuristic = []\n",
    "        #model=torch.load(\"save/pytorchNN=5dp1\");\n",
    "        #model.eval()\n",
    "        ##order1name=[\"costsharing\",\"dp\",\"heuristic\",\"random initializing\"]\n",
    "        for epoch in range(1, supervisionEpochs + 1):\n",
    "#             print(\"distributionRatio\",distributionRatio)\n",
    "            if(order1==\"costsharing\"):\n",
    "                supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "            elif(order1==\"dp\"):\n",
    "                supervisionTrain(epoch, dpSupervisionRule)\n",
    "            elif(order1==\"heuristic\"):\n",
    "                supervisionTrain(epoch, heuristicSupervisionRule)\n",
    "            elif(order1==\"random initializing\"):\n",
    "                print(\"do nothing\");\n",
    "\n",
    "        test()\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(epoch)\n",
    "            test()\n",
    "        losslistname.append(order+\" \"+order1);\n",
    "        losslist.append(losslisttemp);\n",
    "        losslisttemp=[];\n",
    "        savepath=\"save/pytorchNN=5all-beta\"+order+str(order1)\n",
    "        torch.save(model, savepath);\n",
    "        print(\"end\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nOzdeZxWdfn4/9f7nHudfWOGZQbZlGUGAQURUQHBMovKXdMM/Sho7pVZWWZWv+oXGZqaWiq5ZKBllpImCi6kIKjoACLKNsDArNyz3Os55/39454ZZvCeYZgVZq7n4zGPYe6zXffhvs913utRWmuEEEKIgxm9HYAQQogjkyQIIYQQCUmCEEIIkZAkCCGEEAlJghBCCJGQq7cD6Eo5OTl62LBhvR2GEEIcNdatW1ehtR6QaFmfShDDhg1j7dq1vR2GEEIcNZRSO1pbJlVMQgghEpIEIYQQIiFJEEIIIRKSBCGEECIhSRBCCCES6lO9mI4WJYFVrN/3OLXRPaR6BjMh73IK0qf3dlhCCNGClCB6WElgFatKfkMwVoHXTCMYq2BVyW8oCazq7dCEEKIFSRA9bP2+xzGUG42No2O4TT+GcrN+3+O9HZoQQrQgVUw9rDa6G8uOYOkgoPC7MvEYadRF9/R2aEII0YKUIHpQ2NqP7USJOXX4XJm4DD8hq4ra2G6S3Lm9HZ4QQrQgCaKHBMI7Wbn9DjxmGl4zDVN5SXbl4jFSsZwIMTtEWf1HvR2mEEI0kQTRA0rr3ueNnXcB8IWRCzn9mDtIcucQdWpI9x3D9PzbSPcVsKrkNxSXPY2jY70csRBCSBtEt9Ja81n1S3xU9lcyfMM4ech38LszyfAN+1y31lHZX6K47Cm2VL1IeXATUwZ/mxTPwF6KXAghpATRbRxt8cG+x/io7CkGp0zmtKE/xu/ObHV9l+Fl4sArOWnIjdRH97Fi+4/ZGXirByMWQoiWpATRDaJ2HWt230t5cCOjs7/K2JzzUepALl5dHGTJ8lr2VloMzHZx0ZxUphYlATAk9SQyfSNZu+cB1pU+SFn9h0zIuwK36e9UTDI4TwhxuJTWurdj6DKTJ0/Wvf08iLpoKW+X/I5grJxJg65iaPppLZavLg5y79Jq3KbC61FEopqYrbnxwsymJAGgtcPmyn/yccU/SXLnMHnwdWT5R3YopsbBeYZy4zJ8WE4YR8eYXnCbJIku0FbC747txKHJuW0/pdQ6rfXkhMv6e4Loyi/3yBE7eGfXIpRSTB1yMzlJYwCwHU1VjU15tc3CJ6vYX2ujFLhMhc+rAE12uou7b8773HEqg5/w7p77CVvV5CVPoDL0SZulAK01UbuW+lg5wVgZwVg575X+mYhdA4BLefG60nG0TZI7h68c99Bhna/D0dFSS2e+3K+sf40NFU9gePbiRAdSmPNNzpxwRmffSpuxtifhd9V24tA6e277W3KRBNGKzny5n1i5nGOO/Qf+5DJC9blUlRcyZsyHpHjySI1eR1V1NuXVFuXVNhUBG8eJb7t1dxTTBLepsGzQAFpjmorvfCOLwhFecjNNlFJNx4vadbyx/S521LyBy/CR7MrF0mFsJ8KorLPxu7MaEkL8x3LCLeKtDn2GqXwYhgvLCQEaU/kwDRffKHqpxbG6SklgFa999mtqg4po1I3bbZGW7HDGyB+0mSRantt9hOrz2PHJuVx82mxOHOtHa3A0OI5u+rd2wNGaNz9eyc7o79GOC7QXZURQhsXYtO92S5KoDTp8d9E+KgJ2PJaG/2Pb1ng9isljfa1uu3ZTmEg0/v9uGOBxK9CQm2Vy73fzuuX/5GjUkZuM7yzaR1XAwjQNYpZGKYhamswUkx9dkY3bpZr9gNulMI34+U703d6x5Vy+OXPOIZNET9+cdBVJEK34zqJ9OJ7VDB75D3xJ+wgH89i66RzCNVO4aE4alq2xHeK/7fgX33bgnU9f59gJD6IdF7btxeuvxO2pp2zPBPZ+8gMcOwmvR5GXaTIg00VupklOpklupos/LK2ips7G5zXRWhOOamrrHTSQlxVvEspONykc4aFwhJfRx3jweQxe+GQB5XXbiVh1QPxDb5gObsNNlv84kt0DSPLkkuzOIcmdS7I7lyT3AJI9Obz06c0EYxW4TT9aO0TsGkKxagxlMDT9NI7N+jJD0qZiqK5pkgrGKnj2o/nUR8tRht30utbgc6dQlPdVfK5McDKIRjIJB9Opq0ujOpDC65veZPSEB7FtF7blxXRFMAyLD9fMx+uc3LAnB8MMY7qCGGYI0xXEdIXIG/kQHk8tWruwLC+O7cU0w9ixLL4w4kEKct1kphkJL76Humt0HM3eKpvPdkXZujvGZ7uilFXb8YRvgMdtYBqN71MTs+GLJye3eo5efqcetwlKKWwHYpbG0RrHgaKRXo4Z5GbYQDfHDIr/ZKaaLbbvjYtRT99ZH6gadeFSfizdetWo42hKKyw+2Rnl7r9WkTNwLaMKnyc5ZR/1dXl8Uvw1SnedyIghnoTHUgo8LkWdfpuiKQ+j7fh32+WK32Rs23ANF546h/Rkk/QUg/QUgxS/gdGQWF5Z/xqban6Hdlxo5/BvTnqz1CIJohXX/+FfjJ70ENGo2eJi9N7bV5OspgEal2nhclu4TQvTbeFyxUjJ/xU+/34cx4PHU4fpDmFbPuoCBVww/k/kZrlI8atWL0StlVpG5nvYuC3Khq0RPt4Rjd9hGjAq30PyyMuorUvBNC1crjCOY+I4BhlpMb416cU27zgTt0FEGZX5ZfZHtlIb3YPflcWorLM4Jn1WhxrEg7FK9tSuYXfNaqrCn1JWtwXb8mJbyWitUMoGZeFyRYjWTUKrKlBWi30oZWB69mAYNo7jBW2AcjCMGI7tIsNbiFZBNGFAg4Lm77re/iy+TcOrtuUlEklFGTE2v/0YAEk+xZABLvJz3eTnucgf4GZ3WYz7/97y/yRqac6dlYLfa7J1dzwphCLx70pqksGIIW5GDnHz3Ot1eFLXMPTY5/D69xEJ5VGy5RyM6NSEVYaN4ne5Nj5vPKtorakLOng9BrMmJ7GjNMbucovGr2d6isExDQmj2vofVeoetD78i1FHE0t331lr7VAfK6cuWkpdtJTaaCkby5cSitVi27rhPBi4XQqfO5ljs75COJRC1f4k9lYkUVLqo74+BTuWSoRNjDvhUcCNtr0YZgSUxa6Pr+Hi0+cQtSLErDBRO0LEimDZYWJ2mJgTYbe1CK+nFke7GuICZcSIRlKo2HE5YKC1AgzQBsk+kySfiZH1EC53LVp7QCu0NkHFsKPZzMj/I6apMA0wDYXLjFcvmyaYpqL40zAvrFvBsNHxUnO4Pq9HSy2SIFqx6NUrUK4q3N4ACgeUBmWDdjE4fWR8wFqC625Z3Ra0c+BCZEUziUWT8XjruG76skMetz13C5at2bo7RvFnETZui2DkfQ+Pvwpt+1AqXi1hmGGwsrh59mOHPGZjUb0uuoeUZkV1rR321a9nS+WLVIQ+xmX4GZ5xBiMzv4jfndXmBzAUq2JXzWq2Va2mIriFmKWxw/nUVk0i6Po3Hm8dtnWgmsV0hYmEshgQW0RmmiIzPUJKSgC/P4DHG8Axqnlj673YjolhOIADGGgnXlqanP8NPGYybiMZt5mE20zG0/hvI4XH3v4RygygHR8uVx2muxalYjh2KtMGLiRQPYrdZRa7yix2l1tEY/HPfsm++MBEv1dhKEUkpglHHVymYmiem0E5LkbmuxkxxMOIIe4WVYCNd47xi4IHlIVSFmPTvtfmF7U9F9xoTLNzb5Tt+2rZWVZJaXUlgWA1eSMfxuOpjZ8bFGgTsLFi6Ryb/CNSfOmk+zNI86eQkmSSkmSQ5FW8+tGKNu9yHUc3vHdNKKIJRxzC0fjfT654hWPG/RHHceM0XHQNI8b2Dddy7rTZ8QueQdNvs+EC+EHJG5TpRU3HNMwQhhlhcNJXGDloACFrL/WxUupjZTj6wA2D20hmX90GQiEvWntQSgM2YOP2RKmvnghGLYYZbmrL83oUPreiJrqDqBX/Hsd78jugbDwuF1lJQ9v8nhz83Y7TKMMhyzcK24m3Kdp2w28nXrOg3dvQ2gD9+e3qq8djxVKxrBTsWApWLPXAbyuVoL2V44r+jmN5sRpuVE3T4uP3F3DisBlNJZbG0ktaikF6isn/PlnJx50otTSSBNGKxe99harqJLz+yvjnQSu0Br/fYmr+AkzlwTDcmMod/7dyYxpuVnx2D9V1+9GOF3Dh2C6UESY3fQDfmPTnbnlv3773X4yZ9BC2ZRKzvJhmvLSzfs18vjplNmOHeRiZ74nXZXdQdWgrn1YtY3ftGgCiwUHsC36AY/vi71VFMMwIntgZuNwBouozLFsTqhtCTeUkaion4jFyGZTjYnvVqng1nHajnXi8KIvyrdfy8299pdUY/vr+VZQFytGOD6Ua7+Dad24/X8wPY7rrSPflkZJkku0fzdicc8lJGofWUL7fZleZxc8fqUApjWXF2zQ8boXHBbYDj985mCRf4uFCddFSlm25ntpIGZYdb4dorPrzmF4GJk9KkMjivwORXWzc9y+CYRexmAuPO4LPZzMyawZ+dwZha3/Tj62jTcd0HCiv34LWRrytRWmUcohXOzqE64Y1i9DAisXvqq1YKslZazDNCI7tjd8FKzBUFMtKombvedhODJSDUnbDj4Uy4v9Oy30VlyuE1i4aqziVsrEsD8Hqyc3icBou5hqUQ3L6hxhmFI2BaogR5aAdk0jwGGLhHGKRXKxoLjo2EB3LBTsPl5GCe9CteLxVOI4Pp2GXhitMLJzFEO7huKEeRuUrkpLriVg1RO0awnaA17f/DNt2E4rYONrBUAY+r8Jlxpgy+HpMw4tpeHApHy7Di2l447+Vj2Uf/5SK2ioc29/w+dMoI8KAtGzOKfodWjto4u9Ta43GQWuHxe/8AGXsB+2FhveojDA4fiYO+SoRK0DEriHmxOO0dRhNfP+ByA4MZdM4LC3+f6OxLQ92/QlEom5ilgdte3EcD47twXG8ZAxchssdPFDadpJRRhjdzpvGRm0liH49DiInZQgG5eyvzSFmgdsFGakxslIGUJR7SavbzRzh8Npnv6amHqJRE48nTFqyw2kjrui2WH3OVLZ+BAXHPkeSfx+hYB6fbPg69dWTefXdev67uh6XGa+OGjfcw9jhXoYMcDXVkban1JLpH8G4rG+TYp/Dp1UvURFdjMsTw7ZiWLEoLncI0xXGcT9PTWAqHuur5PimkD94CAPHuxiU4yI1yWg43hyeWEmCO+S272xOG3FFw7kNE4168Xgi7T63Z044A9bTosQzOuNmzhh/GtsDK9hS+QJvlfyKbP9xjMk5l9zMQvKyXIwY4m5R3QMQjjhkpZufSw510b3srlnN7trVBCI7qY3uxmX4SfZkoJSB1g6Otok5IXKSxhB1gsTsOuqie4k5QaJ2PbaOEAjvQCuLpKQD+9faYXvgVQannoTPlUGmbyReVzo+V0aLn0dX3Y4y96OdA6UzZQbBTuW8iT+kJhSgJryf+nANwViAcKyGiB2gTu2P3+G6m3di0Hh8tWSMfAlDGRjKxDBcmIYL0zBxGS5chps99bU4tot4qa4xXgOvN0xRYWr8oqZNtFbNfgy2Bt7DcXxA/DXHcaMdE2VEGZf0RyyviWXFq/RiliZm0fBbs+ajrzNh6sNAGBwfLk8E07DZvuFcfnhDerP34CPJnd30V5Z/FMFYBRlJB6pKY3aIJHcOYwec1+Zn6PQRCxo+f9GGz1+UtGQ4fcTVLY5xsHGZV8VvTrSDtuN38+BhbNrNnDbi859524kRsWuIWAGe/mA+UcuLoXRDgtZobeNyRZk8tgDbiRCxwoRj9YRjlUSteJVYvVMWL7UQQjsG0VByvJTm2dvmezwc/boE0ZnxAa1V2XSXttouJhzn49OSGJu2R9i4LUppRbyonppkMOYYD4ap+e87Qbzu+LbhqEMkCl+bkUJ2msneKpt9lRb7qizqQgc+D2NOmYdC4/bUYxgOjuNuqDLS3HDqf9sVc0ca3rrr3NpOjB2BlXxS+W9CVhVZ/uMYm3MOW7eO4MnXX221uqcuuo89tavZVbOaQGQHAFm+UQxJm8rG8mcIW4EW7TaNF6PWuhA7Osbfir+Ox0hGN9xtG5iAScyp5eKif7f5PjraINpYpaq1N16ligIjgrayuGnWoy0Gcx6soyW7pmM2T2btvMtt7ERScJjtO50d99PRz19H2wM6d24r46UWNFqbXV6C6NcJAnr+Qt8Z7b3gVtfafLw9yqbtETZti7JxWwTL1njdCg1YdrzXh8tUFOS5SU0yGJhtkpflIjfLRV6WycBsF399/2qMxgtKQ7VARz6AR5p4onidTyr/RciqwmOkUVb3GXX1vqZSS2pyjNED5hC2q9kf3gZApm8kQ1KnMiRtCknuAUDHL0YvfLKgqWdZo0MlluY6cjHqTE+bxm7LNfVGi5Ldobotd+aYnRnPcDR9r3vj3DbXKwlCKfUo8BWgTGtdlGD5rcClDX+6gLHAAK11lVLqFuAq4sMEPgKu0FqHD97HwY6EkdRHGsfRXPzjPZiGJhKL15G7zXhPiqgFi+8Y1Gode1d9AI9UthNjZ+AN3tz5SywnhMvw4zb9xOwQlhPCUC6OyZhFfupUBqeeRLJnQML9dORi1Fuj2zvT66Wn76yh/wxa641z26i3EsTpQB3weKIEcdC6c4FbtNZnKKWGAG8B47TWIaXUUmCZ1nrxoY4pCSKxg7tUwoE69raK6nD0Dv45HE8XfwW0QcQOoLEwlReXkQQ4fGP8oXulddTRdJcr+q5eaaTWWr+hlBrWztUvAZ5u9rcL8CulYkASIM/j7ISL5qRy79JqiDgtiuoXzUk95LZnTjiDM+lbCeFgqZ4hBGMVpLvycXAwlNlU3dOdCtKnS0IQR7Ren+5bKZUEnAX8HUBrvRtYCOwESoGA1vrQLaKiVVOLkrjxwkyy0k1qg/GSg8z5c8CEvMtxdIyYE0ZhELNDODrGhLzLezs0IXrVkdDNdS6wSmtdBaCUygS+BgwH9gPPKKUu01o/mWhjpdR8YD7A0KFtD4Lpz6YWJUlCaEVB+nSmc5tU9whxkCMhQVxMy+qlOcA2rXU5gFLqH8ApQMIEobV+GHgY4m0Q3Ruq6KukukeIz+vVKialVDowA3i+2cs7gZOVUkkqPp/BbGBTb8QnhBD9WbeVIJRSTwMzgRyl1C7gp4AbQGv9YMNq5wD/1VrXN26ntV6tlHoWeA+wgPdpKCEIIYToOf1+oJwQQvRnbXVz7fVeTEIIIY5MkiCEEEIkJAlCCCFEQpIghBBCJCQJQgghREKSIIQQQiQkCUIIIURCkiCEEEIkJAlCCCFEQpIghBBCJCQJQgghREKSIIQQQiQkCUIIIURCkiCEEEIkJAlCCCFEQpIghBBCJCQJQgghREKSIIQQQiQkCUIIIURCkiCEEEIkJAlCCCFEQpIghBBCJCQJQgghREKSIIQQQiQkCUIIIURCkiCEEEIkJAlCCCFEQpIghBBCJCQJQgghREKSIIQQQiQkCUIIIURCkiCEEEIkJAlCCCFEQpIghBBCJCQJQgghREKSIIQQQiQkCUIIIURCkiCEEEIk1G0JQin1qFKqTClV3MryW5VSHzT8FCulbKVUVsOyDKXUs0qpj5VSm5RS07orTiGEEIl1ZwliMXBWawu11r/VWk/UWk8Efgi8rrWualh8D/CS1noMMAHY1I1xCiGESMDVXTvWWr+hlBrWztUvAZ4GUEqlAacD8xr2EwWiXR9h59QUr6Bi+UNEK0vwZBeQM2cBaUWzejssIYToMr3eBqGUSiJe0vh7w0sjgHLgMaXU+0qpPyulknstwARqilewZ+kdWIEyzKQMrEAZe5beQU3xit4OTQghukyvJwhgLrCqWfWSCzgB+KPWehJQD/ygtY2VUvOVUmuVUmvLy8u7P1qgYvlDKNONHaknVlmC8vgxTDcVyx/qkeMLIURPOBISxMU0VC812AXs0lqvbvj7WeIJIyGt9cNa68la68kDBgzoxjAPiFbsxKqrwg4GcKwoOhZGefxEK3f1yPGFEKIn9GqCUEqlAzOA5xtf01rvBUqUUqMbXpoNbOyF8BKKVuzEiQRxIkFcqTkAONEQOhrCk53fy9EJIUTX6bZGaqXU08BMIEcptQv4KeAG0Fo/2LDaOcB/tdb1B21+A/CUUsoDbAWu6K44D0f9ltXs/fdCXJlDUHUVGC5PvKopVAeGSc6cBb0dohBCdJnu7MV0STvWWUy8O+zBr38ATO76qDpGa031O89Q9foTeAeOZNB5Pya44yMqlj9ErLYCHIfBF9wpvZiEEH1KtyWIvsKJRShbdg91m94gZezp5J59E4bbS1rRLNKKZlG76U32Pf8bPDkFvR2qEEJ0KUkQbbBqKyj9+y+J7P2UrBmXk3nyBSilWqzjLygEILSzGN+g43ojTCGE6BaSIFoR3rOZ0r//AicWZtB5Pyb52KkJ13OlZOHOHEyopJjMqef2cJSir4jFYuzatYtwONzboYg+yufzkZ+fj9vtbvc2kiASqCl+jfL//AEzNZuCS36JJ2dom+v7Cgqp/+RttOOgjCOh57A42uzatYvU1FSGDRv2uVKqEJ2ltaayspJdu3YxfPjwdm8nV7NmtONQseIxyl64G9+QMRR86+5DJgcAf0ERTriOaMXOHohS9EXhcJjs7GxJDqJbKKXIzs4+7BJqvy9BNM2pVLET7dgol5fsUy8hZ/bVKLN9p8dfUARAqKQYb+6wboxW9GWSHER36sjnq1+XIBrnVIpW7cEK1mDVVeNE6vEOHtPu5ADgSs/FlZpDuGRDN0YrhBA9q18niIrlD6EME7u2AqUdvDkFuJLSD3tOJaUUvqFFhEqK0Vp3U7RCdJ/t27dTVFR0WNssXryYPXv2dOq4KSkpndpedK9+nSCilSUY3mRcqdl4cgowPP4Oz6nkzy/Erq8mVt25L4wQ7VFTvIKtiy7m459MZ+uii3tlJuGuSBDiyNavE4QnuwAdDWEmpaPMeNevjs6p5B8av/uSaibR3bprunnLsvjWt77F8ccfz/nnn08wGARg3bp1zJgxgxNPPJEvfvGLlJaW8uyzz7J27VouvfRSJk6cSCgU4q677mLKlCkUFRUxf/78hKXpbdu2MW3aNKZMmcJPfvKTptdXrlzJ6aefzjnnnMO4ceO45pprcBynU+9HdF6/bqTOmbOAPUvvgEgQ5fGjoyEcO9ahOZXcWfmYSemESopJm/CFbohW9Bflyx8msm9rq8tri1/FiYRxTBdQDYC2LXY/9X1qimYn3MabN4IBc+a3edzNmzfzyCOPMH36dK688koeeOABbrrpJm644Qaef/55BgwYwJIlS7j99tt59NFHue+++1i4cCGTJ8dnxbn++uu54447APjmN7/JCy+8wNy5c1sc46abbuLaa6/l8ssv5/7772+xbM2aNWzcuJFjjjmGs846i3/84x+cf/75bcYsule/LkGkFc1i8IV34UrPxQ4GcKXnMvjCuzo0p5JSCl9+ISEpQYhu5oTrwTBbvmiY8dc7oaCggOnTpwNw2WWX8dZbb7F582aKi4s588wzmThxIr/4xS/YtStxFeyKFSuYOnUq48eP57XXXmPDhs9/F1atWsUll8SnafvmN7/ZYtlJJ53EiBEjME2TSy65hLfeeqtT70d0Xr8uQQBNcyp1BX9BIfWf/I9YTTnutJ55NoXoew51px8t344VKMPwJjW95kSCuNJzyb/01x0+7sHdIJVSaK0pLCzk7bffbnPbcDjMt7/9bdauXUtBQQF33nlnq33uW+tumej4onf16xJEV5N2CNETcuYswLFjOJEgWuv480k6WDXa3M6dO5sSwdNPP82pp57K6NGjKS8vb3o9Fos1lQxSU1Opra0FaEoGOTk51NXV8eyzzyY8xvTp0/nb3/4GwFNPPdVi2Zo1a9i2bRuO47BkyRJOPfXUTr0f0XmSILqQZ8BwDE8SoZLi3g5F9GFdWTXa3NixY/nLX/7C8ccfT1VVFddeey0ej4dnn32W2267jQkTJjBx4kT+97//ATBv3jyuueYaJk6ciNfr5eqrr2b8+PF8/etfZ8qUKQmPcc8993D//fczZcoUAoFAi2XTpk3jBz/4AUVFRQwfPpxzzjmnU+9HdJ5qT799pZSptbZ7IJ5OmTx5sl67dm2vxrBn6Z3EAvs45uo/9moc4uiyadMmxo4d29th9JqVK1eycOFCXnjhhd4OpU9L9DlTSq3TWid8/k57SxCfKqV+q5Qa19kA+zr/0CJilSVY9ft7OxQhhOiU9iaI44FPgD8rpd5RSs1XSqV1Y1xHLV9+/PkQ4V3SDiFEe82cOVNKD0egdiUIrXWt1vpPWutTgO8Tf750qVLqL0qpUd0a4VHGN2gUyuWR7q5CiKNeuxKEUspUSn1VKfUccA/wO2AE8G9gWTfGd9RRphvfkLHSUC2EOOq1dxzEFmAF8Fut9f+avf6sUur0rg/r6OYvKKTqraexw/WYvuTeDkcIITqkvQnieK11XaIFWusbuzCePsFXUARowrs3kjwycXc/IYQ40rW3kdpSSl2nlHpAKfVo40+3RnYU8w0+DgwXoZ1SzSSODr013XdnLV68mOuvv77L93vVVVexcePGNtd58MEHefzxx5viaH4u2rP9zJkzaeyWf/bZZ7N/f+s9Hw+1vLu0twTxBPAx8EXgLuBSYFN3BXW0M9w+fINGSU8m0W1WFwdZsryWvZUWA7NdXDQnlalFSYfesAstXryYoqIiBg8e3KHtLcvC5ToyZ/v585//fMh1rrnmmqZ/H3wu2rN9c8uWtd2Ue6jl3aW9JYhRWuufAPVa678AXwbGd19YRz9fQRHh0i04scN7BqwQh7K6OMi9S6upCtikJhlUBWzuXVrN6uJgp/bbE9N9z5s3j+985zvMmjWL2267jTVr1nDKKacwadIkTjnlFDZv3gzEL7jnnnsuZ511Fsceeyzf//73m/bx2GOPcdxxxzFjxgxWrVrV9PqOHTuYPXs2xx9/PLNnz2bnzp1Nx7z22muZNWsWI0aM4PXXX+fKK69k7NixzGjQDJkAACAASURBVJs3L+G5aH53n5KSwu23386ECRM4+eST2bdvHwB33nknCxcuTHgumm9/7bXXMnnyZAoLC/npT3+a8HjDhg2joqKCBx98kIkTJzJx4kSGDx/OrFmzWizfvn07Y8eO5eqrr6awsJAvfOELhEIhAN59912OP/54pk2bxq233nrYJcJE2psgYg2/9yulioB0YFinj96H+QuKwLEJ797c26GIo8zS5TX87qnKVn8WPlVFTZ1DoM6hrMomUOdQU+ew8KmqVrdZurzmkMfdvHkz8+fP58MPPyQtLY0HHniAWCzGDTfcwLPPPsu6deu48soruf322zn//POZPHkyTz31FB988AF+v5/rr7+ed999l+LiYkKhUKvjGj755BOWL1/O7373O8aMGcMbb7zB+++/z1133cWPfvSjpvU++OADlixZwkcffcSSJUsoKSmhtLSUn/70p6xatYpXXnmlRTXO9ddfz+WXX86HH37IpZdeyo03Hmgera6u5rXXXuP3v/89c+fO5ZZbbmHDhg189NFHfPDBB22el/r6ek4++WTWr1/P6aefzp/+9KcWyxOdi+Z++ctfsnbtWj788ENef/11Pvzww1aPdc011/DBBx/w7rvvkp+fz3e+853PrbNlyxauu+46NmzYQEZGBn//+98BuOKKK3jwwQd5++23MU3zc9t1RHsTxMNKqUzgJ8C/gI3A/98lEfRRviFjQRlSzSS6XDCsMQ/65ppG/PXO6InpvgEuuOCCpgtYIBDgggsuoKioqOmi3Wj27Nmkp6fj8/kYN24cO3bsYPXq1cycOZMBAwbg8Xi46KKLmtZ/++23+cY3vgHEpxJvPl343LlzUUoxfvx48vLyGD9+PIZhUFhYyPbt29s8Lx6Ph6985SsAnHjiiYdc/2BLly7lhBNOYNKkSWzYsOGQbRMQf27GGWec8bnnaQAMHz6ciRMntohn//791NbWcsoppwA0nYfOalcFoNa6sULtdeLjH8QhmL5kvLnDpaFaHLYL57Q9ScHucouqgI3PeyBLhCMOWekm3700u8PH7anpvpOTD3T9/slPfsKsWbN47rnn2L59OzNnzmxa5vV6m/5tmiaWZSWMsz3vp3FfhmG02K9hGE37bY3b7W7aV/M42mPbtm0sXLiQd999l8zMTObNm9fqeWm0ePFiduzYwX333Zdw+cHnJRQKJazO6wptliCUUt9p66dbIupDfEOLCO/ehLZjh15ZiHa6aE4qMVsTjjhoHf8dszUXzUnt1H57YrrvgwUCAYYMGQLEL4yHMnXqVFauXEllZSWxWIxnnnmmadkpp5zSYirxnpwuvPm5aK6mpobk5GTS09PZt28f//nPf9rcz7p161i4cCFPPvkkhtH+ybYzMzNJTU3lnXfeAWg6D511qAhSD/Ej2uDPL0TbMcKln/Z2KKIPmVqUxI0XZpKVblIbjJccbrwws9O9mHpiuu+Dff/73+eHP/wh06dPx7YPPWH0oEGDuPPOO5k2bRpz5szhhBNOaFp277338thjj3H88cfzxBNPcM8993TsRHRA83PR2GgMMGHCBCZNmkRhYSFXXnllUxVea+677z6qqqqYNWsWEydO5Kqrrmp3DI888gjz589n2rRpaK1JT0/v8Ptp1K7pvo8WR8J0383ZwQDb7r2U7BnfInPaBb0djjiC9ffpvkXn1dXVkZKSAsCvf/1rSktLP5cku2W6b6XUcUqpV5VSxQ1/H6+U+nEH3kO/Yial48kZKvMyCSG63YsvvsjEiRMpKirizTff5Mc/7vwlur2jVP4E3Ao8BKC1/lAp9VfgF52OoI/z5RdSu3El2nFQh1GnKIQQh+Oiiy5q0aurK7T3ipWktV5z0Gvtb8rvx/xDi9DREJGyrb0dihBCHJb2JogKpdRIQAMopc4HSrstqj7EX9DwACGpZhJCHGXamyCuI169NEYptRu4Gbim7U0EgCs1B1d6njxASAhx1GmzDeKgsQ7LiD8TwgDqgfOAu7svtL7DX1BE/WfvorVu9yAfIYTobe0dBzEZuBbIBDKIlx7GdW9ofYd/aBFOqIZYZUmn9lNTvIKtiy7m459MZ+uii6kpXtFFEYr+7mia7nvRokVNEwm2pvlkeR3VOG1Ff9ZmgtBa/0xr/TMgBzhBa/09rfV3gROB/J4IsC/w5cfbITpTzVRTvII9S+/ACpRhJmVgBcrYs/QOSRL9VElgFS98soCni+fywicLKAmsOvRGXexIThCd0Thgr3FAYH/W3jaIoUC02d9RDjGba8NDhcoax04kWH6rUuqDhp9ipZStlMpqttxUSr2vlEo8JeRRxJ05CDMlq1PjISqWP4RhulEuD040hOFNwjDdVCx/qAsjFUeDksAqVpX8hmCsAq+ZRjBWwaqS33Q6SfTEdN/79u3jnHPOYcKECUyYMKHpInz33XdTVFREUVERixYtAuKzqH75y19mwoQJFBUVsWTJEu6991727NnDrFmzmDVrFrZtM2/ePIqKihg/fjy///3vm471zDPPcNJJJ3Hcccfx5ptvAvGS0mmnncYJJ5zACSec0HT8lStXMmvWLL7xjW8wfnz8SQaNg85WrlzJzJkzOf/88xkzZgyXXnpp03tbtmwZY8aM4dRTT+XGG29smtSvrzicBwatUUo9R7wn0znAXw6xzWLgPuDxRAu11r8FfguglJoL3KK1rmq2yk3EH0rU9sxlRwGlFP6CIkIlxR1uh4hWlmB4U4hW7kI7FsaAYSiPn2hl4pk1xdHrw31PEAjvaHX5zsCbWE4YQ7mINLzmaIs3dtzF0PTTEm6T7juG4/O+2eZxN2/ezCOPPML06dO58soreeCBB7jpppu44YYbeP755xkwYABLlizh9ttv59FHH+W+++5j4cKFTJ4cH4R7/fXXc8cddwDx2VRfeOGFz81GeuONNzJjxgyee+45bNumrq6OdevW8dhjj7F69Wq01kydOpUZM2awdetWBg8ezIsvvgjE521KT0/n7rvvZsWKFeTk5LBu3Tp2795NcXH85qv5U9csy2LNmjUsW7aMn/3sZyxfvpzc3FxeeeUVfD4fW7Zs4ZJLLmmqilqzZg3FxcUMHz78c+fm/fffZ8OGDQwePJjp06ezatUqJk+ezIIFC3jjjTcYPnw4l1xySZvn92jUrhKE1vqXwBVANbAfuEJr/atDbPMGUNXWOs1cAjzd+IdSKp/4Q4kO77FMRzB/QSF2bSVWoKxD27szBhGtLEE78eEnTiyMjobwZEtNX38Tc4IoWs73rzCJOZ2rdumJ6b5fe+01rr32WiA+E2l6ejpvvfUW55xzDsnJyaSkpHDuuefy5ptvMn78eJYvX85tt93Gm2++mXBuoREjRrB161ZuuOEGXnrpJdLSDtxPnnvuuUDLKbpjsVjTnFEXXHBBi6m3TzrppITJoXFZfn4+hmEwceJEtm/fzscff8yIESOatumLCaLdz/vTWr8HvNfVASilkoCzgOYPll0EfJ92TAiolJoPzAcYOnRoV4fXZXwF8QbAUEkx7oy8w9rWDteBMsCO4UrPxaqtxA7WYPiSyZmzoDvCFb3oUHf6gUgJwVgFbvPAg2lidogkdw6nHdPx6RV6arrvg7U2H9xxxx3HunXrWLZsGT/84Q/5whe+0FRCaZSZmcn69et5+eWXuf/++1m6dCmPPvoocGBa7OZTdP/+978nLy+P9evX4zgOPp+vaV/NpyE/WKKpx/vSPHatORLmfpgLrGqsXlJKfQUo01qva8/GWuuHtdaTtdaTBwwY0J1xdoonuwDDl3LY7RBONEzpM3eirSi5X/s+nuwClDJAweAL7yKtaFY3RSyOVBPyLsfRMWJ2/DkAMTuEo2NMyLu8U/vtiem+Z8+ezR//+Ecg3hhcU1PD6aefzj//+U+CwSD19fU899xznHbaaezZs4ekpCQuu+wyvve97/Hee+997rgVFRU4jsN5553Hz3/+86Z1WhMIBBg0aBCGYfDEE0+0awbZ1owZM4atW7c2lU6WLFnS4X0dqY6EJ4ZfTLPqJWA68FWl1NmAD0hTSj2ptb6sV6LrIsow8BcUEj6MnkyOFaX0H78gvOcTBn79NlJGTyd39tVUvfVXqlb9jeRRJ3VjxOJIVZA+nencxvp9j1MX3UOKZzAT8i6nIL3tqaQPpXG67wULFnDssce2mO77xhtvJBAIYFkWN998M4WFhU1TXPv9ft5+++2mqpthw4a1Ot33Pffcw/z583nkkUcwTZM//vGPTJs2jXnz5nHSSfHP81VXXcWkSZN4+eWXufXWWzEMA7fb3ZRY5s+fz5e+9CUGDRrEokWLuOKKK3AcB4Bf/arNmm++/e1vc9555/HMM88wa9asNksNh+L3+3nggQc466yzyMnJaYq/L+nW6b6VUsOAF7TWCTtYK6XSgW1Agda6PsHymcD3tNbt6hpwpE33fbDqNc9R+dojDLvuL7hS237yl3Zs9j73K+q3vEPul28hbfzspmXB7R+w528/ZtAFd5I8MuEsveIoI9N9H50ap9jWWnPddddx7LHHcsstt/R2WK3qlum+O0Ip9TTwNjBaKbVLKfV/SqlrlFLNp+g4B/hvouTQFzXOy3So8RDacSh7cRH1W94h58xrWiQHAN/g0Q3Puz70s22FEN3nT3/6ExMnTqSwsJBAIMCCBX2rTbDbqpi01ods0tdaLybeHba15SuBlV0VU2/z5o1EefyEd20gddzpCdfRWlP+yoPUblhB1unfJOPEzxeeDI8fb95IQpIghOhVt9xyyxFdYuisI6GRut9Qhol/yFhCO1tvqK58/S/UvL+MjKnnkTntwlbX8xWMI7JnM44VbXUdcXTpD71iRO/pyOdLEkQP8xUUEq3YgR2q+dyyqreXsv+dZ0mb+CWyZ85rc0Bd4/OuI3s/685wRQ/x+XxUVlZKkhDdQmtNZWVli2697XEk9GLqV/xN4yE2knLcyU2v71/3AlWvP05K4UwGfOHaQ4629uXH50oM79qAP18aN492+fn57Nq1i/Ly8t4ORfRRPp+P/PzDG1grCaKHeQcdizLdhEuKmxJEzUevUvHKgyQfO5W8s29u16NJXckZuLOGENq1kczuDlp0O7fb3eooXiF6i1Qx9TDD5cE7eHRTT6a6zasoW3YP/mMmkPe121Bm+3O2L38c4V0b0Q19wIUQoitJgugFynRTs/6/bPrBZHY8NB/Dn8qg836M4fIc1n78BUU44TqinXzOhBBCJCIJoofVFK+gZv3LOHYMK1QLWhMt307dJ+8c9r7keddCiO4kCaKHVSx/CMOXjGGYGC4PngHHYLi8HXqugys9L/6cCRkPIYToBtJI3cOilSWYSRm4s/NRpgtlmNDB5zoopfDnH978TkII0V5SguhhnuwCdDSE4fbGkwN06rkOvvxxWLUVxDr4nAkhhGiNJIgeljNnAY4dw4kE0VrjRII4dqzDz3VoaofYJaUIIUTXkgTRw9KKZjH4wrtwpediBwO40nM79VwHz4BhGJ4kQiXSDiGE6FrSBtEL0opmddmDfpRh4MsfKyUIIUSXkxJEH+DLH0e0Yid2qLa3QxFC9CGSIPoAf9O8TFLNJIToOpIg+gDv4NFgumQ8hBCiS0mC6AMMlwffwFHSDiGE6FKSIPoIX34h4dJPcWKR3g5FCNFHSILoI/wFheBYREo/6e1QhBB9hCSIPsI3JP7QoJBMuyGE6CKSIPoI05+KJ+cY6ckkhOgykiD6EF9BIaHdm+QBQkKILiEJog/x549DR0NEy7f1dihCiD5AEkQf4muYuE/aIYQQXUESRB/iThuAK22AtEMIIbqEJIg+xldQSKhkA1rr3g5FCHGUkwTRx/jzC7Hrq7H27+3tUIQQRzlJEH2MX9ohhBBdRBJEH+POysfwpRCSeZmEEJ0kCaKPiT9AaBxhKUEIITpJEkQf5M8vJFa9B6t+f2+HIoQ4ikmC6IN8TQ8QklKEEKLjJEH0Qb5Bo1CmWx4gJIToFEkQfZAy3XgHjyZcIglCCNFxkiD6KH9BIZF9n+FEQ70dihDiKCUJoo/y5Y8D7RDes7m3QxFCHKUkQfRR/iFjQRkyYE4I0WGSIPoow5uEN3e4TNwnhOiwbksQSqlHlVJlSqniVpbfqpT6oOGnWCllK6WylFIFSqkVSqlNSqkNSqmbuivGvs6XP47wno/RttXboQghjkLdWYJYDJzV2kKt9W+11hO11hOBHwKva62rAAv4rtZ6LHAycJ1Salw3xtln+QsK0bEIkX1bezsUITqkpngFWxddzMc/mc7WRRdTU7yit0PqV7otQWit3wCq2rn6JcDTDduVaq3fa/h3LbAJGNItQfZxvvzGifsSFuKEOKLVFK9gz9I7sAJlmEkZWIEy9iy9Q5JED+r1NgilVBLxksbfEywbBkwCVrex/Xyl1Fql1Nry8vLuCvOo5ErJxJ05WNohRJOj6Y68YvlDGKYbTBcQb1czTDcVyx/q5cj6j15PEMBcYFVD9VITpVQK8aRxs9a6prWNtdYPa60na60nDxgwoJtDPfr48scR2rVRHiAkjro78mhlCRpFtGInVm0FAMrjJ1q5q5cj6z+OhARxMQ3VS42UUm7iyeEprfU/eiWqPsJfUIgTqiFWJV+q/q7xjlzbFrH9e1GeI/uO3JNVgLW/FAA7GMCxYuhoCE92fi9H1n/0aoJQSqUDM4Dnm72mgEeATVrru3srtr6iceI+GQ8hopUlaMPEqinHidTjROqP6Dty/8gTcawohj8NACuwF8eOkTNnQS9H1n90ZzfXp4G3gdFKqV1Kqf9TSl2jlLqm2WrnAP/VWtc3e2068E3gjGbdYM/urjj7OnfmYMykDGmHEPE78upSUApluLDr9x+xd+R2qJbQtvdIPu4UfHkjMNw+tBUj58xrSCua1dvh9Ruu7tqx1vqSdqyzmHh32OavvQWo7omq/1FKxdshpATR7/mHHU/91rW4UnOgoSSh3F4GHoF35FVvPokTrqfginvw5g7HiQTZ/uD/EZEbnR51JLRBiG7mLxiHFdjX1NAn+p9YdSnBz9aSMm4m3txhKDSG20PyqJOOuDvyyL6tBN7/D+mTzsabOxyI92DKOuUiQjvWE9z2fi9H2H90WwlCHDkOjIfYSOq403s5GtHTtONQ9p8/oAwXBZf9Ol6CAMpfeZDA+y9h1VXjSsns5SjjtNaUL38Iw5dC1mmXtliWNuls9r/7LypXLsZ/zASUIfe33U3OcD/gzRuB8vilHaKfqln/MqGdH5J9xpVNyQEg/cS54NgE3l/Wi9G1VLfpDcIlG8iecTmmP7XFMsPlIev0y4js+4y6j9/qpQj7F0kQ/YAyTHyDRxOSR5D2O7GacipWPIr/mAmkTfhii2WerCEkjZpCzfvLcKxoL0V4gBMNUfHaI3jzRpJ2/BcSrpM6biae3OFUvfEE2o71cIT9jySI/sIwqf3oVT7+8SlH/Aha0TW01pS/dB9oh9wv3Ui8B3lLGVO+jh0MULfx9V6IsKWq/y3Brqsi58xrWq0+UoZB9ozLie0vJfDByz0cYf8jCaIfqCleQc37L+HYFhguYoF97R5BezRNzSBaqi1+jeDWdWTPmIc7Iy/hOv6h4/HkDmf/u8/36mj7aNVu9q/5J6lFZ+DPH9vmukkjJuMrKKJ61d/kiYndTBqp+4GK5Q9h+JIwQjVYNWXxF7XDrr/cQmrRGZjJGZhJ6ZhJ6biSMzCTMjCTMwjt+pjyl+5FuX0tpmaAu464ni+iJauuiorlD+MbMpb0E77c6npKKTImf5WyZfcQ2vEhScMm9GCUB1S8+meUy032zHmHXFcpRc6sK9j1+HfZv+afZJ16yB71ooMkQfQD0coSzKQM3Nn5aCuCdmy0beFEgpi+FKzaCiJ7P8UOBsCxm7aL7NuKY1sow0CZbtwZAzGIJxxJEEcurTXlLz+AtqLknn3TIXv7pIybQeXKxexf+3yvJIj6T98l+Nm7ZM+6EldKVru28Q0eTfJxp1C95h+kTfoSruSMbo6yf5IE0Q94sguwAmUY3iRwewFwIkG8ucMZfNFdTetprXHCddjBAHZwP9vvn4fp9oF2sIM1xCp3YablHLFTM4i4uo/fon7LO2TPvKJdo6QNl4e0SWdTvepvRKt248nqudn1HStKxat/wp01hIzJcw9r2+wZl1O/5R2q/7eEAWceeYP9+gJpg+gHcuYswLFjOJFgPAlEggnntFFKYfpT8WTn4y8owjtwFIbLgyslC09OAcrjI7Z/HyglPUiOUHYwQMUrD+IdeCwZJ3293dulTzobTJPA2n93Y3Sft//dfxKr3sOAMxegTPdhbevJzift+DMJvP8fYvv3dlOE/ZskiH4grWgWgy+8C1d6LnYwgCs9l8EXHrodoXliQRmYyZmY3iRAsfuvP8SqreyZNyDarXz5w9jh+oaqJbPd27lSMkkdO4Oaj5Zjh+u6McIDrNoKqv+3hORjTyZp+Akd2kfWqZegDIPKN5/s4ugESBVTv5FWNOuw2w3i699FxfKHiFbuwpOdz6DzfoLhcrPvxUWUPHYTA8/5Af6Cou4JugfVFK9oeJ8leLILyJmz4KhrZ6nfspq6ja+TdeqleHOHHfb2GVO+Sm3xq9Ss/y+ZU8/t+gAPUvHao+A45My+qsP7cKXmkDHla1S//QyRk87BmzeyCyMUUoIQbUormsWIm//GmJ+/xYib/0Za0SxSxpxKwbfuxvAmsfvp29m/9t9H9QOJjrYH6SRih+soe/l+PLnDyZx2fof24c0bia+giMC6F9DNOit0h9DOj6jb9AYZU8/DnTGwU/vKmHoehi+FypV/6aLoRCNJEKJDPDlDyf/W70kacSIVyx+i7IW7cWLhLtl3T429sIMBgtveZ+8/fhF/qFJNOdGKHWg7hjqCH6STSMVrj2DX748PiDvMuvzmMqZ8DaumjPot73RhdC1px6Z8+cO4UnM6nMyaM30pZE67kOC29whuX98FEYpGUsUkOsz0JTPo3B9T/fZSqt58ikj5dgade3un7ggb7+YN092hsReJqopSC2di11UR2fcZkb2fxX/v+wyrJv4M82j5DnB7Md0+tGMTqykHw4UTqkFrnXAE8pEkuO09aj98hYyTz8c36NhO7St51FRc6XnsX/NPUkZP76IIWwq8/x+iZdsY+PUfYLh9XbLP9BO/QmBtw0R+37r7iP8/O1pIghCdogyDrOkX4x04in3/+i0li28mb+73SB45uUP7K//vA+DYOI6Nbmgs1bZF6bM/w9q/F+XyoFzuht8ejIbfyuUhuP0Dyv/7YPwO2nARLt1CyaPX4845BqNpLIDCnTUY35CxeE+cizdvBKX/+CV2bSWGNwkNOOF6YoF9OJEgu5+4lexZ847YdhYnGqLsP3/AnTWErFO/0en9KcMgY/JcKl79M+HST/ANOq4LojzADgaoevNJ/EOPJ7kLE5Dh8pB12mWULVtE/eZVpIw5tcv23Z+po7nu+GCTJ0/Wa9eu7e0w+q1YdSmlz/1/RMu2k3X6ZbhSB1Dx6sOtNvxqxyZavp3wns2Ed28mXLqZ2g+Xow0TpVRTLxzt2GDb+IaMbvP4zQf2NVLKwEzJZODXfoA3bwTevBEYHn+L7ZqXWpTHj46GcKwo6VO+Tnjnh9h1VSSNOomcmfPw5AztwjPWcY0lpdCujWBb5H39B+TMuLxL9u1Egmy7/1skHzuVgXO/1yX7bFT20n3UrP8vQ//vvi4/l9pxKHn0erRtMfSqB1Cm3P+2h1JqndY64R2dJAjRpZxYmLL//IH9a/+NXVeFK21A/M48GsKJhck67VIMt59w6WYiez9FxyIAmEnpeAePpnbDSnQsjJmU0XShdyJBXOm5DL/xr2g7irYaf2I4VjQ+OtyKsf2BeShPMkqBMl0oV3xQoB0MMObnbU8PfaBqKt5bqzGZObEw+9f+i/1vP4sTC5M6fjbZp13aYtrsntaY0NAOsZoKTG8yhi+5XV2X26v81T8RWPcCw659FFdqdqfjrVj+EJGybTihWtJOnEvBZb/pkjgPVr9lNaV//zkDvvjt+NgOcUhtJQhJsaJLGW4feXO/R836l4lZUazaCoyQBycWxrFilL90P97Bx8WndJ7wRXyDR+MbPBpXeh5KKWrGnMaepXegY2FovJtvGNSnDANl+KCVemtv3sgDI8YbOJFgu0YTt9YN2HD7yJp2IekTz6Lqf0sIvPcidRtWkj75q2SefP7nnlnQE+KN5wo7GMBwuXFnDkRHw106BUrGiXMJvPsvAu+9SHYnSiaNyUyZbpxoGK0d6je/RU3xim7pRpw06iR8+eOoeutpUgvPwPB0TRtHfyUJQnQ5pRRONIQnOx+rpgzHimJ4/BhJGWBbjLhlKYbLk3DbRGMv2jsmIWfOgviddSR4oKoowYjxjjD9aQyYfTUZJ36VqjefZP/qf1Cz/mUyp12I4UulcsUjPTKGQjs24d0fY0dDGKYLd8YglDLA4+/SKVDcGQNJPnYqgQ/+Q+YpF2E0TNFyuCqWP4Qy3Q2lvAjutNz/197dB8dRnwcc/z57p5eThCTbegGDjY2BAUoTJw0uFBLsGdc2DKkJpW46SZukMwnU0MRT/miayUwwM0nIJMHOTEiK0ySmCYQ4E5ISnFIXCjjEQ8B2DHYw2I6RwS9ItmQJvd3b7tM/dk+cxcqST3fas+75zNzc7t7t3aPfrfa53f39nkNi8ZLV8xIRZi3+FIc23Mb+r96IZlNn7biWcmAJwpRErv5TTeu8kWVeaoh4y5wxk0NOIYP6cusVmlwmqqq5nfYP30Xzoo/Q/eyDdG5eT7av06+I29BS0oq36RNv0Pn4OjSbJlZdS9WM2e9cpwkScjE1X7WSwf3P0/+Hp2lauKKg10gfP4SbTqKZYZyaOpxEI6AlreeV6X0Ld6AHdTNUt82f1lWISz3A0xKEKYlSfps/nUKTy5mqqxs7SgAAC0RJREFUab+I2avWMnhwB25/N+5Qn58AG2aOjKEoVhzqufS+8Au6f/NjnOo6WpevpmfbT/3rNyVs29o5V1LdNp++7Y/R+N7lZ9R1VFUZeOUZ3OQAmkkRb24nlmhCBLxU8ZNZvhNPPkCsYSbZt7v862ANMxFKX4V4MjvrQtadbJfwibAEYUpiKr7NlwN38CRVrfPQ9CDuQA+Zvk5wYrhDvSMD7iYj3f0mnZvXkzr6GvWX/gWty1cTr2+m9oI/KXnbigjNV91M1+Z1DHfsom7++ya0njvUR9cT9zO4bxuJC99DuvMgTqwKULxU6b8o5MrbayaFO9yHO/w2IGR73+LE0z+kpn0BNecu8E/PhZRCn+qd9UTWVc/DSw/hpYfR9DBucpDOX30DdbMogib7iSUaITVU1ERovZiMmYSD6z86cmFcFbzUINm+LkCpv3gRzVffSuN7l417Wm009Tx6X/wlPVt/hFTV0LpsNQ2Xf3DKB4B52TSHvvOP1Jx7MbNX3T3u8wf2Pc/xJ76Nmxpg1gc/TvOiW+h/5dkp/aKQ+0ykug7NJNFsCnd4AHEcqmbNAS8LgFQnRpJFTfsCas+9mOSx/Rz92d2ndnl2M6E9xNTN4qWH8VJDHNpwG9n+40i8GlRBFS+TJFbXRNsNnwUvi7ou6mVRNxvM+7furT/GSw743XLVQ9VDM2mkqprE3D/1qzCHVClIHnn1lC7hNW3zUdUJ9drLZ91cjSmR0DEUboaZ1/8D6aP7SB7ZS6x+Bs1/fgtNC2+YUK+adM8RujavI3nkVeovuZrW5XcQb5gxBX9NuJ7nfkLPcw8x99P/PuapITc5wIknN9C/5/+obptP+013FVQwsBjG+kxmr7qHcy6/jvSJN4IR9QdJdR4g1XkQzaYBSHW9DojfE04EPA8vm8KpqqXh0mtGEoKXHhpZB07dWeeoKuK51J5/2buDdOJILIY4cYY6fu/PO47f4UAcFAEvQ+vSz+BU1/nXb2rqRqalOsGxn60lO9jrV1gWB3GckS7hF615ZMLtZQnCmBIaawyFqjL8xm5Obvspw4dewkk00rzoZprff9MpXXFz1PPo2/EY3c886B81/OXtNFxxfeRlI7KDvXR855M0vmcZbctXv+vxoY5ddG5ehztwkhnXrGLmtX876VNrkzXWZxJGPY9Mz2GSbx3g8MY1qDiQ2/k7DiDguX5vrupEsLOuf2e6OkHn4/f53Y5r6gAHBDSTJN7YyoW3f98fl+PE/aOEUYkk/yg0ZyI7+tMlwjM5QrMEYUzEhg/v5eS2Rxg6uAOntoHmD/wVTl0zPVv/k3T3m8Qb23BqG/AGT1J38SLaVtw54Z/fnAqdv/4WA3u3Mm/1xpGxH146SfczP6Rv52aqZl1A+03/UvTSHFMtip31ZNed7Ok7SxDGlInksX2c3LaJvpe2kO09hpNoRKpqcYPCgS3L/4n2G9dEftQwWqrzIK/f/0mceBVeJuknL4mB59J81UpmfujvCx4rUU6i2lkXY0dfKBtJbUyZqD3vUs776y8y+MfteMN9eKlBSA3i1DYQS5zD0P7flV1yAEgdP4Q70E3Wc4nVNTF85DVEHM69+V9pWfKpqMMrmsn0vptMF+up6p59pixBGBOBbP9xqmZe4P+2t+siNQnQ0g4gm4wTTz6Ak2jCHezxf7a2vhmntp63X/qfaZUgoHx31lGwBGFMBHIjzZ2aupH/Qq8Eo6GLJd39JrH6GQiKVCeI1dShZZzQTHHYL8oZE4GWpbfhuRm/j7uq33VyCkaaF6p61hzIDBM/Z5bfrZLSlPcw5cUShDERaLxyCbNX3UO8qc0/ZdPUVtRy3cV2tiU0Uxx2ismYiJxN57orpXSKOZUlCGPMhJxNCc0Uh51iMsYYE8oShDHGmFCWIIwxxoSyBGGMMSaUJQhjjDGhplWxPhE5DhwqcPUW4EQRw5lurH3GZ210etY+44uijS5U1dawB6ZVgpgMEdk+VkVDY+0zEdZGp2ftM75yayM7xWSMMSaUJQhjjDGhLEG8Y0PUAZQ5a5/xWRudnrXP+MqqjewahDHGmFB2BGGMMSaUJQhjjDGhKj5BiMgKEXlNRA6IyOejjqcciUiHiOwWkV0isj3qeKImIj8QkS4R2ZO3bKaI/K+I7A/uZ0QZY9TGaKO7ReRIsB3tEpEbo4wxSiIyR0SeFpG9IvIHEflcsLystqOKThAiEgPuB24ArgD+TkSuiDaqsrVEVReWUx/tCG0EVoxa9nngKVW9BHgqmK9kG3l3GwGsC7ajhar66ymOqZxkgbtU9XLgauCOYN9TVttRRScIYBFwQFUPqmoaeARYGXFMpsyp6lagZ9TilcCDwfSDwM1TGlSZGaONTEBVj6nqzmC6H9gLnE+ZbUeVniDOB97Mmz8cLDOnUmCLiOwQkc9EHUyZalfVY+D/8wNtEcdTru4UkZeDU1AVfRouR0TmAe8DfkeZbUeVniAkZJn1+323a1X1/fin4u4QkQ9FHZA5K30XWAAsBI4B34w2nOiJSAPwc2CNqr4ddTyjVXqCOAzMyZu/ADgaUSxlS1WPBvddwC/wT82ZU3WKyHkAwX1XxPGUHVXtVFVXVT3ge1T4diQiVfjJ4SFVfTRYXFbbUaUniBeBS0RkvohUAx8FHos4prIiIvUick5uGlgG7Dn9WhXpMeATwfQngP+KMJaylNvxBT5CBW9HIiLA94G9qnpf3kNltR1V/EjqoKvdeiAG/EBVvxxxSGVFRC7CP2oAiAMPV3obichPgMX4pZk7gS8BvwQ2AXOBN4C/UdWKvUg7Rhstxj+9pEAHcFvufHulEZHrgN8AuwEvWPwF/OsQZbMdVXyCMMYYE67STzEZY4wZgyUIY4wxoSxBGGOMCWUJwhhjTChLEMYYY0JZgjDTlog0i8jqcZ6zbRKvf4+ILC10/VGv9YVR8wXHZUyxWDdXM20FNW4eV9UrQx6Lqao75UGNQUQGVLUh6jiMyWdHEGY6uxdYEPz2wNdFZHFQg/9h/AFKiMhAcN8gIk+JyM7gty9WBsvnBTX7vxfU7d8iIongsY0icmsw3SEia/PWvyxY3hrU9d8pIg+IyCERackPUkTuBRJBnA+NimuxiDwrIptEZJ+I3CsiHxORF4L3WZD3Pj8XkReD27XB8uvzfn/h97lR8cZMiKrazW7T8gbMA/bkzS8GBoH5ecsGgvs40BhMtwAH8Is5zsOv3b8weGwT8PFgeiNwazDdAfxzML0a+I9g+tvAvwXTK/BHEbeExDoQNh/E3AucB9QAR4C1wWOfA9YH0w8D1wXTc/FLOAD8Cr/YIkADEI/6c7Hb2XOLTya5GHMWekFVXw9ZLsBXgkq1Hn7Z9/bgsddVdVcwvQM/aYR5NO85twTT1+HXHUJVnxCRkwXE/KIGJSlE5I/AlmD5bmBJML0UuMIv8QNAY3C08FvgvuDI5FFVPVzA+5sKZQnCVJrBMZZ/DGgF/kxVMyLSAdQGj6XynucCiTFeI5X3nNz/VlhJ+TOV//5e3ryX9z4OcI2qDo9a914R2QzcCDwvIktV9dUixGQqgF2DMNNZPzDRc+5NQFeQHJYAFxYphueAVQAisgwY60dyMkH550JtAe7MzYjIwuB+garuVtWvAduByybxHqbCWIIw05aqdgO/FZE9IvL1cZ7+EPABEdmOfzRRrG/Za4FlIrIT/weXjuEnrtE2AC/nLlIX4LP48b8sIq8AtwfL1wR//0vAMPDfBb6+qUDWzdWYEhKRGsBV1ayIXAN8V1UXRh2XMRNh1yCMKa25wCYRcYA08OmI4zFmwuwIwhhjTCi7BmGMMSaUJQhjjDGhLEEYY4wJZQnCGGNMKEsQxhhjQv0/UUgez79e5gMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colorlist=[\"#D2691E\",'#4169E1',\"#9ACD32\",\"#B22222\",\"#FF00FF\",\"#708090\"]\n",
    "for i in range(len(losslist)):\n",
    "    plt.plot(losslist[i], 'ro-', color=colorlist[i], alpha=0.8, label=losslistname[i])\n",
    "\n",
    "# 显示标签，如果不加这句，即使加了label='一些数字'的参数，最终还是不会显示标签\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('trianing times')\n",
    "plt.ylabel('delay')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
