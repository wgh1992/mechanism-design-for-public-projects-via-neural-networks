{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from random import randint\n",
    "import random\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "from pylab import *\n",
    "from scipy.stats import beta\n",
    "n = 5\n",
    "epochs = 5\n",
    "supervisionEpochs = 5\n",
    "lr = 0.001\n",
    "log_interval = 20\n",
    "trainSize = 40000#100000\n",
    "percentage_train_test= 0.25\n",
    "penaltyLambda = 10\n",
    "doublePeakHighMean = 0.9\n",
    "doublePeakLowMean = 0.1\n",
    "doublePeakStd = 0.1\n",
    "uniformlow=0\n",
    "uniformhigh=1.0\n",
    "normalloc = 0.2\n",
    "normalscale = 0.1\n",
    "\n",
    "cauchyloc = 1/n\n",
    "cauchyscalen = 0.004\n",
    "\n",
    "exponentialhigh = 15 #Symbol(\"b\", real=True)\n",
    "exponentiallow  = 15 #Symbol(\"a\", real=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "independentnormalloc1=[(float(ii)+1)/(2*n+1) for ii in range(n,0,-1)]\n",
    "independentnormalscale1=[0.05 for ii in range(n)]\n",
    "\n",
    "independentnormalloc2=[(float(ii)+1)/(2*n+1) for ii in range(1,n+1,1)]\n",
    "independentnormalscale2=[0.05 for ii in range(n)]\n",
    "stage=[\"U-exponential\"]\n",
    "# \"twopeak\",\"normal\",\"uniform\",\"independent1\",\"independent2\",\"cauchy\",\"U-exponential\"\n",
    "order1name=[\"costsharing\",\"dp\",\"random initializing\"]\n",
    "# \"costsharing\",\"dp\",\"heuristic\",\"random initializing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5000)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio1 = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase1 = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "d3 = D.normal.Normal(normalloc, normalscale)\n",
    "distributionRatio3 = d3.cdf(1) - d3.cdf(0)\n",
    "distributionBase3 = d3.cdf(0)\n",
    "\n",
    "d4 = D.uniform.Uniform(uniformlow,uniformhigh)\n",
    "distributionRatio4 = d4.cdf(1) - d4.cdf(0)\n",
    "distributionBase4 = d4.cdf(0)\n",
    "\n",
    "d5 = [D.normal.Normal(independentnormalloc1[ii], independentnormalscale1[ii]) for ii in range(n)]\n",
    "d6 = [D.normal.Normal(independentnormalloc2[ii], independentnormalscale2[ii]) for ii in range(n)]\n",
    "\n",
    "d7 = D.cauchy.Cauchy(cauchyloc,cauchyscalen)\n",
    "\n",
    "d81 = D.exponential.Exponential(exponentiallow)\n",
    "d82 = D.exponential.Exponential(exponentialhigh)\n",
    "\n",
    "#d8 = beta(betahigh,betalow)\n",
    "#d9 = D.beta.Beta(betahigh,betalow)\n",
    "def cdf(x,y, i=None):\n",
    "    if(y==\"twopeak\"):\n",
    "        return (d1.cdf(x) + d2.cdf(x) - distributionBase1) / 2 / distributionRatio1\n",
    "    elif(y==\"normal\"):\n",
    "        return (d3.cdf(x)-distributionBase3)/distributionRatio3;\n",
    "    elif(y==\"uniform\"):\n",
    "        return (d4.cdf(x)-distributionBase4)/distributionRatio4;\n",
    "    elif(y==\"independent1\"):\n",
    "        return d5[i].cdf(x);\n",
    "    elif(y==\"independent2\"):\n",
    "        return d6[i].cdf(x);\n",
    "    elif(y==\"cauchy\"):\n",
    "        return d7.cdf(x);\n",
    "#     elif(y==\"beta\"):\n",
    "#         return torch.tensor(d8.cdf(x));\n",
    "    elif(y==\"U-exponential\"):\n",
    "        return (d81.cdf(x) + (1.0 - d82.cdf(1.0-x)))  / 2 \n",
    "print(cdf(0.5,\"U-exponential\"))\n",
    "\n",
    "print(d81.cdf(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    if torch.sum(bits).item() == 0:\n",
    "        return torch.ones(n)\n",
    "    bits = bits.type(torch.float32)\n",
    "    negBits = torch.ones(n) - bits\n",
    "    payments = model(bits)\n",
    "    payments = payments - 1000 * negBits\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    payments = payments + negBits\n",
    "    return payments\n",
    "\n",
    "def tpToBits(tp, bits=torch.ones(n).type(torch.uint8)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = (tp >= payments).type(torch.uint8)\n",
    "    if torch.equal(newBits, bits):\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits(tp, newBits)#bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    return n - torch.sum(tpToBits(tp).type(torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    global  dp,decision\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money, yes]\n",
    "        offer = float(offerIndex) / dpPrecision\n",
    "        if tp[i] >= offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits[i] = 0\n",
    "            payments[i] = 1\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "#     print()\n",
    "#     print(tp)\n",
    "#     print(bits)\n",
    "#     print(payments)\n",
    "#     print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def heuristicSupervisionRule(tp):\n",
    "    global  dp_H,decision_H\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    tempold=-1;\n",
    "    for turns in range(n,0,-1):\n",
    "        money = dpPrecision\n",
    "        j=0\n",
    "        tempo=sum(bits)\n",
    "        #print(\"bits\",tempo)\n",
    "        for i in range(n):\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            \n",
    "            offerIndex = decision_H[tempo ,tempo - i, money]\n",
    "            offer = float(offerIndex) / dpPrecision\n",
    "            while(j<n):\n",
    "                if(bits[j]!=0):\n",
    "                    break;\n",
    "                j+=1;\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            if tp[j] >= offer:\n",
    "                #print(money,j,tp[j],offer)\n",
    "                money -= offerIndex\n",
    "                bits[j] = 1\n",
    "                payments[j] = offer\n",
    "            else:\n",
    "                bits[j] = 0;\n",
    "                payments[j] = 1.0;\n",
    "            j+=1;\n",
    "        #print(\"money\",money)\n",
    "        if(money==0 and tempold==tempo):\n",
    "            break;\n",
    "        tempold=tempo;\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    for k in range(n, -1, -1):\n",
    "        if k == 0:\n",
    "            break\n",
    "        bits = [1 if tp[ii] >= 1.0 / k else 0 for ii in range(n)]\n",
    "        if sum(bits) == k:\n",
    "            break\n",
    "    if k == 0:\n",
    "        payments = [1 for ii in range(n)]\n",
    "    else:\n",
    "        payments = [1.0 / k if bits[ii] == 1 else 1 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    return float(n - torch.sum(costSharingSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def dpDelay(tp):\n",
    "    return float(n - torch.sum(dpSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def heuristicDelay(tp):\n",
    "    return float(n - torch.sum(heuristicSupervisionRule(tp)[0]).item())\n",
    "\n",
    "\n",
    "# templire=0;\n",
    "# num=1\n",
    "# for i in range(num):\n",
    "#     temp=torch.tensor(samplesJoint[i])\n",
    "#     #print(temp)\n",
    "#     #print(temp);\n",
    "#     #print(heuristicSupervisionRule(temp))\n",
    "#     #print(dpSupervisionRule(temp))\n",
    "#     #print(dpDelay(temp))\n",
    "#     res=dpDelay(temp)\n",
    "#     templire+=res\n",
    "#     #print(\"delay\",res)\n",
    "#     #print()\n",
    "# print(templire/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordAndReport(name, source, loss):\n",
    "    source.append(loss)\n",
    "    realLength = len(source)\n",
    "    #print(f\"{name} ({realLength}): {loss}\")\n",
    "    print(name,realLength,\":\" ,loss)\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    \n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda /100000\n",
    "        \n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "losslist=[];\n",
    "losslistname=[];\n",
    "losslisttemp=[];\n",
    "def test_batch():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lenLoss= 0\n",
    "        nnLoss = 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "            for tp in tp_batch:\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        nnLoss/=lenLoss\n",
    "    return nnLoss\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                delay1 = tpToTotalDelay(tp1)\n",
    "                delay0 = tpToTotalDelay(tp0)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                    loss = loss + (1 - cdf(offer,order)) * delay1 + cdf(offer,order) * delay0\n",
    "                else:\n",
    "                    loss = loss + (1 - cdf(offer,order,i)) * delay1 + cdf(offer,order,i) * delay0\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "    \n",
    "allBits = [torch.tensor(bits, dtype=torch.int16) for bits in itertools.product([0, 1], repeat=n)]\n",
    "\n",
    "def test():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        costSharingLoss = 0\n",
    "        dpLoss = 0\n",
    "        nnLoss = 0\n",
    "        heuristicLoss=0\n",
    "        lenLoss= 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "                heuristicLoss+= heuristicDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        costSharingLoss /= lenLoss\n",
    "        dpLoss /= lenLoss\n",
    "        nnLoss /= lenLoss\n",
    "        heuristicLoss /= lenLoss\n",
    "        #print(lenLoss)\n",
    "        losslisttemp.append(nnLoss);\n",
    "        recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "        recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "        recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "        recordAndReport(\"heuristic\", runningLossHeuristic, heuristicLoss)\n",
    "        print(\"DP:\",dp[n, dpPrecision,0])\n",
    "        #for i in range(n, 0, -1):\n",
    "        #    print(\"Heuristic:\",i,5*(1-dp_H[i, i, dpPrecision]))\n",
    "        for i in range(n, 0, -1):\n",
    "            print(\n",
    "                    tpToPayments(\n",
    "                            torch.tensor([0 if ii >= i else 1 for ii in range(n)], dtype=torch.float32)\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "dpPrecision = 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def producedata(order):\n",
    "    global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "    dpPrecision = 100 \n",
    "    if(order==\"twopeak\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = np.random.normal(\n",
    "            loc=doublePeakLowMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=doublePeakLowMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samples2 = np.random.normal(\n",
    "            loc=doublePeakHighMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = np.random.normal(\n",
    "                        loc=doublePeakHighMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "    elif(order==\"normal\"):\n",
    "        print(\"loc\",normalloc, \"scale\",normalscale)\n",
    "        samples1 = np.random.normal(\n",
    "            loc=normalloc, scale=normalscale, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=normalloc, scale=normalscale\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"uniform\"):  \n",
    "        print(\"uniformlow\",uniformlow, \"uniformhigh\",uniformhigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        uniformlow, uniformhigh\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent1\"):\n",
    "        print(\"loc\",independentnormalloc1,\"scale\",independentnormalscale1)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent2\"):\n",
    "        print(\"loc\",independentnormalloc2, \"scale\",independentnormalscale2)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"cauchy\"):\n",
    "        print(\"cauchyloc\",cauchyloc, \"cauchyscale\",cauchyscalen)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"beta\"):\n",
    "        print(\"betalow\",betalow, \"betahigh\",betahigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(betahigh,betalow,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(betahigh,betalow,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"U-exponential\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = d81.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d81.rsample(torch.Size([1])).numpy()\n",
    "                    \n",
    "        samples2 = d82.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        \n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = d82.rsample(torch.Size([1])).numpy()\n",
    "        samples2 = 1.0 - samples2\n",
    "        samplesJoint = signals * samples1 - (signals - 1.0) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        plt.hist(samplesJoint,bins=500)\n",
    "        plt.show()\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "        \n",
    "        \n",
    "    tp_dataset = TensorDataset(tp_tensor[: int(trainSize * percentage_train_test)])\n",
    "    tp_dataset_testing = TensorDataset(tp_tensor[int(trainSize * (1.0-percentage_train_test)) :])\n",
    "    tp_dataloader = DataLoader(tp_dataset, batch_size=128, shuffle=True)\n",
    "    tp_dataloader_testing = DataLoader(tp_dataset_testing, batch_size=256, shuffle=False)\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "    decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for yes in range(n + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp[0, 0, yes] = 0\n",
    "            else:\n",
    "                dp[0, money, yes] = yes# + 1.0\n",
    "    for ppl in range(1,  n + 1):\n",
    "        for yes in range(n + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                minSoFar = 1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1 - cdf(offer,order)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order) * (1 + dp[ppl - 1, money, yes])\n",
    "                    else:\n",
    "                        res = (1 - cdf(offer,order,n-ppl)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order,n-ppl) * (1 + dp[ppl - 1, money, yes])\n",
    "                    if minSoFar > res.item():\n",
    "                        minSoFar = res.item()\n",
    "                        decision[ppl, money, yes] = offerIndex\n",
    "                dp[ppl, money, yes] = minSoFar\n",
    "    \n",
    "    print(\"dp\",dp[n, dpPrecision, 0])\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp_H = np.zeros([n + 1 , n + 1, dpPrecision + 1])\n",
    "    decision_H = np.zeros([n + 1 , n + 1, dpPrecision + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for i in range(1,n+1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp_H[i, 0, 0] = 1\n",
    "            else:\n",
    "                offer = money / dpPrecision\n",
    "                dp_H[i, 0, money] = 0#cdf(offer)# + 1.0\n",
    "    for i in range(1,n+1):\n",
    "        for ppl in range(1, i + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                maxSoFar = -1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1-cdf(offer,order)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    else:\n",
    "                        res = (1-cdf(offer,order,ppl-1)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    if maxSoFar < res.item():\n",
    "                        maxSoFar = res.item()\n",
    "                        decision_H[i, ppl, money] = offerIndex\n",
    "                dp_H[i, ppl, money] = maxSoFar\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loc 0.1 scale 0.1\n",
      "loc 0.9 scale 0.1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dp 1.9375001192092896\n",
      "Supervised Aim: U-exponential costsharing\n",
      "Train Epoch: 1 [0/10000 (0%)]\tLoss: 0.001899\n",
      "Train Epoch: 1 [2560/10000 (25%)]\tLoss: 0.000016\n",
      "Train Epoch: 1 [5120/10000 (51%)]\tLoss: 0.000004\n",
      "Train Epoch: 1 [7680/10000 (76%)]\tLoss: 0.000001\n",
      "Train Epoch: 2 [0/10000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [2560/10000 (25%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [5120/10000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [7680/10000 (76%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [0/10000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [2560/10000 (25%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [5120/10000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [7680/10000 (76%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [0/10000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [2560/10000 (25%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [5120/10000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [7680/10000 (76%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [0/10000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [2560/10000 (25%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [5120/10000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [7680/10000 (76%)]\tLoss: 0.000000\n",
      "NN 1 : tensor(2.6341)\n",
      "CS 1 : 2.6341\n",
      "DP 1 : 1.9423\n",
      "heuristic 1 : 2.6955\n",
      "DP: 1.9375001192092896\n",
      "tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500, 1.0000])\n",
      "tensor([0.3333, 0.3333, 0.3333, 1.0000, 1.0000])\n",
      "tensor([0.5000, 0.5000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/10000 (0%)]\tLoss: 2.654058 testing loss: tensor(2.6334)\n",
      "Train Epoch: 1 [2560/10000 (25%)]\tLoss: 2.239132 testing loss: tensor(2.1428)\n",
      "Train Epoch: 1 [5120/10000 (51%)]\tLoss: 2.002737 testing loss: tensor(2.0599)\n",
      "Train Epoch: 1 [7680/10000 (76%)]\tLoss: 2.110441 testing loss: tensor(2.0246)\n",
      "penalty: 0.06137726828455925\n",
      "NN 2 : tensor(1.9952)\n",
      "CS 2 : 2.6341\n",
      "DP 2 : 1.9423\n",
      "heuristic 2 : 2.6955\n",
      "DP: 1.9375001192092896\n",
      "tensor([5.8066e-04, 7.7549e-01, 6.3560e-06, 2.1710e-01, 6.8164e-03])\n",
      "tensor([5.3094e-03, 7.8423e-01, 2.4874e-04, 2.1021e-01, 1.0000e+00])\n",
      "tensor([0.1230, 0.8638, 0.0131, 1.0000, 1.0000])\n",
      "tensor([0.1361, 0.8639, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 2 [0/10000 (0%)]\tLoss: 1.859046 testing loss: tensor(1.9967)\n",
      "Train Epoch: 2 [2560/10000 (25%)]\tLoss: 1.950326 testing loss: tensor(1.9912)\n",
      "Train Epoch: 2 [5120/10000 (51%)]\tLoss: 2.130051 testing loss: tensor(1.9752)\n",
      "Train Epoch: 2 [7680/10000 (76%)]\tLoss: 1.968184 testing loss: tensor(1.9718)\n",
      "penalty: 0.024851635098457336\n",
      "NN 3 : tensor(1.9670)\n",
      "CS 3 : 2.6341\n",
      "DP 3 : 1.9423\n",
      "heuristic 3 : 2.6955\n",
      "DP: 1.9375001192092896\n",
      "tensor([5.7090e-06, 7.2660e-01, 1.2500e-08, 2.7019e-01, 3.2046e-03])\n",
      "tensor([5.4544e-04, 7.3673e-01, 7.2049e-06, 2.6272e-01, 1.0000e+00])\n",
      "tensor([0.2030, 0.7910, 0.0059, 1.0000, 1.0000])\n",
      "tensor([0.1936, 0.8064, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 3 [0/10000 (0%)]\tLoss: 1.979110 testing loss: tensor(1.9653)\n",
      "Train Epoch: 3 [2560/10000 (25%)]\tLoss: 2.053305 testing loss: tensor(1.9628)\n",
      "Train Epoch: 3 [5120/10000 (51%)]\tLoss: 2.262871 testing loss: tensor(1.9598)\n",
      "Train Epoch: 3 [7680/10000 (76%)]\tLoss: 1.921779 testing loss: tensor(1.9580)\n",
      "penalty: 0.020715704187750816\n",
      "NN 4 : tensor(1.9614)\n",
      "CS 4 : 2.6341\n",
      "DP 4 : 1.9423\n",
      "heuristic 4 : 2.6955\n",
      "DP: 1.9375001192092896\n",
      "tensor([9.6097e-08, 7.8305e-01, 1.3748e-10, 2.1577e-01, 1.1793e-03])\n",
      "tensor([4.3075e-05, 7.5719e-01, 3.0577e-07, 2.4276e-01, 1.0000e+00])\n",
      "tensor([0.1766, 0.8217, 0.0017, 1.0000, 1.0000])\n",
      "tensor([0.1983, 0.8017, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 4 [0/10000 (0%)]\tLoss: 1.891231 testing loss: tensor(1.9574)\n",
      "Train Epoch: 4 [2560/10000 (25%)]\tLoss: 1.788223 testing loss: tensor(1.9573)\n",
      "Train Epoch: 4 [5120/10000 (51%)]\tLoss: 1.934995 testing loss: tensor(1.9508)\n",
      "Train Epoch: 4 [7680/10000 (76%)]\tLoss: 2.110153 testing loss: tensor(1.9528)\n",
      "penalty: 0.06148046255111694\n",
      "NN 5 : tensor(1.9513)\n",
      "CS 5 : 2.6341\n",
      "DP 5 : 1.9423\n",
      "heuristic 5 : 2.6955\n",
      "DP: 1.9375001192092896\n",
      "tensor([2.2596e-08, 5.5907e-01, 4.4448e-11, 4.4000e-01, 9.3011e-04])\n",
      "tensor([2.5329e-05, 6.1435e-01, 1.6892e-07, 3.8562e-01, 1.0000e+00])\n",
      "tensor([0.2233, 0.7753, 0.0014, 1.0000, 1.0000])\n",
      "tensor([0.2198, 0.7802, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 5 [0/10000 (0%)]\tLoss: 1.639276 testing loss: tensor(1.9547)\n",
      "Train Epoch: 5 [2560/10000 (25%)]\tLoss: 1.816597 testing loss: tensor(1.9557)\n",
      "Train Epoch: 5 [5120/10000 (51%)]\tLoss: 1.729219 testing loss: tensor(1.9582)\n",
      "Train Epoch: 5 [7680/10000 (76%)]\tLoss: 2.008591 testing loss: tensor(1.9534)\n",
      "penalty: 0.03913591802120209\n",
      "NN 6 : tensor(1.9478)\n",
      "CS 6 : 2.6341\n",
      "DP 6 : 1.9423\n",
      "heuristic 6 : 2.6955\n",
      "DP: 1.9375001192092896\n",
      "tensor([1.1192e-08, 4.8911e-01, 3.4246e-11, 5.0992e-01, 9.6732e-04])\n",
      "tensor([1.2891e-05, 4.5227e-01, 1.1736e-07, 5.4772e-01, 1.0000e+00])\n",
      "tensor([0.2418, 0.7567, 0.0015, 1.0000, 1.0000])\n",
      "tensor([0.2314, 0.7686, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: U-exponential dp\n",
      "Train Epoch: 1 [0/10000 (0%)]\tLoss: 0.033303\n",
      "Train Epoch: 1 [2560/10000 (25%)]\tLoss: 0.009443\n",
      "Train Epoch: 1 [5120/10000 (51%)]\tLoss: 0.004115\n",
      "Train Epoch: 1 [7680/10000 (76%)]\tLoss: 0.001355\n",
      "Train Epoch: 2 [0/10000 (0%)]\tLoss: 0.000417\n",
      "Train Epoch: 2 [2560/10000 (25%)]\tLoss: 0.000201\n",
      "Train Epoch: 2 [5120/10000 (51%)]\tLoss: 0.000122\n",
      "Train Epoch: 2 [7680/10000 (76%)]\tLoss: 0.000086\n",
      "Train Epoch: 3 [0/10000 (0%)]\tLoss: 0.000065\n",
      "Train Epoch: 3 [2560/10000 (25%)]\tLoss: 0.000042\n",
      "Train Epoch: 3 [5120/10000 (51%)]\tLoss: 0.000041\n",
      "Train Epoch: 3 [7680/10000 (76%)]\tLoss: 0.000030\n",
      "Train Epoch: 4 [0/10000 (0%)]\tLoss: 0.000029\n",
      "Train Epoch: 4 [2560/10000 (25%)]\tLoss: 0.000021\n",
      "Train Epoch: 4 [5120/10000 (51%)]\tLoss: 0.000019\n",
      "Train Epoch: 4 [7680/10000 (76%)]\tLoss: 0.000016\n",
      "Train Epoch: 5 [0/10000 (0%)]\tLoss: 0.000015\n",
      "Train Epoch: 5 [2560/10000 (25%)]\tLoss: 0.000013\n",
      "Train Epoch: 5 [5120/10000 (51%)]\tLoss: 0.000010\n",
      "Train Epoch: 5 [7680/10000 (76%)]\tLoss: 0.000010\n",
      "NN 1 : tensor(1.9710)\n",
      "CS 1 : 2.6341\n",
      "DP 1 : 1.9423\n",
      "heuristic 1 : 2.6955\n",
      "DP: 1.9375001192092896\n",
      "tensor([4.9699e-01, 4.9715e-01, 5.6943e-03, 1.6955e-04, 1.3722e-06])\n",
      "tensor([4.8187e-01, 4.9136e-01, 2.6139e-02, 6.2888e-04, 1.0000e+00])\n",
      "tensor([0.4826, 0.4919, 0.0255, 1.0000, 1.0000])\n",
      "tensor([0.5039, 0.4961, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/10000 (0%)]\tLoss: 1.869728 testing loss: tensor(1.9568)\n",
      "Train Epoch: 1 [2560/10000 (25%)]\tLoss: 1.961102 testing loss: tensor(1.9665)\n",
      "Train Epoch: 1 [5120/10000 (51%)]\tLoss: 2.067348 testing loss: tensor(1.9548)\n",
      "Train Epoch: 1 [7680/10000 (76%)]\tLoss: 1.948152 testing loss: tensor(1.9461)\n",
      "penalty: 0.07317239046096802\n",
      "NN 2 : tensor(1.9538)\n",
      "CS 2 : 2.6341\n",
      "DP 2 : 1.9423\n",
      "heuristic 2 : 2.6955\n",
      "DP: 1.9375001192092896\n",
      "tensor([4.4582e-01, 5.5290e-01, 1.2680e-03, 8.8996e-06, 1.7759e-08])\n",
      "tensor([4.5027e-01, 5.4271e-01, 6.9729e-03, 5.1931e-05, 1.0000e+00])\n",
      "tensor([0.4370, 0.5570, 0.0060, 1.0000, 1.0000])\n",
      "tensor([0.3770, 0.6230, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 2 [0/10000 (0%)]\tLoss: 2.062842 testing loss: tensor(1.9576)\n",
      "Train Epoch: 2 [2560/10000 (25%)]\tLoss: 1.815319 testing loss: tensor(1.9496)\n",
      "Train Epoch: 2 [5120/10000 (51%)]\tLoss: 1.993810 testing loss: tensor(1.9491)\n",
      "Train Epoch: 2 [7680/10000 (76%)]\tLoss: 1.970006 testing loss: tensor(1.9463)\n",
      "penalty: 0.05799335241317749\n",
      "NN 3 : tensor(1.9509)\n",
      "CS 3 : 2.6341\n",
      "DP 3 : 1.9423\n",
      "heuristic 3 : 2.6955\n",
      "DP: 1.9375001192092896\n",
      "tensor([2.0899e-01, 7.9065e-01, 3.5713e-04, 2.6730e-06, 5.4266e-09])\n",
      "tensor([2.1814e-01, 7.8016e-01, 1.6838e-03, 1.7714e-05, 1.0000e+00])\n",
      "tensor([0.2325, 0.7655, 0.0020, 1.0000, 1.0000])\n",
      "tensor([0.2605, 0.7395, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 3 [0/10000 (0%)]\tLoss: 1.904994 testing loss: tensor(1.9737)\n",
      "Train Epoch: 3 [2560/10000 (25%)]\tLoss: 1.972361 testing loss: tensor(1.9473)\n",
      "Train Epoch: 3 [5120/10000 (51%)]\tLoss: 1.903754 testing loss: tensor(1.9442)\n",
      "Train Epoch: 3 [7680/10000 (76%)]\tLoss: 1.797510 testing loss: tensor(1.9437)\n",
      "penalty: 0.04059342294931412\n",
      "NN 4 : tensor(1.9440)\n",
      "CS 4 : 2.6341\n",
      "DP 4 : 1.9423\n",
      "heuristic 4 : 2.6955\n",
      "DP: 1.9375001192092896\n",
      "tensor([6.4894e-01, 3.5066e-01, 3.9756e-04, 2.3662e-06, 2.7347e-09])\n",
      "tensor([6.4770e-01, 3.5054e-01, 1.7419e-03, 1.4810e-05, 1.0000e+00])\n",
      "tensor([0.6400, 0.3582, 0.0019, 1.0000, 1.0000])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6341, 0.3659, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 4 [0/10000 (0%)]\tLoss: 1.845292 testing loss: tensor(1.9504)\n",
      "Train Epoch: 4 [2560/10000 (25%)]\tLoss: 1.970660 testing loss: tensor(1.9470)\n",
      "Train Epoch: 4 [5120/10000 (51%)]\tLoss: 2.003734 testing loss: tensor(1.9444)\n",
      "Train Epoch: 4 [7680/10000 (76%)]\tLoss: 1.645297 testing loss: tensor(1.9426)\n",
      "penalty: 0.05506784841418266\n",
      "NN 5 : tensor(1.9445)\n",
      "CS 5 : 2.6341\n",
      "DP 5 : 1.9423\n",
      "heuristic 5 : 2.6955\n",
      "DP: 1.9375001192092896\n",
      "tensor([4.5055e-01, 5.4919e-01, 2.5560e-04, 1.5900e-06, 1.6663e-09])\n",
      "tensor([4.5229e-01, 5.4651e-01, 1.1880e-03, 1.1645e-05, 1.0000e+00])\n",
      "tensor([0.4537, 0.5449, 0.0014, 1.0000, 1.0000])\n",
      "tensor([0.4666, 0.5334, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 5 [0/10000 (0%)]\tLoss: 2.052033 testing loss: tensor(1.9612)\n",
      "Train Epoch: 5 [2560/10000 (25%)]\tLoss: 2.076589 testing loss: tensor(1.9439)\n",
      "Train Epoch: 5 [5120/10000 (51%)]\tLoss: 1.932214 testing loss: tensor(1.9468)\n",
      "Train Epoch: 5 [7680/10000 (76%)]\tLoss: 2.145993 testing loss: tensor(1.9429)\n",
      "penalty: 0.04115447402000427\n",
      "NN 6 : tensor(1.9407)\n",
      "CS 6 : 2.6341\n",
      "DP 6 : 1.9423\n",
      "heuristic 6 : 2.6955\n",
      "DP: 1.9375001192092896\n",
      "tensor([6.2641e-01, 3.7316e-01, 4.2034e-04, 1.9721e-06, 7.9296e-10])\n",
      "tensor([6.2202e-01, 3.7608e-01, 1.8912e-03, 1.3462e-05, 1.0000e+00])\n",
      "tensor([0.6188, 0.3794, 0.0019, 1.0000, 1.0000])\n",
      "tensor([0.6092, 0.3908, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: U-exponential random initializing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "NN 1 : tensor(2.6188)\n",
      "CS 1 : 2.6341\n",
      "DP 1 : 1.9423\n",
      "heuristic 1 : 2.6955\n",
      "DP: 1.9375001192092896\n",
      "tensor([0.3218, 0.1593, 0.2058, 0.1612, 0.1519])\n",
      "tensor([0.3582, 0.1861, 0.2465, 0.2091, 1.0000])\n",
      "tensor([0.5013, 0.2131, 0.2856, 1.0000, 1.0000])\n",
      "tensor([0.6784, 0.3216, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/10000 (0%)]\tLoss: 2.575697 testing loss: tensor(2.5968)\n",
      "Train Epoch: 1 [2560/10000 (25%)]\tLoss: 2.076586 testing loss: tensor(2.1923)\n",
      "Train Epoch: 1 [5120/10000 (51%)]\tLoss: 2.311751 testing loss: tensor(2.1505)\n",
      "Train Epoch: 1 [7680/10000 (76%)]\tLoss: 2.057197 testing loss: tensor(2.1071)\n",
      "penalty: 0.015449315309524536\n",
      "NN 2 : tensor(2.0835)\n",
      "CS 2 : 2.6341\n",
      "DP 2 : 1.9423\n",
      "heuristic 2 : 2.6955\n",
      "DP: 1.9375001192092896\n",
      "tensor([8.4131e-01, 5.6183e-04, 1.4650e-01, 1.3798e-03, 1.0243e-02])\n",
      "tensor([0.8408, 0.0027, 0.1509, 0.0056, 1.0000])\n",
      "tensor([0.8381, 0.0077, 0.1542, 1.0000, 1.0000])\n",
      "tensor([0.9227, 0.0773, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 2 [0/10000 (0%)]\tLoss: 2.053191 testing loss: tensor(2.0816)\n",
      "Train Epoch: 2 [2560/10000 (25%)]\tLoss: 2.146663 testing loss: tensor(2.0470)\n",
      "Train Epoch: 2 [5120/10000 (51%)]\tLoss: 2.091110 testing loss: tensor(2.0292)\n",
      "Train Epoch: 2 [7680/10000 (76%)]\tLoss: 1.757222 testing loss: tensor(2.0297)\n",
      "penalty: 0.048071932047605515\n",
      "NN 3 : tensor(2.0014)\n",
      "CS 3 : 2.6341\n",
      "DP 3 : 1.9423\n",
      "heuristic 3 : 2.6955\n",
      "DP: 1.9375001192092896\n",
      "tensor([7.5049e-01, 1.4127e-05, 2.4251e-01, 2.5348e-04, 6.7338e-03])\n",
      "tensor([7.5932e-01, 3.7161e-04, 2.3459e-01, 5.7190e-03, 1.0000e+00])\n",
      "tensor([0.7664, 0.0054, 0.2283, 1.0000, 1.0000])\n",
      "tensor([0.8401, 0.1599, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 3 [0/10000 (0%)]\tLoss: 2.073087 testing loss: tensor(2.0027)\n",
      "Train Epoch: 3 [2560/10000 (25%)]\tLoss: 2.218195 testing loss: tensor(2.0020)\n",
      "Train Epoch: 3 [5120/10000 (51%)]\tLoss: 1.975111 testing loss: tensor(1.9892)\n",
      "Train Epoch: 3 [7680/10000 (76%)]\tLoss: 2.037301 testing loss: tensor(1.9823)\n",
      "penalty: 0.0364157110452652\n",
      "NN 4 : tensor(1.9776)\n",
      "CS 4 : 2.6341\n",
      "DP 4 : 1.9423\n",
      "heuristic 4 : 2.6955\n",
      "DP: 1.9375001192092896\n",
      "tensor([7.6524e-01, 1.3396e-07, 2.3214e-01, 1.1227e-05, 2.6112e-03])\n",
      "tensor([7.5835e-01, 1.7881e-05, 2.4051e-01, 1.1153e-03, 1.0000e+00])\n",
      "tensor([7.4789e-01, 7.7789e-04, 2.5133e-01, 1.0000e+00, 1.0000e+00])\n",
      "tensor([0.8440, 0.1560, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 4 [0/10000 (0%)]\tLoss: 1.930408 testing loss: tensor(1.9841)\n",
      "Train Epoch: 4 [2560/10000 (25%)]\tLoss: 2.104358 testing loss: tensor(1.9688)\n",
      "Train Epoch: 4 [5120/10000 (51%)]\tLoss: 1.873129 testing loss: tensor(1.9608)\n",
      "Train Epoch: 4 [7680/10000 (76%)]\tLoss: 1.925643 testing loss: tensor(1.9602)\n",
      "penalty: 0.027373015880584717\n",
      "NN 5 : tensor(1.9547)\n",
      "CS 5 : 2.6341\n",
      "DP 5 : 1.9423\n",
      "heuristic 5 : 2.6955\n",
      "DP: 1.9375001192092896\n",
      "tensor([6.5900e-01, 4.0364e-09, 3.3919e-01, 1.4985e-06, 1.8036e-03])\n",
      "tensor([7.0247e-01, 2.5679e-06, 2.9699e-01, 5.3337e-04, 1.0000e+00])\n",
      "tensor([7.2285e-01, 2.5127e-04, 2.7690e-01, 1.0000e+00, 1.0000e+00])\n",
      "tensor([0.7690, 0.2310, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 5 [0/10000 (0%)]\tLoss: 1.984011 testing loss: tensor(1.9576)\n",
      "Train Epoch: 5 [2560/10000 (25%)]\tLoss: 1.878650 testing loss: tensor(1.9536)\n",
      "Train Epoch: 5 [5120/10000 (51%)]\tLoss: 1.978780 testing loss: tensor(1.9534)\n",
      "Train Epoch: 5 [7680/10000 (76%)]\tLoss: 1.906180 testing loss: tensor(1.9499)\n",
      "penalty: 0.04683980345726013\n",
      "NN 6 : tensor(1.9521)\n",
      "CS 6 : 2.6341\n",
      "DP 6 : 1.9423\n",
      "heuristic 6 : 2.6955\n",
      "DP: 1.9375001192092896\n",
      "tensor([7.2596e-01, 2.7982e-10, 2.7281e-01, 2.5832e-07, 1.2301e-03])\n",
      "tensor([7.1370e-01, 3.1428e-07, 2.8616e-01, 1.4064e-04, 1.0000e+00])\n",
      "tensor([6.9578e-01, 5.7715e-05, 3.0416e-01, 1.0000e+00, 1.0000e+00])\n",
      "tensor([0.7842, 0.2158, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.1)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "    \n",
    "for order in stage:\n",
    "    producedata(order)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "    #running\n",
    "    for order1 in order1name:\n",
    "        print(\"Supervised Aim:\",order,order1)\n",
    "        \n",
    "        # for mapping binary to payments before softmax\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(n, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, n),\n",
    "        )\n",
    "        model.apply(init_weights)\n",
    "        # optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        runningLossNN = []\n",
    "        runningLossCS = []\n",
    "        runningLossDP = []\n",
    "        runningLossHeuristic = []\n",
    "        #model=torch.load(\"save/pytorchNN=5dp1\");\n",
    "        #model.eval()\n",
    "        ##order1name=[\"costsharing\",\"dp\",\"heuristic\",\"random initializing\"]\n",
    "        for epoch in range(1, supervisionEpochs + 1):\n",
    "#             print(\"distributionRatio\",distributionRatio)\n",
    "            if(order1==\"costsharing\"):\n",
    "                supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "            elif(order1==\"dp\"):\n",
    "                supervisionTrain(epoch, dpSupervisionRule)\n",
    "            elif(order1==\"heuristic\"):\n",
    "                supervisionTrain(epoch, heuristicSupervisionRule)\n",
    "            elif(order1==\"random initializing\"):\n",
    "                print(\"do nothing\");\n",
    "\n",
    "        test()\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(epoch)\n",
    "            test()\n",
    "        losslistname.append(order+\" \"+order1);\n",
    "        losslist.append(losslisttemp);\n",
    "        losslisttemp=[];\n",
    "        savepath=\"save/pytorchNN=5all-phoenix\"+order+str(order1)\n",
    "        torch.save(model, savepath);\n",
    "        print(\"end\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colorlist=[\"#D2691E\",'#4169E1',\"#9ACD32\",\"#B22222\",\"#FF00FF\",\"#708090\"]\n",
    "for i in range(len(losslist)):\n",
    "    plt.plot(losslist[i], 'ro-', color=colorlist[i], alpha=0.8, label=losslistname[i])\n",
    "\n",
    "# 显示标签，如果不加这句，即使加了label='一些数字'的参数，最终还是不会显示标签\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel('trianing times')\n",
    "plt.ylabel('delay')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
