{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3540388371733616\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from matplotlib import pylab\n",
    "#from random import randint\n",
    "import random\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "from pylab import *\n",
    "from scipy.stats import beta\n",
    "import math\n",
    "n = 5\n",
    "epochs = 5\n",
    "supervisionEpochs = 10\n",
    "lr = 0.0005\n",
    "log_interval = 20\n",
    "trainSize = 60000#100000\n",
    "percentage_train_test= 0.25\n",
    "penaltyLambda = 10\n",
    "doublePeakHighMean = 0.9\n",
    "doublePeakLowMean = 0.1\n",
    "doublePeakStd = 0.1\n",
    "uniformlow=0\n",
    "uniformhigh=1.0\n",
    "normalloc = 0.2\n",
    "normalscale = 0.1\n",
    "\n",
    "cauchyloc = 1/n\n",
    "cauchyscalen = 0.004\n",
    "\n",
    "exponentialhigh = 15 #Symbol(\"b\", real=True)\n",
    "exponentiallow  = 15 #Symbol(\"a\", real=True)\n",
    "\n",
    "\n",
    "beta_a = 0.1\n",
    "beta_b  = 0.1\n",
    "kumaraswamy_a = beta_a \n",
    "kumaraswamy_b = (1.0+(beta_a-1.0)*math.pow( (beta_a+beta_b-2.0)/(beta_a-1.0), beta_a) )/beta_a \n",
    "print(kumaraswamy_b)\n",
    "\n",
    "independentnormalloc1=[(float(ii)+1)/(2*n+1) for ii in range(n,0,-1)]\n",
    "independentnormalscale1=[0.05 for ii in range(n)]\n",
    "\n",
    "independentnormalloc2=[(float(ii)+1)/(2*n+1) for ii in range(1,n+1,1)]\n",
    "independentnormalscale2=[0.05 for ii in range(n)]\n",
    "stage=[\"beta\"]\n",
    "# \"twopeak\",\"normal\",\"uniform\",\"independent1\",\"independent2\",\"cauchy\",\"beta\",\"U-exponential\",\"arcsine\"\n",
    "order1name=[\"dp\",\"random initializing\",\"costsharing\"]\n",
    "# \"costsharing\",\"dp\",\"heuristic\",\"random initializing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio1 = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase1 = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "d3 = D.normal.Normal(normalloc, normalscale)\n",
    "distributionRatio3 = d3.cdf(1) - d3.cdf(0)\n",
    "distributionBase3 = d3.cdf(0)\n",
    "\n",
    "d4 = D.uniform.Uniform(uniformlow,uniformhigh)\n",
    "distributionRatio4 = d4.cdf(1) - d4.cdf(0)\n",
    "distributionBase4 = d4.cdf(0)\n",
    "\n",
    "d5 = [D.normal.Normal(independentnormalloc1[ii], independentnormalscale1[ii]) for ii in range(n)]\n",
    "d6 = [D.normal.Normal(independentnormalloc2[ii], independentnormalscale2[ii]) for ii in range(n)]\n",
    "\n",
    "d7 = D.cauchy.Cauchy(cauchyloc,cauchyscalen)\n",
    "\n",
    "d81 = D.exponential.Exponential(exponentiallow)\n",
    "d82 = D.exponential.Exponential(exponentialhigh)\n",
    "\n",
    "d9 = D.beta.Beta(beta_a,beta_b)\n",
    "#sample_d9=d9.rsample(torch.Size([100000]))\n",
    "\n",
    "#d9_sample=np.linspace(0.0001, 0.9999, 10000)\n",
    "#d9_pdf=torch.exp(d9.log_prob(torch.tensor(d9_sample,dtype=torch.float32)))\n",
    "#d9_delta=d9_sample[1]-d9_sample[0]\n",
    "#d9_sum_pdf=torch.sum(d9_pdf*d9_delta)\n",
    "\n",
    "d10 = D.beta.Beta(0.5,0.5)\n",
    "\n",
    "def cdf(x,y, i=None):\n",
    "    if(y==\"twopeak\"):\n",
    "        return (d1.cdf(x) + d2.cdf(x) - distributionBase1) / 2 / distributionRatio1\n",
    "    elif(y==\"normal\"):\n",
    "        return (d3.cdf(x)-distributionBase3)/distributionRatio3;\n",
    "    elif(y==\"uniform\"):\n",
    "        return (d4.cdf(x)-distributionBase4)/distributionRatio4;\n",
    "    elif(y==\"independent1\"):\n",
    "        return d5[i].cdf(x);\n",
    "    elif(y==\"independent2\"):\n",
    "        return d6[i].cdf(x);\n",
    "    elif(y==\"cauchy\"):\n",
    "        return d7.cdf(x);\n",
    "    elif(y==\"beta\"):\n",
    "#         sum_cdf=0.0;\n",
    "#         if(x<0.0001):\n",
    "#             x=0.00011;\n",
    "#         if(x>0.9999):\n",
    "#             x=0.99989;\n",
    "#         for i in range(len(d9_pdf)):\n",
    "#             if(d9_sample[i]<x):\n",
    "#                 sum_cdf+=d9_pdf[i]*d9_delta;\n",
    "#             else:\n",
    "#                 sum_cdf+=(d9_pdf[i]+d9_pdf[i-1])/ 2 *(x-d9_sample[i-1])\n",
    "#                 break;\n",
    "#         return sum_cdf/d9_sum_pdf\n",
    "#         cdf_v=torch.sum((sample_d9<(x)), dtype=torch.float32)/100000\n",
    "#         return cdf_v\n",
    "#    F(x|a,b)=1–(1–x^a)^b\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(x,kumaraswamy_a),kumaraswamy_b);\n",
    "        except:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(torch.tensor(x,dtype=torch.float32),kumaraswamy_a),kumaraswamy_b);\n",
    "    elif(y==\"arcsine\"):\n",
    "        #\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            res=2.0/math.pi * torch.asin(torch.sqrt(x))\n",
    "            #print(x)\n",
    "            return res# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "        except:\n",
    "            return 2.0/math.pi * torch.asin(torch.sqrt(torch.tensor(x,dtype=torch.float32)))# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "    elif(y==\"U-exponential\"):\n",
    "        return (d81.cdf(x) + (1.0 - d82.cdf(1.0-x)))  / 2 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"beta_a\",beta_a, \"beta_b\",beta_b)\n",
    "# print(\"kumaraswamy_a\",kumaraswamy_a, \"kumaraswamy_b\",kumaraswamy_b)\n",
    "# samples1 = np.random.uniform(\n",
    "#     uniformlow, uniformhigh, size=(trainSize, n)\n",
    "# )\n",
    "# for i in range(trainSize):\n",
    "#     for j in range(n):\n",
    "#         samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "#         while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "#             samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "\n",
    "# samplesJoint = samples1\n",
    "# tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "\n",
    "# dpPrecision=100\n",
    "# # howManyPpl left, money left, yes already\n",
    "# dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "# decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "# # ppl = 0 left\n",
    "# for yes in range(n + 1):\n",
    "#     for money in range(dpPrecision + 1):\n",
    "#         if money == 0:\n",
    "#             dp[0, 0, yes] = 0\n",
    "#         else:\n",
    "#             dp[0, money, yes] = yes#+ 1.0\n",
    "# order=\"beta\"\n",
    "# for ppl in range(1,  n + 1):\n",
    "#     for yes in range(n + 1):\n",
    "#         for money in range(dpPrecision + 1):\n",
    "#             minSoFar = 1000000\n",
    "#             for offerIndex in range(money + 1):\n",
    "#                 offer = float(offerIndex) / dpPrecision\n",
    "#                 if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "#                     res = (1 - cdf(offer,order)) * dp[\n",
    "#                     ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "#                     ] + cdf(offer,order) * (1.0 + dp[ppl - 1, money, yes])\n",
    "#                 else:\n",
    "#                     res = (1 - cdf(offer,order,n-ppl)) * dp[\n",
    "#                     ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "#                     ] + cdf(offer,order,n-ppl) * (1.0 + dp[ppl - 1, money, yes])\n",
    "#                 if minSoFar > res.item():\n",
    "#                     minSoFar = res.item()\n",
    "#                     decision[ppl, money, yes] = offerIndex\n",
    "#             dp[ppl, money, yes] = minSoFar\n",
    "\n",
    "# print(\"dp\",dp[n, dpPrecision, 0])\n",
    "\n",
    "# def plan_dp(temp):\n",
    "#     #print(temp)\n",
    "#     remain=dpPrecision\n",
    "#     yes=0;\n",
    "#     ans =0;\n",
    "#     o_list=[];\n",
    "#     remain_list=[];\n",
    "#     for ppl in range(n,0,-1):\n",
    "#         o=decision[ppl, remain, yes]\n",
    "#         #print(o,remain)\n",
    "#         o_list.append(o)\n",
    "#         remain_list.append(remain);\n",
    "#         if(o<temp[n-ppl]):\n",
    "#             remain-=int(o);\n",
    "#             yes+=1;\n",
    "#         elif (remain>0):\n",
    "#             ans+=1;\n",
    "#     if(remain<=0):\n",
    "#         return ans,o_list;\n",
    "#     else:\n",
    "#         return n,o_list;\n",
    "# ans_list=[];\n",
    "# for i in range(10000):\n",
    "#     temp=samplesJoint[i]*dpPrecision\n",
    "#     #print(temp)\n",
    "#     ans_list.append(plan_dp(temp)[0]);\n",
    "#     #print(\"\\n\",temp)\n",
    "#     #print(plan_dp(temp))\n",
    "# print(sum(ans_list)/len(ans_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    if torch.sum(bits).item() == 0:\n",
    "        return torch.ones(n)\n",
    "    bits = bits.type(torch.float32)\n",
    "    negBits = torch.ones(n) - bits\n",
    "    payments = model(bits)\n",
    "    payments = payments - 1000 * negBits\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    payments = payments + negBits\n",
    "    return payments\n",
    "\n",
    "def tpToBits(tp, bits=torch.ones(n).type(torch.uint8)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = (tp >= payments).type(torch.uint8)\n",
    "    if torch.equal(newBits, bits):\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits(tp, newBits)#bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    return n - torch.sum(tpToBits(tp).type(torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    global  dp,decision\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money, yes]\n",
    "        offer = float(offerIndex) / dpPrecision\n",
    "        if tp[i] >= offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits[i] = 0\n",
    "            payments[i] = 1\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "#     print()\n",
    "#     print(tp)\n",
    "#     print(bits)\n",
    "#     print(payments)\n",
    "#     print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def heuristicSupervisionRule(tp):\n",
    "    global  dp_H,decision_H\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    tempold=-1;\n",
    "    for turns in range(n,0,-1):\n",
    "        money = dpPrecision\n",
    "        j=0\n",
    "        tempo=sum(bits)\n",
    "        #print(\"bits\",tempo)\n",
    "        for i in range(n):\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            \n",
    "            offerIndex = decision_H[tempo ,tempo - i, money]\n",
    "            offer = float(offerIndex) / dpPrecision\n",
    "            while(j<n):\n",
    "                if(bits[j]!=0):\n",
    "                    break;\n",
    "                j+=1;\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            if tp[j] >= offer:\n",
    "                #print(money,j,tp[j],offer)\n",
    "                money -= offerIndex\n",
    "                bits[j] = 1\n",
    "                payments[j] = offer\n",
    "            else:\n",
    "                bits[j] = 0;\n",
    "                payments[j] = 1.0;\n",
    "            j+=1;\n",
    "        #print(\"money\",money)\n",
    "        if(money==0 and tempold==tempo):\n",
    "            break;\n",
    "        tempold=tempo;\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    for k in range(n, -1, -1):\n",
    "        if k == 0:\n",
    "            break\n",
    "        bits = [1 if tp[ii] >= 1.0 / k else 0 for ii in range(n)]\n",
    "        if sum(bits) == k:\n",
    "            break\n",
    "    if k == 0:\n",
    "        payments = [1 for ii in range(n)]\n",
    "    else:\n",
    "        payments = [1.0 / k if bits[ii] == 1 else 1 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    return float(n - torch.sum(costSharingSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def dpDelay(tp):\n",
    "    return float(n - torch.sum(dpSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def heuristicDelay(tp):\n",
    "    return float(n - torch.sum(heuristicSupervisionRule(tp)[0]).item())\n",
    "\n",
    "\n",
    "# templire=0;\n",
    "# num=1\n",
    "# for i in range(num):\n",
    "#     temp=torch.tensor(samplesJoint[i])\n",
    "#     #print(temp)\n",
    "#     #print(temp);\n",
    "#     #print(heuristicSupervisionRule(temp))\n",
    "#     #print(dpSupervisionRule(temp))\n",
    "#     #print(dpDelay(temp))\n",
    "#     res=dpDelay(temp)\n",
    "#     templire+=res\n",
    "#     #print(\"delay\",res)\n",
    "#     #print()\n",
    "# print(templire/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordAndReport(name, source, loss):\n",
    "    source.append(loss)\n",
    "    realLength = len(source)\n",
    "    #print(f\"{name} ({realLength}): {loss}\")\n",
    "    print(name,realLength,\":\" ,loss)\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    \n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda /100000\n",
    "        \n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "losslist=[];\n",
    "losslistname=[];\n",
    "losslisttemp=[];\n",
    "def test_batch():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lenLoss= 0\n",
    "        nnLoss = 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "            for tp in tp_batch:\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        nnLoss/=lenLoss\n",
    "    return nnLoss\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                delay1 = tpToTotalDelay(tp1)\n",
    "                delay0 = tpToTotalDelay(tp0)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                    loss = loss + (1 - cdf(offer,order)) * delay1 + cdf(offer,order) * delay0\n",
    "                else:\n",
    "                    loss = loss + (1 - cdf(offer,order,i)) * delay1 + cdf(offer,order,i) * delay0\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "    \n",
    "allBits = [torch.tensor(bits, dtype=torch.int16) for bits in itertools.product([0, 1], repeat=n)]\n",
    "\n",
    "def test():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        costSharingLoss = 0\n",
    "        dpLoss = 0\n",
    "        nnLoss = 0\n",
    "        heuristicLoss=0\n",
    "        lenLoss= 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "                heuristicLoss+= heuristicDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        costSharingLoss /= lenLoss\n",
    "        dpLoss /= lenLoss\n",
    "        nnLoss /= lenLoss\n",
    "        heuristicLoss /= lenLoss\n",
    "        #print(lenLoss)\n",
    "        losslisttemp.append(nnLoss);\n",
    "        recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "        recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "        recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "        recordAndReport(\"heuristic\", runningLossHeuristic, heuristicLoss)\n",
    "        print(\"DP:\",dp[n, dpPrecision,0])\n",
    "        #for i in range(n, 0, -1):\n",
    "        #    print(\"Heuristic:\",i,5*(1-dp_H[i, i, dpPrecision]))\n",
    "        for i in range(n, 0, -1):\n",
    "            print(\n",
    "                    tpToPayments(\n",
    "                            torch.tensor([0 if ii >= i else 1 for ii in range(n)], dtype=torch.float32)\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "dpPrecision = 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def producedata(order):\n",
    "    global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "    dpPrecision = 100 \n",
    "    if(order==\"twopeak\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = np.random.normal(\n",
    "            loc=doublePeakLowMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=doublePeakLowMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samples2 = np.random.normal(\n",
    "            loc=doublePeakHighMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = np.random.normal(\n",
    "                        loc=doublePeakHighMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "    elif(order==\"normal\"):\n",
    "        print(\"loc\",normalloc, \"scale\",normalscale)\n",
    "        samples1 = np.random.normal(\n",
    "            loc=normalloc, scale=normalscale, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=normalloc, scale=normalscale\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"uniform\"):  \n",
    "        print(\"uniformlow\",uniformlow, \"uniformhigh\",uniformhigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        uniformlow, uniformhigh\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent1\"):\n",
    "        print(\"loc\",independentnormalloc1,\"scale\",independentnormalscale1)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent2\"):\n",
    "        print(\"loc\",independentnormalloc2, \"scale\",independentnormalscale2)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"cauchy\"):\n",
    "        print(\"cauchyloc\",cauchyloc, \"cauchyscale\",cauchyscalen)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"beta\"):\n",
    "        print(\"beta_a\",beta_a, \"beta_b\",beta_b)\n",
    "        print(\"kumaraswamy_a\",kumaraswamy_a, \"kumaraswamy_b\",kumaraswamy_b)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"arcsine\"):\n",
    "        print(\"betalow\",0.5, \"betahigh\",0.5)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"U-exponential\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = d81.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d81.rsample(torch.Size([1])).numpy()\n",
    "                    \n",
    "        samples2 = d82.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        \n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = d82.rsample(torch.Size([1])).numpy()\n",
    "        samples2 = 1.0 - samples2\n",
    "        samplesJoint = signals * samples1 - (signals - 1.0) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "        \n",
    "    plt.hist(samplesJoint,bins=500)\n",
    "    fig = plt.gcf()\n",
    "    plt.show()\n",
    "    savepath=\"beta2 \"+order+str(order)\n",
    "    fig.savefig(savepath+' distribution2.png')\n",
    "    \n",
    "    tp_dataset = TensorDataset(tp_tensor[: int(trainSize * percentage_train_test)])\n",
    "    tp_dataset_testing = TensorDataset(tp_tensor[int(trainSize * (1.0-percentage_train_test)) :])\n",
    "    tp_dataloader = DataLoader(tp_dataset, batch_size=128, shuffle=True)\n",
    "    tp_dataloader_testing = DataLoader(tp_dataset_testing, batch_size=256, shuffle=False)\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "    decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for yes in range(n + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp[0, 0, yes] = 0\n",
    "            else:\n",
    "                dp[0, money, yes] = yes# + 1.0\n",
    "    for ppl in range(1,  n + 1):\n",
    "        for yes in range(n + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                minSoFar = 1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1 - cdf(offer,order)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order) * (1 + dp[ppl - 1, money, yes])\n",
    "                    else:\n",
    "                        res = (1 - cdf(offer,order,n-ppl)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order,n-ppl) * (1 + dp[ppl - 1, money, yes])\n",
    "                    if minSoFar > res.item():\n",
    "                        minSoFar = res.item()\n",
    "                        decision[ppl, money, yes] = offerIndex\n",
    "                dp[ppl, money, yes] = minSoFar\n",
    "    \n",
    "    print(\"dp\",dp[n, dpPrecision, 0])\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp_H = np.zeros([n + 1 , n + 1, dpPrecision + 1])\n",
    "    decision_H = np.zeros([n + 1 , n + 1, dpPrecision + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for i in range(1,n+1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp_H[i, 0, 0] = 1\n",
    "            else:\n",
    "                offer = money / dpPrecision\n",
    "                dp_H[i, 0, money] = 0#cdf(offer)# + 1.0\n",
    "    for i in range(1,n+1):\n",
    "        for ppl in range(1, i + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                maxSoFar = -1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1-cdf(offer,order)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    else:\n",
    "                        res = (1-cdf(offer,order,ppl-1)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    if maxSoFar < res.item():\n",
    "                        maxSoFar = res.item()\n",
    "                        decision_H[i, ppl, money] = offerIndex\n",
    "                dp_H[i, ppl, money] = maxSoFar\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_a 0.1 beta_b 0.1\n",
      "kumaraswamy_a 0.1 kumaraswamy_b 0.3540388371733616\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dp 2.8306477069854736\n",
      "Supervised Aim: beta dp\n",
      "Train Epoch: 1 [0/15000 (0%)]\tLoss: 0.029657\n",
      "Train Epoch: 1 [2560/15000 (17%)]\tLoss: 0.011544\n",
      "Train Epoch: 1 [5120/15000 (34%)]\tLoss: 0.005876\n",
      "Train Epoch: 1 [7680/15000 (51%)]\tLoss: 0.004126\n",
      "Train Epoch: 1 [10240/15000 (68%)]\tLoss: 0.002767\n",
      "Train Epoch: 1 [12800/15000 (85%)]\tLoss: 0.001005\n",
      "Train Epoch: 2 [0/15000 (0%)]\tLoss: 0.000592\n",
      "Train Epoch: 2 [2560/15000 (17%)]\tLoss: 0.000410\n",
      "Train Epoch: 2 [5120/15000 (34%)]\tLoss: 0.000216\n",
      "Train Epoch: 2 [7680/15000 (51%)]\tLoss: 0.000160\n",
      "Train Epoch: 2 [10240/15000 (68%)]\tLoss: 0.000110\n",
      "Train Epoch: 2 [12800/15000 (85%)]\tLoss: 0.000087\n",
      "Train Epoch: 3 [0/15000 (0%)]\tLoss: 0.000075\n",
      "Train Epoch: 3 [2560/15000 (17%)]\tLoss: 0.000064\n",
      "Train Epoch: 3 [5120/15000 (34%)]\tLoss: 0.000060\n",
      "Train Epoch: 3 [7680/15000 (51%)]\tLoss: 0.000045\n",
      "Train Epoch: 3 [10240/15000 (68%)]\tLoss: 0.000041\n",
      "Train Epoch: 3 [12800/15000 (85%)]\tLoss: 0.000035\n",
      "Train Epoch: 4 [0/15000 (0%)]\tLoss: 0.000030\n",
      "Train Epoch: 4 [2560/15000 (17%)]\tLoss: 0.000030\n",
      "Train Epoch: 4 [5120/15000 (34%)]\tLoss: 0.000022\n",
      "Train Epoch: 4 [7680/15000 (51%)]\tLoss: 0.000023\n",
      "Train Epoch: 4 [10240/15000 (68%)]\tLoss: 0.000021\n",
      "Train Epoch: 4 [12800/15000 (85%)]\tLoss: 0.000017\n",
      "Train Epoch: 5 [0/15000 (0%)]\tLoss: 0.000017\n",
      "Train Epoch: 5 [2560/15000 (17%)]\tLoss: 0.000015\n",
      "Train Epoch: 5 [5120/15000 (34%)]\tLoss: 0.000015\n",
      "Train Epoch: 5 [7680/15000 (51%)]\tLoss: 0.000013\n",
      "Train Epoch: 5 [10240/15000 (68%)]\tLoss: 0.000013\n",
      "Train Epoch: 5 [12800/15000 (85%)]\tLoss: 0.000012\n",
      "Train Epoch: 6 [0/15000 (0%)]\tLoss: 0.000012\n",
      "Train Epoch: 6 [2560/15000 (17%)]\tLoss: 0.000011\n",
      "Train Epoch: 6 [5120/15000 (34%)]\tLoss: 0.000009\n",
      "Train Epoch: 6 [7680/15000 (51%)]\tLoss: 0.000009\n",
      "Train Epoch: 6 [10240/15000 (68%)]\tLoss: 0.000008\n",
      "Train Epoch: 6 [12800/15000 (85%)]\tLoss: 0.000008\n",
      "Train Epoch: 7 [0/15000 (0%)]\tLoss: 0.000007\n",
      "Train Epoch: 7 [2560/15000 (17%)]\tLoss: 0.000006\n",
      "Train Epoch: 7 [5120/15000 (34%)]\tLoss: 0.000007\n",
      "Train Epoch: 7 [7680/15000 (51%)]\tLoss: 0.000006\n",
      "Train Epoch: 7 [10240/15000 (68%)]\tLoss: 0.000006\n",
      "Train Epoch: 7 [12800/15000 (85%)]\tLoss: 0.000006\n",
      "Train Epoch: 8 [0/15000 (0%)]\tLoss: 0.000006\n",
      "Train Epoch: 8 [2560/15000 (17%)]\tLoss: 0.000006\n",
      "Train Epoch: 8 [5120/15000 (34%)]\tLoss: 0.000006\n",
      "Train Epoch: 8 [7680/15000 (51%)]\tLoss: 0.000005\n",
      "Train Epoch: 8 [10240/15000 (68%)]\tLoss: 0.000006\n",
      "Train Epoch: 8 [12800/15000 (85%)]\tLoss: 0.000009\n",
      "Train Epoch: 9 [0/15000 (0%)]\tLoss: 0.000008\n",
      "Train Epoch: 9 [2560/15000 (17%)]\tLoss: 0.000006\n",
      "Train Epoch: 9 [5120/15000 (34%)]\tLoss: 0.000005\n",
      "Train Epoch: 9 [7680/15000 (51%)]\tLoss: 0.000004\n",
      "Train Epoch: 9 [10240/15000 (68%)]\tLoss: 0.000005\n",
      "Train Epoch: 9 [12800/15000 (85%)]\tLoss: 0.000005\n",
      "Train Epoch: 10 [0/15000 (0%)]\tLoss: 0.000005\n",
      "Train Epoch: 10 [2560/15000 (17%)]\tLoss: 0.000008\n",
      "Train Epoch: 10 [5120/15000 (34%)]\tLoss: 0.000006\n",
      "Train Epoch: 10 [7680/15000 (51%)]\tLoss: 0.000006\n",
      "Train Epoch: 10 [10240/15000 (68%)]\tLoss: 0.000008\n",
      "Train Epoch: 10 [12800/15000 (85%)]\tLoss: 0.000004\n",
      "NN 1 : tensor(2.2879)\n",
      "CS 1 : 2.4738\n",
      "DP 1 : 1.9430666666666667\n",
      "heuristic 1 : 2.0665333333333336\n",
      "DP: 2.8306477069854736\n",
      "tensor([4.9734e-01, 4.9981e-01, 2.7203e-03, 1.2555e-04, 2.9964e-06])\n",
      "tensor([4.9430e-01, 5.0093e-01, 4.6447e-03, 1.2200e-04, 1.0000e+00])\n",
      "tensor([0.4931, 0.4959, 0.0110, 1.0000, 1.0000])\n",
      "tensor([0.4978, 0.5022, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/15000 (0%)]\tLoss: 2.441929 testing loss: tensor(2.2701)\n",
      "Train Epoch: 1 [2560/15000 (17%)]\tLoss: 2.506293 testing loss: tensor(2.2451)\n",
      "Train Epoch: 1 [5120/15000 (34%)]\tLoss: 2.319394 testing loss: tensor(2.1731)\n",
      "Train Epoch: 1 [7680/15000 (51%)]\tLoss: 2.153316 testing loss: tensor(2.1371)\n",
      "Train Epoch: 1 [10240/15000 (68%)]\tLoss: 2.271091 testing loss: tensor(2.1060)\n",
      "Train Epoch: 1 [12800/15000 (85%)]\tLoss: 2.285394 testing loss: tensor(2.0970)\n",
      "penalty: 0.1739102602005005\n",
      "NN 2 : tensor(2.0733)\n",
      "CS 2 : 2.4738\n",
      "DP 2 : 1.9430666666666667\n",
      "heuristic 2 : 2.0665333333333336\n",
      "DP: 2.8306477069854736\n",
      "tensor([5.3405e-01, 4.6594e-01, 1.2020e-06, 4.9580e-11, 3.1393e-14])\n",
      "tensor([5.3455e-01, 4.6544e-01, 7.8970e-06, 6.2032e-10, 1.0000e+00])\n",
      "tensor([5.2582e-01, 4.7415e-01, 3.3464e-05, 1.0000e+00, 1.0000e+00])\n",
      "tensor([0.5030, 0.4970, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 2 [0/15000 (0%)]\tLoss: 2.155375 testing loss: tensor(2.0847)\n",
      "Train Epoch: 2 [2560/15000 (17%)]\tLoss: 2.165214 testing loss: tensor(2.0686)\n",
      "Train Epoch: 2 [5120/15000 (34%)]\tLoss: 2.268757 testing loss: tensor(2.0515)\n",
      "Train Epoch: 2 [7680/15000 (51%)]\tLoss: 2.116325 testing loss: tensor(2.0375)\n",
      "Train Epoch: 2 [10240/15000 (68%)]\tLoss: 2.057665 testing loss: tensor(2.0281)\n",
      "Train Epoch: 2 [12800/15000 (85%)]\tLoss: 2.097810 testing loss: tensor(2.0229)\n",
      "penalty: 0.13246303796768188\n",
      "NN 3 : tensor(2.0198)\n",
      "CS 3 : 2.4738\n",
      "DP 3 : 1.9430666666666667\n",
      "heuristic 3 : 2.0665333333333336\n",
      "DP: 2.8306477069854736\n",
      "tensor([4.8360e-01, 5.1640e-01, 8.0639e-08, 3.8413e-14, 1.8725e-18])\n",
      "tensor([4.8312e-01, 5.1688e-01, 7.9371e-07, 7.8005e-13, 1.0000e+00])\n",
      "tensor([4.8338e-01, 5.1662e-01, 5.4578e-06, 1.0000e+00, 1.0000e+00])\n",
      "tensor([0.4874, 0.5126, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 3 [0/15000 (0%)]\tLoss: 2.451334 testing loss: tensor(2.0249)\n",
      "Train Epoch: 3 [2560/15000 (17%)]\tLoss: 2.160666 testing loss: tensor(2.0228)\n",
      "Train Epoch: 3 [5120/15000 (34%)]\tLoss: 2.146478 testing loss: tensor(2.0181)\n",
      "Train Epoch: 3 [7680/15000 (51%)]\tLoss: 2.080301 testing loss: tensor(2.0155)\n",
      "Train Epoch: 3 [10240/15000 (68%)]\tLoss: 2.048267 testing loss: tensor(2.0116)\n",
      "Train Epoch: 3 [12800/15000 (85%)]\tLoss: 2.086460 testing loss: tensor(2.0130)\n",
      "penalty: 0.0998656153678894\n",
      "NN 4 : tensor(2.0100)\n",
      "CS 4 : 2.4738\n",
      "DP 4 : 1.9430666666666667\n",
      "heuristic 4 : 2.0665333333333336\n",
      "DP: 2.8306477069854736\n",
      "tensor([5.8907e-01, 4.1093e-01, 7.2962e-08, 7.7945e-15, 1.9940e-20])\n",
      "tensor([5.8450e-01, 4.1549e-01, 7.0101e-07, 1.8264e-13, 1.0000e+00])\n",
      "tensor([5.7135e-01, 4.2865e-01, 4.8926e-06, 1.0000e+00, 1.0000e+00])\n",
      "tensor([0.5544, 0.4456, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 4 [0/15000 (0%)]\tLoss: 2.186889 testing loss: tensor(2.0122)\n",
      "Train Epoch: 4 [2560/15000 (17%)]\tLoss: 2.093589 testing loss: tensor(2.0109)\n",
      "Train Epoch: 4 [5120/15000 (34%)]\tLoss: 2.191509 testing loss: tensor(2.0097)\n",
      "Train Epoch: 4 [7680/15000 (51%)]\tLoss: 2.020594 testing loss: tensor(2.0103)\n",
      "Train Epoch: 4 [10240/15000 (68%)]\tLoss: 2.097539 testing loss: tensor(2.0097)\n",
      "Train Epoch: 4 [12800/15000 (85%)]\tLoss: 2.094939 testing loss: tensor(2.0160)\n",
      "penalty: 0.2200498878955841\n",
      "NN 5 : tensor(2.0215)\n",
      "CS 5 : 2.4738\n",
      "DP 5 : 1.9430666666666667\n",
      "heuristic 5 : 2.0665333333333336\n",
      "DP: 2.8306477069854736\n",
      "tensor([5.3521e-01, 4.6479e-01, 2.5152e-08, 4.8214e-15, 5.0313e-21])\n",
      "tensor([5.3290e-01, 4.6710e-01, 1.9809e-07, 7.9616e-14, 1.0000e+00])\n",
      "tensor([5.2466e-01, 4.7534e-01, 1.5966e-06, 1.0000e+00, 1.0000e+00])\n",
      "tensor([0.5027, 0.4973, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 5 [0/15000 (0%)]\tLoss: 2.240434 testing loss: tensor(2.0487)\n",
      "Train Epoch: 5 [2560/15000 (17%)]\tLoss: 2.330976 testing loss: tensor(2.1353)\n",
      "Train Epoch: 5 [5120/15000 (34%)]\tLoss: 2.200941 testing loss: tensor(2.0424)\n",
      "Train Epoch: 5 [7680/15000 (51%)]\tLoss: 2.300884 testing loss: tensor(2.0309)\n",
      "Train Epoch: 5 [10240/15000 (68%)]\tLoss: 2.145222 testing loss: tensor(2.0201)\n",
      "Train Epoch: 5 [12800/15000 (85%)]\tLoss: 2.174637 testing loss: tensor(2.0173)\n",
      "penalty: 0.06894713640213013\n",
      "NN 6 : tensor(2.0140)\n",
      "CS 6 : 2.4738\n",
      "DP 6 : 1.9430666666666667\n",
      "heuristic 6 : 2.0665333333333336\n",
      "DP: 2.8306477069854736\n",
      "tensor([4.8415e-01, 5.1585e-01, 8.5149e-08, 2.3605e-14, 9.1564e-20])\n",
      "tensor([4.8497e-01, 5.1503e-01, 6.5471e-07, 3.5486e-13, 1.0000e+00])\n",
      "tensor([4.9401e-01, 5.0598e-01, 5.2192e-06, 1.0000e+00, 1.0000e+00])\n",
      "tensor([0.5032, 0.4968, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: beta random initializing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "NN 1 : tensor(2.4739)\n",
      "CS 1 : 2.4738\n",
      "DP 1 : 1.9430666666666667\n",
      "heuristic 1 : 2.0665333333333336\n",
      "DP: 2.8306477069854736\n",
      "tensor([0.2625, 0.2276, 0.1605, 0.2660, 0.0834])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2472, 0.2483, 0.2023, 0.3022, 1.0000])\n",
      "tensor([0.3587, 0.3448, 0.2965, 1.0000, 1.0000])\n",
      "tensor([0.4878, 0.5122, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/15000 (0%)]\tLoss: 2.659778 testing loss: tensor(2.4702)\n",
      "Train Epoch: 1 [2560/15000 (17%)]\tLoss: 2.542385 testing loss: tensor(2.4507)\n",
      "Train Epoch: 1 [5120/15000 (34%)]\tLoss: 2.639128 testing loss: tensor(2.4149)\n",
      "Train Epoch: 1 [7680/15000 (51%)]\tLoss: 2.259342 testing loss: tensor(2.3734)\n",
      "Train Epoch: 1 [10240/15000 (68%)]\tLoss: 2.326454 testing loss: tensor(2.3461)\n",
      "Train Epoch: 1 [12800/15000 (85%)]\tLoss: 2.563153 testing loss: tensor(2.3456)\n",
      "penalty: 0.08528298139572144\n",
      "NN 2 : tensor(2.3631)\n",
      "CS 2 : 2.4738\n",
      "DP 2 : 1.9430666666666667\n",
      "heuristic 2 : 2.0665333333333336\n",
      "DP: 2.8306477069854736\n",
      "tensor([3.4908e-01, 3.6267e-01, 7.2968e-09, 2.8825e-01, 6.5405e-09])\n",
      "tensor([3.5065e-01, 3.5993e-01, 3.4317e-08, 2.8941e-01, 1.0000e+00])\n",
      "tensor([5.1467e-01, 4.8533e-01, 1.1268e-06, 1.0000e+00, 1.0000e+00])\n",
      "tensor([0.5138, 0.4862, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 2 [0/15000 (0%)]\tLoss: 2.486171 testing loss: tensor(2.3669)\n",
      "Train Epoch: 2 [2560/15000 (17%)]\tLoss: 2.662212 testing loss: tensor(2.3748)\n",
      "Train Epoch: 2 [5120/15000 (34%)]\tLoss: 2.430687 testing loss: tensor(2.3700)\n",
      "Train Epoch: 2 [7680/15000 (51%)]\tLoss: 2.594055 testing loss: tensor(2.3706)\n",
      "Train Epoch: 2 [10240/15000 (68%)]\tLoss: 2.465670 testing loss: tensor(2.3675)\n",
      "Train Epoch: 2 [12800/15000 (85%)]\tLoss: 2.546257 testing loss: tensor(2.3667)\n",
      "penalty: 0.018511682748794556\n",
      "NN 3 : tensor(2.3553)\n",
      "CS 3 : 2.4738\n",
      "DP 3 : 1.9430666666666667\n",
      "heuristic 3 : 2.0665333333333336\n",
      "DP: 2.8306477069854736\n",
      "tensor([3.8541e-01, 2.4201e-01, 1.5914e-09, 3.7258e-01, 1.2391e-09])\n",
      "tensor([3.8393e-01, 2.4099e-01, 7.7667e-09, 3.7509e-01, 1.0000e+00])\n",
      "tensor([6.1664e-01, 3.8336e-01, 4.1333e-07, 1.0000e+00, 1.0000e+00])\n",
      "tensor([0.6184, 0.3816, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 3 [0/15000 (0%)]\tLoss: 2.568880 testing loss: tensor(2.3556)\n",
      "Train Epoch: 3 [2560/15000 (17%)]\tLoss: 2.446416 testing loss: tensor(2.3623)\n",
      "Train Epoch: 3 [5120/15000 (34%)]\tLoss: 2.446130 testing loss: tensor(2.3494)\n",
      "Train Epoch: 3 [7680/15000 (51%)]\tLoss: 2.188090 testing loss: tensor(2.3463)\n",
      "Train Epoch: 3 [10240/15000 (68%)]\tLoss: 2.569052 testing loss: tensor(2.3369)\n",
      "Train Epoch: 3 [12800/15000 (85%)]\tLoss: 2.458001 testing loss: tensor(2.3239)\n",
      "penalty: 0.055354148149490356\n",
      "NN 4 : tensor(2.3191)\n",
      "CS 4 : 2.4738\n",
      "DP 4 : 1.9430666666666667\n",
      "heuristic 4 : 2.0665333333333336\n",
      "DP: 2.8306477069854736\n",
      "tensor([3.6340e-01, 3.4742e-01, 4.9270e-10, 2.8917e-01, 5.3564e-10])\n",
      "tensor([3.6621e-01, 3.4503e-01, 2.2919e-09, 2.8876e-01, 1.0000e+00])\n",
      "tensor([5.2340e-01, 4.7660e-01, 1.7151e-07, 1.0000e+00, 1.0000e+00])\n",
      "tensor([0.5256, 0.4744, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 4 [0/15000 (0%)]\tLoss: 2.901200 testing loss: tensor(2.3198)\n",
      "Train Epoch: 4 [2560/15000 (17%)]\tLoss: 2.279480 testing loss: tensor(2.3003)\n",
      "Train Epoch: 4 [5120/15000 (34%)]\tLoss: 2.377826 testing loss: tensor(2.2700)\n",
      "Train Epoch: 4 [7680/15000 (51%)]\tLoss: 2.393826 testing loss: tensor(2.2587)\n",
      "Train Epoch: 4 [10240/15000 (68%)]\tLoss: 2.274383 testing loss: tensor(2.2397)\n",
      "Train Epoch: 4 [12800/15000 (85%)]\tLoss: 2.202878 testing loss: tensor(2.2170)\n",
      "penalty: 0.0674329474568367\n",
      "NN 5 : tensor(2.1963)\n",
      "CS 5 : 2.4738\n",
      "DP 5 : 1.9430666666666667\n",
      "heuristic 5 : 2.0665333333333336\n",
      "DP: 2.8306477069854736\n",
      "tensor([2.4814e-01, 2.9940e-01, 7.8588e-11, 4.5246e-01, 3.1943e-11])\n",
      "tensor([2.4985e-01, 2.9989e-01, 3.4599e-10, 4.5026e-01, 1.0000e+00])\n",
      "tensor([4.8771e-01, 5.1229e-01, 4.4532e-07, 1.0000e+00, 1.0000e+00])\n",
      "tensor([0.4886, 0.5114, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 5 [0/15000 (0%)]\tLoss: 2.463918 testing loss: tensor(2.1959)\n",
      "Train Epoch: 5 [2560/15000 (17%)]\tLoss: 2.265989 testing loss: tensor(2.1813)\n",
      "Train Epoch: 5 [5120/15000 (34%)]\tLoss: 2.363633 testing loss: tensor(2.1678)\n",
      "Train Epoch: 5 [7680/15000 (51%)]\tLoss: 2.328966 testing loss: tensor(2.1725)\n",
      "Train Epoch: 5 [10240/15000 (68%)]\tLoss: 2.358636 testing loss: tensor(2.1619)\n",
      "Train Epoch: 5 [12800/15000 (85%)]\tLoss: 2.375433 testing loss: tensor(2.1566)\n",
      "penalty: 0.02463337779045105\n",
      "NN 6 : tensor(2.1596)\n",
      "CS 6 : 2.4738\n",
      "DP 6 : 1.9430666666666667\n",
      "heuristic 6 : 2.0665333333333336\n",
      "DP: 2.8306477069854736\n",
      "tensor([2.3384e-01, 3.1388e-01, 9.9479e-12, 4.5227e-01, 5.4232e-13])\n",
      "tensor([2.3382e-01, 3.1396e-01, 5.6550e-11, 4.5222e-01, 1.0000e+00])\n",
      "tensor([4.8105e-01, 5.1895e-01, 6.2295e-07, 1.0000e+00, 1.0000e+00])\n",
      "tensor([0.4783, 0.5217, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: beta costsharing\n",
      "Train Epoch: 1 [0/15000 (0%)]\tLoss: 0.002460\n",
      "Train Epoch: 1 [2560/15000 (17%)]\tLoss: 0.000043\n",
      "Train Epoch: 1 [5120/15000 (34%)]\tLoss: 0.000019\n",
      "Train Epoch: 1 [7680/15000 (51%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [10240/15000 (68%)]\tLoss: 0.000001\n",
      "Train Epoch: 1 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "NN 1 : tensor(2.4738)\n",
      "CS 1 : 2.4738\n",
      "DP 1 : 1.9430666666666667\n",
      "heuristic 1 : 2.0665333333333336\n",
      "DP: 2.8306477069854736\n",
      "tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500, 1.0000])\n",
      "tensor([0.3333, 0.3333, 0.3333, 1.0000, 1.0000])\n",
      "tensor([0.5000, 0.5000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/15000 (0%)]\tLoss: 2.620306 testing loss: tensor(2.4757)\n",
      "Train Epoch: 1 [2560/15000 (17%)]\tLoss: 2.533772 testing loss: tensor(2.4718)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [5120/15000 (34%)]\tLoss: 2.419804 testing loss: tensor(2.3606)\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.1)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "    \n",
    "for order in stage:\n",
    "    producedata(order)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "    #running\n",
    "    for order1 in order1name:\n",
    "        print(\"Supervised Aim:\",order,order1)\n",
    "        \n",
    "        # for mapping binary to payments before softmax\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(n, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, n),\n",
    "        )\n",
    "        model.apply(init_weights)\n",
    "        # optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        runningLossNN = []\n",
    "        runningLossCS = []\n",
    "        runningLossDP = []\n",
    "        runningLossHeuristic = []\n",
    "        #model=torch.load(\"save/pytorchNN=5dp1\");\n",
    "        #model.eval()\n",
    "        ##order1name=[\"costsharing\",\"dp\",\"heuristic\",\"random initializing\"]\n",
    "        for epoch in range(1, supervisionEpochs + 1):\n",
    "#             print(\"distributionRatio\",distributionRatio)\n",
    "            if(order1==\"costsharing\"):\n",
    "                supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "            elif(order1==\"dp\"):\n",
    "                supervisionTrain(epoch, dpSupervisionRule)\n",
    "            elif(order1==\"heuristic\"):\n",
    "                supervisionTrain(epoch, heuristicSupervisionRule)\n",
    "            elif(order1==\"random initializing\"):\n",
    "                print(\"do nothing\");\n",
    "\n",
    "        test()\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(epoch)\n",
    "            test()\n",
    "        losslistname.append(order+\" \"+order1);\n",
    "        losslist.append(losslisttemp);\n",
    "        losslisttemp=[];\n",
    "        savepath=\"save/pytorchNN=5all-beta\"+order+str(order1)\n",
    "        torch.save(model, savepath);\n",
    "        print(\"end\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colorlist=[\"#D2691E\",'#4169E1',\"#9ACD32\",\"#B22222\",\"#FF00FF\",\"#708090\"]\n",
    "for i in range(len(losslist)):\n",
    "    plt.plot(losslist[i], 'ro-', color=colorlist[i], alpha=0.8, label=losslistname[i])\n",
    "\n",
    "# 显示标签，如果不加这句，即使加了label='一些数字'的参数，最终还是不会显示标签\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('trianing times')\n",
    "plt.ylabel('delay')\n",
    "\n",
    "fig = plt.gcf()\n",
    "plt.show()\n",
    "fig.savefig('result2.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
