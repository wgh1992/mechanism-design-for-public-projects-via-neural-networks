{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40058530825361593\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from random import randint\n",
    "import random\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "from pylab import *\n",
    "from scipy.stats import beta\n",
    "import math\n",
    "n = 3\n",
    "epochs = 5\n",
    "supervisionEpochs = 10\n",
    "lr = 0.0005\n",
    "log_interval = 20\n",
    "trainSize = 60000#100000\n",
    "percentage_train_test= 0.25\n",
    "penaltyLambda = 10\n",
    "doublePeakHighMean = 0.9\n",
    "doublePeakLowMean = 0.1\n",
    "doublePeakStd = 0.1\n",
    "uniformlow=0\n",
    "uniformhigh=1.0\n",
    "normalloc = 0.2\n",
    "normalscale = 0.1\n",
    "\n",
    "cauchyloc = 1/n\n",
    "cauchyscalen = 0.004\n",
    "\n",
    "exponentialhigh = 15 #Symbol(\"b\", real=True)\n",
    "exponentiallow  = 15 #Symbol(\"a\", real=True)\n",
    "\n",
    "\n",
    "beta_a = 0.3\n",
    "beta_b  = 0.2\n",
    "kumaraswamy_a = beta_a \n",
    "kumaraswamy_b = (1.0+(beta_a-1.0)*math.pow( (beta_a+beta_b-2.0)/(beta_a-1.0), beta_a) )/beta_a \n",
    "print(kumaraswamy_b)\n",
    "\n",
    "\n",
    "independentnormalloc1=[(float(ii)+1)/(2*n+1) for ii in range(n,0,-1)]\n",
    "independentnormalscale1=[0.05 for ii in range(n)]\n",
    "\n",
    "independentnormalloc2=[(float(ii)+1)/(2*n+1) for ii in range(1,n+1,1)]\n",
    "independentnormalscale2=[0.05 for ii in range(n)]\n",
    "stage=[\"beta\"]\n",
    "# \"twopeak\",\"normal\",\"uniform\",\"independent1\",\"independent2\",\"cauchy\",\"beta\",\"U-exponential\",\"arcsine\"\n",
    "order1name=[\"dp\",\"random initializing\",\"costsharing\"]\n",
    "# \"costsharing\",\"dp\",\"heuristic\",\"random initializing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio1 = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase1 = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "d3 = D.normal.Normal(normalloc, normalscale)\n",
    "distributionRatio3 = d3.cdf(1) - d3.cdf(0)\n",
    "distributionBase3 = d3.cdf(0)\n",
    "\n",
    "d4 = D.uniform.Uniform(uniformlow,uniformhigh)\n",
    "distributionRatio4 = d4.cdf(1) - d4.cdf(0)\n",
    "distributionBase4 = d4.cdf(0)\n",
    "\n",
    "d5 = [D.normal.Normal(independentnormalloc1[ii], independentnormalscale1[ii]) for ii in range(n)]\n",
    "d6 = [D.normal.Normal(independentnormalloc2[ii], independentnormalscale2[ii]) for ii in range(n)]\n",
    "\n",
    "d7 = D.cauchy.Cauchy(cauchyloc,cauchyscalen)\n",
    "\n",
    "d81 = D.exponential.Exponential(exponentiallow)\n",
    "d82 = D.exponential.Exponential(exponentialhigh)\n",
    "\n",
    "d9 = D.beta.Beta(beta_a,beta_b)\n",
    "#sample_d9=d9.rsample(torch.Size([100000]))\n",
    "\n",
    "#d9_sample=np.linspace(0.0001, 0.9999, 10000)\n",
    "#d9_pdf=torch.exp(d9.log_prob(torch.tensor(d9_sample,dtype=torch.float32)))\n",
    "#d9_delta=d9_sample[1]-d9_sample[0]\n",
    "#d9_sum_pdf=torch.sum(d9_pdf*d9_delta)\n",
    "\n",
    "d10 = D.beta.Beta(0.5,0.5)\n",
    "\n",
    "def cdf(x,y, i=None):\n",
    "    if(y==\"twopeak\"):\n",
    "        return (d1.cdf(x) + d2.cdf(x) - distributionBase1) / 2 / distributionRatio1\n",
    "    elif(y==\"normal\"):\n",
    "        return (d3.cdf(x)-distributionBase3)/distributionRatio3;\n",
    "    elif(y==\"uniform\"):\n",
    "        return (d4.cdf(x)-distributionBase4)/distributionRatio4;\n",
    "    elif(y==\"independent1\"):\n",
    "        return d5[i].cdf(x);\n",
    "    elif(y==\"independent2\"):\n",
    "        return d6[i].cdf(x);\n",
    "    elif(y==\"cauchy\"):\n",
    "        return d7.cdf(x);\n",
    "    elif(y==\"beta\"):\n",
    "#         sum_cdf=0.0;\n",
    "#         if(x<0.0001):\n",
    "#             x=0.00011;\n",
    "#         if(x>0.9999):\n",
    "#             x=0.99989;\n",
    "#         for i in range(len(d9_pdf)):\n",
    "#             if(d9_sample[i]<x):\n",
    "#                 sum_cdf+=d9_pdf[i]*d9_delta;\n",
    "#             else:\n",
    "#                 sum_cdf+=(d9_pdf[i]+d9_pdf[i-1])/ 2 *(x-d9_sample[i-1])\n",
    "#                 break;\n",
    "#         return sum_cdf/d9_sum_pdf\n",
    "#         cdf_v=torch.sum((sample_d9<(x)), dtype=torch.float32)/100000\n",
    "#         return cdf_v\n",
    "#    F(x|a,b)=1–(1–x^a)^b\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(x,kumaraswamy_a),kumaraswamy_b);\n",
    "        except:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(torch.tensor(x,dtype=torch.float32),kumaraswamy_a),kumaraswamy_b);\n",
    "    elif(y==\"arcsine\"):\n",
    "        #\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            res=2.0/math.pi * torch.asin(torch.sqrt(x))\n",
    "            #print(x)\n",
    "            return res# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "        except:\n",
    "            return 2.0/math.pi * torch.asin(torch.sqrt(torch.tensor(x,dtype=torch.float32)))# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "    elif(y==\"U-exponential\"):\n",
    "        return (d81.cdf(x) + (1.0 - d82.cdf(1.0-x)))  / 2 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    if torch.sum(bits).item() == 0:\n",
    "        return torch.ones(n)\n",
    "    bits = bits.type(torch.float32)\n",
    "    negBits = torch.ones(n) - bits\n",
    "    payments = model(bits)\n",
    "    payments = payments - 1000 * negBits\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    payments = payments + negBits\n",
    "    return payments\n",
    "\n",
    "def tpToBits(tp, bits=torch.ones(n).type(torch.uint8)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = (tp >= payments).type(torch.uint8)\n",
    "    if torch.equal(newBits, bits):\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits(tp, newBits)#bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    return n - torch.sum(tpToBits(tp).type(torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    global  dp,decision\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money, yes]\n",
    "        offer = float(offerIndex) / dpPrecision\n",
    "        if tp[i] >= offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits[i] = 0\n",
    "            payments[i] = 1\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "#     print()\n",
    "#     print(tp)\n",
    "#     print(bits)\n",
    "#     print(payments)\n",
    "#     print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def heuristicSupervisionRule(tp):\n",
    "    global  dp_H,decision_H\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    tempold=-1;\n",
    "    for turns in range(n,0,-1):\n",
    "        money = dpPrecision\n",
    "        j=0\n",
    "        tempo=sum(bits)\n",
    "        #print(\"bits\",tempo)\n",
    "        for i in range(n):\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            \n",
    "            offerIndex = decision_H[tempo ,tempo - i, money]\n",
    "            offer = float(offerIndex) / dpPrecision\n",
    "            while(j<n):\n",
    "                if(bits[j]!=0):\n",
    "                    break;\n",
    "                j+=1;\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            if tp[j] >= offer:\n",
    "                #print(money,j,tp[j],offer)\n",
    "                money -= offerIndex\n",
    "                bits[j] = 1\n",
    "                payments[j] = offer\n",
    "            else:\n",
    "                bits[j] = 0;\n",
    "                payments[j] = 1.0;\n",
    "            j+=1;\n",
    "        #print(\"money\",money)\n",
    "        if(money==0 and tempold==tempo):\n",
    "            break;\n",
    "        tempold=tempo;\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    for k in range(n, -1, -1):\n",
    "        if k == 0:\n",
    "            break\n",
    "        bits = [1 if tp[ii] >= 1.0 / k else 0 for ii in range(n)]\n",
    "        if sum(bits) == k:\n",
    "            break\n",
    "    if k == 0:\n",
    "        payments = [1 for ii in range(n)]\n",
    "    else:\n",
    "        payments = [1.0 / k if bits[ii] == 1 else 1 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    return float(n - torch.sum(costSharingSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def dpDelay(tp):\n",
    "    return float(n - torch.sum(dpSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def heuristicDelay(tp):\n",
    "    return float(n - torch.sum(heuristicSupervisionRule(tp)[0]).item())\n",
    "\n",
    "\n",
    "# templire=0;\n",
    "# num=1\n",
    "# for i in range(num):\n",
    "#     temp=torch.tensor(samplesJoint[i])\n",
    "#     #print(temp)\n",
    "#     #print(temp);\n",
    "#     #print(heuristicSupervisionRule(temp))\n",
    "#     #print(dpSupervisionRule(temp))\n",
    "#     #print(dpDelay(temp))\n",
    "#     res=dpDelay(temp)\n",
    "#     templire+=res\n",
    "#     #print(\"delay\",res)\n",
    "#     #print()\n",
    "# print(templire/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordAndReport(name, source, loss):\n",
    "    source.append(loss)\n",
    "    realLength = len(source)\n",
    "    #print(f\"{name} ({realLength}): {loss}\")\n",
    "    print(name,realLength,\":\" ,loss)\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    \n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda /100000\n",
    "        \n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "losslist=[];\n",
    "losslistname=[];\n",
    "losslisttemp=[];\n",
    "def test_batch():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lenLoss= 0\n",
    "        nnLoss = 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "            for tp in tp_batch:\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        nnLoss/=lenLoss\n",
    "    return nnLoss\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                delay1 = tpToTotalDelay(tp1)\n",
    "                delay0 = tpToTotalDelay(tp0)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                    loss = loss + (1 - cdf(offer,order)) * delay1 + cdf(offer,order) * delay0\n",
    "                else:\n",
    "                    loss = loss + (1 - cdf(offer,order,i)) * delay1 + cdf(offer,order,i) * delay0\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "    \n",
    "allBits = [torch.tensor(bits, dtype=torch.int16) for bits in itertools.product([0, 1], repeat=n)]\n",
    "\n",
    "def test():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        costSharingLoss = 0\n",
    "        dpLoss = 0\n",
    "        nnLoss = 0\n",
    "        heuristicLoss=0\n",
    "        lenLoss= 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "                heuristicLoss+= heuristicDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        costSharingLoss /= lenLoss\n",
    "        dpLoss /= lenLoss\n",
    "        nnLoss /= lenLoss\n",
    "        heuristicLoss /= lenLoss\n",
    "        #print(lenLoss)\n",
    "        losslisttemp.append(nnLoss);\n",
    "        recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "        recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "        recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "        recordAndReport(\"heuristic\", runningLossHeuristic, heuristicLoss)\n",
    "        print(\"DP:\",dp[n, dpPrecision,0])\n",
    "        #for i in range(n, 0, -1):\n",
    "        #    print(\"Heuristic:\",i,5*(1-dp_H[i, i, dpPrecision]))\n",
    "        for i in range(n, 0, -1):\n",
    "            print(\n",
    "                    tpToPayments(\n",
    "                            torch.tensor([0 if ii >= i else 1 for ii in range(n)], dtype=torch.float32)\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "dpPrecision = 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def producedata(order):\n",
    "    global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "    dpPrecision = 100 \n",
    "    if(order==\"twopeak\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = np.random.normal(\n",
    "            loc=doublePeakLowMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=doublePeakLowMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samples2 = np.random.normal(\n",
    "            loc=doublePeakHighMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = np.random.normal(\n",
    "                        loc=doublePeakHighMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "    elif(order==\"normal\"):\n",
    "        print(\"loc\",normalloc, \"scale\",normalscale)\n",
    "        samples1 = np.random.normal(\n",
    "            loc=normalloc, scale=normalscale, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=normalloc, scale=normalscale\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"uniform\"):  \n",
    "        print(\"uniformlow\",uniformlow, \"uniformhigh\",uniformhigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        uniformlow, uniformhigh\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent1\"):\n",
    "        print(\"loc\",independentnormalloc1,\"scale\",independentnormalscale1)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent2\"):\n",
    "        print(\"loc\",independentnormalloc2, \"scale\",independentnormalscale2)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"cauchy\"):\n",
    "        print(\"cauchyloc\",cauchyloc, \"cauchyscale\",cauchyscalen)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"beta\"):\n",
    "        print(\"beta_a\",beta_a, \"beta_b\",beta_b)\n",
    "        print(\"kumaraswamy_a\",kumaraswamy_a, \"kumaraswamy_b\",kumaraswamy_b)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"arcsine\"):\n",
    "        print(\"betalow\",0.5, \"betahigh\",0.5)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"U-exponential\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = d81.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d81.rsample(torch.Size([1])).numpy()\n",
    "                    \n",
    "        samples2 = d82.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        \n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = d82.rsample(torch.Size([1])).numpy()\n",
    "        samples2 = 1.0 - samples2\n",
    "        samplesJoint = signals * samples1 - (signals - 1.0) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "        \n",
    "    plt.hist(samplesJoint,bins=500)\n",
    "    plt.show()\n",
    "    \n",
    "    tp_dataset = TensorDataset(tp_tensor[: int(trainSize * percentage_train_test)])\n",
    "    tp_dataset_testing = TensorDataset(tp_tensor[int(trainSize * (1.0-percentage_train_test)) :])\n",
    "    tp_dataloader = DataLoader(tp_dataset, batch_size=128, shuffle=True)\n",
    "    tp_dataloader_testing = DataLoader(tp_dataset_testing, batch_size=256, shuffle=False)\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "    decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for yes in range(n + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp[0, 0, yes] = 0\n",
    "            else:\n",
    "                dp[0, money, yes] = yes# + 1.0\n",
    "    for ppl in range(1,  n + 1):\n",
    "        for yes in range(n + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                minSoFar = 1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1 - cdf(offer,order)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order) * (1 + dp[ppl - 1, money, yes])\n",
    "                    else:\n",
    "                        res = (1 - cdf(offer,order,n-ppl)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order,n-ppl) * (1 + dp[ppl - 1, money, yes])\n",
    "                    if minSoFar > res.item():\n",
    "                        minSoFar = res.item()\n",
    "                        decision[ppl, money, yes] = offerIndex\n",
    "                dp[ppl, money, yes] = minSoFar\n",
    "    \n",
    "    print(\"dp\",dp[n, dpPrecision, 0])\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp_H = np.zeros([n + 1 , n + 1, dpPrecision + 1])\n",
    "    decision_H = np.zeros([n + 1 , n + 1, dpPrecision + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for i in range(1,n+1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp_H[i, 0, 0] = 1\n",
    "            else:\n",
    "                offer = money / dpPrecision\n",
    "                dp_H[i, 0, money] = 0#cdf(offer)# + 1.0\n",
    "    for i in range(1,n+1):\n",
    "        for ppl in range(1, i + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                maxSoFar = -1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1-cdf(offer,order)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    else:\n",
    "                        res = (1-cdf(offer,order,ppl-1)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    if maxSoFar < res.item():\n",
    "                        maxSoFar = res.item()\n",
    "                        decision_H[i, ppl, money] = offerIndex\n",
    "                dp_H[i, ppl, money] = maxSoFar\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_a 0.3 beta_b 0.2\n",
      "kumaraswamy_a 0.3 kumaraswamy_b 0.40058530825361593\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dp 1.7036516666412354\n",
      "Supervised Aim: beta dp\n",
      "Train Epoch: 1 [0/15000 (0%)]\tLoss: 0.030208\n",
      "Train Epoch: 1 [2560/15000 (17%)]\tLoss: 0.007505\n",
      "Train Epoch: 1 [5120/15000 (34%)]\tLoss: 0.002657\n",
      "Train Epoch: 1 [7680/15000 (51%)]\tLoss: 0.000887\n",
      "Train Epoch: 1 [10240/15000 (68%)]\tLoss: 0.000270\n",
      "Train Epoch: 1 [12800/15000 (85%)]\tLoss: 0.000169\n",
      "Train Epoch: 2 [0/15000 (0%)]\tLoss: 0.000102\n",
      "Train Epoch: 2 [2560/15000 (17%)]\tLoss: 0.000098\n",
      "Train Epoch: 2 [5120/15000 (34%)]\tLoss: 0.000063\n",
      "Train Epoch: 2 [7680/15000 (51%)]\tLoss: 0.000058\n",
      "Train Epoch: 2 [10240/15000 (68%)]\tLoss: 0.000031\n",
      "Train Epoch: 2 [12800/15000 (85%)]\tLoss: 0.000028\n",
      "Train Epoch: 3 [0/15000 (0%)]\tLoss: 0.000027\n",
      "Train Epoch: 3 [2560/15000 (17%)]\tLoss: 0.000022\n",
      "Train Epoch: 3 [5120/15000 (34%)]\tLoss: 0.000016\n",
      "Train Epoch: 3 [7680/15000 (51%)]\tLoss: 0.000016\n",
      "Train Epoch: 3 [10240/15000 (68%)]\tLoss: 0.000015\n",
      "Train Epoch: 3 [12800/15000 (85%)]\tLoss: 0.000012\n",
      "Train Epoch: 4 [0/15000 (0%)]\tLoss: 0.000009\n",
      "Train Epoch: 4 [2560/15000 (17%)]\tLoss: 0.000010\n",
      "Train Epoch: 4 [5120/15000 (34%)]\tLoss: 0.000007\n",
      "Train Epoch: 4 [7680/15000 (51%)]\tLoss: 0.000008\n",
      "Train Epoch: 4 [10240/15000 (68%)]\tLoss: 0.000008\n",
      "Train Epoch: 4 [12800/15000 (85%)]\tLoss: 0.000006\n",
      "Train Epoch: 5 [0/15000 (0%)]\tLoss: 0.000007\n",
      "Train Epoch: 5 [2560/15000 (17%)]\tLoss: 0.000005\n",
      "Train Epoch: 5 [5120/15000 (34%)]\tLoss: 0.000005\n",
      "Train Epoch: 5 [7680/15000 (51%)]\tLoss: 0.000004\n",
      "Train Epoch: 5 [10240/15000 (68%)]\tLoss: 0.000004\n",
      "Train Epoch: 5 [12800/15000 (85%)]\tLoss: 0.000004\n",
      "Train Epoch: 6 [0/15000 (0%)]\tLoss: 0.000003\n",
      "Train Epoch: 6 [2560/15000 (17%)]\tLoss: 0.000004\n",
      "Train Epoch: 6 [5120/15000 (34%)]\tLoss: 0.000003\n",
      "Train Epoch: 6 [7680/15000 (51%)]\tLoss: 0.000002\n",
      "Train Epoch: 6 [10240/15000 (68%)]\tLoss: 0.000002\n",
      "Train Epoch: 6 [12800/15000 (85%)]\tLoss: 0.000002\n",
      "Train Epoch: 7 [0/15000 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 7 [2560/15000 (17%)]\tLoss: 0.000002\n",
      "Train Epoch: 7 [5120/15000 (34%)]\tLoss: 0.000002\n",
      "Train Epoch: 7 [7680/15000 (51%)]\tLoss: 0.000002\n",
      "Train Epoch: 7 [10240/15000 (68%)]\tLoss: 0.000002\n",
      "Train Epoch: 7 [12800/15000 (85%)]\tLoss: 0.000001\n",
      "Train Epoch: 8 [0/15000 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 8 [2560/15000 (17%)]\tLoss: 0.000001\n",
      "Train Epoch: 8 [5120/15000 (34%)]\tLoss: 0.000001\n",
      "Train Epoch: 8 [7680/15000 (51%)]\tLoss: 0.000001\n",
      "Train Epoch: 8 [10240/15000 (68%)]\tLoss: 0.000001\n",
      "Train Epoch: 8 [12800/15000 (85%)]\tLoss: 0.000001\n",
      "Train Epoch: 9 [0/15000 (0%)]\tLoss: 0.000001\n",
      "Train Epoch: 9 [2560/15000 (17%)]\tLoss: 0.000001\n",
      "Train Epoch: 9 [5120/15000 (34%)]\tLoss: 0.000001\n",
      "Train Epoch: 9 [7680/15000 (51%)]\tLoss: 0.000002\n",
      "Train Epoch: 9 [10240/15000 (68%)]\tLoss: 0.000001\n",
      "Train Epoch: 9 [12800/15000 (85%)]\tLoss: 0.000001\n",
      "Train Epoch: 10 [0/15000 (0%)]\tLoss: 0.000001\n",
      "Train Epoch: 10 [2560/15000 (17%)]\tLoss: 0.000001\n",
      "Train Epoch: 10 [5120/15000 (34%)]\tLoss: 0.000001\n",
      "Train Epoch: 10 [7680/15000 (51%)]\tLoss: 0.000001\n",
      "Train Epoch: 10 [10240/15000 (68%)]\tLoss: 0.000001\n",
      "Train Epoch: 10 [12800/15000 (85%)]\tLoss: 0.000001\n",
      "NN 1 : tensor(1.3191)\n",
      "CS 1 : 1.3570666666666666\n",
      "DP 1 : 1.3002666666666667\n",
      "heuristic 1 : 1.5662666666666667\n",
      "DP: 1.7036516666412354\n",
      "tensor([0.4986, 0.4994, 0.0020])\n",
      "tensor([0.5004, 0.4996, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 1 [0/15000 (0%)]\tLoss: 1.308660 testing loss: tensor(1.3347)\n",
      "Train Epoch: 1 [2560/15000 (17%)]\tLoss: 1.423996 testing loss: tensor(1.3131)\n",
      "Train Epoch: 1 [5120/15000 (34%)]\tLoss: 1.442855 testing loss: tensor(1.3079)\n",
      "Train Epoch: 1 [7680/15000 (51%)]\tLoss: 1.410719 testing loss: tensor(1.3031)\n",
      "Train Epoch: 1 [10240/15000 (68%)]\tLoss: 1.387592 testing loss: tensor(1.3038)\n",
      "Train Epoch: 1 [12800/15000 (85%)]\tLoss: 1.367266 testing loss: tensor(1.3085)\n",
      "penalty: 0.013610273599624634\n",
      "NN 2 : tensor(1.2991)\n",
      "CS 2 : 1.3570666666666666\n",
      "DP 2 : 1.3002666666666667\n",
      "heuristic 2 : 1.5662666666666667\n",
      "DP: 1.7036516666412354\n",
      "tensor([4.9563e-01, 5.0437e-01, 2.7667e-06])\n",
      "tensor([0.4954, 0.5046, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 2 [0/15000 (0%)]\tLoss: 1.418325 testing loss: tensor(1.3053)\n",
      "Train Epoch: 2 [2560/15000 (17%)]\tLoss: 1.390282 testing loss: tensor(1.2979)\n",
      "Train Epoch: 2 [5120/15000 (34%)]\tLoss: 1.444048 testing loss: tensor(1.3056)\n",
      "Train Epoch: 2 [7680/15000 (51%)]\tLoss: 1.508839 testing loss: tensor(1.2966)\n",
      "Train Epoch: 2 [10240/15000 (68%)]\tLoss: 1.354496 testing loss: tensor(1.2977)\n",
      "Train Epoch: 2 [12800/15000 (85%)]\tLoss: 1.545416 testing loss: tensor(1.3059)\n",
      "penalty: 0.011838793754577637\n",
      "NN 3 : tensor(1.3003)\n",
      "CS 3 : 1.3570666666666666\n",
      "DP 3 : 1.3002666666666667\n",
      "heuristic 3 : 1.5662666666666667\n",
      "DP: 1.7036516666412354\n",
      "tensor([4.8626e-01, 5.1374e-01, 7.6661e-07])\n",
      "tensor([0.4900, 0.5100, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 3 [0/15000 (0%)]\tLoss: 1.429927 testing loss: tensor(1.3097)\n",
      "Train Epoch: 3 [2560/15000 (17%)]\tLoss: 1.333415 testing loss: tensor(1.2977)\n",
      "Train Epoch: 3 [5120/15000 (34%)]\tLoss: 1.472200 testing loss: tensor(1.2966)\n",
      "Train Epoch: 3 [7680/15000 (51%)]\tLoss: 1.394102 testing loss: tensor(1.3001)\n",
      "Train Epoch: 3 [10240/15000 (68%)]\tLoss: 1.453923 testing loss: tensor(1.2993)\n",
      "Train Epoch: 3 [12800/15000 (85%)]\tLoss: 1.353750 testing loss: tensor(1.2939)\n",
      "penalty: 0.008749842643737793\n",
      "NN 4 : tensor(1.2991)\n",
      "CS 4 : 1.3570666666666666\n",
      "DP 4 : 1.3002666666666667\n",
      "heuristic 4 : 1.5662666666666667\n",
      "DP: 1.7036516666412354\n",
      "tensor([4.5845e-01, 5.4155e-01, 2.3728e-07])\n",
      "tensor([0.4653, 0.5347, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 4 [0/15000 (0%)]\tLoss: 1.578662 testing loss: tensor(1.3003)\n",
      "Train Epoch: 4 [2560/15000 (17%)]\tLoss: 1.383249 testing loss: tensor(1.3005)\n",
      "Train Epoch: 4 [5120/15000 (34%)]\tLoss: 1.501958 testing loss: tensor(1.3143)\n",
      "Train Epoch: 4 [7680/15000 (51%)]\tLoss: 1.550187 testing loss: tensor(1.2975)\n",
      "Train Epoch: 4 [10240/15000 (68%)]\tLoss: 1.439973 testing loss: tensor(1.2941)\n",
      "Train Epoch: 4 [12800/15000 (85%)]\tLoss: 1.413360 testing loss: tensor(1.2979)\n",
      "penalty: 0.0006002187728881836\n",
      "NN 5 : tensor(1.2937)\n",
      "CS 5 : 1.3570666666666666\n",
      "DP 5 : 1.3002666666666667\n",
      "heuristic 5 : 1.5662666666666667\n",
      "DP: 1.7036516666412354\n",
      "tensor([5.0537e-01, 4.9463e-01, 1.5486e-07])\n",
      "tensor([0.5033, 0.4967, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 5 [0/15000 (0%)]\tLoss: 1.476973 testing loss: tensor(1.2965)\n",
      "Train Epoch: 5 [2560/15000 (17%)]\tLoss: 1.452678 testing loss: tensor(1.3033)\n",
      "Train Epoch: 5 [5120/15000 (34%)]\tLoss: 1.305709 testing loss: tensor(1.3017)\n",
      "Train Epoch: 5 [7680/15000 (51%)]\tLoss: 1.532116 testing loss: tensor(1.2976)\n",
      "Train Epoch: 5 [10240/15000 (68%)]\tLoss: 1.377565 testing loss: tensor(1.2993)\n",
      "Train Epoch: 5 [12800/15000 (85%)]\tLoss: 1.376199 testing loss: tensor(1.2979)\n",
      "penalty: 0.0025746822357177734\n",
      "NN 6 : tensor(1.2961)\n",
      "CS 6 : 1.3570666666666666\n",
      "DP 6 : 1.3002666666666667\n",
      "heuristic 6 : 1.5662666666666667\n",
      "DP: 1.7036516666412354\n",
      "tensor([4.8092e-01, 5.1908e-01, 1.1088e-07])\n",
      "tensor([0.4835, 0.5165, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: beta random initializing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "NN 1 : tensor(1.3597)\n",
      "CS 1 : 1.3570666666666666\n",
      "DP 1 : 1.3002666666666667\n",
      "heuristic 1 : 1.5662666666666667\n",
      "DP: 1.7036516666412354\n",
      "tensor([0.3005, 0.2803, 0.4192])\n",
      "tensor([0.4777, 0.5223, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 1 [0/15000 (0%)]\tLoss: 1.477711 testing loss: tensor(1.3578)\n",
      "Train Epoch: 1 [2560/15000 (17%)]\tLoss: 1.487837 testing loss: tensor(1.3561)\n",
      "Train Epoch: 1 [5120/15000 (34%)]\tLoss: 1.362566 testing loss: tensor(1.3591)\n",
      "Train Epoch: 1 [7680/15000 (51%)]\tLoss: 1.483814 testing loss: tensor(1.3601)\n",
      "Train Epoch: 1 [10240/15000 (68%)]\tLoss: 1.498448 testing loss: tensor(1.3628)\n",
      "Train Epoch: 1 [12800/15000 (85%)]\tLoss: 1.494760 testing loss: tensor(1.3588)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.3593)\n",
      "CS 2 : 1.3570666666666666\n",
      "DP 2 : 1.3002666666666667\n",
      "heuristic 2 : 1.5662666666666667\n",
      "DP: 1.7036516666412354\n",
      "tensor([0.3213, 0.3490, 0.3297])\n",
      "tensor([0.4907, 0.5093, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 2 [0/15000 (0%)]\tLoss: 1.373714 testing loss: tensor(1.3589)\n",
      "Train Epoch: 2 [2560/15000 (17%)]\tLoss: 1.555896 testing loss: tensor(1.3571)\n",
      "Train Epoch: 2 [5120/15000 (34%)]\tLoss: 1.554865 testing loss: tensor(1.3592)\n",
      "Train Epoch: 2 [7680/15000 (51%)]\tLoss: 1.504886 testing loss: tensor(1.3622)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [10240/15000 (68%)]\tLoss: 1.471372 testing loss: tensor(1.3583)\n",
      "Train Epoch: 2 [12800/15000 (85%)]\tLoss: 1.476037 testing loss: tensor(1.3618)\n",
      "penalty: 0.0\n",
      "NN 3 : tensor(1.3598)\n",
      "CS 3 : 1.3570666666666666\n",
      "DP 3 : 1.3002666666666667\n",
      "heuristic 3 : 1.5662666666666667\n",
      "DP: 1.7036516666412354\n",
      "tensor([0.3135, 0.3728, 0.3137])\n",
      "tensor([0.4335, 0.5665, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 3 [0/15000 (0%)]\tLoss: 1.509266 testing loss: tensor(1.3617)\n",
      "Train Epoch: 3 [2560/15000 (17%)]\tLoss: 1.496601 testing loss: tensor(1.3573)\n",
      "Train Epoch: 3 [5120/15000 (34%)]\tLoss: 1.370527 testing loss: tensor(1.3599)\n",
      "Train Epoch: 3 [7680/15000 (51%)]\tLoss: 1.480344 testing loss: tensor(1.3590)\n",
      "Train Epoch: 3 [10240/15000 (68%)]\tLoss: 1.447389 testing loss: tensor(1.3588)\n",
      "Train Epoch: 3 [12800/15000 (85%)]\tLoss: 1.457195 testing loss: tensor(1.3569)\n",
      "penalty: 0.0\n",
      "NN 4 : tensor(1.3583)\n",
      "CS 4 : 1.3570666666666666\n",
      "DP 4 : 1.3002666666666667\n",
      "heuristic 4 : 1.5662666666666667\n",
      "DP: 1.7036516666412354\n",
      "tensor([0.3227, 0.3406, 0.3367])\n",
      "tensor([0.4716, 0.5284, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 4 [0/15000 (0%)]\tLoss: 1.384062 testing loss: tensor(1.3599)\n",
      "Train Epoch: 4 [2560/15000 (17%)]\tLoss: 1.524187 testing loss: tensor(1.3592)\n",
      "Train Epoch: 4 [5120/15000 (34%)]\tLoss: 1.414085 testing loss: tensor(1.3575)\n",
      "Train Epoch: 4 [7680/15000 (51%)]\tLoss: 1.471327 testing loss: tensor(1.3569)\n",
      "Train Epoch: 4 [10240/15000 (68%)]\tLoss: 1.574686 testing loss: tensor(1.3581)\n",
      "Train Epoch: 4 [12800/15000 (85%)]\tLoss: 1.466817 testing loss: tensor(1.3604)\n",
      "penalty: 0.0\n",
      "NN 5 : tensor(1.3568)\n",
      "CS 5 : 1.3570666666666666\n",
      "DP 5 : 1.3002666666666667\n",
      "heuristic 5 : 1.5662666666666667\n",
      "DP: 1.7036516666412354\n",
      "tensor([0.3493, 0.3171, 0.3336])\n",
      "tensor([0.5046, 0.4954, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 5 [0/15000 (0%)]\tLoss: 1.438265 testing loss: tensor(1.3581)\n",
      "Train Epoch: 5 [2560/15000 (17%)]\tLoss: 1.440991 testing loss: tensor(1.3565)\n",
      "Train Epoch: 5 [5120/15000 (34%)]\tLoss: 1.463543 testing loss: tensor(1.3569)\n",
      "Train Epoch: 5 [7680/15000 (51%)]\tLoss: 1.627434 testing loss: tensor(1.3579)\n",
      "Train Epoch: 5 [10240/15000 (68%)]\tLoss: 1.336552 testing loss: tensor(1.3563)\n",
      "Train Epoch: 5 [12800/15000 (85%)]\tLoss: 1.404995 testing loss: tensor(1.3587)\n",
      "penalty: 0.0\n",
      "NN 6 : tensor(1.3571)\n",
      "CS 6 : 1.3570666666666666\n",
      "DP 6 : 1.3002666666666667\n",
      "heuristic 6 : 1.5662666666666667\n",
      "DP: 1.7036516666412354\n",
      "tensor([0.3364, 0.3394, 0.3242])\n",
      "tensor([0.5077, 0.4923, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: beta costsharing\n",
      "Train Epoch: 1 [0/15000 (0%)]\tLoss: 0.004683\n",
      "Train Epoch: 1 [2560/15000 (17%)]\tLoss: 0.000156\n",
      "Train Epoch: 1 [5120/15000 (34%)]\tLoss: 0.000011\n",
      "Train Epoch: 1 [7680/15000 (51%)]\tLoss: 0.000001\n",
      "Train Epoch: 1 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "NN 1 : tensor(1.3571)\n",
      "CS 1 : 1.3570666666666666\n",
      "DP 1 : 1.3002666666666667\n",
      "heuristic 1 : 1.5662666666666667\n",
      "DP: 1.7036516666412354\n",
      "tensor([0.3333, 0.3333, 0.3333])\n",
      "tensor([0.5000, 0.5000, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 1 [0/15000 (0%)]\tLoss: 1.521622 testing loss: tensor(1.3599)\n",
      "Train Epoch: 1 [2560/15000 (17%)]\tLoss: 1.460430 testing loss: tensor(1.3587)\n",
      "Train Epoch: 1 [5120/15000 (34%)]\tLoss: 1.397183 testing loss: tensor(1.3582)\n",
      "Train Epoch: 1 [7680/15000 (51%)]\tLoss: 1.381556 testing loss: tensor(1.3602)\n",
      "Train Epoch: 1 [10240/15000 (68%)]\tLoss: 1.475791 testing loss: tensor(1.3583)\n",
      "Train Epoch: 1 [12800/15000 (85%)]\tLoss: 1.557955 testing loss: tensor(1.3593)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.3616)\n",
      "CS 2 : 1.3570666666666666\n",
      "DP 2 : 1.3002666666666667\n",
      "heuristic 2 : 1.5662666666666667\n",
      "DP: 1.7036516666412354\n",
      "tensor([0.3173, 0.3532, 0.3296])\n",
      "tensor([0.4550, 0.5450, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 2 [0/15000 (0%)]\tLoss: 1.605935 testing loss: tensor(1.3613)\n",
      "Train Epoch: 2 [2560/15000 (17%)]\tLoss: 1.422467 testing loss: tensor(1.3573)\n",
      "Train Epoch: 2 [5120/15000 (34%)]\tLoss: 1.524615 testing loss: tensor(1.3586)\n",
      "Train Epoch: 2 [7680/15000 (51%)]\tLoss: 1.509621 testing loss: tensor(1.3615)\n",
      "Train Epoch: 2 [10240/15000 (68%)]\tLoss: 1.506585 testing loss: tensor(1.3599)\n",
      "Train Epoch: 2 [12800/15000 (85%)]\tLoss: 1.625088 testing loss: tensor(1.3583)\n",
      "penalty: 0.0\n",
      "NN 3 : tensor(1.3557)\n",
      "CS 3 : 1.3570666666666666\n",
      "DP 3 : 1.3002666666666667\n",
      "heuristic 3 : 1.5662666666666667\n",
      "DP: 1.7036516666412354\n",
      "tensor([0.3505, 0.3083, 0.3413])\n",
      "tensor([0.5061, 0.4939, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 3 [0/15000 (0%)]\tLoss: 1.350471 testing loss: tensor(1.3560)\n",
      "Train Epoch: 3 [2560/15000 (17%)]\tLoss: 1.529693 testing loss: tensor(1.3620)\n",
      "Train Epoch: 3 [5120/15000 (34%)]\tLoss: 1.466548 testing loss: tensor(1.3594)\n",
      "Train Epoch: 3 [7680/15000 (51%)]\tLoss: 1.431795 testing loss: tensor(1.3581)\n",
      "Train Epoch: 3 [10240/15000 (68%)]\tLoss: 1.427615 testing loss: tensor(1.3553)\n",
      "Train Epoch: 3 [12800/15000 (85%)]\tLoss: 1.370086 testing loss: tensor(1.3583)\n",
      "penalty: 0.0\n",
      "NN 4 : tensor(1.3591)\n",
      "CS 4 : 1.3570666666666666\n",
      "DP 4 : 1.3002666666666667\n",
      "heuristic 4 : 1.5662666666666667\n",
      "DP: 1.7036516666412354\n",
      "tensor([0.3124, 0.3494, 0.3382])\n",
      "tensor([0.4635, 0.5365, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 4 [0/15000 (0%)]\tLoss: 1.576097 testing loss: tensor(1.3593)\n",
      "Train Epoch: 4 [2560/15000 (17%)]\tLoss: 1.465442 testing loss: tensor(1.3567)\n",
      "Train Epoch: 4 [5120/15000 (34%)]\tLoss: 1.408050 testing loss: tensor(1.3573)\n",
      "Train Epoch: 4 [7680/15000 (51%)]\tLoss: 1.499097 testing loss: tensor(1.3589)\n",
      "Train Epoch: 4 [10240/15000 (68%)]\tLoss: 1.462888 testing loss: tensor(1.3576)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [12800/15000 (85%)]\tLoss: 1.434773 testing loss: tensor(1.3587)\n",
      "penalty: 0.0\n",
      "NN 5 : tensor(1.3579)\n",
      "CS 5 : 1.3570666666666666\n",
      "DP 5 : 1.3002666666666667\n",
      "heuristic 5 : 1.5662666666666667\n",
      "DP: 1.7036516666412354\n",
      "tensor([0.3330, 0.3295, 0.3375])\n",
      "tensor([0.4890, 0.5110, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 5 [0/15000 (0%)]\tLoss: 1.400685 testing loss: tensor(1.3580)\n",
      "Train Epoch: 5 [2560/15000 (17%)]\tLoss: 1.457744 testing loss: tensor(1.3591)\n",
      "Train Epoch: 5 [5120/15000 (34%)]\tLoss: 1.541222 testing loss: tensor(1.3601)\n",
      "Train Epoch: 5 [7680/15000 (51%)]\tLoss: 1.373537 testing loss: tensor(1.3575)\n",
      "Train Epoch: 5 [10240/15000 (68%)]\tLoss: 1.444366 testing loss: tensor(1.3569)\n",
      "Train Epoch: 5 [12800/15000 (85%)]\tLoss: 1.491250 testing loss: tensor(1.3596)\n",
      "penalty: 0.0\n",
      "NN 6 : tensor(1.3580)\n",
      "CS 6 : 1.3570666666666666\n",
      "DP 6 : 1.3002666666666667\n",
      "heuristic 6 : 1.5662666666666667\n",
      "DP: 1.7036516666412354\n",
      "tensor([0.3443, 0.3308, 0.3249])\n",
      "tensor([0.4987, 0.5013, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.1)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "    \n",
    "for order in stage:\n",
    "    producedata(order)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "    #running\n",
    "    for order1 in order1name:\n",
    "        print(\"Supervised Aim:\",order,order1)\n",
    "        \n",
    "        # for mapping binary to payments before softmax\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(n, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, n),\n",
    "        )\n",
    "        model.apply(init_weights)\n",
    "        # optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        runningLossNN = []\n",
    "        runningLossCS = []\n",
    "        runningLossDP = []\n",
    "        runningLossHeuristic = []\n",
    "        #model=torch.load(\"save/pytorchNN=5dp1\");\n",
    "        #model.eval()\n",
    "        ##order1name=[\"costsharing\",\"dp\",\"heuristic\",\"random initializing\"]\n",
    "        for epoch in range(1, supervisionEpochs + 1):\n",
    "#             print(\"distributionRatio\",distributionRatio)\n",
    "            if(order1==\"costsharing\"):\n",
    "                supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "            elif(order1==\"dp\"):\n",
    "                supervisionTrain(epoch, dpSupervisionRule)\n",
    "            elif(order1==\"heuristic\"):\n",
    "                supervisionTrain(epoch, heuristicSupervisionRule)\n",
    "            elif(order1==\"random initializing\"):\n",
    "                print(\"do nothing\");\n",
    "\n",
    "        test()\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(epoch)\n",
    "            test()\n",
    "        losslistname.append(order+\" \"+order1);\n",
    "        losslist.append(losslisttemp);\n",
    "        losslisttemp=[];\n",
    "        savepath=\"save/pytorchNN=5all-beta\"+order+str(order1)\n",
    "        torch.save(model, savepath);\n",
    "        print(\"end\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colorlist=[\"#D2691E\",'#4169E1',\"#9ACD32\",\"#B22222\",\"#FF00FF\",\"#708090\"]\n",
    "for i in range(len(losslist)):\n",
    "    plt.plot(losslist[i], 'ro-', color=colorlist[i], alpha=0.8, label=losslistname[i])\n",
    "\n",
    "# 显示标签，如果不加这句，即使加了label='一些数字'的参数，最终还是不会显示标签\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('trianing times')\n",
    "plt.ylabel('delay')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
