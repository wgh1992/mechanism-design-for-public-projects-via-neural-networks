{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "from pylab import *\n",
    "from scipy.stats import beta\n",
    "import random \n",
    "n = 5\n",
    "epochs = 1\n",
    "supervisionEpochs = 3\n",
    "lr = 0.0005\n",
    "log_interval = 10\n",
    "trainSize = 40000#100000\n",
    "percentage_train_test= 0.75\n",
    "penaltyLambda = 10\n",
    "doublePeakHighMean = 0.85\n",
    "doublePeakLowMean = 0.15\n",
    "doublePeakStd = 0.1\n",
    "uniformlow=0\n",
    "uniformhigh=1.0\n",
    "normalloc = 0.2\n",
    "normalscale = 0.1\n",
    "\n",
    "cauchyloc = 1/n\n",
    "cauchyscalen = 0.004\n",
    "\n",
    "exponentialhigh = 15 #Symbol(\"b\", real=True)\n",
    "exponentiallow  = 15 #Symbol(\"a\", real=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "independentnormalloc1=[(float(ii)+1)/(2*n+1) for ii in range(n,0,-1)]\n",
    "independentnormalscale1=[0.05 for ii in range(n)]\n",
    "\n",
    "independentnormalloc2=[(float(ii)+1)/(2*n+1) for ii in range(1,n+1,1)]\n",
    "independentnormalscale2=[0.05 for ii in range(n)]\n",
    "stage=[\"twopeak\"]\n",
    "# \"twopeak\",\"normal\",\"uniform\",\"independent1\",\"independent2\",\"cauchy\",\"U-exponential\"\n",
    "order1name=[\"random initializing\"]\n",
    "numberofpeople=['2','1','0']\n",
    "# \"costsharing\",\"dp\",\"heuristic\",\"random initializing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5000)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio1 = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase1 = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "d3 = D.normal.Normal(normalloc, normalscale)\n",
    "distributionRatio3 = d3.cdf(1) - d3.cdf(0)\n",
    "distributionBase3 = d3.cdf(0)\n",
    "\n",
    "d4 = D.uniform.Uniform(uniformlow,uniformhigh)\n",
    "distributionRatio4 = d4.cdf(1) - d4.cdf(0)\n",
    "distributionBase4 = d4.cdf(0)\n",
    "\n",
    "d5 = [D.normal.Normal(independentnormalloc1[ii], independentnormalscale1[ii]) for ii in range(n)]\n",
    "d6 = [D.normal.Normal(independentnormalloc2[ii], independentnormalscale2[ii]) for ii in range(n)]\n",
    "\n",
    "d7 = D.cauchy.Cauchy(cauchyloc,cauchyscalen)\n",
    "\n",
    "d81 = D.exponential.Exponential(exponentiallow)\n",
    "d82 = D.exponential.Exponential(exponentialhigh)\n",
    "\n",
    "#d8 = beta(betahigh,betalow)\n",
    "#d9 = D.beta.Beta(betahigh,betalow)\n",
    "def cdf(x,y, i=None):\n",
    "    if(y==\"twopeak\"):\n",
    "        return (d1.cdf(x) + d2.cdf(x) - distributionBase1) / 2 / distributionRatio1\n",
    "    elif(y==\"normal\"):\n",
    "        return (d3.cdf(x)-distributionBase3)/distributionRatio3;\n",
    "    elif(y==\"uniform\"):\n",
    "        return (d4.cdf(x)-distributionBase4)/distributionRatio4;\n",
    "    elif(y==\"independent1\"):\n",
    "        return d5[i].cdf(x);\n",
    "    elif(y==\"independent2\"):\n",
    "        return d6[i].cdf(x);\n",
    "    elif(y==\"cauchy\"):\n",
    "        return d7.cdf(x);\n",
    "#     elif(y==\"beta\"):\n",
    "#         return torch.tensor(d8.cdf(x));\n",
    "    elif(y==\"U-exponential\"):\n",
    "        return (d81.cdf(x) + (1.0 - d82.cdf(1.0-x)))  / 2 \n",
    "print(cdf(0.5,\"U-exponential\"))\n",
    "\n",
    "print(d81.cdf(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    if torch.sum(bits).item() == 0:\n",
    "        return torch.ones(n)\n",
    "    bits = bits.type(torch.float32)\n",
    "    negBits = torch.ones(n) - bits\n",
    "    payments = model(bits)\n",
    "    payments = payments - 1000 * negBits\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    payments = payments + negBits\n",
    "    return payments\n",
    "\n",
    "def tpToBits(tp, bits=torch.ones(n).type(torch.uint8)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = (tp >= payments).type(torch.uint8)\n",
    "    if torch.equal(newBits, bits):\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits(tp, newBits)#bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    return n - torch.sum(tpToBits(tp).type(torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    global  dp,decision\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money, yes]\n",
    "        offer = float(offerIndex) / dpPrecision\n",
    "        if tp[i] >= offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits[i] = 0\n",
    "            payments[i] = 1\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "#     print()\n",
    "#     print(tp)\n",
    "#     print(bits)\n",
    "#     print(payments)\n",
    "#     print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def heuristicSupervisionRule(tp):\n",
    "    global  dp_H,decision_H\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    tempold=-1;\n",
    "    for turns in range(n,0,-1):\n",
    "        money = dpPrecision\n",
    "        j=0\n",
    "        tempo=sum(bits)\n",
    "        #print(\"bits\",tempo)\n",
    "        for i in range(n):\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            \n",
    "            offerIndex = decision_H[tempo ,tempo - i, money]\n",
    "            offer = float(offerIndex) / dpPrecision\n",
    "            while(j<n):\n",
    "                if(bits[j]!=0):\n",
    "                    break;\n",
    "                j+=1;\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            if tp[j] >= offer:\n",
    "                #print(money,j,tp[j],offer)\n",
    "                money -= offerIndex\n",
    "                bits[j] = 1\n",
    "                payments[j] = offer\n",
    "            else:\n",
    "                bits[j] = 0;\n",
    "                payments[j] = 1.0;\n",
    "            j+=1;\n",
    "        #print(\"money\",money)\n",
    "        if(money==0 and tempold==tempo):\n",
    "            break;\n",
    "        tempold=tempo;\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    for k in range(n, -1, -1):\n",
    "        if k == 0:\n",
    "            break\n",
    "        bits = [1 if tp[ii] >= 1.0 / k else 0 for ii in range(n)]\n",
    "        if sum(bits) == k:\n",
    "            break\n",
    "    if k == 0:\n",
    "        payments = [1 for ii in range(n)]\n",
    "    else:\n",
    "        payments = [1.0 / k if bits[ii] == 1 else 1 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    return float(n - torch.sum(costSharingSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def dpDelay(tp):\n",
    "    return float(n - torch.sum(dpSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def heuristicDelay(tp):\n",
    "    return float(n - torch.sum(heuristicSupervisionRule(tp)[0]).item())\n",
    "\n",
    "\n",
    "# templire=0;\n",
    "# num=1\n",
    "# for i in range(num):\n",
    "#     temp=torch.tensor(samplesJoint[i])\n",
    "#     #print(temp)\n",
    "#     #print(temp);\n",
    "#     #print(heuristicSupervisionRule(temp))\n",
    "#     #print(dpSupervisionRule(temp))\n",
    "#     #print(dpDelay(temp))\n",
    "#     res=dpDelay(temp)\n",
    "#     templire+=res\n",
    "#     #print(\"delay\",res)\n",
    "#     #print()\n",
    "# print(templire/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordAndReport(name, source, loss):\n",
    "    source.append(loss)\n",
    "    realLength = len(source)\n",
    "    #print(f\"{name} ({realLength}): {loss}\")\n",
    "    print(name,realLength,\":\" ,loss)\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    \n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda /100000\n",
    "        \n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "losslist=[];\n",
    "losslistname=[];\n",
    "losslisttemp=[];\n",
    "def test_batch():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lenLoss= 0\n",
    "        nnLoss = 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "            for tp in tp_batch:\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        nnLoss/=lenLoss\n",
    "    return nnLoss\n",
    "\n",
    "\n",
    "def train0(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            delay1 = tpToTotalDelay(tp)\n",
    "            loss = loss + delay1 \n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "\n",
    "def train1(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                delay1 = tpToTotalDelay(tp1)\n",
    "                delay0 = tpToTotalDelay(tp0)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                    loss = loss + (1 - cdf(offer,order)) * delay1 + cdf(offer,order) * delay0\n",
    "                else:\n",
    "                    loss = loss + (1 - cdf(offer,order,i)) * delay1 + cdf(offer,order,i) * delay0\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "    \n",
    "def train2(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                x1=random.randint(0,n-1);\n",
    "                x2=random.randint(0,n-1);\n",
    "                while(x2==x1):\n",
    "                    x2=random.randint(0,n-1);\n",
    "                    \n",
    "                tp11 = tp.clone()\n",
    "                tp11[x1] = 1\n",
    "                tp11[x2] = 1\n",
    "                tp10 = tp.clone()\n",
    "                tp10[x1] = 1\n",
    "                tp10[x2] = 0\n",
    "                tp01 = tp.clone()\n",
    "                tp01[x1] = 0\n",
    "                tp01[x2] = 1\n",
    "                tp00 = tp.clone()\n",
    "                tp00[x1] = 0\n",
    "                tp00[x2] = 0\n",
    "                \n",
    "                offer1 = tpToPayments(tp11)[x1]\n",
    "                offer2 = tpToPayments(tp11)[x2]\n",
    "                offer3 = tpToPayments(tp10)[x1]\n",
    "                offer4 = tpToPayments(tp01)[x2]\n",
    "                \n",
    "                #print(tp)\n",
    "                #print(offer1,offer2)\n",
    "                \n",
    "                delay11 = tpToTotalDelay(tp11)\n",
    "                delay01 = tpToTotalDelay(tp01)\n",
    "                delay10 = tpToTotalDelay(tp10)\n",
    "                delay00 = tpToTotalDelay(tp00)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "#                     print ((1.0-cdf(offer1,order)) * (1.0-cdf(offer2,order))+                                                   \n",
    "#                            (1.0-cdf(offer3,order)) * cdf(offer2,order)+\n",
    "#                            cdf(offer1,order) * (1.0-cdf(offer4,order))+\n",
    "#                            (cdf(offer3,order) * cdf(offer4,order) -\n",
    "#                                      (cdf(offer3,order)-cdf(offer1,order))*(cdf(offer4,order)-cdf(offer2,order)) )\n",
    "#                           )\n",
    "                    loss = loss +(      (1.0-cdf(offer1,order)) * (1.0-cdf(offer2,order)) * delay11 +\n",
    "                                  (1.0-cdf(offer3,order)) * cdf(offer2,order) * delay10 +\n",
    "                                  cdf(offer1,order) * (1.0-cdf(offer4,order)) * delay01 +\n",
    "                                    (cdf(offer3,order) * cdf(offer4,order) -\n",
    "                                     (cdf(offer3,order)-cdf(offer1,order))*(cdf(offer4,order)-cdf(offer2,order)) )* delay00 \n",
    "                                                       )\n",
    "                else:\n",
    "                    loss = loss +(      (1.0-cdf(offer1,order,x1)) * (1.0-cdf(offer2,order),x2) * delay11 +\n",
    "                                  (1.0-cdf(offer3,order,x1)) * cdf(offer2,order,x2) * delay10 +\n",
    "                                  cdf(offer1,order,x1) * (1.0-cdf(offer4,order,x2)) * delay01 +\n",
    "                                    (cdf(offer3,order,x1) * cdf(offer4,order,x2) -\n",
    "                                     (cdf(offer3,order,x1)-cdf(offer1,order,x1))*(cdf(offer4,order,x2)-cdf(offer2,order,x2)) )* delay00 \n",
    "                                                       )\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "    \n",
    "allBits = [torch.tensor(bits, dtype=torch.int16) for bits in itertools.product([0, 1], repeat=n)]\n",
    "\n",
    "def test():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        costSharingLoss = 0\n",
    "        dpLoss = 0\n",
    "        nnLoss = 0\n",
    "        heuristicLoss=0\n",
    "        lenLoss= 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "                heuristicLoss+= heuristicDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        costSharingLoss /= lenLoss\n",
    "        dpLoss /= lenLoss\n",
    "        nnLoss /= lenLoss\n",
    "        heuristicLoss /= lenLoss\n",
    "        #print(lenLoss)\n",
    "        losslisttemp.append(nnLoss);\n",
    "        recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "        recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "        recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "        recordAndReport(\"heuristic\", runningLossHeuristic, heuristicLoss)\n",
    "        print(\"DP:\",dp[n, dpPrecision,0])\n",
    "        #for i in range(n, 0, -1):\n",
    "        #    print(\"Heuristic:\",i,5*(1-dp_H[i, i, dpPrecision]))\n",
    "        for i in range(n, 0, -1):\n",
    "            print(\n",
    "                    tpToPayments(\n",
    "                            torch.tensor([0 if ii >= i else 1 for ii in range(n)], dtype=torch.float32)\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "dpPrecision = 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def producedata(order):\n",
    "    global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "    dpPrecision = 100 \n",
    "    if(order==\"twopeak\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = np.random.normal(\n",
    "            loc=doublePeakLowMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=doublePeakLowMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samples2 = np.random.normal(\n",
    "            loc=doublePeakHighMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = np.random.normal(\n",
    "                        loc=doublePeakHighMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "    elif(order==\"normal\"):\n",
    "        print(\"loc\",normalloc, \"scale\",normalscale)\n",
    "        samples1 = np.random.normal(\n",
    "            loc=normalloc, scale=normalscale, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=normalloc, scale=normalscale\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"uniform\"):  \n",
    "        print(\"uniformlow\",uniformlow, \"uniformhigh\",uniformhigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        uniformlow, uniformhigh\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent1\"):\n",
    "        print(\"loc\",independentnormalloc1,\"scale\",independentnormalscale1)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent2\"):\n",
    "        print(\"loc\",independentnormalloc2, \"scale\",independentnormalscale2)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"cauchy\"):\n",
    "        print(\"cauchyloc\",cauchyloc, \"cauchyscale\",cauchyscalen)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"beta\"):\n",
    "        print(\"betalow\",betalow, \"betahigh\",betahigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(betahigh,betalow,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(betahigh,betalow,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"U-exponential\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = d81.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d81.rsample(torch.Size([1])).numpy()\n",
    "                    \n",
    "        samples2 = d82.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        \n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = d82.rsample(torch.Size([1])).numpy()\n",
    "        samples2 = 1.0 - samples2\n",
    "        samplesJoint = signals * samples1 - (signals - 1.0) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        plt.hist(samplesJoint,bins=500)\n",
    "        plt.show()\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "        \n",
    "        \n",
    "    tp_dataset = TensorDataset(tp_tensor[: int(trainSize * percentage_train_test)])\n",
    "    tp_dataset_testing = TensorDataset(tp_tensor[int(trainSize * (1.0-percentage_train_test)) :])\n",
    "    tp_dataloader = DataLoader(tp_dataset, batch_size=128, shuffle=True)\n",
    "    tp_dataloader_testing = DataLoader(tp_dataset_testing, batch_size=256, shuffle=False)\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "    decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for yes in range(n + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp[0, 0, yes] = 0\n",
    "            else:\n",
    "                dp[0, money, yes] = yes# + 1.0\n",
    "    for ppl in range(1,  n + 1):\n",
    "        for yes in range(n + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                minSoFar = 1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1 - cdf(offer,order)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order) * (1 + dp[ppl - 1, money, yes])\n",
    "                    else:\n",
    "                        res = (1 - cdf(offer,order,n-ppl)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order,n-ppl) * (1 + dp[ppl - 1, money, yes])\n",
    "                    if minSoFar > res.item():\n",
    "                        minSoFar = res.item()\n",
    "                        decision[ppl, money, yes] = offerIndex\n",
    "                dp[ppl, money, yes] = minSoFar\n",
    "    \n",
    "    print(\"dp\",dp[n, dpPrecision, 0])\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp_H = np.zeros([n + 1 , n + 1, dpPrecision + 1])\n",
    "    decision_H = np.zeros([n + 1 , n + 1, dpPrecision + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for i in range(1,n+1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp_H[i, 0, 0] = 1\n",
    "            else:\n",
    "                offer = money / dpPrecision\n",
    "                dp_H[i, 0, money] = 0#cdf(offer)# + 1.0\n",
    "    for i in range(1,n+1):\n",
    "        for ppl in range(1, i + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                maxSoFar = -1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1-cdf(offer,order)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    else:\n",
    "                        res = (1-cdf(offer,order,ppl-1)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    if maxSoFar < res.item():\n",
    "                        maxSoFar = res.item()\n",
    "                        decision_H[i, ppl, money] = offerIndex\n",
    "                dp_H[i, ppl, money] = maxSoFar\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loc 0.1 scale 0.1\n",
      "loc 0.9 scale 0.1\n",
      "dp 1.8651759624481201\n",
      "Supervised Aim: twopeak random initializing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "NN 1 : tensor(2.5354)\n",
      "CS 1 : 2.5398\n",
      "DP 1 : 1.8724\n",
      "heuristic 1 : 1.8543\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.1991, 0.1844, 0.2310, 0.1863, 0.1991])\n",
      "tensor([0.2358, 0.2223, 0.3145, 0.2274, 1.0000])\n",
      "tensor([0.3163, 0.2902, 0.3935, 1.0000, 1.0000])\n",
      "tensor([0.5005, 0.4995, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/10000 (0%)]\tLoss: 2.580120 testing loss: tensor(2.5252)\n",
      "Train Epoch: 1 [1280/10000 (13%)]\tLoss: 1.923901 testing loss: tensor(2.0572)\n",
      "Train Epoch: 1 [2560/10000 (25%)]\tLoss: 1.889549 testing loss: tensor(1.9363)\n",
      "Train Epoch: 1 [3840/10000 (38%)]\tLoss: 1.786453 testing loss: tensor(1.8406)\n",
      "Train Epoch: 1 [5120/10000 (51%)]\tLoss: 1.887991 testing loss: tensor(1.8188)\n",
      "Train Epoch: 1 [6400/10000 (63%)]\tLoss: 1.791925 testing loss: tensor(1.7807)\n",
      "Train Epoch: 1 [7680/10000 (76%)]\tLoss: 1.786326 testing loss: tensor(1.7718)\n",
      "Train Epoch: 1 [8960/10000 (89%)]\tLoss: 1.711544 testing loss: tensor(1.7593)\n",
      "penalty: 0.009554684162139893\n",
      "NN 2 : tensor(1.7564)\n",
      "CS 2 : 2.5398\n",
      "DP 2 : 1.8724\n",
      "heuristic 2 : 1.8543\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0659, 0.0695, 0.7441, 0.0495, 0.0710])\n",
      "tensor([0.0730, 0.0709, 0.7509, 0.1051, 1.0000])\n",
      "tensor([0.1113, 0.1122, 0.7765, 1.0000, 1.0000])\n",
      "tensor([0.4988, 0.5012, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 2 [0/10000 (0%)]\tLoss: 1.639704 testing loss: tensor(1.7566)\n",
      "Train Epoch: 2 [1280/10000 (13%)]\tLoss: 1.833986 testing loss: tensor(1.7641)\n",
      "Train Epoch: 2 [2560/10000 (25%)]\tLoss: 1.612975 testing loss: tensor(1.7602)\n",
      "Train Epoch: 2 [3840/10000 (38%)]\tLoss: 1.807385 testing loss: tensor(1.7575)\n",
      "Train Epoch: 2 [5120/10000 (51%)]\tLoss: 1.676737 testing loss: tensor(1.7564)\n",
      "Train Epoch: 2 [6400/10000 (63%)]\tLoss: 1.712106 testing loss: tensor(1.7595)\n",
      "Train Epoch: 2 [7680/10000 (76%)]\tLoss: 1.831616 testing loss: tensor(1.7558)\n",
      "Train Epoch: 2 [8960/10000 (89%)]\tLoss: 1.679034 testing loss: tensor(1.7595)\n",
      "penalty: 0.005288660526275635\n",
      "NN 3 : tensor(1.7666)\n",
      "CS 3 : 2.5398\n",
      "DP 3 : 1.8724\n",
      "heuristic 3 : 1.8543\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0678, 0.0661, 0.7300, 0.0651, 0.0710])\n",
      "tensor([0.0740, 0.0751, 0.7432, 0.1076, 1.0000])\n",
      "tensor([0.1208, 0.1212, 0.7580, 1.0000, 1.0000])\n",
      "tensor([0.4642, 0.5358, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 3 [0/10000 (0%)]\tLoss: 1.669350 testing loss: tensor(1.7683)\n",
      "Train Epoch: 3 [1280/10000 (13%)]\tLoss: 1.722276 testing loss: tensor(1.7547)\n",
      "Train Epoch: 3 [2560/10000 (25%)]\tLoss: 1.682887 testing loss: tensor(1.7544)\n",
      "Train Epoch: 3 [3840/10000 (38%)]\tLoss: 1.867052 testing loss: tensor(1.7598)\n",
      "Train Epoch: 3 [5120/10000 (51%)]\tLoss: 1.671312 testing loss: tensor(1.7683)\n",
      "Train Epoch: 3 [6400/10000 (63%)]\tLoss: 1.628500 testing loss: tensor(1.7624)\n",
      "Train Epoch: 3 [7680/10000 (76%)]\tLoss: 1.857953 testing loss: tensor(1.7551)\n",
      "Train Epoch: 3 [8960/10000 (89%)]\tLoss: 1.867460 testing loss: tensor(1.7572)\n",
      "penalty: 0.0\n",
      "NN 4 : tensor(1.7608)\n",
      "CS 4 : 2.5398\n",
      "DP 4 : 1.8724\n",
      "heuristic 4 : 1.8543\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0657, 0.0637, 0.7420, 0.0636, 0.0650])\n",
      "tensor([0.0760, 0.0760, 0.7490, 0.0990, 1.0000])\n",
      "tensor([0.1194, 0.1188, 0.7618, 1.0000, 1.0000])\n",
      "tensor([0.4812, 0.5188, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 4 [0/10000 (0%)]\tLoss: 1.674495 testing loss: tensor(1.7589)\n",
      "Train Epoch: 4 [1280/10000 (13%)]\tLoss: 1.943474 testing loss: tensor(1.7606)\n",
      "Train Epoch: 4 [2560/10000 (25%)]\tLoss: 1.757018 testing loss: tensor(1.7615)\n",
      "Train Epoch: 4 [3840/10000 (38%)]\tLoss: 1.582209 testing loss: tensor(1.7567)\n",
      "Train Epoch: 4 [5120/10000 (51%)]\tLoss: 1.866570 testing loss: tensor(1.7607)\n",
      "Train Epoch: 4 [6400/10000 (63%)]\tLoss: 1.778130 testing loss: tensor(1.7575)\n",
      "Train Epoch: 4 [7680/10000 (76%)]\tLoss: 1.693366 testing loss: tensor(1.7604)\n",
      "Train Epoch: 4 [8960/10000 (89%)]\tLoss: 1.699638 testing loss: tensor(1.7577)\n",
      "penalty: 0.003354012966156006\n",
      "NN 5 : tensor(1.7564)\n",
      "CS 5 : 2.5398\n",
      "DP 5 : 1.8724\n",
      "heuristic 5 : 1.8543\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0625, 0.0639, 0.7507, 0.0615, 0.0614])\n",
      "tensor([0.0723, 0.0756, 0.7606, 0.0915, 1.0000])\n",
      "tensor([0.1155, 0.1176, 0.7669, 1.0000, 1.0000])\n",
      "tensor([0.4598, 0.5402, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "NN 1 : tensor(2.5188)\n",
      "CS 1 : 2.5398\n",
      "DP 1 : 1.8724\n",
      "heuristic 1 : 1.8543\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.1616, 0.2312, 0.1707, 0.2611, 0.1753])\n",
      "tensor([0.1992, 0.2692, 0.2111, 0.3205, 1.0000])\n",
      "tensor([0.2956, 0.4044, 0.3000, 1.0000, 1.0000])\n",
      "tensor([0.4469, 0.5531, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/10000 (0%)]\tLoss: 2.600687 testing loss: tensor(2.4961)\n",
      "Train Epoch: 1 [1280/10000 (13%)]\tLoss: 1.929259 testing loss: tensor(1.9620)\n",
      "Train Epoch: 1 [2560/10000 (25%)]\tLoss: 1.911206 testing loss: tensor(1.9010)\n",
      "Train Epoch: 1 [3840/10000 (38%)]\tLoss: 1.773359 testing loss: tensor(1.8420)\n",
      "Train Epoch: 1 [5120/10000 (51%)]\tLoss: 1.753941 testing loss: tensor(1.8425)\n",
      "Train Epoch: 1 [6400/10000 (63%)]\tLoss: 1.938259 testing loss: tensor(1.8189)\n",
      "Train Epoch: 1 [7680/10000 (76%)]\tLoss: 1.865343 testing loss: tensor(1.8105)\n",
      "Train Epoch: 1 [8960/10000 (89%)]\tLoss: 1.895418 testing loss: tensor(1.8046)\n",
      "penalty: 0.009835243225097656\n",
      "NN 2 : tensor(1.7891)\n",
      "CS 2 : 2.5398\n",
      "DP 2 : 1.8724\n",
      "heuristic 2 : 1.8543\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0497, 0.0893, 0.0537, 0.7486, 0.0587])\n",
      "tensor([0.0659, 0.1046, 0.0681, 0.7613, 1.0000])\n",
      "tensor([0.1216, 0.7673, 0.1112, 1.0000, 1.0000])\n",
      "tensor([0.2053, 0.7947, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 2 [0/10000 (0%)]\tLoss: 1.818138 testing loss: tensor(1.7896)\n",
      "Train Epoch: 2 [1280/10000 (13%)]\tLoss: 1.806665 testing loss: tensor(1.7875)\n",
      "Train Epoch: 2 [2560/10000 (25%)]\tLoss: 1.713661 testing loss: tensor(1.7859)\n",
      "Train Epoch: 2 [3840/10000 (38%)]\tLoss: 1.777551 testing loss: tensor(1.7915)\n",
      "Train Epoch: 2 [5120/10000 (51%)]\tLoss: 1.814674 testing loss: tensor(1.7901)\n",
      "Train Epoch: 2 [6400/10000 (63%)]\tLoss: 2.077929 testing loss: tensor(1.7871)\n",
      "Train Epoch: 2 [7680/10000 (76%)]\tLoss: 1.965104 testing loss: tensor(1.7842)\n",
      "Train Epoch: 2 [8960/10000 (89%)]\tLoss: 1.846025 testing loss: tensor(1.7812)\n",
      "penalty: 0.0\n",
      "NN 3 : tensor(1.7782)\n",
      "CS 3 : 2.5398\n",
      "DP 3 : 1.8724\n",
      "heuristic 3 : 1.8543\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0648, 0.0665, 0.0681, 0.7300, 0.0706])\n",
      "tensor([0.0842, 0.0832, 0.0825, 0.7501, 1.0000])\n",
      "tensor([0.1161, 0.7595, 0.1244, 1.0000, 1.0000])\n",
      "tensor([0.2297, 0.7703, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 3 [0/10000 (0%)]\tLoss: 1.705962 testing loss: tensor(1.7780)\n",
      "Train Epoch: 3 [1280/10000 (13%)]\tLoss: 1.721561 testing loss: tensor(1.7755)\n",
      "Train Epoch: 3 [2560/10000 (25%)]\tLoss: 1.598782 testing loss: tensor(1.7590)\n",
      "Train Epoch: 3 [3840/10000 (38%)]\tLoss: 1.791211 testing loss: tensor(1.7396)\n",
      "Train Epoch: 3 [5120/10000 (51%)]\tLoss: 1.803647 testing loss: tensor(1.7375)\n",
      "Train Epoch: 3 [6400/10000 (63%)]\tLoss: 1.637978 testing loss: tensor(1.7344)\n",
      "Train Epoch: 3 [7680/10000 (76%)]\tLoss: 1.939183 testing loss: tensor(1.7285)\n",
      "Train Epoch: 3 [8960/10000 (89%)]\tLoss: 1.763071 testing loss: tensor(1.7274)\n",
      "penalty: 0.0021656155586242676\n",
      "NN 4 : tensor(1.7252)\n",
      "CS 4 : 2.5398\n",
      "DP 4 : 1.8724\n",
      "heuristic 4 : 1.8543\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0601, 0.0622, 0.0675, 0.7514, 0.0589])\n",
      "tensor([0.0796, 0.0745, 0.0819, 0.7639, 1.0000])\n",
      "tensor([0.1294, 0.7598, 0.1108, 1.0000, 1.0000])\n",
      "tensor([0.2370, 0.7630, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 4 [0/10000 (0%)]\tLoss: 1.829615 testing loss: tensor(1.7235)\n",
      "Train Epoch: 4 [1280/10000 (13%)]\tLoss: 2.000576 testing loss: tensor(1.7236)\n",
      "Train Epoch: 4 [2560/10000 (25%)]\tLoss: 1.906279 testing loss: tensor(1.7269)\n",
      "Train Epoch: 4 [3840/10000 (38%)]\tLoss: 1.615271 testing loss: tensor(1.7247)\n",
      "Train Epoch: 4 [5120/10000 (51%)]\tLoss: 1.651291 testing loss: tensor(1.7274)\n",
      "Train Epoch: 4 [6400/10000 (63%)]\tLoss: 1.889350 testing loss: tensor(1.7329)\n",
      "Train Epoch: 4 [7680/10000 (76%)]\tLoss: 1.948299 testing loss: tensor(1.7338)\n",
      "Train Epoch: 4 [8960/10000 (89%)]\tLoss: 1.675610 testing loss: tensor(1.7290)\n",
      "penalty: 0.0\n",
      "NN 5 : tensor(1.7326)\n",
      "CS 5 : 2.5398\n",
      "DP 5 : 1.8724\n",
      "heuristic 5 : 1.8543\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0615, 0.0676, 0.0631, 0.7439, 0.0640])\n",
      "tensor([0.0853, 0.0854, 0.0804, 0.7489, 1.0000])\n",
      "tensor([0.1166, 0.7829, 0.1006, 1.0000, 1.0000])\n",
      "tensor([0.2071, 0.7929, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "NN 1 : tensor(2.5321)\n",
      "CS 1 : 2.5398\n",
      "DP 1 : 1.8724\n",
      "heuristic 1 : 1.8543\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.2234, 0.1819, 0.1862, 0.1891, 0.2194])\n",
      "tensor([0.2866, 0.2242, 0.2626, 0.2265, 1.0000])\n",
      "tensor([0.3825, 0.2688, 0.3487, 1.0000, 1.0000])\n",
      "tensor([0.5688, 0.4312, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/10000 (0%)]\tLoss: 2.562500 testing loss: tensor(2.5321)\n",
      "Train Epoch: 1 [1280/10000 (13%)]\tLoss: 2.421875 testing loss: tensor(2.5321)\n",
      "Train Epoch: 1 [2560/10000 (25%)]\tLoss: 2.453125 testing loss: tensor(2.5321)\n",
      "Train Epoch: 1 [3840/10000 (38%)]\tLoss: 2.554688 testing loss: tensor(2.5321)\n",
      "Train Epoch: 1 [5120/10000 (51%)]\tLoss: 2.523438 testing loss: tensor(2.5321)\n",
      "Train Epoch: 1 [6400/10000 (63%)]\tLoss: 2.710938 testing loss: tensor(2.5321)\n",
      "Train Epoch: 1 [7680/10000 (76%)]\tLoss: 2.390625 testing loss: tensor(2.5321)\n",
      "Train Epoch: 1 [8960/10000 (89%)]\tLoss: 2.507812 testing loss: tensor(2.5321)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(2.5321)\n",
      "CS 2 : 2.5398\n",
      "DP 2 : 1.8724\n",
      "heuristic 2 : 1.8543\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.2234, 0.1819, 0.1862, 0.1891, 0.2194])\n",
      "tensor([0.2866, 0.2242, 0.2626, 0.2265, 1.0000])\n",
      "tensor([0.3825, 0.2688, 0.3487, 1.0000, 1.0000])\n",
      "tensor([0.5688, 0.4312, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 2 [0/10000 (0%)]\tLoss: 2.718750 testing loss: tensor(2.5321)\n",
      "Train Epoch: 2 [1280/10000 (13%)]\tLoss: 2.609375 testing loss: tensor(2.5321)\n",
      "Train Epoch: 2 [2560/10000 (25%)]\tLoss: 2.609375 testing loss: tensor(2.5321)\n",
      "Train Epoch: 2 [3840/10000 (38%)]\tLoss: 2.476562 testing loss: tensor(2.5321)\n",
      "Train Epoch: 2 [5120/10000 (51%)]\tLoss: 2.421875 testing loss: tensor(2.5321)\n",
      "Train Epoch: 2 [6400/10000 (63%)]\tLoss: 2.390625 testing loss: tensor(2.5321)\n",
      "Train Epoch: 2 [7680/10000 (76%)]\tLoss: 2.453125 testing loss: tensor(2.5321)\n",
      "Train Epoch: 2 [8960/10000 (89%)]\tLoss: 2.687500 testing loss: tensor(2.5321)\n",
      "penalty: 0.0\n",
      "NN 3 : tensor(2.5321)\n",
      "CS 3 : 2.5398\n",
      "DP 3 : 1.8724\n",
      "heuristic 3 : 1.8543\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.2234, 0.1819, 0.1862, 0.1891, 0.2194])\n",
      "tensor([0.2866, 0.2242, 0.2626, 0.2265, 1.0000])\n",
      "tensor([0.3825, 0.2688, 0.3487, 1.0000, 1.0000])\n",
      "tensor([0.5688, 0.4312, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 3 [0/10000 (0%)]\tLoss: 2.500000 testing loss: tensor(2.5321)\n",
      "Train Epoch: 3 [1280/10000 (13%)]\tLoss: 2.398438 testing loss: tensor(2.5321)\n",
      "Train Epoch: 3 [2560/10000 (25%)]\tLoss: 2.570312 testing loss: tensor(2.5321)\n",
      "Train Epoch: 3 [3840/10000 (38%)]\tLoss: 2.445312 testing loss: tensor(2.5321)\n",
      "Train Epoch: 3 [5120/10000 (51%)]\tLoss: 2.507812 testing loss: tensor(2.5321)\n",
      "Train Epoch: 3 [6400/10000 (63%)]\tLoss: 2.476562 testing loss: tensor(2.5321)\n",
      "Train Epoch: 3 [7680/10000 (76%)]\tLoss: 2.664062 testing loss: tensor(2.5321)\n",
      "Train Epoch: 3 [8960/10000 (89%)]\tLoss: 2.679688 testing loss: tensor(2.5321)\n",
      "penalty: 0.0\n",
      "NN 4 : tensor(2.5321)\n",
      "CS 4 : 2.5398\n",
      "DP 4 : 1.8724\n",
      "heuristic 4 : 1.8543\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.2234, 0.1819, 0.1862, 0.1891, 0.2194])\n",
      "tensor([0.2866, 0.2242, 0.2626, 0.2265, 1.0000])\n",
      "tensor([0.3825, 0.2688, 0.3487, 1.0000, 1.0000])\n",
      "tensor([0.5688, 0.4312, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 4 [0/10000 (0%)]\tLoss: 2.554688 testing loss: tensor(2.5321)\n",
      "Train Epoch: 4 [1280/10000 (13%)]\tLoss: 2.695312 testing loss: tensor(2.5321)\n",
      "Train Epoch: 4 [2560/10000 (25%)]\tLoss: 2.406250 testing loss: tensor(2.5321)\n",
      "Train Epoch: 4 [3840/10000 (38%)]\tLoss: 2.578125 testing loss: tensor(2.5321)\n",
      "Train Epoch: 4 [5120/10000 (51%)]\tLoss: 2.390625 testing loss: tensor(2.5321)\n",
      "Train Epoch: 4 [6400/10000 (63%)]\tLoss: 2.687500 testing loss: tensor(2.5321)\n",
      "Train Epoch: 4 [7680/10000 (76%)]\tLoss: 2.406250 testing loss: tensor(2.5321)\n",
      "Train Epoch: 4 [8960/10000 (89%)]\tLoss: 2.359375 testing loss: tensor(2.5321)\n",
      "penalty: 0.0\n",
      "NN 5 : tensor(2.5321)\n",
      "CS 5 : 2.5398\n",
      "DP 5 : 1.8724\n",
      "heuristic 5 : 1.8543\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.2234, 0.1819, 0.1862, 0.1891, 0.2194])\n",
      "tensor([0.2866, 0.2242, 0.2626, 0.2265, 1.0000])\n",
      "tensor([0.3825, 0.2688, 0.3487, 1.0000, 1.0000])\n",
      "tensor([0.5688, 0.4312, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "    \n",
    "for order in stage:\n",
    "    producedata(order)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "    #running\n",
    "    for order1 in order1name:\n",
    "        print(\"Supervised Aim:\",order,order1)\n",
    "        for trainingnumber in numberofpeople:\n",
    "            # for mapping binary to payments before softmax\n",
    "            model = nn.Sequential(\n",
    "                nn.Linear(n, 100),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(100, 100),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(100, 100),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(100, n),\n",
    "            )\n",
    "            model.apply(init_weights)\n",
    "            # optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "            runningLossNN = []\n",
    "            runningLossCS = []\n",
    "            runningLossDP = []\n",
    "            runningLossHeuristic = []\n",
    "            #model=torch.load(\"save/pytorchNN=5dp1\");\n",
    "            #model.eval()\n",
    "            ##order1name=[\"costsharing\",\"dp\",\"heuristic\",\"random initializing\"]\n",
    "            for epoch in range(1, supervisionEpochs + 1):\n",
    "        #             print(\"distributionRatio\",distributionRatio)\n",
    "                if(order1==\"costsharing\"):\n",
    "                    supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "                elif(order1==\"dp\"):\n",
    "                    supervisionTrain(epoch, dpSupervisionRule)\n",
    "                elif(order1==\"heuristic\"):\n",
    "                    supervisionTrain(epoch, heuristicSupervisionRule)\n",
    "                elif(order1==\"random initializing\"):\n",
    "                    print(\"do nothing\");\n",
    "\n",
    "            test()\n",
    "\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                if(trainingnumber=='0'):\n",
    "                    train0(epoch)\n",
    "                if(trainingnumber=='1'):\n",
    "                    train1(epoch)\n",
    "                if(trainingnumber=='2'):\n",
    "                    train2(epoch)\n",
    "                test()\n",
    "            losslistname.append(order+\" \"+order1+\" choose people:\"+trainingnumber);\n",
    "            losslist.append(losslisttemp);\n",
    "            losslisttemp=[];\n",
    "            savepath=\"save/pytorchNN=5all-phoenix\"+order+str(order1)+trainingnumber\n",
    "            torch.save(model, savepath);\n",
    "            print(\"end\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colorlist=[\"#D2691E\",'#4169E1',\"#9ACD32\",\"#B22222\",\"#FF00FF\",\"#708090\"]\n",
    "for i in range(len(losslist)):\n",
    "    plt.plot(losslist[i], '.-', color=colorlist[i], alpha=0.8, label=losslistname[i])\n",
    "\n",
    "# 显示标签，如果不加这句，即使加了label='一些数字'的参数，最终还是不会显示标签\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('trianing times')\n",
    "plt.ylabel('delay')\n",
    "plt.title(\"n=\"+str(n))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
