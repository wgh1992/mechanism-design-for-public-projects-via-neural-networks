{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from random import randint\n",
    "import random\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "from pylab import *\n",
    "n = 5\n",
    "epochs = 4\n",
    "supervisionEpochs = 3\n",
    "lr = 0.001\n",
    "log_interval = 10\n",
    "trainSize = 40000#100000\n",
    "percentage_train_test= 0.25\n",
    "penaltyLambda = 10\n",
    "doublePeakHighMean = 0.9\n",
    "doublePeakLowMean = 0.1\n",
    "doublePeakStd = 0.1\n",
    "uniformlow=0\n",
    "uniformhigh=1.0\n",
    "normalloc = 0.2\n",
    "normalscale = 0.1\n",
    "\n",
    "cauchyloc = 1/n\n",
    "cauchyscalen = 0.004\n",
    "\n",
    "independentnormalloc1=[(float(ii)+1)/(2*n+1) for ii in range(n,0,-1)]\n",
    "independentnormalscale1=[0.05 for ii in range(n)]\n",
    "\n",
    "independentnormalloc2=[(float(ii)+1)/(2*n+1) for ii in range(1,n+1,1)]\n",
    "independentnormalscale2=[0.05 for ii in range(n)]\n",
    "stage=[\"twopeak\"]\n",
    "# \"twopeak\",\"normal\",\"uniform\",\"independent1\",\"independent2\",\"cauchy\"\n",
    "order1name=[\"costsharing\",\"dp\",\"random initializing\"]\n",
    "# \"costsharing\",\"dp\",\"heuristic\",\"random initializing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "d3 = D.normal.Normal(normalloc, normalscale)\n",
    "d4 = D.uniform.Uniform(uniformlow,uniformhigh)\n",
    "\n",
    "d5 = [D.normal.Normal(independentnormalloc1[ii], independentnormalscale1[ii]) for ii in range(n)]\n",
    "d6 = [D.normal.Normal(independentnormalloc2[ii], independentnormalscale2[ii]) for ii in range(n)]\n",
    "\n",
    "d7 = D.cauchy.Cauchy(cauchyloc,cauchyscalen)\n",
    "def cdf(x,y, i=None):\n",
    "    if(y==\"twopeak\"):\n",
    "        return (d1.cdf(x) + d2.cdf(x) - distributionBase) / 2 / distributionRatio\n",
    "    elif(y==\"normal\"):\n",
    "        return d3.cdf(x);\n",
    "    elif(y==\"uniform\"):\n",
    "        return d4.cdf(x);\n",
    "    elif(y==\"independent1\"):\n",
    "        return d5[i].cdf(x);\n",
    "    elif(y==\"independent2\"):\n",
    "        return d6[i].cdf(x);\n",
    "    elif(y==\"cauchy\"):\n",
    "        return d7.cdf(x);\n",
    "#print(cdf(0.1,\"independent\",1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    if torch.sum(bits).item() == 0:\n",
    "        return torch.ones(n)\n",
    "    bits = bits.type(torch.float32)\n",
    "    negBits = torch.ones(n) - bits\n",
    "    payments = model(bits)\n",
    "    payments = payments - 1000 * negBits\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    payments = payments + negBits\n",
    "    return payments\n",
    "\n",
    "def tpToBits(tp, bits=torch.ones(n).type(torch.uint8)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = (tp >= payments).type(torch.uint8)\n",
    "    if torch.equal(newBits, bits):\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits(tp, newBits)#bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    return n - torch.sum(tpToBits(tp).type(torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    global  dp,decision\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money, yes]\n",
    "        offer = float(offerIndex) / dpPrecision\n",
    "        if tp[i] >= offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits[i] = 0\n",
    "            payments[i] = 1\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "#     print()\n",
    "#     print(tp)\n",
    "#     print(bits)\n",
    "#     print(payments)\n",
    "#     print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def heuristicSupervisionRule(tp):\n",
    "    global  dp_H,decision_H\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    tempold=-1;\n",
    "    for turns in range(n,0,-1):\n",
    "        money = dpPrecision\n",
    "        j=0\n",
    "        tempo=sum(bits)\n",
    "        #print(\"bits\",tempo)\n",
    "        for i in range(n):\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            \n",
    "            offerIndex = decision_H[tempo ,tempo - i, money]\n",
    "            offer = float(offerIndex) / dpPrecision\n",
    "            while(j<n):\n",
    "                if(bits[j]!=0):\n",
    "                    break;\n",
    "                j+=1;\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            if tp[j] >= offer:\n",
    "                #print(money,j,tp[j],offer)\n",
    "                money -= offerIndex\n",
    "                bits[j] = 1\n",
    "                payments[j] = offer\n",
    "            else:\n",
    "                bits[j] = 0;\n",
    "                payments[j] = 1.0;\n",
    "            j+=1;\n",
    "        #print(\"money\",money)\n",
    "        if(money==0 and tempold==tempo):\n",
    "            break;\n",
    "        tempold=tempo;\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    for k in range(n, -1, -1):\n",
    "        if k == 0:\n",
    "            break\n",
    "        bits = [1 if tp[ii] >= 1.0 / k else 0 for ii in range(n)]\n",
    "        if sum(bits) == k:\n",
    "            break\n",
    "    if k == 0:\n",
    "        payments = [1 for ii in range(n)]\n",
    "    else:\n",
    "        payments = [1.0 / k if bits[ii] == 1 else 1 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    return float(n - torch.sum(costSharingSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def dpDelay(tp):\n",
    "    return float(n - torch.sum(dpSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def heuristicDelay(tp):\n",
    "    return float(n - torch.sum(heuristicSupervisionRule(tp)[0]).item())\n",
    "\n",
    "\n",
    "# templire=0;\n",
    "# num=1\n",
    "# for i in range(num):\n",
    "#     temp=torch.tensor(samplesJoint[i])\n",
    "#     #print(temp)\n",
    "#     #print(temp);\n",
    "#     #print(heuristicSupervisionRule(temp))\n",
    "#     #print(dpSupervisionRule(temp))\n",
    "#     #print(dpDelay(temp))\n",
    "#     res=dpDelay(temp)\n",
    "#     templire+=res\n",
    "#     #print(\"delay\",res)\n",
    "#     #print()\n",
    "# print(templire/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordAndReport(name, source, loss):\n",
    "    source.append(loss)\n",
    "    realLength = len(source)\n",
    "    #print(f\"{name} ({realLength}): {loss}\")\n",
    "    print(name,realLength,\":\" ,loss)\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    \n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda /100000\n",
    "        \n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "losslist=[];\n",
    "losslistname=[];\n",
    "losslisttemp=[];\n",
    "def test_batch():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lenLoss= 0\n",
    "        nnLoss = 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "            for tp in tp_batch:\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        nnLoss/=lenLoss\n",
    "    return nnLoss\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                delay1 = tpToTotalDelay(tp1)\n",
    "                delay0 = tpToTotalDelay(tp0)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                    loss = loss + (1 - cdf(offer,order)) * delay1 + cdf(offer,order) * delay0\n",
    "                else:\n",
    "                    loss = loss + (1 - cdf(offer,order,i)) * delay1 + cdf(offer,order,i) * delay0\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "    \n",
    "allBits = [torch.tensor(bits, dtype=torch.int16) for bits in itertools.product([0, 1], repeat=n)]\n",
    "\n",
    "def test():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        costSharingLoss = 0\n",
    "        dpLoss = 0\n",
    "        nnLoss = 0\n",
    "        heuristicLoss=0\n",
    "        lenLoss= 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "                heuristicLoss+= heuristicDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        costSharingLoss /= lenLoss\n",
    "        dpLoss /= lenLoss\n",
    "        nnLoss /= lenLoss\n",
    "        heuristicLoss /= lenLoss\n",
    "        #print(lenLoss)\n",
    "        losslisttemp.append(nnLoss);\n",
    "        recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "        recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "        recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "        recordAndReport(\"heuristic\", runningLossHeuristic, heuristicLoss)\n",
    "        print(\"DP:\",dp[n, dpPrecision,0])\n",
    "        #for i in range(n, 0, -1):\n",
    "        #    print(\"Heuristic:\",i,5*(1-dp_H[i, i, dpPrecision]))\n",
    "        for i in range(n, 0, -1):\n",
    "            print(\n",
    "                    tpToPayments(\n",
    "                            torch.tensor([0 if ii >= i else 1 for ii in range(n)], dtype=torch.float32)\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "dpPrecision = 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def producedata(order):\n",
    "    global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "    dpPrecision = 100 \n",
    "    if(order==\"twopeak\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = np.random.normal(\n",
    "            loc=doublePeakLowMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=doublePeakLowMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samples2 = np.random.normal(\n",
    "            loc=doublePeakHighMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = np.random.normal(\n",
    "                        loc=doublePeakHighMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "    elif(order==\"normal\"):\n",
    "        print(\"loc\",normalloc, \"scale\",normalscale)\n",
    "        samples1 = np.random.normal(\n",
    "            loc=normalloc, scale=normalscale, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=normalloc, scale=normalscale\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"uniform\"):  \n",
    "        print(\"uniformlow\",uniformlow, \"uniformhigh\",uniformhigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        uniformlow, uniformhigh\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent1\"):\n",
    "        print(\"loc\",independentnormalloc1,\"scale\",independentnormalscale1)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent2\"):\n",
    "        print(\"loc\",independentnormalloc2, \"scale\",independentnormalscale2)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"cauchy\"):\n",
    "        print(\"cauchyloc\",cauchyloc, \"cauchyscale\",cauchyscalen)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        \n",
    "    tp_dataset = TensorDataset(tp_tensor[: int(trainSize * percentage_train_test)])\n",
    "    tp_dataset_testing = TensorDataset(tp_tensor[int(trainSize * (1.0-percentage_train_test)) :])\n",
    "    tp_dataloader = DataLoader(tp_dataset, batch_size=128, shuffle=True)\n",
    "    tp_dataloader_testing = DataLoader(tp_dataset_testing, batch_size=256, shuffle=False)\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "    decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for yes in range(n + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp[0, 0, yes] = 0\n",
    "            else:\n",
    "                dp[0, money, yes] = yes# + 1.0\n",
    "    for ppl in range(1,  n + 1):\n",
    "        for yes in range(n + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                minSoFar = 1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1 - cdf(offer,order)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order) * (1 + dp[ppl - 1, money, yes])\n",
    "                    else:\n",
    "                        res = (1 - cdf(offer,order,n-ppl)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order,n-ppl) * (1 + dp[ppl - 1, money, yes])\n",
    "                    if minSoFar > res.item():\n",
    "                        minSoFar = res.item()\n",
    "                        decision[ppl, money, yes] = offerIndex\n",
    "                dp[ppl, money, yes] = minSoFar\n",
    "    \n",
    "    print(\"dp\",dp[n, dpPrecision, 0])\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp_H = np.zeros([n + 1 , n + 1, dpPrecision + 1])\n",
    "    decision_H = np.zeros([n + 1 , n + 1, dpPrecision + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for i in range(1,n+1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp_H[i, 0, 0] = 1\n",
    "            else:\n",
    "                offer = money / dpPrecision\n",
    "                dp_H[i, 0, money] = 0#cdf(offer)# + 1.0\n",
    "    for i in range(1,n+1):\n",
    "        for ppl in range(1, i + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                maxSoFar = -1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1-cdf(offer,order)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    else:\n",
    "                        res = (1-cdf(offer,order,ppl-1)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    if maxSoFar < res.item():\n",
    "                        maxSoFar = res.item()\n",
    "                        decision_H[i, ppl, money] = offerIndex\n",
    "                dp_H[i, ppl, money] = maxSoFar\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loc 0.1 scale 0.1\n",
      "loc 0.9 scale 0.1\n",
      "dp 1.8651759624481201\n",
      "Supervised Aim: twopeak costsharing\n",
      "Train Epoch: 1 [0/5000 (0%)]\tLoss: 0.000129\n",
      "Train Epoch: 1 [1280/5000 (25%)]\tLoss: 0.000016\n",
      "Train Epoch: 1 [2560/5000 (50%)]\tLoss: 0.000005\n",
      "Train Epoch: 1 [3840/5000 (75%)]\tLoss: 0.000001\n",
      "Train Epoch: 2 [0/5000 (0%)]\tLoss: 0.000001\n",
      "Train Epoch: 2 [1280/5000 (25%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [2560/5000 (50%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [3840/5000 (75%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [0/5000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [1280/5000 (25%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [2560/5000 (50%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [3840/5000 (75%)]\tLoss: 0.000000\n",
      "NN 1 : tensor(2.5702)\n",
      "CS 1 : 2.5702\n",
      "DP 1 : 1.9104\n",
      "heuristic 1 : 1.8748\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500, 1.0000])\n",
      "tensor([0.3333, 0.3333, 0.3334, 1.0000, 1.0000])\n",
      "tensor([0.5000, 0.5000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/5000 (0%)]\tLoss: 2.473871 testing loss: tensor(2.5706)\n",
      "Train Epoch: 1 [1280/5000 (25%)]\tLoss: 2.129407 testing loss: tensor(2.2018)\n",
      "Train Epoch: 1 [2560/5000 (50%)]\tLoss: 2.074615 testing loss: tensor(2.0006)\n",
      "Train Epoch: 1 [3840/5000 (75%)]\tLoss: 1.560843 testing loss: tensor(1.9128)\n",
      "penalty: 0.04004853963851929\n",
      "NN 2 : tensor(1.8856)\n",
      "CS 2 : 2.5702\n",
      "DP 2 : 1.9104\n",
      "heuristic 2 : 1.8748\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.1014, 0.7181, 0.0405, 0.1169, 0.0231])\n",
      "tensor([0.1130, 0.7209, 0.0474, 0.1188, 1.0000])\n",
      "tensor([0.1581, 0.7346, 0.1073, 1.0000, 1.0000])\n",
      "tensor([0.2017, 0.7983, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 2 [0/5000 (0%)]\tLoss: 1.768729 testing loss: tensor(1.9064)\n",
      "Train Epoch: 2 [1280/5000 (25%)]\tLoss: 1.642394 testing loss: tensor(1.8622)\n",
      "Train Epoch: 2 [2560/5000 (50%)]\tLoss: 1.663583 testing loss: tensor(1.8530)\n",
      "Train Epoch: 2 [3840/5000 (75%)]\tLoss: 1.723503 testing loss: tensor(1.8220)\n",
      "penalty: 0.048569656908512115\n",
      "NN 3 : tensor(1.8286)\n",
      "CS 3 : 2.5702\n",
      "DP 3 : 1.9104\n",
      "heuristic 3 : 1.8748\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0694, 0.7346, 0.0584, 0.0799, 0.0579])\n",
      "tensor([0.0884, 0.7556, 0.0719, 0.0842, 1.0000])\n",
      "tensor([0.1040, 0.7804, 0.1156, 1.0000, 1.0000])\n",
      "tensor([0.1587, 0.8413, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 3 [0/5000 (0%)]\tLoss: 2.011135 testing loss: tensor(1.8336)\n",
      "Train Epoch: 3 [1280/5000 (25%)]\tLoss: 1.765428 testing loss: tensor(1.8208)\n",
      "Train Epoch: 3 [2560/5000 (50%)]\tLoss: 1.677580 testing loss: tensor(1.8076)\n",
      "Train Epoch: 3 [3840/5000 (75%)]\tLoss: 1.766574 testing loss: tensor(1.8192)\n",
      "penalty: 0.017748281359672546\n",
      "NN 4 : tensor(1.8100)\n",
      "CS 4 : 2.5702\n",
      "DP 4 : 1.9104\n",
      "heuristic 4 : 1.8748\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0665, 0.7550, 0.0583, 0.0610, 0.0592])\n",
      "tensor([0.0875, 0.7727, 0.0736, 0.0662, 1.0000])\n",
      "tensor([0.1043, 0.7824, 0.1133, 1.0000, 1.0000])\n",
      "tensor([0.1610, 0.8390, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 4 [0/5000 (0%)]\tLoss: 1.857479 testing loss: tensor(1.8080)\n",
      "Train Epoch: 4 [1280/5000 (25%)]\tLoss: 1.668142 testing loss: tensor(1.8064)\n",
      "Train Epoch: 4 [2560/5000 (50%)]\tLoss: 1.639956 testing loss: tensor(1.7968)\n",
      "Train Epoch: 4 [3840/5000 (75%)]\tLoss: 1.794384 testing loss: tensor(1.7916)\n",
      "penalty: 0.0\n",
      "NN 5 : tensor(1.8028)\n",
      "CS 5 : 2.5702\n",
      "DP 5 : 1.9104\n",
      "heuristic 5 : 1.8748\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0620, 0.7508, 0.0622, 0.0623, 0.0627])\n",
      "tensor([0.0816, 0.7697, 0.0792, 0.0695, 1.0000])\n",
      "tensor([0.1013, 0.7821, 0.1166, 1.0000, 1.0000])\n",
      "tensor([0.1580, 0.8420, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: twopeak dp\n",
      "Train Epoch: 1 [0/5000 (0%)]\tLoss: 0.049642\n",
      "Train Epoch: 1 [1280/5000 (25%)]\tLoss: 0.016805\n",
      "Train Epoch: 1 [2560/5000 (50%)]\tLoss: 0.004080\n",
      "Train Epoch: 1 [3840/5000 (75%)]\tLoss: 0.003836\n",
      "Train Epoch: 2 [0/5000 (0%)]\tLoss: 0.002284\n",
      "Train Epoch: 2 [1280/5000 (25%)]\tLoss: 0.001517\n",
      "Train Epoch: 2 [2560/5000 (50%)]\tLoss: 0.001011\n",
      "Train Epoch: 2 [3840/5000 (75%)]\tLoss: 0.000765\n",
      "Train Epoch: 3 [0/5000 (0%)]\tLoss: 0.000419\n",
      "Train Epoch: 3 [1280/5000 (25%)]\tLoss: 0.000212\n",
      "Train Epoch: 3 [2560/5000 (50%)]\tLoss: 0.000165\n",
      "Train Epoch: 3 [3840/5000 (75%)]\tLoss: 0.000157\n",
      "NN 1 : tensor(1.8696)\n",
      "CS 1 : 2.5702\n",
      "DP 1 : 1.9104\n",
      "heuristic 1 : 1.8748\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.7572, 0.0972, 0.0806, 0.0620, 0.0030])\n",
      "tensor([0.6841, 0.1276, 0.1091, 0.0792, 1.0000])\n",
      "tensor([0.7173, 0.1501, 0.1325, 1.0000, 1.0000])\n",
      "tensor([0.8159, 0.1841, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/5000 (0%)]\tLoss: 1.788681 testing loss: tensor(1.8746)\n",
      "Train Epoch: 1 [1280/5000 (25%)]\tLoss: 1.856220 testing loss: tensor(1.8670)\n",
      "Train Epoch: 1 [2560/5000 (50%)]\tLoss: 1.934088 testing loss: tensor(1.8568)\n",
      "Train Epoch: 1 [3840/5000 (75%)]\tLoss: 1.592918 testing loss: tensor(1.8422)\n",
      "penalty: 0.0049874186515808105\n",
      "NN 2 : tensor(1.8288)\n",
      "CS 2 : 2.5702\n",
      "DP 2 : 1.9104\n",
      "heuristic 2 : 1.8748\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.7412, 0.0636, 0.0798, 0.0697, 0.0456])\n",
      "tensor([0.7466, 0.0756, 0.0951, 0.0828, 1.0000])\n",
      "tensor([0.7708, 0.1141, 0.1151, 1.0000, 1.0000])\n",
      "tensor([0.8474, 0.1526, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 2 [0/5000 (0%)]\tLoss: 1.693636 testing loss: tensor(1.8380)\n",
      "Train Epoch: 2 [1280/5000 (25%)]\tLoss: 1.791635 testing loss: tensor(1.8256)\n",
      "Train Epoch: 2 [2560/5000 (50%)]\tLoss: 1.719682 testing loss: tensor(1.8146)\n",
      "Train Epoch: 2 [3840/5000 (75%)]\tLoss: 1.804618 testing loss: tensor(1.8246)\n",
      "penalty: 0.03206387162208557\n",
      "NN 3 : tensor(1.8184)\n",
      "CS 3 : 2.5702\n",
      "DP 3 : 1.9104\n",
      "heuristic 3 : 1.8748\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.7515, 0.0603, 0.0647, 0.0631, 0.0604])\n",
      "tensor([0.7559, 0.0778, 0.0824, 0.0838, 1.0000])\n",
      "tensor([0.7650, 0.1231, 0.1119, 1.0000, 1.0000])\n",
      "tensor([0.8373, 0.1627, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 3 [0/5000 (0%)]\tLoss: 1.658346 testing loss: tensor(1.8156)\n",
      "Train Epoch: 3 [1280/5000 (25%)]\tLoss: 1.805421 testing loss: tensor(1.8108)\n",
      "Train Epoch: 3 [2560/5000 (50%)]\tLoss: 1.809667 testing loss: tensor(1.8306)\n",
      "Train Epoch: 3 [3840/5000 (75%)]\tLoss: 1.689013 testing loss: tensor(1.8142)\n",
      "penalty: 0.007751531898975372\n",
      "NN 4 : tensor(1.8208)\n",
      "CS 4 : 2.5702\n",
      "DP 4 : 1.9104\n",
      "heuristic 4 : 1.8748\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.7501, 0.0650, 0.0606, 0.0632, 0.0610])\n",
      "tensor([0.7707, 0.0768, 0.0724, 0.0801, 1.0000])\n",
      "tensor([0.7737, 0.1222, 0.1040, 1.0000, 1.0000])\n",
      "tensor([0.8369, 0.1631, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 4 [0/5000 (0%)]\tLoss: 1.842347 testing loss: tensor(1.8222)\n",
      "Train Epoch: 4 [1280/5000 (25%)]\tLoss: 1.717571 testing loss: tensor(1.8238)\n",
      "Train Epoch: 4 [2560/5000 (50%)]\tLoss: 1.624380 testing loss: tensor(1.8208)\n",
      "Train Epoch: 4 [3840/5000 (75%)]\tLoss: 1.592531 testing loss: tensor(1.8114)\n",
      "penalty: 0.022850502282381058\n",
      "NN 5 : tensor(1.7954)\n",
      "CS 5 : 2.5702\n",
      "DP 5 : 1.9104\n",
      "heuristic 5 : 1.8748\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.7444, 0.0619, 0.0613, 0.0727, 0.0597])\n",
      "tensor([0.7551, 0.0789, 0.0731, 0.0928, 1.0000])\n",
      "tensor([0.7631, 0.1274, 0.1095, 1.0000, 1.0000])\n",
      "tensor([0.8234, 0.1766, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: twopeak random initializing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "NN 1 : tensor(2.5498)\n",
      "CS 1 : 2.5702\n",
      "DP 1 : 1.9104\n",
      "heuristic 1 : 1.8748\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.2552, 0.2087, 0.1950, 0.1677, 0.1733])\n",
      "tensor([0.3017, 0.2271, 0.2411, 0.2301, 1.0000])\n",
      "tensor([0.3767, 0.3239, 0.2994, 1.0000, 1.0000])\n",
      "tensor([0.5369, 0.4631, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/5000 (0%)]\tLoss: 2.659602 testing loss: tensor(2.5376)\n",
      "Train Epoch: 1 [1280/5000 (25%)]\tLoss: 1.945352 testing loss: tensor(1.9880)\n",
      "Train Epoch: 1 [2560/5000 (50%)]\tLoss: 2.135695 testing loss: tensor(1.9550)\n",
      "Train Epoch: 1 [3840/5000 (75%)]\tLoss: 1.822073 testing loss: tensor(1.9110)\n",
      "penalty: 0.021319061517715454\n",
      "NN 2 : tensor(1.8940)\n",
      "CS 2 : 2.5702\n",
      "DP 2 : 1.9104\n",
      "heuristic 2 : 1.8748\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.7567, 0.1476, 0.0380, 0.0270, 0.0307])\n",
      "tensor([0.7764, 0.1456, 0.0442, 0.0338, 1.0000])\n",
      "tensor([0.7892, 0.1606, 0.0502, 1.0000, 1.0000])\n",
      "tensor([0.8090, 0.1910, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [0/5000 (0%)]\tLoss: 1.681788 testing loss: tensor(1.8928)\n",
      "Train Epoch: 2 [1280/5000 (25%)]\tLoss: 1.701211 testing loss: tensor(1.8802)\n",
      "Train Epoch: 2 [2560/5000 (50%)]\tLoss: 1.875496 testing loss: tensor(1.8530)\n",
      "Train Epoch: 2 [3840/5000 (75%)]\tLoss: 1.801189 testing loss: tensor(1.8256)\n",
      "penalty: 0.005115032196044922\n",
      "NN 3 : tensor(1.8080)\n",
      "CS 3 : 2.5702\n",
      "DP 3 : 1.9104\n",
      "heuristic 3 : 1.8748\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.7551, 0.0905, 0.0470, 0.0573, 0.0501])\n",
      "tensor([0.7678, 0.1101, 0.0541, 0.0680, 1.0000])\n",
      "tensor([0.7905, 0.1437, 0.0657, 1.0000, 1.0000])\n",
      "tensor([0.8260, 0.1740, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 3 [0/5000 (0%)]\tLoss: 1.739719 testing loss: tensor(1.8082)\n",
      "Train Epoch: 3 [1280/5000 (25%)]\tLoss: 1.747869 testing loss: tensor(1.8060)\n",
      "Train Epoch: 3 [2560/5000 (50%)]\tLoss: 1.879523 testing loss: tensor(1.8046)\n",
      "Train Epoch: 3 [3840/5000 (75%)]\tLoss: 1.637243 testing loss: tensor(1.8082)\n",
      "penalty: 0.0017070174217224121\n",
      "NN 4 : tensor(1.7960)\n",
      "CS 4 : 2.5702\n",
      "DP 4 : 1.9104\n",
      "heuristic 4 : 1.8748\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.7449, 0.0696, 0.0546, 0.0659, 0.0649])\n",
      "tensor([0.7491, 0.0960, 0.0722, 0.0827, 1.0000])\n",
      "tensor([0.7750, 0.1339, 0.0912, 1.0000, 1.0000])\n",
      "tensor([0.8318, 0.1682, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 4 [0/5000 (0%)]\tLoss: 1.778946 testing loss: tensor(1.8038)\n",
      "Train Epoch: 4 [1280/5000 (25%)]\tLoss: 1.807430 testing loss: tensor(1.8042)\n",
      "Train Epoch: 4 [2560/5000 (50%)]\tLoss: 1.756843 testing loss: tensor(1.7986)\n",
      "Train Epoch: 4 [3840/5000 (75%)]\tLoss: 1.895239 testing loss: tensor(1.7998)\n",
      "penalty: 0.0007152333855628967\n",
      "NN 5 : tensor(1.7968)\n",
      "CS 5 : 2.5702\n",
      "DP 5 : 1.9104\n",
      "heuristic 5 : 1.8748\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.7174, 0.0737, 0.0640, 0.0733, 0.0716])\n",
      "tensor([0.7401, 0.0938, 0.0795, 0.0865, 1.0000])\n",
      "tensor([0.7629, 0.1342, 0.1029, 1.0000, 1.0000])\n",
      "tensor([0.8215, 0.1785, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "    \n",
    "for order in stage:\n",
    "    producedata(order)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "    #running\n",
    "    for order1 in order1name:\n",
    "        print(\"Supervised Aim:\",order,order1)\n",
    "        \n",
    "        # for mapping binary to payments before softmax\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(n, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, n),\n",
    "        )\n",
    "        model.apply(init_weights)\n",
    "        # optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        runningLossNN = []\n",
    "        runningLossCS = []\n",
    "        runningLossDP = []\n",
    "        runningLossHeuristic = []\n",
    "        #model=torch.load(\"save/pytorchNN=5dp1\");\n",
    "        #model.eval()\n",
    "        ##order1name=[\"costsharing\",\"dp\",\"heuristic\",\"random initializing\"]\n",
    "        for epoch in range(1, supervisionEpochs + 1):\n",
    "#             print(\"distributionRatio\",distributionRatio)\n",
    "            if(order1==\"costsharing\"):\n",
    "                supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "            elif(order1==\"dp\"):\n",
    "                supervisionTrain(epoch, dpSupervisionRule)\n",
    "            elif(order1==\"heuristic\"):\n",
    "                supervisionTrain(epoch, heuristicSupervisionRule)\n",
    "            elif(order1==\"random initializing\"):\n",
    "                print(\"do nothing\");\n",
    "\n",
    "        test()\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(epoch)\n",
    "            test()\n",
    "        losslistname.append(order+\" \"+order1);\n",
    "        losslist.append(losslisttemp);\n",
    "        losslisttemp=[];\n",
    "        savepath=\"save/pytorchNN=5all-phoenix\"+order+str(order1)\n",
    "        torch.save(model, savepath);\n",
    "        print(\"end\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colorlist=[\"#D2691E\",'#4169E1',\"#9ACD32\",\"#B22222\",\"#FF00FF\",\"#708090\"]\n",
    "for i in range(len(losslist)):\n",
    "    plt.plot(losslist[i], 'ro-', color=colorlist[i], alpha=0.8, label=losslistname[i])\n",
    "\n",
    "# label=''\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel('trianing times')\n",
    "plt.ylabel('delay')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
