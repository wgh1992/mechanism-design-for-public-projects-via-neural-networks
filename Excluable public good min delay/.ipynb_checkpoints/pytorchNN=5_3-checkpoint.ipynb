{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40058530825361593\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from random import randint\n",
    "import random\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "from pylab import *\n",
    "from scipy.stats import beta\n",
    "import math\n",
    "n = 5\n",
    "epochs = 1\n",
    "supervisionEpochs = 1\n",
    "lr1 = 0.01\n",
    "lr2 = 0.0005\n",
    "log_interval = 5\n",
    "trainSize = 50000#100000\n",
    "percentage_train_test= 0.5\n",
    "penaltyLambda = 100\n",
    "doublePeakHighMean = 0.85\n",
    "doublePeakLowMean = 0.15\n",
    "doublePeakStd = 0.1\n",
    "uniformlow=0\n",
    "uniformhigh=1.0\n",
    "normalloc = 0.2\n",
    "normalscale = 0.1\n",
    "\n",
    "cauchyloc = 1/n\n",
    "cauchyscalen = 0.004\n",
    "\n",
    "exponentialhigh = 15 #Symbol(\"b\", real=True)\n",
    "exponentiallow  = 15 #Symbol(\"a\", real=True)\n",
    "\n",
    "\n",
    "beta_a = 0.3\n",
    "beta_b  = 0.2\n",
    "kumaraswamy_a = beta_a \n",
    "kumaraswamy_b = (1.0+(beta_a-1.0)*math.pow( (beta_a+beta_b-2.0)/(beta_a-1.0), beta_a) )/beta_a \n",
    "print(kumaraswamy_b)\n",
    "\n",
    "\n",
    "independentnormalloc1=[(float(ii)+1)/(2*n+1) for ii in range(n,0,-1)]\n",
    "independentnormalscale1=[0.05 for ii in range(n)]\n",
    "\n",
    "independentnormalloc2=[(float(ii)+1)/(2*n+1) for ii in range(1,n+1,1)]\n",
    "independentnormalscale2=[0.05 for ii in range(n)]\n",
    "stage=[\"twopeak\"]\n",
    "# \"twopeak\",\"normal\",\"uniform\",\"independent1\",\"independent2\",\"cauchy\",\"beta\",\"U-exponential\",\"arcsine\"\n",
    "order1name=[\"dp\",\"random initializing\",\"costsharing\",\"heuristic\"]\n",
    "# \"costsharing\",\"dp\",\"heuristic\",\"random initializing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio1 = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase1 = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "d3 = D.normal.Normal(normalloc, normalscale)\n",
    "distributionRatio3 = d3.cdf(1) - d3.cdf(0)\n",
    "distributionBase3 = d3.cdf(0)\n",
    "\n",
    "d4 = D.uniform.Uniform(uniformlow,uniformhigh)\n",
    "distributionRatio4 = d4.cdf(1) - d4.cdf(0)\n",
    "distributionBase4 = d4.cdf(0)\n",
    "\n",
    "d5 = [D.normal.Normal(independentnormalloc1[ii], independentnormalscale1[ii]) for ii in range(n)]\n",
    "d6 = [D.normal.Normal(independentnormalloc2[ii], independentnormalscale2[ii]) for ii in range(n)]\n",
    "\n",
    "d7 = D.cauchy.Cauchy(cauchyloc,cauchyscalen)\n",
    "\n",
    "d81 = D.exponential.Exponential(exponentiallow)\n",
    "d82 = D.exponential.Exponential(exponentialhigh)\n",
    "\n",
    "d9 = D.beta.Beta(beta_a,beta_b)\n",
    "#sample_d9=d9.rsample(torch.Size([100000]))\n",
    "\n",
    "#d9_sample=np.linspace(0.0001, 0.9999, 10000)\n",
    "#d9_pdf=torch.exp(d9.log_prob(torch.tensor(d9_sample,dtype=torch.float32)))\n",
    "#d9_delta=d9_sample[1]-d9_sample[0]\n",
    "#d9_sum_pdf=torch.sum(d9_pdf*d9_delta)\n",
    "\n",
    "d10 = D.beta.Beta(0.5,0.5)\n",
    "\n",
    "def cdf(x,y, i=None):\n",
    "    if(y==\"twopeak\"):\n",
    "        return (d1.cdf(x) + d2.cdf(x) - distributionBase1) / 2 / distributionRatio1\n",
    "    elif(y==\"normal\"):\n",
    "        return (d3.cdf(x)-distributionBase3)/distributionRatio3;\n",
    "    elif(y==\"uniform\"):\n",
    "        return (d4.cdf(x)-distributionBase4)/distributionRatio4;\n",
    "    elif(y==\"independent1\"):\n",
    "        return d5[i].cdf(x);\n",
    "    elif(y==\"independent2\"):\n",
    "        return d6[i].cdf(x);\n",
    "    elif(y==\"cauchy\"):\n",
    "        return d7.cdf(x);\n",
    "    elif(y==\"beta\"):\n",
    "#         sum_cdf=0.0;\n",
    "#         if(x<0.0001):\n",
    "#             x=0.00011;\n",
    "#         if(x>0.9999):\n",
    "#             x=0.99989;\n",
    "#         for i in range(len(d9_pdf)):\n",
    "#             if(d9_sample[i]<x):\n",
    "#                 sum_cdf+=d9_pdf[i]*d9_delta;\n",
    "#             else:\n",
    "#                 sum_cdf+=(d9_pdf[i]+d9_pdf[i-1])/ 2 *(x-d9_sample[i-1])\n",
    "#                 break;\n",
    "#         return sum_cdf/d9_sum_pdf\n",
    "#         cdf_v=torch.sum((sample_d9<(x)), dtype=torch.float32)/100000\n",
    "#         return cdf_v\n",
    "#    F(x|a,b)=1–(1–x^a)^b\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(x,kumaraswamy_a),kumaraswamy_b);\n",
    "        except:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(torch.tensor(x,dtype=torch.float32),kumaraswamy_a),kumaraswamy_b);\n",
    "    elif(y==\"arcsine\"):\n",
    "        #\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            res=2.0/math.pi * torch.asin(torch.sqrt(x))\n",
    "            #print(x)\n",
    "            return res# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "        except:\n",
    "            return 2.0/math.pi * torch.asin(torch.sqrt(torch.tensor(x,dtype=torch.float32)))# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "    elif(y==\"U-exponential\"):\n",
    "        return (d81.cdf(x) + (1.0 - d82.cdf(1.0-x)))  / 2 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    if torch.sum(bits).item() == 0:\n",
    "        return torch.ones(n)\n",
    "    bits = bits.type(torch.float32)\n",
    "    negBits = torch.ones(n) - bits\n",
    "    payments = model(bits)\n",
    "    payments = payments - 1000 * negBits\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    payments = payments + negBits\n",
    "    return payments\n",
    "\n",
    "def tpToBits(tp, bits=torch.ones(n).type(torch.uint8)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = (tp >= payments).type(torch.uint8)\n",
    "    if torch.equal(newBits, bits):\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits(tp, newBits)#bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    return n - torch.sum(tpToBits(tp).type(torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    global  dp,decision\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money, yes]\n",
    "        offer = float(offerIndex) / dpPrecision\n",
    "        if tp[i] >= offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits[i] = 0\n",
    "            payments[i] = 1\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "#     print()\n",
    "#     print(tp)\n",
    "#     print(bits)\n",
    "#     print(payments)\n",
    "#     print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def heuristicSupervisionRule(tp):\n",
    "    global  dp_H,decision_H\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    tempold=-1;\n",
    "    for turns in range(n,0,-1):\n",
    "        money = dpPrecision\n",
    "        j=0\n",
    "        tempo=sum(bits)\n",
    "        #print(\"bits\",tempo)\n",
    "        for i in range(n):\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            \n",
    "            offerIndex = decision_H[tempo ,tempo - i, money]\n",
    "            offer = float(offerIndex) / dpPrecision\n",
    "            while(j<n):\n",
    "                if(bits[j]!=0):\n",
    "                    break;\n",
    "                j+=1;\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            if tp[j] >= offer:\n",
    "                #print(money,j,tp[j],offer)\n",
    "                money -= offerIndex\n",
    "                bits[j] = 1\n",
    "                payments[j] = offer\n",
    "            else:\n",
    "                bits[j] = 0;\n",
    "                payments[j] = 1.0;\n",
    "            j+=1;\n",
    "        #print(\"money\",money)\n",
    "        if(money==0 and tempold==tempo):\n",
    "            break;\n",
    "        tempold=tempo;\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    for k in range(n, -1, -1):\n",
    "        if k == 0:\n",
    "            break\n",
    "        bits = [1 if tp[ii] >= 1.0 / k else 0 for ii in range(n)]\n",
    "        if sum(bits) == k:\n",
    "            break\n",
    "    if k == 0:\n",
    "        payments = [1 for ii in range(n)]\n",
    "    else:\n",
    "        payments = [1.0 / k if bits[ii] == 1 else 1 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    return float(n - torch.sum(costSharingSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def dpDelay(tp):\n",
    "    return float(n - torch.sum(dpSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def heuristicDelay(tp):\n",
    "    return float(n - torch.sum(heuristicSupervisionRule(tp)[0]).item())\n",
    "\n",
    "\n",
    "# templire=0;\n",
    "# num=1\n",
    "# for i in range(num):\n",
    "#     temp=torch.tensor(samplesJoint[i])\n",
    "#     #print(temp)\n",
    "#     #print(temp);\n",
    "#     #print(heuristicSupervisionRule(temp))\n",
    "#     #print(dpSupervisionRule(temp))\n",
    "#     #print(dpDelay(temp))\n",
    "#     res=dpDelay(temp)\n",
    "#     templire+=res\n",
    "#     #print(\"Delay\",res)\n",
    "#     #print()\n",
    "# print(templire/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordAndReport(name, source, loss):\n",
    "    source.append(loss)\n",
    "    realLength = len(source)\n",
    "    #print(f\"{name} ({realLength}): {loss}\")\n",
    "    print(name,realLength,\":\" ,loss)\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    \n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda /100000\n",
    "        \n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % (log_interval*5) == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "losslist=[];\n",
    "losslistname=[];\n",
    "losslisttemp=[];\n",
    "def test_batch():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lenLoss= 0\n",
    "        nnLoss = 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "            for tp in tp_batch:\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        nnLoss/=lenLoss\n",
    "    return nnLoss\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                Delay1 = tpToTotalDelay(tp1)\n",
    "                Delay0 = tpToTotalDelay(tp0)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                    loss = loss + (1 - cdf(offer,order)) * Delay1 + cdf(offer,order) * Delay0\n",
    "                else:\n",
    "                    loss = loss + (1 - cdf(offer,order,i)) * Delay1 + cdf(offer,order,i) * Delay0\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "    \n",
    "allBits = [torch.tensor(bits, dtype=torch.int16) for bits in itertools.product([0, 1], repeat=n)]\n",
    "\n",
    "def test():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        costSharingLoss = 0\n",
    "        dpLoss = 0\n",
    "        nnLoss = 0\n",
    "        heuristicLoss=0\n",
    "        lenLoss= 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "                heuristicLoss+= heuristicDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        costSharingLoss /= lenLoss\n",
    "        dpLoss /= lenLoss\n",
    "        nnLoss /= lenLoss\n",
    "        heuristicLoss /= lenLoss\n",
    "        #print(lenLoss)\n",
    "        losslisttemp.append(nnLoss);\n",
    "        recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "        recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "        recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "        recordAndReport(\"heuristic\", runningLossHeuristic, heuristicLoss)\n",
    "        print(\"DP:\",dp[n, dpPrecision,0])\n",
    "        #for i in range(n, 0, -1):\n",
    "        #    print(\"Heuristic:\",i,5*(1-dp_H[i, i, dpPrecision]))\n",
    "        for i in range(n, 0, -1):\n",
    "            print(\n",
    "                    tpToPayments(\n",
    "                            torch.tensor([0 if ii >= i else 1 for ii in range(n)], dtype=torch.float32)\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "dpPrecision = 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def producedata(order):\n",
    "    global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "    dpPrecision = 100 \n",
    "    if(order==\"twopeak\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = np.random.normal(\n",
    "            loc=doublePeakLowMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=doublePeakLowMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samples2 = np.random.normal(\n",
    "            loc=doublePeakHighMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = np.random.normal(\n",
    "                        loc=doublePeakHighMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "    elif(order==\"normal\"):\n",
    "        print(\"loc\",normalloc, \"scale\",normalscale)\n",
    "        samples1 = np.random.normal(\n",
    "            loc=normalloc, scale=normalscale, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=normalloc, scale=normalscale\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"uniform\"):  \n",
    "        print(\"uniformlow\",uniformlow, \"uniformhigh\",uniformhigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        uniformlow, uniformhigh\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent1\"):\n",
    "        print(\"loc\",independentnormalloc1,\"scale\",independentnormalscale1)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent2\"):\n",
    "        print(\"loc\",independentnormalloc2, \"scale\",independentnormalscale2)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"cauchy\"):\n",
    "        print(\"cauchyloc\",cauchyloc, \"cauchyscale\",cauchyscalen)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"beta\"):\n",
    "        print(\"beta_a\",beta_a, \"beta_b\",beta_b)\n",
    "        print(\"kumaraswamy_a\",kumaraswamy_a, \"kumaraswamy_b\",kumaraswamy_b)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"arcsine\"):\n",
    "        print(\"betalow\",0.5, \"betahigh\",0.5)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"U-exponential\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = d81.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d81.rsample(torch.Size([1])).numpy()\n",
    "                    \n",
    "        samples2 = d82.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        \n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = d82.rsample(torch.Size([1])).numpy()\n",
    "        samples2 = 1.0 - samples2\n",
    "        samplesJoint = signals * samples1 - (signals - 1.0) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "        \n",
    "    plt.hist(samplesJoint,bins=500)\n",
    "    plt.show()\n",
    "    \n",
    "    tp_dataset = TensorDataset(tp_tensor[: int(trainSize * percentage_train_test)])\n",
    "    tp_dataset_testing = TensorDataset(tp_tensor[int(trainSize * (1.0-percentage_train_test)) :])\n",
    "    tp_dataloader = DataLoader(tp_dataset, batch_size=128, shuffle=True)\n",
    "    tp_dataloader_testing = DataLoader(tp_dataset_testing, batch_size=256, shuffle=False)\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "    decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for yes in range(n + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp[0, 0, yes] = 0\n",
    "            else:\n",
    "                dp[0, money, yes] = yes# + 1.0\n",
    "    for ppl in range(1,  n + 1):\n",
    "        for yes in range(n + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                minSoFar = 1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1 - cdf(offer,order)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order) * (1 + dp[ppl - 1, money, yes])\n",
    "                    else:\n",
    "                        res = (1 - cdf(offer,order,n-ppl)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order,n-ppl) * (1 + dp[ppl - 1, money, yes])\n",
    "                    if minSoFar > res.item():\n",
    "                        minSoFar = res.item()\n",
    "                        decision[ppl, money, yes] = offerIndex\n",
    "                dp[ppl, money, yes] = minSoFar\n",
    "    \n",
    "    print(\"dp\",dp[n, dpPrecision, 0])\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp_H = np.zeros([n + 1 , n + 1, dpPrecision + 1])\n",
    "    decision_H = np.zeros([n + 1 , n + 1, dpPrecision + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for i in range(1,n+1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp_H[i, 0, 0] = 1\n",
    "            else:\n",
    "                offer = money / dpPrecision\n",
    "                dp_H[i, 0, money] = 0#cdf(offer)# + 1.0\n",
    "    for i in range(1,n+1):\n",
    "        for ppl in range(1, i + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                maxSoFar = -1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1-cdf(offer,order)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    else:\n",
    "                        res = (1-cdf(offer,order,ppl-1)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    if maxSoFar < res.item():\n",
    "                        maxSoFar = res.item()\n",
    "                        decision_H[i, ppl, money] = offerIndex\n",
    "                dp_H[i, ppl, money] = maxSoFar\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loc 0.15 scale 0.1\n",
      "loc 0.85 scale 0.1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dp 1.7567987442016602\n",
      "Supervised Aim: twopeak dp\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 0.045341\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 0.000577\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 0.000045\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 0.000018\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 0.000007\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 0.000006\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 0.000005\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 0.000004\n",
      "NN 1 : tensor(1.6980)\n",
      "CS 1 : 2.34964\n",
      "DP 1 : 1.7516\n",
      "heuristic 1 : 1.7118\n",
      "DP: 1.7567987442016602\n",
      "tensor([0.6986, 0.1188, 0.0988, 0.0787, 0.0052])\n",
      "tensor([0.6953, 0.1184, 0.0978, 0.0884, 1.0000])\n",
      "tensor([0.7592, 0.1240, 0.1169, 1.0000, 1.0000])\n",
      "tensor([0.8548, 0.1452, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 1.645445 testing loss: tensor(1.6978)\n",
      "Train Epoch: 1 [640/25000 (3%)]\tLoss: 1.733336 testing loss: tensor(1.6992)\n",
      "Train Epoch: 1 [1280/25000 (5%)]\tLoss: 1.740193 testing loss: tensor(1.6896)\n",
      "Train Epoch: 1 [1920/25000 (8%)]\tLoss: 1.713834 testing loss: tensor(1.6850)\n",
      "Train Epoch: 1 [2560/25000 (10%)]\tLoss: 1.679190 testing loss: tensor(1.6878)\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 1.663525 testing loss: tensor(1.6802)\n",
      "Train Epoch: 1 [3840/25000 (15%)]\tLoss: 1.725745 testing loss: tensor(1.6756)\n",
      "Train Epoch: 1 [4480/25000 (18%)]\tLoss: 1.564562 testing loss: tensor(1.6750)\n",
      "Train Epoch: 1 [5120/25000 (20%)]\tLoss: 1.680663 testing loss: tensor(1.6699)\n",
      "Train Epoch: 1 [5760/25000 (23%)]\tLoss: 1.688353 testing loss: tensor(1.6685)\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 1.745939 testing loss: tensor(1.6639)\n",
      "Train Epoch: 1 [7040/25000 (28%)]\tLoss: 1.699975 testing loss: tensor(1.6612)\n",
      "Train Epoch: 1 [7680/25000 (31%)]\tLoss: 1.674283 testing loss: tensor(1.6594)\n",
      "Train Epoch: 1 [8320/25000 (33%)]\tLoss: 1.541785 testing loss: tensor(1.6574)\n",
      "Train Epoch: 1 [8960/25000 (36%)]\tLoss: 1.697940 testing loss: tensor(1.6542)\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 1.424992 testing loss: tensor(1.6550)\n",
      "Train Epoch: 1 [10240/25000 (41%)]\tLoss: 1.926455 testing loss: tensor(1.6550)\n",
      "Train Epoch: 1 [10880/25000 (43%)]\tLoss: 1.551317 testing loss: tensor(1.6512)\n",
      "Train Epoch: 1 [11520/25000 (46%)]\tLoss: 1.660087 testing loss: tensor(1.6485)\n",
      "Train Epoch: 1 [12160/25000 (48%)]\tLoss: 1.573910 testing loss: tensor(1.6447)\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 1.689177 testing loss: tensor(1.6427)\n",
      "Train Epoch: 1 [13440/25000 (54%)]\tLoss: 1.843520 testing loss: tensor(1.6380)\n",
      "Train Epoch: 1 [14080/25000 (56%)]\tLoss: 1.388297 testing loss: tensor(1.6363)\n",
      "Train Epoch: 1 [14720/25000 (59%)]\tLoss: 1.589433 testing loss: tensor(1.6382)\n",
      "Train Epoch: 1 [15360/25000 (61%)]\tLoss: 1.733672 testing loss: tensor(1.6334)\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 1.700989 testing loss: tensor(1.6358)\n",
      "Train Epoch: 1 [16640/25000 (66%)]\tLoss: 1.577199 testing loss: tensor(1.6321)\n",
      "Train Epoch: 1 [17280/25000 (69%)]\tLoss: 1.676728 testing loss: tensor(1.6304)\n",
      "Train Epoch: 1 [17920/25000 (71%)]\tLoss: 1.727140 testing loss: tensor(1.6321)\n",
      "Train Epoch: 1 [18560/25000 (74%)]\tLoss: 1.522392 testing loss: tensor(1.6304)\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 1.640353 testing loss: tensor(1.6290)\n",
      "Train Epoch: 1 [19840/25000 (79%)]\tLoss: 1.713926 testing loss: tensor(1.6271)\n",
      "Train Epoch: 1 [20480/25000 (82%)]\tLoss: 1.490494 testing loss: tensor(1.6291)\n",
      "Train Epoch: 1 [21120/25000 (84%)]\tLoss: 1.734244 testing loss: tensor(1.6294)\n",
      "Train Epoch: 1 [21760/25000 (87%)]\tLoss: 1.685950 testing loss: tensor(1.6303)\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 1.647681 testing loss: tensor(1.6317)\n",
      "Train Epoch: 1 [23040/25000 (92%)]\tLoss: 1.638559 testing loss: tensor(1.6282)\n",
      "Train Epoch: 1 [23680/25000 (94%)]\tLoss: 1.463912 testing loss: tensor(1.6287)\n",
      "Train Epoch: 1 [24320/25000 (97%)]\tLoss: 1.664317 testing loss: tensor(1.6294)\n",
      "Train Epoch: 1 [7800/25000 (99%)]\tLoss: 1.818632 testing loss: tensor(1.6276)\n",
      "penalty: 0.00033539533615112305\n",
      "NN 2 : tensor(1.6276)\n",
      "CS 2 : 2.34964\n",
      "DP 2 : 1.7516\n",
      "heuristic 2 : 1.7118\n",
      "DP: 1.7567987442016602\n",
      "tensor([0.6752, 0.0862, 0.0847, 0.0799, 0.0740])\n",
      "tensor([0.6871, 0.1082, 0.1068, 0.0979, 1.0000])\n",
      "tensor([0.7219, 0.1440, 0.1341, 1.0000, 1.0000])\n",
      "tensor([0.8191, 0.1809, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: twopeak random initializing\n",
      "do nothing\n",
      "NN 1 : tensor(2.3072)\n",
      "CS 1 : 2.34964\n",
      "DP 1 : 1.7516\n",
      "heuristic 1 : 1.7118\n",
      "DP: 1.7567987442016602\n",
      "tensor([0.2011, 0.1704, 0.2780, 0.1899, 0.1605])\n",
      "tensor([0.2368, 0.1985, 0.3268, 0.2379, 1.0000])\n",
      "tensor([0.3139, 0.2402, 0.4459, 1.0000, 1.0000])\n",
      "tensor([0.5676, 0.4324, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 2.275265 testing loss: tensor(2.2748)\n",
      "Train Epoch: 1 [640/25000 (3%)]\tLoss: 2.036259 testing loss: tensor(1.9962)\n",
      "Train Epoch: 1 [1280/25000 (5%)]\tLoss: 1.922234 testing loss: tensor(1.7993)\n",
      "Train Epoch: 1 [1920/25000 (8%)]\tLoss: 1.721726 testing loss: tensor(1.7977)\n",
      "Train Epoch: 1 [2560/25000 (10%)]\tLoss: 1.630031 testing loss: tensor(1.7343)\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 1.708176 testing loss: tensor(1.7232)\n",
      "Train Epoch: 1 [3840/25000 (15%)]\tLoss: 1.505217 testing loss: tensor(1.6946)\n",
      "Train Epoch: 1 [4480/25000 (18%)]\tLoss: 1.718184 testing loss: tensor(1.6920)\n",
      "Train Epoch: 1 [5120/25000 (20%)]\tLoss: 1.759922 testing loss: tensor(1.6795)\n",
      "Train Epoch: 1 [5760/25000 (23%)]\tLoss: 1.718532 testing loss: tensor(1.6779)\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 1.741644 testing loss: tensor(1.6682)\n",
      "Train Epoch: 1 [7040/25000 (28%)]\tLoss: 1.747894 testing loss: tensor(1.6652)\n",
      "Train Epoch: 1 [7680/25000 (31%)]\tLoss: 1.497393 testing loss: tensor(1.6621)\n",
      "Train Epoch: 1 [8320/25000 (33%)]\tLoss: 1.612757 testing loss: tensor(1.6615)\n",
      "Train Epoch: 1 [8960/25000 (36%)]\tLoss: 1.440212 testing loss: tensor(1.6584)\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 1.594294 testing loss: tensor(1.6546)\n",
      "Train Epoch: 1 [10240/25000 (41%)]\tLoss: 1.465066 testing loss: tensor(1.6522)\n",
      "Train Epoch: 1 [10880/25000 (43%)]\tLoss: 1.594235 testing loss: tensor(1.6419)\n",
      "Train Epoch: 1 [11520/25000 (46%)]\tLoss: 1.615693 testing loss: tensor(1.6358)\n",
      "Train Epoch: 1 [12160/25000 (48%)]\tLoss: 1.570502 testing loss: tensor(1.6302)\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 1.594370 testing loss: tensor(1.6251)\n",
      "Train Epoch: 1 [13440/25000 (54%)]\tLoss: 1.767691 testing loss: tensor(1.6205)\n",
      "Train Epoch: 1 [14080/25000 (56%)]\tLoss: 1.673113 testing loss: tensor(1.6248)\n",
      "Train Epoch: 1 [14720/25000 (59%)]\tLoss: 1.546502 testing loss: tensor(1.6193)\n",
      "Train Epoch: 1 [15360/25000 (61%)]\tLoss: 1.632310 testing loss: tensor(1.6205)\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 1.666999 testing loss: tensor(1.6190)\n",
      "Train Epoch: 1 [16640/25000 (66%)]\tLoss: 1.665861 testing loss: tensor(1.6188)\n",
      "Train Epoch: 1 [17280/25000 (69%)]\tLoss: 1.543310 testing loss: tensor(1.6205)\n",
      "Train Epoch: 1 [17920/25000 (71%)]\tLoss: 1.584581 testing loss: tensor(1.6168)\n",
      "Train Epoch: 1 [18560/25000 (74%)]\tLoss: 1.652065 testing loss: tensor(1.6153)\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 1.527741 testing loss: tensor(1.6160)\n",
      "Train Epoch: 1 [19840/25000 (79%)]\tLoss: 1.497954 testing loss: tensor(1.6148)\n",
      "Train Epoch: 1 [20480/25000 (82%)]\tLoss: 1.497268 testing loss: tensor(1.6160)\n",
      "Train Epoch: 1 [21120/25000 (84%)]\tLoss: 1.549063 testing loss: tensor(1.6173)\n",
      "Train Epoch: 1 [21760/25000 (87%)]\tLoss: 1.601239 testing loss: tensor(1.6206)\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 1.552259 testing loss: tensor(1.6186)\n",
      "Train Epoch: 1 [23040/25000 (92%)]\tLoss: 1.492435 testing loss: tensor(1.6166)\n",
      "Train Epoch: 1 [23680/25000 (94%)]\tLoss: 1.723117 testing loss: tensor(1.6170)\n",
      "Train Epoch: 1 [24320/25000 (97%)]\tLoss: 1.705750 testing loss: tensor(1.6155)\n",
      "Train Epoch: 1 [7800/25000 (99%)]\tLoss: 1.525785 testing loss: tensor(1.6171)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.6171)\n",
      "CS 2 : 2.34964\n",
      "DP 2 : 1.7516\n",
      "heuristic 2 : 1.7118\n",
      "DP: 1.7567987442016602\n",
      "tensor([0.0806, 0.0792, 0.6694, 0.0872, 0.0836])\n",
      "tensor([0.1097, 0.0911, 0.6833, 0.1159, 1.0000])\n",
      "tensor([0.1460, 0.1521, 0.7018, 1.0000, 1.0000])\n",
      "tensor([0.7347, 0.2653, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: twopeak costsharing\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 0.003857\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 0.000022\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 0.000001\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 0.000000\n",
      "NN 1 : tensor(2.3497)\n",
      "CS 1 : 2.34964\n",
      "DP 1 : 1.7516\n",
      "heuristic 1 : 1.7118\n",
      "DP: 1.7567987442016602\n",
      "tensor([0.2001, 0.2002, 0.1999, 0.1999, 0.2000])\n",
      "tensor([0.2497, 0.2505, 0.2495, 0.2503, 1.0000])\n",
      "tensor([0.3332, 0.3337, 0.3331, 1.0000, 1.0000])\n",
      "tensor([0.5000, 0.5000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 2.208085 testing loss: tensor(2.3500)\n",
      "Train Epoch: 1 [640/25000 (3%)]\tLoss: 2.341510 testing loss: tensor(2.3490)\n",
      "Train Epoch: 1 [1280/25000 (5%)]\tLoss: 2.059052 testing loss: tensor(2.3482)\n",
      "Train Epoch: 1 [1920/25000 (8%)]\tLoss: 2.238620 testing loss: tensor(2.3473)\n",
      "Train Epoch: 1 [2560/25000 (10%)]\tLoss: 2.382588 testing loss: tensor(2.3462)\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 2.436976 testing loss: tensor(2.3444)\n",
      "Train Epoch: 1 [3840/25000 (15%)]\tLoss: 2.416959 testing loss: tensor(2.3419)\n",
      "Train Epoch: 1 [4480/25000 (18%)]\tLoss: 2.300020 testing loss: tensor(2.3393)\n",
      "Train Epoch: 1 [5120/25000 (20%)]\tLoss: 2.249177 testing loss: tensor(2.3323)\n",
      "Train Epoch: 1 [5760/25000 (23%)]\tLoss: 2.243047 testing loss: tensor(2.3225)\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 2.136254 testing loss: tensor(2.3080)\n",
      "Train Epoch: 1 [7040/25000 (28%)]\tLoss: 2.239197 testing loss: tensor(2.2802)\n",
      "Train Epoch: 1 [7680/25000 (31%)]\tLoss: 2.255729 testing loss: tensor(2.2379)\n",
      "Train Epoch: 1 [8320/25000 (33%)]\tLoss: 2.314059 testing loss: tensor(2.1764)\n",
      "Train Epoch: 1 [8960/25000 (36%)]\tLoss: 2.226147 testing loss: tensor(2.0889)\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 2.010110 testing loss: tensor(1.9924)\n",
      "Train Epoch: 1 [10240/25000 (41%)]\tLoss: 1.983823 testing loss: tensor(1.9072)\n",
      "Train Epoch: 1 [10880/25000 (43%)]\tLoss: 1.826228 testing loss: tensor(1.8820)\n",
      "Train Epoch: 1 [11520/25000 (46%)]\tLoss: 1.875022 testing loss: tensor(1.8722)\n",
      "Train Epoch: 1 [12160/25000 (48%)]\tLoss: 1.933393 testing loss: tensor(1.8541)\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 1.760605 testing loss: tensor(1.8311)\n",
      "Train Epoch: 1 [13440/25000 (54%)]\tLoss: 1.811579 testing loss: tensor(1.8199)\n",
      "Train Epoch: 1 [14080/25000 (56%)]\tLoss: 1.599390 testing loss: tensor(1.8096)\n",
      "Train Epoch: 1 [14720/25000 (59%)]\tLoss: 1.720489 testing loss: tensor(1.7979)\n",
      "Train Epoch: 1 [15360/25000 (61%)]\tLoss: 1.752055 testing loss: tensor(1.7883)\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 1.673295 testing loss: tensor(1.7791)\n",
      "Train Epoch: 1 [16640/25000 (66%)]\tLoss: 1.645708 testing loss: tensor(1.7672)\n",
      "Train Epoch: 1 [17280/25000 (69%)]\tLoss: 1.640617 testing loss: tensor(1.7583)\n",
      "Train Epoch: 1 [17920/25000 (71%)]\tLoss: 1.892286 testing loss: tensor(1.7544)\n",
      "Train Epoch: 1 [18560/25000 (74%)]\tLoss: 1.772637 testing loss: tensor(1.7520)\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 1.711413 testing loss: tensor(1.7509)\n",
      "Train Epoch: 1 [19840/25000 (79%)]\tLoss: 1.717605 testing loss: tensor(1.7484)\n",
      "Train Epoch: 1 [20480/25000 (82%)]\tLoss: 1.788102 testing loss: tensor(1.7461)\n",
      "Train Epoch: 1 [21120/25000 (84%)]\tLoss: 1.718162 testing loss: tensor(1.7438)\n",
      "Train Epoch: 1 [21760/25000 (87%)]\tLoss: 1.648401 testing loss: tensor(1.7418)\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 1.682631 testing loss: tensor(1.7394)\n",
      "Train Epoch: 1 [23040/25000 (92%)]\tLoss: 1.714682 testing loss: tensor(1.7397)\n",
      "Train Epoch: 1 [23680/25000 (94%)]\tLoss: 1.824344 testing loss: tensor(1.7359)\n",
      "Train Epoch: 1 [24320/25000 (97%)]\tLoss: 1.823759 testing loss: tensor(1.7331)\n",
      "Train Epoch: 1 [7800/25000 (99%)]\tLoss: 1.668143 testing loss: tensor(1.7333)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.7333)\n",
      "CS 2 : 2.34964\n",
      "DP 2 : 1.7516\n",
      "heuristic 2 : 1.7118\n",
      "DP: 1.7567987442016602\n",
      "tensor([0.0494, 0.0522, 0.0543, 0.6783, 0.1658])\n",
      "tensor([0.1004, 0.1090, 0.1086, 0.6821, 1.0000])\n",
      "tensor([0.3248, 0.3394, 0.3358, 1.0000, 1.0000])\n",
      "tensor([0.4913, 0.5087, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: twopeak heuristic\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 0.020719\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 0.002810\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 0.000120\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 0.000235\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 0.000059\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 0.000036\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 0.000045\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 0.000033\n",
      "NN 1 : tensor(1.6637)\n",
      "CS 1 : 2.34964\n",
      "DP 1 : 1.7516\n",
      "heuristic 1 : 1.7118\n",
      "DP: 1.7567987442016602\n",
      "tensor([0.0701, 0.0684, 0.0699, 0.0813, 0.7104])\n",
      "tensor([0.0895, 0.0880, 0.0992, 0.7233, 1.0000])\n",
      "tensor([0.1297, 0.1304, 0.7399, 1.0000, 1.0000])\n",
      "tensor([0.4892, 0.5108, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 2.129597 testing loss: tensor(1.6470)\n",
      "Train Epoch: 1 [640/25000 (3%)]\tLoss: 1.978604 testing loss: tensor(1.6654)\n",
      "Train Epoch: 1 [1280/25000 (5%)]\tLoss: 2.008100 testing loss: tensor(1.7602)\n",
      "Train Epoch: 1 [1920/25000 (8%)]\tLoss: 1.954646 testing loss: tensor(1.8501)\n",
      "Train Epoch: 1 [2560/25000 (10%)]\tLoss: 1.929232 testing loss: tensor(1.9064)\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 1.819802 testing loss: tensor(1.9155)\n",
      "Train Epoch: 1 [3840/25000 (15%)]\tLoss: 1.886371 testing loss: tensor(1.8862)\n",
      "Train Epoch: 1 [4480/25000 (18%)]\tLoss: 1.643887 testing loss: tensor(1.8497)\n",
      "Train Epoch: 1 [5120/25000 (20%)]\tLoss: 1.632970 testing loss: tensor(1.8289)\n",
      "Train Epoch: 1 [5760/25000 (23%)]\tLoss: 1.937026 testing loss: tensor(1.8250)\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 1.753202 testing loss: tensor(1.8171)\n",
      "Train Epoch: 1 [7040/25000 (28%)]\tLoss: 1.706421 testing loss: tensor(1.7996)\n",
      "Train Epoch: 1 [7680/25000 (31%)]\tLoss: 1.699993 testing loss: tensor(1.7772)\n",
      "Train Epoch: 1 [8320/25000 (33%)]\tLoss: 1.733208 testing loss: tensor(1.7490)\n",
      "Train Epoch: 1 [8960/25000 (36%)]\tLoss: 1.691701 testing loss: tensor(1.7221)\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 1.727050 testing loss: tensor(1.7047)\n",
      "Train Epoch: 1 [10240/25000 (41%)]\tLoss: 1.693959 testing loss: tensor(1.6997)\n",
      "Train Epoch: 1 [10880/25000 (43%)]\tLoss: 1.750497 testing loss: tensor(1.6894)\n",
      "Train Epoch: 1 [11520/25000 (46%)]\tLoss: 1.569033 testing loss: tensor(1.6808)\n",
      "Train Epoch: 1 [12160/25000 (48%)]\tLoss: 1.719531 testing loss: tensor(1.6769)\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 1.616453 testing loss: tensor(1.6744)\n",
      "Train Epoch: 1 [13440/25000 (54%)]\tLoss: 1.751418 testing loss: tensor(1.6702)\n",
      "Train Epoch: 1 [14080/25000 (56%)]\tLoss: 1.602666 testing loss: tensor(1.6659)\n",
      "Train Epoch: 1 [14720/25000 (59%)]\tLoss: 1.485270 testing loss: tensor(1.6642)\n",
      "Train Epoch: 1 [15360/25000 (61%)]\tLoss: 1.625910 testing loss: tensor(1.6628)\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 1.599736 testing loss: tensor(1.6598)\n",
      "Train Epoch: 1 [16640/25000 (66%)]\tLoss: 1.793496 testing loss: tensor(1.6574)\n",
      "Train Epoch: 1 [17280/25000 (69%)]\tLoss: 1.824321 testing loss: tensor(1.6541)\n",
      "Train Epoch: 1 [17920/25000 (71%)]\tLoss: 1.557144 testing loss: tensor(1.6531)\n",
      "Train Epoch: 1 [18560/25000 (74%)]\tLoss: 1.412537 testing loss: tensor(1.6506)\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 1.698215 testing loss: tensor(1.6507)\n",
      "Train Epoch: 1 [19840/25000 (79%)]\tLoss: 1.806642 testing loss: tensor(1.6513)\n",
      "Train Epoch: 1 [20480/25000 (82%)]\tLoss: 1.701275 testing loss: tensor(1.6493)\n",
      "Train Epoch: 1 [21120/25000 (84%)]\tLoss: 1.653770 testing loss: tensor(1.6484)\n",
      "Train Epoch: 1 [21760/25000 (87%)]\tLoss: 1.379183 testing loss: tensor(1.6464)\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 1.453140 testing loss: tensor(1.6462)\n",
      "Train Epoch: 1 [23040/25000 (92%)]\tLoss: 1.564633 testing loss: tensor(1.6462)\n",
      "Train Epoch: 1 [23680/25000 (94%)]\tLoss: 1.548661 testing loss: tensor(1.6449)\n",
      "Train Epoch: 1 [24320/25000 (97%)]\tLoss: 1.556841 testing loss: tensor(1.6428)\n",
      "Train Epoch: 1 [7800/25000 (99%)]\tLoss: 1.649402 testing loss: tensor(1.6414)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.6414)\n",
      "CS 2 : 2.34964\n",
      "DP 2 : 1.7516\n",
      "heuristic 2 : 1.7118\n",
      "DP: 1.7567987442016602\n",
      "tensor([0.0740, 0.0679, 0.0802, 0.1053, 0.6725])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0874, 0.0829, 0.1359, 0.6938, 1.0000])\n",
      "tensor([0.1526, 0.1475, 0.6999, 1.0000, 1.0000])\n",
      "tensor([0.4991, 0.5009, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.1)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "    \n",
    "for order in stage:\n",
    "    producedata(order)\n",
    "    #running\n",
    "    for order1 in order1name:\n",
    "        print(\"Supervised Aim:\",order,order1)\n",
    "        \n",
    "        # for mapping binary to payments before softmax\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(n, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, n),\n",
    "        )\n",
    "        model.apply(init_weights)\n",
    "        # optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr1)\n",
    "        \n",
    "        runningLossNN = []\n",
    "        runningLossCS = []\n",
    "        runningLossDP = []\n",
    "        runningLossHeuristic = []\n",
    "        #model=torch.load(\"save/pytorchNN=5dp1\");\n",
    "        #model.eval()\n",
    "        ##order1name=[\"costsharing\",\"dp\",\"heuristic\",\"random initializing\"]\n",
    "        for epoch in range(1, supervisionEpochs + 1):\n",
    "#             print(\"distributionRatio\",distributionRatio)\n",
    "            if(order1==\"costsharing\"):\n",
    "                supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "            elif(order1==\"dp\"):\n",
    "                supervisionTrain(epoch, dpSupervisionRule)\n",
    "            elif(order1==\"heuristic\"):\n",
    "                supervisionTrain(epoch, heuristicSupervisionRule)\n",
    "            elif(order1==\"random initializing\"):\n",
    "                print(\"do nothing\");\n",
    "\n",
    "        test()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr2)\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(epoch)\n",
    "            test()\n",
    "        losslistname.append(order+\" \"+order1);\n",
    "        losslist.append(losslisttemp);\n",
    "        losslisttemp=[];\n",
    "        savepath=\"save/pytorchNN=5all-beta\"+order+str(order1)\n",
    "        torch.save(model, savepath);\n",
    "        print(\"end\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colorlist=[\"#D2691E\",'#4169E1',\"#9ACD32\",\"#B22222\",\"#FF00FF\",\"#708090\"]\n",
    "for i in range(len(losslist)):\n",
    "    plt.plot(losslist[i], '.-', color=colorlist[i], alpha=0.8, label=losslistname[i])\n",
    "\n",
    "# 显示标签，如果不加这句，即使加了label='一些数字'的参数，最终还是不会显示标签\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('Trianing Times')\n",
    "plt.ylabel('Delay')\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('pytorchNN=5_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
