{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from random import randint\n",
    "import random\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "from pylab import *\n",
    "n = 5\n",
    "epochs = 4\n",
    "supervisionEpochs = 3\n",
    "lr = 0.001\n",
    "log_interval = 10\n",
    "trainSize = 40000#100000\n",
    "percentage_train_test= 0.25\n",
    "penaltyLambda = 10\n",
    "doublePeakHighMean = 0.9\n",
    "doublePeakLowMean = 0.1\n",
    "doublePeakStd = 0.1\n",
    "uniformlow=0\n",
    "uniformhigh=1.0\n",
    "normalloc = 0.2\n",
    "normalscale = 0.1\n",
    "\n",
    "cauchyloc = 1/n\n",
    "cauchyscalen = 0.004\n",
    "\n",
    "independentnormalloc1=[(float(ii)+1)/(2*n+1) for ii in range(n,0,-1)]\n",
    "independentnormalscale1=[0.05 for ii in range(n)]\n",
    "\n",
    "independentnormalloc2=[(float(ii)+1)/(2*n+1) for ii in range(1,n+1,1)]\n",
    "independentnormalscale2=[0.05 for ii in range(n)]\n",
    "stage=[\"twopeak\"]\n",
    "# \"twopeak\",\"normal\",\"uniform\",\"independent1\",\"independent2\",\"cauchy\"\n",
    "order1name=[\"costsharing\",\"dp\",\"random initializing\"]\n",
    "# \"costsharing\",\"dp\",\"heuristic\",\"random initializing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "d3 = D.normal.Normal(normalloc, normalscale)\n",
    "d4 = D.uniform.Uniform(uniformlow,uniformhigh)\n",
    "\n",
    "d5 = [D.normal.Normal(independentnormalloc1[ii], independentnormalscale1[ii]) for ii in range(n)]\n",
    "d6 = [D.normal.Normal(independentnormalloc2[ii], independentnormalscale2[ii]) for ii in range(n)]\n",
    "\n",
    "d7 = D.cauchy.Cauchy(cauchyloc,cauchyscalen)\n",
    "def cdf(x,y, i=None):\n",
    "    if(y==\"twopeak\"):\n",
    "        return (d1.cdf(x) + d2.cdf(x) - distributionBase) / 2 / distributionRatio\n",
    "    elif(y==\"normal\"):\n",
    "        return d3.cdf(x);\n",
    "    elif(y==\"uniform\"):\n",
    "        return d4.cdf(x);\n",
    "    elif(y==\"independent1\"):\n",
    "        return d5[i].cdf(x);\n",
    "    elif(y==\"independent2\"):\n",
    "        return d6[i].cdf(x);\n",
    "    elif(y==\"cauchy\"):\n",
    "        return d7.cdf(x);\n",
    "#print(cdf(0.1,\"independent\",1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    if torch.sum(bits).item() == 0:\n",
    "        return torch.ones(n)\n",
    "    bits = bits.type(torch.float32)\n",
    "    negBits = torch.ones(n) - bits\n",
    "    payments = model(bits)\n",
    "    payments = payments - 1000 * negBits\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    payments = payments + negBits\n",
    "    return payments\n",
    "\n",
    "def tpToBits(tp, bits=torch.ones(n).type(torch.uint8)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = (tp >= payments).type(torch.uint8)\n",
    "    if torch.equal(newBits, bits):\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits(tp, newBits)#bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    return n - torch.sum(tpToBits(tp).type(torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    global  dp,decision\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money, yes]\n",
    "        offer = float(offerIndex) / dpPrecision\n",
    "        if tp[i] >= offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits[i] = 0\n",
    "            payments[i] = 1\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "#     print()\n",
    "#     print(tp)\n",
    "#     print(bits)\n",
    "#     print(payments)\n",
    "#     print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def heuristicSupervisionRule(tp):\n",
    "    global  dp_H,decision_H\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    tempold=-1;\n",
    "    for turns in range(n,0,-1):\n",
    "        money = dpPrecision\n",
    "        j=0\n",
    "        tempo=sum(bits)\n",
    "        #print(\"bits\",tempo)\n",
    "        for i in range(n):\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            \n",
    "            offerIndex = decision_H[tempo ,tempo - i, money]\n",
    "            offer = float(offerIndex) / dpPrecision\n",
    "            while(j<n):\n",
    "                if(bits[j]!=0):\n",
    "                    break;\n",
    "                j+=1;\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            if tp[j] >= offer:\n",
    "                #print(money,j,tp[j],offer)\n",
    "                money -= offerIndex\n",
    "                bits[j] = 1\n",
    "                payments[j] = offer\n",
    "            else:\n",
    "                bits[j] = 0;\n",
    "                payments[j] = 1.0;\n",
    "            j+=1;\n",
    "        #print(\"money\",money)\n",
    "        if(money==0 and tempold==tempo):\n",
    "            break;\n",
    "        tempold=tempo;\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    for k in range(n, -1, -1):\n",
    "        if k == 0:\n",
    "            break\n",
    "        bits = [1 if tp[ii] >= 1.0 / k else 0 for ii in range(n)]\n",
    "        if sum(bits) == k:\n",
    "            break\n",
    "    if k == 0:\n",
    "        payments = [1 for ii in range(n)]\n",
    "    else:\n",
    "        payments = [1.0 / k if bits[ii] == 1 else 1 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    return float(n - torch.sum(costSharingSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def dpDelay(tp):\n",
    "    return float(n - torch.sum(dpSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def heuristicDelay(tp):\n",
    "    return float(n - torch.sum(heuristicSupervisionRule(tp)[0]).item())\n",
    "\n",
    "\n",
    "# templire=0;\n",
    "# num=1\n",
    "# for i in range(num):\n",
    "#     temp=torch.tensor(samplesJoint[i])\n",
    "#     #print(temp)\n",
    "#     #print(temp);\n",
    "#     #print(heuristicSupervisionRule(temp))\n",
    "#     #print(dpSupervisionRule(temp))\n",
    "#     #print(dpDelay(temp))\n",
    "#     res=dpDelay(temp)\n",
    "#     templire+=res\n",
    "#     #print(\"delay\",res)\n",
    "#     #print()\n",
    "# print(templire/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordAndReport(name, source, loss):\n",
    "    source.append(loss)\n",
    "    realLength = len(source)\n",
    "    #print(f\"{name} ({realLength}): {loss}\")\n",
    "    print(name,realLength,\":\" ,loss)\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    \n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda /100000\n",
    "        \n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "losslist=[];\n",
    "losslistname=[];\n",
    "losslisttemp=[];\n",
    "def test_batch():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lenLoss= 0\n",
    "        nnLoss = 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "            for tp in tp_batch:\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        nnLoss/=lenLoss\n",
    "    return nnLoss\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                delay1 = tpToTotalDelay(tp1)\n",
    "                delay0 = tpToTotalDelay(tp0)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                    loss = loss + (1 - cdf(offer,order)) * delay1 + cdf(offer,order) * delay0\n",
    "                else:\n",
    "                    loss = loss + (1 - cdf(offer,order,i)) * delay1 + cdf(offer,order,i) * delay0\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "    \n",
    "allBits = [torch.tensor(bits, dtype=torch.int16) for bits in itertools.product([0, 1], repeat=n)]\n",
    "\n",
    "def test():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        costSharingLoss = 0\n",
    "        dpLoss = 0\n",
    "        nnLoss = 0\n",
    "        heuristicLoss=0\n",
    "        lenLoss= 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "                heuristicLoss+= heuristicDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        costSharingLoss /= lenLoss\n",
    "        dpLoss /= lenLoss\n",
    "        nnLoss /= lenLoss\n",
    "        heuristicLoss /= lenLoss\n",
    "        #print(lenLoss)\n",
    "        losslisttemp.append(nnLoss);\n",
    "        recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "        recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "        recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "        recordAndReport(\"heuristic\", runningLossHeuristic, heuristicLoss)\n",
    "        print(\"DP:\",dp[n, dpPrecision,0])\n",
    "        #for i in range(n, 0, -1):\n",
    "        #    print(\"Heuristic:\",i,5*(1-dp_H[i, i, dpPrecision]))\n",
    "        for i in range(n, 0, -1):\n",
    "            print(\n",
    "                    tpToPayments(\n",
    "                            torch.tensor([0 if ii >= i else 1 for ii in range(n)], dtype=torch.float32)\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "dpPrecision = 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def producedata(order):\n",
    "    global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "    dpPrecision = 100 \n",
    "    if(order==\"twopeak\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = np.random.normal(\n",
    "            loc=doublePeakLowMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=doublePeakLowMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samples2 = np.random.normal(\n",
    "            loc=doublePeakHighMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = np.random.normal(\n",
    "                        loc=doublePeakHighMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "    elif(order==\"normal\"):\n",
    "        print(\"loc\",normalloc, \"scale\",normalscale)\n",
    "        samples1 = np.random.normal(\n",
    "            loc=normalloc, scale=normalscale, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=normalloc, scale=normalscale\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"uniform\"):  \n",
    "        print(\"uniformlow\",uniformlow, \"uniformhigh\",uniformhigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        uniformlow, uniformhigh\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent1\"):\n",
    "        print(\"loc\",independentnormalloc1,\"scale\",independentnormalscale1)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent2\"):\n",
    "        print(\"loc\",independentnormalloc2, \"scale\",independentnormalscale2)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"cauchy\"):\n",
    "        print(\"cauchyloc\",cauchyloc, \"cauchyscale\",cauchyscalen)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        \n",
    "    tp_dataset = TensorDataset(tp_tensor[: int(trainSize * percentage_train_test)])\n",
    "    tp_dataset_testing = TensorDataset(tp_tensor[int(trainSize * (1.0-percentage_train_test)) :])\n",
    "    tp_dataloader = DataLoader(tp_dataset, batch_size=128, shuffle=True)\n",
    "    tp_dataloader_testing = DataLoader(tp_dataset_testing, batch_size=256, shuffle=False)\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "    decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for yes in range(n + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp[0, 0, yes] = 0\n",
    "            else:\n",
    "                dp[0, money, yes] = yes# + 1.0\n",
    "    for ppl in range(1,  n + 1):\n",
    "        for yes in range(n + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                minSoFar = 1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1 - cdf(offer,order)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order) * (1 + dp[ppl - 1, money, yes])\n",
    "                    else:\n",
    "                        res = (1 - cdf(offer,order,n-ppl)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order,n-ppl) * (1 + dp[ppl - 1, money, yes])\n",
    "                    if minSoFar > res.item():\n",
    "                        minSoFar = res.item()\n",
    "                        decision[ppl, money, yes] = offerIndex\n",
    "                dp[ppl, money, yes] = minSoFar\n",
    "    \n",
    "    print(\"dp\",dp[n, dpPrecision, 0])\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp_H = np.zeros([n + 1 , n + 1, dpPrecision + 1])\n",
    "    decision_H = np.zeros([n + 1 , n + 1, dpPrecision + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for i in range(1,n+1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp_H[i, 0, 0] = 1\n",
    "            else:\n",
    "                offer = money / dpPrecision\n",
    "                dp_H[i, 0, money] = 0#cdf(offer)# + 1.0\n",
    "    for i in range(1,n+1):\n",
    "        for ppl in range(1, i + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                maxSoFar = -1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1-cdf(offer,order)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    else:\n",
    "                        res = (1-cdf(offer,order,ppl-1)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    if maxSoFar < res.item():\n",
    "                        maxSoFar = res.item()\n",
    "                        decision_H[i, ppl, money] = offerIndex\n",
    "                dp_H[i, ppl, money] = maxSoFar\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loc 0.1 scale 0.1\n",
      "loc 0.9 scale 0.1\n",
      "dp 1.8651759624481201\n",
      "Supervised Aim: twopeak costsharing\n",
      "Train Epoch: 1 [0/10000 (0%)]\tLoss: 0.024273\n",
      "Train Epoch: 1 [1280/10000 (13%)]\tLoss: 0.001183\n",
      "Train Epoch: 1 [2560/10000 (25%)]\tLoss: 0.000769\n",
      "Train Epoch: 1 [3840/10000 (38%)]\tLoss: 0.000376\n",
      "Train Epoch: 1 [5120/10000 (51%)]\tLoss: 0.000063\n",
      "Train Epoch: 1 [6400/10000 (63%)]\tLoss: 0.000023\n",
      "Train Epoch: 1 [7680/10000 (76%)]\tLoss: 0.000034\n",
      "Train Epoch: 1 [8960/10000 (89%)]\tLoss: 0.000010\n",
      "Train Epoch: 2 [0/10000 (0%)]\tLoss: 0.000009\n",
      "Train Epoch: 2 [1280/10000 (13%)]\tLoss: 0.000006\n",
      "Train Epoch: 2 [2560/10000 (25%)]\tLoss: 0.000004\n",
      "Train Epoch: 2 [3840/10000 (38%)]\tLoss: 0.000004\n",
      "Train Epoch: 2 [5120/10000 (51%)]\tLoss: 0.000003\n",
      "Train Epoch: 2 [6400/10000 (63%)]\tLoss: 0.000002\n",
      "Train Epoch: 2 [7680/10000 (76%)]\tLoss: 0.000002\n",
      "Train Epoch: 2 [8960/10000 (89%)]\tLoss: 0.000002\n",
      "Train Epoch: 3 [0/10000 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 3 [1280/10000 (13%)]\tLoss: 0.000002\n",
      "Train Epoch: 3 [2560/10000 (25%)]\tLoss: 0.000001\n",
      "Train Epoch: 3 [3840/10000 (38%)]\tLoss: 0.000001\n",
      "Train Epoch: 3 [5120/10000 (51%)]\tLoss: 0.000001\n",
      "Train Epoch: 3 [6400/10000 (63%)]\tLoss: 0.000001\n",
      "Train Epoch: 3 [7680/10000 (76%)]\tLoss: 0.000001\n",
      "Train Epoch: 3 [8960/10000 (89%)]\tLoss: 0.000001\n",
      "NN 1 : tensor(2.5322)\n",
      "CS 1 : 2.5331\n",
      "DP 1 : 1.8676\n",
      "heuristic 1 : 1.8589\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.1997, 0.1992, 0.1999, 0.1992, 0.2020])\n",
      "tensor([0.2477, 0.2487, 0.2505, 0.2530, 1.0000])\n",
      "tensor([0.3334, 0.3358, 0.3308, 1.0000, 1.0000])\n",
      "tensor([0.4993, 0.5007, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/10000 (0%)]\tLoss: 2.719185 testing loss: tensor(2.5121)\n",
      "Train Epoch: 1 [1280/10000 (13%)]\tLoss: 1.901936 testing loss: tensor(1.9581)\n",
      "Train Epoch: 1 [2560/10000 (25%)]\tLoss: 1.875365 testing loss: tensor(1.8820)\n",
      "Train Epoch: 1 [3840/10000 (38%)]\tLoss: 1.852557 testing loss: tensor(1.8786)\n",
      "Train Epoch: 1 [5120/10000 (51%)]\tLoss: 1.698837 testing loss: tensor(1.8214)\n",
      "Train Epoch: 1 [6400/10000 (63%)]\tLoss: 1.826846 testing loss: tensor(1.8001)\n",
      "Train Epoch: 1 [7680/10000 (76%)]\tLoss: 1.811018 testing loss: tensor(1.7967)\n",
      "Train Epoch: 1 [8960/10000 (89%)]\tLoss: 1.961275 testing loss: tensor(1.8006)\n",
      "penalty: 0.0025187134742736816\n",
      "NN 2 : tensor(1.7930)\n",
      "CS 2 : 2.5331\n",
      "DP 2 : 1.8676\n",
      "heuristic 2 : 1.8589\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0701, 0.0687, 0.0617, 0.0630, 0.7366])\n",
      "tensor([0.0757, 0.1371, 0.0704, 0.7167, 1.0000])\n",
      "tensor([0.1938, 0.6430, 0.1632, 1.0000, 1.0000])\n",
      "tensor([0.2774, 0.7226, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 2 [0/10000 (0%)]\tLoss: 1.820309 testing loss: tensor(1.7833)\n",
      "Train Epoch: 2 [1280/10000 (13%)]\tLoss: 1.759284 testing loss: tensor(1.7781)\n",
      "Train Epoch: 2 [2560/10000 (25%)]\tLoss: 1.801915 testing loss: tensor(1.7771)\n",
      "Train Epoch: 2 [3840/10000 (38%)]\tLoss: 1.839796 testing loss: tensor(1.7644)\n",
      "Train Epoch: 2 [5120/10000 (51%)]\tLoss: 1.764553 testing loss: tensor(1.7632)\n",
      "Train Epoch: 2 [6400/10000 (63%)]\tLoss: 1.740238 testing loss: tensor(1.7694)\n",
      "Train Epoch: 2 [7680/10000 (76%)]\tLoss: 1.599212 testing loss: tensor(1.7585)\n",
      "Train Epoch: 2 [8960/10000 (89%)]\tLoss: 1.657639 testing loss: tensor(1.7631)\n",
      "penalty: 0.008340775966644287\n",
      "NN 3 : tensor(1.7594)\n",
      "CS 3 : 2.5331\n",
      "DP 3 : 1.8676\n",
      "heuristic 3 : 1.8589\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0630, 0.0582, 0.0645, 0.0741, 0.7402])\n",
      "tensor([0.0703, 0.0903, 0.0713, 0.7681, 1.0000])\n",
      "tensor([0.1287, 0.7574, 0.1139, 1.0000, 1.0000])\n",
      "tensor([0.2347, 0.7653, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 3 [0/10000 (0%)]\tLoss: 1.749224 testing loss: tensor(1.7802)\n",
      "Train Epoch: 3 [1280/10000 (13%)]\tLoss: 1.746792 testing loss: tensor(1.7572)\n",
      "Train Epoch: 3 [2560/10000 (25%)]\tLoss: 1.601356 testing loss: tensor(1.7667)\n",
      "Train Epoch: 3 [3840/10000 (38%)]\tLoss: 1.626640 testing loss: tensor(1.7531)\n",
      "Train Epoch: 3 [5120/10000 (51%)]\tLoss: 2.055386 testing loss: tensor(1.7759)\n",
      "Train Epoch: 3 [6400/10000 (63%)]\tLoss: 1.718825 testing loss: tensor(1.7669)\n",
      "Train Epoch: 3 [7680/10000 (76%)]\tLoss: 1.857400 testing loss: tensor(1.7596)\n",
      "Train Epoch: 3 [8960/10000 (89%)]\tLoss: 1.770885 testing loss: tensor(1.7641)\n",
      "penalty: 0.0002549886703491211\n",
      "NN 4 : tensor(1.7667)\n",
      "CS 4 : 2.5331\n",
      "DP 4 : 1.8676\n",
      "heuristic 4 : 1.8589\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0704, 0.0682, 0.0698, 0.0680, 0.7236])\n",
      "tensor([0.0893, 0.0935, 0.0845, 0.7327, 1.0000])\n",
      "tensor([0.1262, 0.7613, 0.1125, 1.0000, 1.0000])\n",
      "tensor([0.2369, 0.7631, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 4 [0/10000 (0%)]\tLoss: 1.553068 testing loss: tensor(1.7679)\n",
      "Train Epoch: 4 [1280/10000 (13%)]\tLoss: 1.709795 testing loss: tensor(1.7622)\n",
      "Train Epoch: 4 [2560/10000 (25%)]\tLoss: 1.894692 testing loss: tensor(1.7647)\n",
      "Train Epoch: 4 [3840/10000 (38%)]\tLoss: 1.706768 testing loss: tensor(1.7571)\n",
      "Train Epoch: 4 [5120/10000 (51%)]\tLoss: 1.908802 testing loss: tensor(1.7608)\n",
      "Train Epoch: 4 [6400/10000 (63%)]\tLoss: 1.718656 testing loss: tensor(1.7584)\n",
      "Train Epoch: 4 [7680/10000 (76%)]\tLoss: 1.798340 testing loss: tensor(1.7551)\n",
      "Train Epoch: 4 [8960/10000 (89%)]\tLoss: 1.757132 testing loss: tensor(1.7545)\n",
      "penalty: 0.010866522789001465\n",
      "NN 5 : tensor(1.7597)\n",
      "CS 5 : 2.5331\n",
      "DP 5 : 1.8676\n",
      "heuristic 5 : 1.8589\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0722, 0.0608, 0.0635, 0.0651, 0.7384])\n",
      "tensor([0.0903, 0.0785, 0.0844, 0.7469, 1.0000])\n",
      "tensor([0.1481, 0.7203, 0.1315, 1.0000, 1.0000])\n",
      "tensor([0.2682, 0.7318, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: twopeak dp\n",
      "Train Epoch: 1 [0/10000 (0%)]\tLoss: 0.107784\n",
      "Train Epoch: 1 [1280/10000 (13%)]\tLoss: 0.006662\n",
      "Train Epoch: 1 [2560/10000 (25%)]\tLoss: 0.007086\n",
      "Train Epoch: 1 [3840/10000 (38%)]\tLoss: 0.005747\n",
      "Train Epoch: 1 [5120/10000 (51%)]\tLoss: 0.004036\n",
      "Train Epoch: 1 [6400/10000 (63%)]\tLoss: 0.003713\n",
      "Train Epoch: 1 [7680/10000 (76%)]\tLoss: 0.002845\n",
      "Train Epoch: 1 [8960/10000 (89%)]\tLoss: 0.002645\n",
      "Train Epoch: 2 [0/10000 (0%)]\tLoss: 0.002083\n",
      "Train Epoch: 2 [1280/10000 (13%)]\tLoss: 0.001894\n",
      "Train Epoch: 2 [2560/10000 (25%)]\tLoss: 0.001496\n",
      "Train Epoch: 2 [3840/10000 (38%)]\tLoss: 0.001291\n",
      "Train Epoch: 2 [5120/10000 (51%)]\tLoss: 0.000823\n",
      "Train Epoch: 2 [6400/10000 (63%)]\tLoss: 0.000696\n",
      "Train Epoch: 2 [7680/10000 (76%)]\tLoss: 0.000801\n",
      "Train Epoch: 2 [8960/10000 (89%)]\tLoss: 0.000566\n",
      "Train Epoch: 3 [0/10000 (0%)]\tLoss: 0.000497\n",
      "Train Epoch: 3 [1280/10000 (13%)]\tLoss: 0.000405\n",
      "Train Epoch: 3 [2560/10000 (25%)]\tLoss: 0.000323\n",
      "Train Epoch: 3 [3840/10000 (38%)]\tLoss: 0.000393\n",
      "Train Epoch: 3 [5120/10000 (51%)]\tLoss: 0.000279\n",
      "Train Epoch: 3 [6400/10000 (63%)]\tLoss: 0.000203\n",
      "Train Epoch: 3 [7680/10000 (76%)]\tLoss: 0.000201\n",
      "Train Epoch: 3 [8960/10000 (89%)]\tLoss: 0.000155\n",
      "NN 1 : tensor(1.8422)\n",
      "CS 1 : 2.5331\n",
      "DP 1 : 1.8676\n",
      "heuristic 1 : 1.8589\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.7586, 0.0958, 0.0859, 0.0572, 0.0026])\n",
      "tensor([0.7456, 0.0740, 0.1096, 0.0708, 1.0000])\n",
      "tensor([0.7580, 0.0968, 0.1451, 1.0000, 1.0000])\n",
      "tensor([0.8856, 0.1144, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/10000 (0%)]\tLoss: 1.935689 testing loss: tensor(1.8783)\n",
      "Train Epoch: 1 [1280/10000 (13%)]\tLoss: 1.874912 testing loss: tensor(1.8454)\n",
      "Train Epoch: 1 [2560/10000 (25%)]\tLoss: 2.019401 testing loss: tensor(1.8488)\n",
      "Train Epoch: 1 [3840/10000 (38%)]\tLoss: 1.643748 testing loss: tensor(1.8228)\n",
      "Train Epoch: 1 [5120/10000 (51%)]\tLoss: 1.785941 testing loss: tensor(1.8231)\n",
      "Train Epoch: 1 [6400/10000 (63%)]\tLoss: 1.706957 testing loss: tensor(1.8122)\n",
      "Train Epoch: 1 [7680/10000 (76%)]\tLoss: 1.771771 testing loss: tensor(1.8184)\n",
      "Train Epoch: 1 [8960/10000 (89%)]\tLoss: 1.882159 testing loss: tensor(1.8105)\n",
      "penalty: 0.015617765486240387\n",
      "NN 2 : tensor(1.8186)\n",
      "CS 2 : 2.5331\n",
      "DP 2 : 1.8676\n",
      "heuristic 2 : 1.8589\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.7717, 0.0644, 0.0694, 0.0520, 0.0425])\n",
      "tensor([0.7781, 0.0738, 0.0872, 0.0608, 1.0000])\n",
      "tensor([0.8104, 0.0767, 0.1129, 1.0000, 1.0000])\n",
      "tensor([0.9141, 0.0859, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 2 [0/10000 (0%)]\tLoss: 1.910234 testing loss: tensor(1.8229)\n",
      "Train Epoch: 2 [1280/10000 (13%)]\tLoss: 1.655297 testing loss: tensor(1.8010)\n",
      "Train Epoch: 2 [2560/10000 (25%)]\tLoss: 1.660329 testing loss: tensor(1.7833)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [3840/10000 (38%)]\tLoss: 1.812686 testing loss: tensor(1.7876)\n",
      "Train Epoch: 2 [5120/10000 (51%)]\tLoss: 1.657602 testing loss: tensor(1.7927)\n",
      "Train Epoch: 2 [6400/10000 (63%)]\tLoss: 1.806708 testing loss: tensor(1.7856)\n",
      "Train Epoch: 2 [7680/10000 (76%)]\tLoss: 1.818127 testing loss: tensor(1.7890)\n",
      "Train Epoch: 2 [8960/10000 (89%)]\tLoss: 1.717715 testing loss: tensor(1.7778)\n",
      "penalty: 0.022265277802944183\n",
      "NN 3 : tensor(1.7729)\n",
      "CS 3 : 2.5331\n",
      "DP 3 : 1.8676\n",
      "heuristic 3 : 1.8589\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.7251, 0.0782, 0.0676, 0.0562, 0.0729])\n",
      "tensor([0.7387, 0.0937, 0.0992, 0.0684, 1.0000])\n",
      "tensor([0.7503, 0.1050, 0.1446, 1.0000, 1.0000])\n",
      "tensor([0.8625, 0.1375, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 3 [0/10000 (0%)]\tLoss: 1.610722 testing loss: tensor(1.7733)\n",
      "Train Epoch: 3 [1280/10000 (13%)]\tLoss: 1.891083 testing loss: tensor(1.7767)\n",
      "Train Epoch: 3 [2560/10000 (25%)]\tLoss: 1.697320 testing loss: tensor(1.7713)\n",
      "Train Epoch: 3 [3840/10000 (38%)]\tLoss: 1.963194 testing loss: tensor(1.7798)\n",
      "Train Epoch: 3 [5120/10000 (51%)]\tLoss: 1.580651 testing loss: tensor(1.7782)\n",
      "Train Epoch: 3 [6400/10000 (63%)]\tLoss: 1.721702 testing loss: tensor(1.7764)\n",
      "Train Epoch: 3 [7680/10000 (76%)]\tLoss: 1.823877 testing loss: tensor(1.7733)\n",
      "Train Epoch: 3 [8960/10000 (89%)]\tLoss: 1.718822 testing loss: tensor(1.7665)\n",
      "penalty: 0.002614259719848633\n",
      "NN 4 : tensor(1.7720)\n",
      "CS 4 : 2.5331\n",
      "DP 4 : 1.8676\n",
      "heuristic 4 : 1.8589\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.7502, 0.0746, 0.0606, 0.0512, 0.0634])\n",
      "tensor([0.7628, 0.0896, 0.0840, 0.0635, 1.0000])\n",
      "tensor([0.7674, 0.1102, 0.1225, 1.0000, 1.0000])\n",
      "tensor([0.8485, 0.1515, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 4 [0/10000 (0%)]\tLoss: 1.756120 testing loss: tensor(1.7682)\n",
      "Train Epoch: 4 [1280/10000 (13%)]\tLoss: 1.741235 testing loss: tensor(1.7774)\n",
      "Train Epoch: 4 [2560/10000 (25%)]\tLoss: 1.504122 testing loss: tensor(1.7733)\n",
      "Train Epoch: 4 [3840/10000 (38%)]\tLoss: 1.754957 testing loss: tensor(1.7801)\n",
      "Train Epoch: 4 [5120/10000 (51%)]\tLoss: 1.805608 testing loss: tensor(1.7790)\n",
      "Train Epoch: 4 [6400/10000 (63%)]\tLoss: 1.770903 testing loss: tensor(1.7790)\n",
      "Train Epoch: 4 [7680/10000 (76%)]\tLoss: 1.916520 testing loss: tensor(1.7760)\n",
      "Train Epoch: 4 [8960/10000 (89%)]\tLoss: 1.891904 testing loss: tensor(1.7693)\n",
      "penalty: 0.0030916929244995117\n",
      "NN 5 : tensor(1.7670)\n",
      "CS 5 : 2.5331\n",
      "DP 5 : 1.8676\n",
      "heuristic 5 : 1.8589\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.7241, 0.0777, 0.0708, 0.0618, 0.0656])\n",
      "tensor([0.7309, 0.0967, 0.0931, 0.0793, 1.0000])\n",
      "tensor([0.7462, 0.1211, 0.1327, 1.0000, 1.0000])\n",
      "tensor([0.8241, 0.1759, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: twopeak random initializing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "NN 1 : tensor(2.1241)\n",
      "CS 1 : 2.5331\n",
      "DP 1 : 1.8676\n",
      "heuristic 1 : 1.8589\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0335, 0.4053, 0.0543, 0.0182, 0.4886])\n",
      "tensor([0.0529, 0.8378, 0.0840, 0.0253, 1.0000])\n",
      "tensor([0.0399, 0.9053, 0.0548, 1.0000, 1.0000])\n",
      "tensor([0.0397, 0.9603, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/10000 (0%)]\tLoss: 2.036935 testing loss: tensor(2.1048)\n",
      "Train Epoch: 1 [1280/10000 (13%)]\tLoss: 1.986041 testing loss: tensor(1.8814)\n",
      "Train Epoch: 1 [2560/10000 (25%)]\tLoss: 1.953883 testing loss: tensor(1.8484)\n",
      "Train Epoch: 1 [3840/10000 (38%)]\tLoss: 1.853985 testing loss: tensor(1.8397)\n",
      "Train Epoch: 1 [5120/10000 (51%)]\tLoss: 2.092322 testing loss: tensor(1.8185)\n",
      "Train Epoch: 1 [6400/10000 (63%)]\tLoss: 1.867488 testing loss: tensor(1.8160)\n",
      "Train Epoch: 1 [7680/10000 (76%)]\tLoss: 1.762565 testing loss: tensor(1.8096)\n",
      "Train Epoch: 1 [8960/10000 (89%)]\tLoss: 1.800218 testing loss: tensor(1.8163)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.8125)\n",
      "CS 2 : 2.5331\n",
      "DP 2 : 1.8676\n",
      "heuristic 2 : 1.8589\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0600, 0.0760, 0.0708, 0.0696, 0.7236])\n",
      "tensor([0.0615, 0.7245, 0.1069, 0.1070, 1.0000])\n",
      "tensor([0.0984, 0.7516, 0.1500, 1.0000, 1.0000])\n",
      "tensor([0.1814, 0.8186, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 2 [0/10000 (0%)]\tLoss: 1.868672 testing loss: tensor(1.8224)\n",
      "Train Epoch: 2 [1280/10000 (13%)]\tLoss: 1.654236 testing loss: tensor(1.8008)\n",
      "Train Epoch: 2 [2560/10000 (25%)]\tLoss: 1.606862 testing loss: tensor(1.8094)\n",
      "Train Epoch: 2 [3840/10000 (38%)]\tLoss: 1.989433 testing loss: tensor(1.8024)\n",
      "Train Epoch: 2 [5120/10000 (51%)]\tLoss: 1.799212 testing loss: tensor(1.8133)\n",
      "Train Epoch: 2 [6400/10000 (63%)]\tLoss: 2.101747 testing loss: tensor(1.7970)\n",
      "Train Epoch: 2 [7680/10000 (76%)]\tLoss: 1.739909 testing loss: tensor(1.8031)\n",
      "Train Epoch: 2 [8960/10000 (89%)]\tLoss: 1.608193 testing loss: tensor(1.8020)\n",
      "penalty: 0.006378602236509323\n",
      "NN 3 : tensor(1.8059)\n",
      "CS 3 : 2.5331\n",
      "DP 3 : 1.8676\n",
      "heuristic 3 : 1.8589\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0536, 0.0843, 0.0638, 0.0691, 0.7292])\n",
      "tensor([0.0484, 0.7642, 0.0867, 0.1006, 1.0000])\n",
      "tensor([0.0881, 0.7949, 0.1170, 1.0000, 1.0000])\n",
      "tensor([0.1834, 0.8166, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 3 [0/10000 (0%)]\tLoss: 1.799538 testing loss: tensor(1.8301)\n",
      "Train Epoch: 3 [1280/10000 (13%)]\tLoss: 1.740261 testing loss: tensor(1.8042)\n",
      "Train Epoch: 3 [2560/10000 (25%)]\tLoss: 1.655714 testing loss: tensor(1.8011)\n",
      "Train Epoch: 3 [3840/10000 (38%)]\tLoss: 1.929999 testing loss: tensor(1.7983)\n",
      "Train Epoch: 3 [5120/10000 (51%)]\tLoss: 1.728896 testing loss: tensor(1.7873)\n",
      "Train Epoch: 3 [6400/10000 (63%)]\tLoss: 1.802646 testing loss: tensor(1.8001)\n",
      "Train Epoch: 3 [7680/10000 (76%)]\tLoss: 1.751434 testing loss: tensor(1.7948)\n",
      "Train Epoch: 3 [8960/10000 (89%)]\tLoss: 1.854572 testing loss: tensor(1.7907)\n",
      "penalty: 0.0022151172161102295\n",
      "NN 4 : tensor(1.7995)\n",
      "CS 4 : 2.5331\n",
      "DP 4 : 1.8676\n",
      "heuristic 4 : 1.8589\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0672, 0.0790, 0.0709, 0.0654, 0.7175])\n",
      "tensor([0.0664, 0.7573, 0.0916, 0.0847, 1.0000])\n",
      "tensor([0.1214, 0.7619, 0.1167, 1.0000, 1.0000])\n",
      "tensor([0.2183, 0.7817, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 4 [0/10000 (0%)]\tLoss: 1.961366 testing loss: tensor(1.7935)\n",
      "Train Epoch: 4 [1280/10000 (13%)]\tLoss: 1.692477 testing loss: tensor(1.7878)\n",
      "Train Epoch: 4 [2560/10000 (25%)]\tLoss: 1.524329 testing loss: tensor(1.7821)\n",
      "Train Epoch: 4 [3840/10000 (38%)]\tLoss: 1.673796 testing loss: tensor(1.7791)\n",
      "Train Epoch: 4 [5120/10000 (51%)]\tLoss: 1.721787 testing loss: tensor(1.7723)\n",
      "Train Epoch: 4 [6400/10000 (63%)]\tLoss: 1.959087 testing loss: tensor(1.7922)\n",
      "Train Epoch: 4 [7680/10000 (76%)]\tLoss: 1.688578 testing loss: tensor(1.7613)\n",
      "Train Epoch: 4 [8960/10000 (89%)]\tLoss: 1.757157 testing loss: tensor(1.7696)\n",
      "penalty: 0.0\n",
      "NN 5 : tensor(1.7801)\n",
      "CS 5 : 2.5331\n",
      "DP 5 : 1.8676\n",
      "heuristic 5 : 1.8589\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0685, 0.0734, 0.0572, 0.0626, 0.7383])\n",
      "tensor([0.0766, 0.7843, 0.0772, 0.0618, 1.0000])\n",
      "tensor([0.1134, 0.7894, 0.0972, 1.0000, 1.0000])\n",
      "tensor([0.1831, 0.8169, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.5)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "    \n",
    "for order in stage:\n",
    "    producedata(order)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "    #running\n",
    "    for order1 in order1name:\n",
    "        print(\"Supervised Aim:\",order,order1)\n",
    "        \n",
    "        # for mapping binary to payments before softmax\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(n, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, n),\n",
    "        )\n",
    "        model.apply(init_weights)\n",
    "        # optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        runningLossNN = []\n",
    "        runningLossCS = []\n",
    "        runningLossDP = []\n",
    "        runningLossHeuristic = []\n",
    "        #model=torch.load(\"save/pytorchNN=5dp1\");\n",
    "        #model.eval()\n",
    "        ##order1name=[\"costsharing\",\"dp\",\"heuristic\",\"random initializing\"]\n",
    "        for epoch in range(1, supervisionEpochs + 1):\n",
    "#             print(\"distributionRatio\",distributionRatio)\n",
    "            if(order1==\"costsharing\"):\n",
    "                supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "            elif(order1==\"dp\"):\n",
    "                supervisionTrain(epoch, dpSupervisionRule)\n",
    "            elif(order1==\"heuristic\"):\n",
    "                supervisionTrain(epoch, heuristicSupervisionRule)\n",
    "            elif(order1==\"random initializing\"):\n",
    "                print(\"do nothing\");\n",
    "\n",
    "        test()\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(epoch)\n",
    "            test()\n",
    "        losslistname.append(order+\" \"+order1);\n",
    "        losslist.append(losslisttemp);\n",
    "        losslisttemp=[];\n",
    "        savepath=\"save/pytorchNN=5all-phoenix\"+order+str(order1)\n",
    "        torch.save(model, savepath);\n",
    "        print(\"end\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colorlist=[\"#D2691E\",'#4169E1',\"#9ACD32\",\"#B22222\",\"#FF00FF\",\"#708090\"]\n",
    "for i in range(len(losslist)):\n",
    "    plt.plot(losslist[i], 'ro-', color=colorlist[i], alpha=0.8, label=losslistname[i])\n",
    "\n",
    "# 显示标签，如果不加这句，即使加了label='一些数字'的参数，最终还是不会显示标签\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel('trianing times')\n",
    "plt.ylabel('delay')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
