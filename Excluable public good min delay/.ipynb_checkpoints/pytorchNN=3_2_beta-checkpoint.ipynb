{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3540388371733616\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from random import randint\n",
    "import random\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "from pylab import *\n",
    "from scipy.stats import beta\n",
    "import math\n",
    "n = 3\n",
    "epochs = 1\n",
    "supervisionEpochs = 1\n",
    "lr1 = 0.005\n",
    "lr2 = 0.0005\n",
    "log_interval = 5\n",
    "trainSize = 50000#100000\n",
    "percentage_train_test= 0.5\n",
    "penaltyLambda = 50\n",
    "doublePeakHighMean = 0.6\n",
    "doublePeakLowMean = 0.2\n",
    "doublePeakStd = 0.1\n",
    "uniformlow=0\n",
    "uniformhigh=1.0\n",
    "normalloc = 0.2\n",
    "normalscale = 0.1\n",
    "\n",
    "cauchyloc = 1/n\n",
    "cauchyscalen = 0.004\n",
    "\n",
    "exponentialhigh = 15 #Symbol(\"b\", real=True)\n",
    "exponentiallow  = 15 #Symbol(\"a\", real=True)\n",
    "\n",
    "\n",
    "beta_a = 0.1\n",
    "beta_b  = 0.1\n",
    "kumaraswamy_a = beta_a \n",
    "kumaraswamy_b = (1.0+(beta_a-1.0)*math.pow( (beta_a+beta_b-2.0)/(beta_a-1.0), beta_a) )/beta_a \n",
    "print(kumaraswamy_b)\n",
    "\n",
    "\n",
    "independentnormalloc1=[(float(ii)+1)/(2*n+1) for ii in range(n,0,-1)]\n",
    "independentnormalscale1=[0.05 for ii in range(n)]\n",
    "\n",
    "independentnormalloc2=[(float(ii)+1)/(2*n+1) for ii in range(1,n+1,1)]\n",
    "independentnormalscale2=[0.05 for ii in range(n)]\n",
    "stage=[\"beta\"]\n",
    "# \"twopeak\",\"normal\",\"uniform\",\"independent1\",\"independent2\",\"cauchy\",\"beta\",\"U-exponential\",\"arcsine\"\n",
    "order1name=[\"dp\",\"random initializing\",\"costsharing\",\"heuristic\"]\n",
    "# \"costsharing\",\"dp\",\"heuristic\",\"random initializing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio1 = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase1 = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "d3 = D.normal.Normal(normalloc, normalscale)\n",
    "distributionRatio3 = d3.cdf(1) - d3.cdf(0)\n",
    "distributionBase3 = d3.cdf(0)\n",
    "\n",
    "d4 = D.uniform.Uniform(uniformlow,uniformhigh)\n",
    "distributionRatio4 = d4.cdf(1) - d4.cdf(0)\n",
    "distributionBase4 = d4.cdf(0)\n",
    "\n",
    "d5 = [D.normal.Normal(independentnormalloc1[ii], independentnormalscale1[ii]) for ii in range(n)]\n",
    "d6 = [D.normal.Normal(independentnormalloc2[ii], independentnormalscale2[ii]) for ii in range(n)]\n",
    "\n",
    "d7 = D.cauchy.Cauchy(cauchyloc,cauchyscalen)\n",
    "\n",
    "d81 = D.exponential.Exponential(exponentiallow)\n",
    "d82 = D.exponential.Exponential(exponentialhigh)\n",
    "\n",
    "d9 = D.beta.Beta(beta_a,beta_b)\n",
    "#sample_d9=d9.rsample(torch.Size([100000]))\n",
    "\n",
    "#d9_sample=np.linspace(0.0001, 0.9999, 10000)\n",
    "#d9_pdf=torch.exp(d9.log_prob(torch.tensor(d9_sample,dtype=torch.float32)))\n",
    "#d9_delta=d9_sample[1]-d9_sample[0]\n",
    "#d9_sum_pdf=torch.sum(d9_pdf*d9_delta)\n",
    "\n",
    "d10 = D.beta.Beta(0.5,0.5)\n",
    "\n",
    "def beta_cdf(x,y, i=None):\n",
    "    return beta.cdf(x, beta_a, beta_b)\n",
    "\n",
    "\n",
    "def cdf(x,y, i=None):\n",
    "    if(y==\"twopeak\"):\n",
    "        return (d1.cdf(x) + d2.cdf(x) - distributionBase1) / 2 / distributionRatio1\n",
    "    elif(y==\"normal\"):\n",
    "        return (d3.cdf(x)-distributionBase3)/distributionRatio3;\n",
    "    elif(y==\"uniform\"):\n",
    "        return (d4.cdf(x)-distributionBase4)/distributionRatio4;\n",
    "    elif(y==\"independent1\"):\n",
    "        return d5[i].cdf(x);\n",
    "    elif(y==\"independent2\"):\n",
    "        return d6[i].cdf(x);\n",
    "    elif(y==\"cauchy\"):\n",
    "        return d7.cdf(x);\n",
    "    elif(y==\"beta\"):\n",
    "#         sum_cdf=0.0;\n",
    "#         if(x<0.0001):\n",
    "#             x=0.00011;\n",
    "#         if(x>0.9999):\n",
    "#             x=0.99989;\n",
    "#         for i in range(len(d9_pdf)):\n",
    "#             if(d9_sample[i]<x):\n",
    "#                 sum_cdf+=d9_pdf[i]*d9_delta;\n",
    "#             else:\n",
    "#                 sum_cdf+=(d9_pdf[i]+d9_pdf[i-1])/ 2 *(x-d9_sample[i-1])\n",
    "#                 break;\n",
    "#         return sum_cdf/d9_sum_pdf\n",
    "#         cdf_v=torch.sum((sample_d9<(x)), dtype=torch.float32)/100000\n",
    "#         return cdf_v\n",
    "#    F(x|a,b)=1–(1–x^a)^b\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(x,kumaraswamy_a),kumaraswamy_b);\n",
    "        except:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(torch.tensor(x,dtype=torch.float32),kumaraswamy_a),kumaraswamy_b);\n",
    "    elif(y==\"arcsine\"):\n",
    "        #\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            res=2.0/math.pi * torch.asin(torch.sqrt(x))\n",
    "            #print(x)\n",
    "            return res# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "        except:\n",
    "            return 2.0/math.pi * torch.asin(torch.sqrt(torch.tensor(x,dtype=torch.float32)))# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "    elif(y==\"U-exponential\"):\n",
    "        return (d81.cdf(x) + (1.0 - d82.cdf(1.0-x)))  / 2 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    if torch.sum(bits).item() == 0:\n",
    "        return torch.ones(n)\n",
    "    bits = bits.type(torch.float32)\n",
    "    negBits = torch.ones(n) - bits\n",
    "    payments = model(bits)\n",
    "    payments = payments - 1000 * negBits\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    payments = payments + negBits\n",
    "    return payments\n",
    "\n",
    "def tpToBits(tp, bits=torch.ones(n).type(torch.uint8)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = (tp >= payments).type(torch.uint8)\n",
    "    if torch.equal(newBits, bits):\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits(tp, newBits)#bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    return n - torch.sum(tpToBits(tp).type(torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    global  dp,decision\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money, yes]\n",
    "        offer = float(offerIndex) / dpPrecision\n",
    "        if tp[i] >= offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits[i] = 0\n",
    "            payments[i] = 1\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "#     print()\n",
    "#     print(tp)\n",
    "#     print(bits)\n",
    "#     print(payments)\n",
    "#     print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def heuristicSupervisionRule(tp):\n",
    "    global  dp_H,decision_H\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    tempold=-1;\n",
    "    for turns in range(n,0,-1):\n",
    "        money = dpPrecision\n",
    "        j=0\n",
    "        tempo=sum(bits)\n",
    "        #print(\"bits\",tempo)\n",
    "        for i in range(n):\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            \n",
    "            offerIndex = decision_H[tempo ,tempo - i, money]\n",
    "            offer = float(offerIndex) / dpPrecision\n",
    "            while(j<n):\n",
    "                if(bits[j]!=0):\n",
    "                    break;\n",
    "                j+=1;\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            if tp[j] >= offer:\n",
    "                #print(money,j,tp[j],offer)\n",
    "                money -= offerIndex\n",
    "                bits[j] = 1\n",
    "                payments[j] = offer\n",
    "            else:\n",
    "                bits[j] = 0;\n",
    "                payments[j] = 1.0;\n",
    "            j+=1;\n",
    "        #print(\"money\",money)\n",
    "        if(money==0 and tempold==tempo):\n",
    "            break;\n",
    "        tempold=tempo;\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    for k in range(n, -1, -1):\n",
    "        if k == 0:\n",
    "            break\n",
    "        bits = [1 if tp[ii] >= 1.0 / k else 0 for ii in range(n)]\n",
    "        if sum(bits) == k:\n",
    "            break\n",
    "    if k == 0:\n",
    "        payments = [1 for ii in range(n)]\n",
    "    else:\n",
    "        payments = [1.0 / k if bits[ii] == 1 else 1 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    return float(n - torch.sum(costSharingSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def dpDelay(tp):\n",
    "    return float(n - torch.sum(dpSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def heuristicDelay(tp):\n",
    "    return float(n - torch.sum(heuristicSupervisionRule(tp)[0]).item())\n",
    "\n",
    "\n",
    "# templire=0;\n",
    "# num=1\n",
    "# for i in range(num):\n",
    "#     temp=torch.tensor(samplesJoint[i])\n",
    "#     #print(temp)\n",
    "#     #print(temp);\n",
    "#     #print(heuristicSupervisionRule(temp))\n",
    "#     #print(dpSupervisionRule(temp))\n",
    "#     #print(dpDelay(temp))\n",
    "#     res=dpDelay(temp)\n",
    "#     templire+=res\n",
    "#     #print(\"Delay\",res)\n",
    "#     #print()\n",
    "# print(templire/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordAndReport(name, source, loss):\n",
    "    source.append(loss)\n",
    "    realLength = len(source)\n",
    "    #print(f\"{name} ({realLength}): {loss}\")\n",
    "    print(name,realLength,\":\" ,loss)\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    \n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda /100000\n",
    "        \n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % (log_interval*5) == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "losslist=[];\n",
    "losslistname=[];\n",
    "losslisttemp=[];\n",
    "def test_batch():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lenLoss= 0\n",
    "        nnLoss = 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "            for tp in tp_batch:\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        nnLoss/=lenLoss\n",
    "    return nnLoss\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                Delay1 = tpToTotalDelay(tp1)\n",
    "                Delay0 = tpToTotalDelay(tp0)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                    loss = loss + (1 - cdf(offer,order)) * Delay1 + cdf(offer,order) * Delay0\n",
    "                else:\n",
    "                    loss = loss + (1 - cdf(offer,order,i)) * Delay1 + cdf(offer,order,i) * Delay0\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "    \n",
    "allBits = [torch.tensor(bits, dtype=torch.int16) for bits in itertools.product([0, 1], repeat=n)]\n",
    "\n",
    "def test():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        costSharingLoss = 0\n",
    "        dpLoss = 0\n",
    "        nnLoss = 0\n",
    "        heuristicLoss=0\n",
    "        lenLoss= 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "                heuristicLoss+= heuristicDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        costSharingLoss /= lenLoss\n",
    "        dpLoss /= lenLoss\n",
    "        nnLoss /= lenLoss\n",
    "        heuristicLoss /= lenLoss\n",
    "        #print(lenLoss)\n",
    "        losslisttemp.append(nnLoss);\n",
    "        recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "        recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "        recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "        recordAndReport(\"heuristic\", runningLossHeuristic, heuristicLoss)\n",
    "        print(\"DP:\",dp[n, dpPrecision,0])\n",
    "        #for i in range(n, 0, -1):\n",
    "        #    print(\"Heuristic:\",i,5*(1-dp_H[i, i, dpPrecision]))\n",
    "        for i in range(n, 0, -1):\n",
    "            print(\n",
    "                    tpToPayments(\n",
    "                            torch.tensor([0 if ii >= i else 1 for ii in range(n)], dtype=torch.float32)\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "dpPrecision = 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def producedata(order):\n",
    "    global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "    dpPrecision = 100 \n",
    "    if(order==\"twopeak\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = np.random.normal(\n",
    "            loc=doublePeakLowMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=doublePeakLowMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samples2 = np.random.normal(\n",
    "            loc=doublePeakHighMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = np.random.normal(\n",
    "                        loc=doublePeakHighMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "    elif(order==\"normal\"):\n",
    "        print(\"loc\",normalloc, \"scale\",normalscale)\n",
    "        samples1 = np.random.normal(\n",
    "            loc=normalloc, scale=normalscale, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=normalloc, scale=normalscale\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"uniform\"):  \n",
    "        print(\"uniformlow\",uniformlow, \"uniformhigh\",uniformhigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        uniformlow, uniformhigh\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent1\"):\n",
    "        print(\"loc\",independentnormalloc1,\"scale\",independentnormalscale1)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent2\"):\n",
    "        print(\"loc\",independentnormalloc2, \"scale\",independentnormalscale2)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"cauchy\"):\n",
    "        print(\"cauchyloc\",cauchyloc, \"cauchyscale\",cauchyscalen)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"beta\"):\n",
    "        print(\"beta_a\",beta_a, \"beta_b\",beta_b)\n",
    "        print(\"kumaraswamy_a\",kumaraswamy_a, \"kumaraswamy_b\",kumaraswamy_b)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"arcsine\"):\n",
    "        print(\"betalow\",0.5, \"betahigh\",0.5)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"U-exponential\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = d81.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d81.rsample(torch.Size([1])).numpy()\n",
    "                    \n",
    "        samples2 = d82.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        \n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = d82.rsample(torch.Size([1])).numpy()\n",
    "        samples2 = 1.0 - samples2\n",
    "        samplesJoint = signals * samples1 - (signals - 1.0) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "        \n",
    "    plt.hist(samplesJoint,bins=500)\n",
    "    plt.show()\n",
    "    \n",
    "    tp_dataset = TensorDataset(tp_tensor[: int(trainSize * percentage_train_test)])\n",
    "    tp_dataset_testing = TensorDataset(tp_tensor[int(trainSize * (1.0-percentage_train_test)) :])\n",
    "    tp_dataloader = DataLoader(tp_dataset, batch_size=128, shuffle=True)\n",
    "    tp_dataloader_testing = DataLoader(tp_dataset_testing, batch_size=256, shuffle=False)\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "    decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for yes in range(n + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp[0, 0, yes] = 0\n",
    "            else:\n",
    "                dp[0, money, yes] = yes# + 1.0\n",
    "    for ppl in range(1,  n + 1):\n",
    "        for yes in range(n + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                minSoFar = 1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order==\"beta\"):\n",
    "                        res = (1 - beta_cdf(offer,order)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + beta_cdf(offer,order) * (1 + dp[ppl - 1, money, yes])\n",
    "#                     if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "#                         res = (1 - cdf(offer,order)) * dp[\n",
    "#                         ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "#                         ] + cdf(offer,order) * (1 + dp[ppl - 1, money, yes])\n",
    "#                     else:\n",
    "#                         res = (1 - cdf(offer,order,n-ppl)) * dp[\n",
    "#                         ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "#                         ] + cdf(offer,order,n-ppl) * (1 + dp[ppl - 1, money, yes])\n",
    "                    if minSoFar > res.item():\n",
    "                        minSoFar = res.item()\n",
    "                        decision[ppl, money, yes] = offerIndex\n",
    "                dp[ppl, money, yes] = minSoFar\n",
    "    \n",
    "    print(\"dp\",dp[n, dpPrecision, 0])\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp_H = np.zeros([n + 1 , n + 1, dpPrecision + 1])\n",
    "    decision_H = np.zeros([n + 1 , n + 1, dpPrecision + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for i in range(1,n+1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp_H[i, 0, 0] = 1\n",
    "            else:\n",
    "                offer = money / dpPrecision\n",
    "                dp_H[i, 0, money] = 0#cdf(offer)# + 1.0\n",
    "    for i in range(1,n+1):\n",
    "        for ppl in range(1, i + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                maxSoFar = -1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1-cdf(offer,order)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    else:\n",
    "                        res = (1-cdf(offer,order,ppl-1)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    if maxSoFar < res.item():\n",
    "                        maxSoFar = res.item()\n",
    "                        decision_H[i, ppl, money] = offerIndex\n",
    "                dp_H[i, ppl, money] = maxSoFar\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_a 0.1 beta_b 0.1\n",
      "kumaraswamy_a 0.1 kumaraswamy_b 0.3540388371733616\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dp 1.7499999999999998\n",
      "Supervised Aim: beta dp\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 0.007448\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 0.000288\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 0.000039\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 0.000011\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 0.000005\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 0.000003\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 0.000029\n",
      "NN 1 : tensor(1.7531)\n",
      "CS 1 : 1.77668\n",
      "DP 1 : 1.74812\n",
      "heuristic 1 : 1.74784\n",
      "DP: 1.7499999999999998\n",
      "tensor([0.4974, 0.4981, 0.0046])\n",
      "tensor([0.4977, 0.5023, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 1.922328 testing loss: tensor(1.7560)\n",
      "Train Epoch: 1 [640/25000 (3%)]\tLoss: 1.938245 testing loss: tensor(1.7493)\n",
      "Train Epoch: 1 [1280/25000 (5%)]\tLoss: 1.746644 testing loss: tensor(1.7490)\n",
      "Train Epoch: 1 [1920/25000 (8%)]\tLoss: 1.882857 testing loss: tensor(1.7500)\n",
      "Train Epoch: 1 [2560/25000 (10%)]\tLoss: 1.920814 testing loss: tensor(1.7489)\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 1.884256 testing loss: tensor(1.7475)\n",
      "Train Epoch: 1 [3840/25000 (15%)]\tLoss: 2.094895 testing loss: tensor(1.7446)\n",
      "Train Epoch: 1 [4480/25000 (18%)]\tLoss: 1.929365 testing loss: tensor(1.7433)\n",
      "Train Epoch: 1 [5120/25000 (20%)]\tLoss: 1.886562 testing loss: tensor(1.7417)\n",
      "Train Epoch: 1 [5760/25000 (23%)]\tLoss: 1.942315 testing loss: tensor(1.7406)\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 1.977451 testing loss: tensor(1.7388)\n",
      "Train Epoch: 1 [7040/25000 (28%)]\tLoss: 1.804642 testing loss: tensor(1.7369)\n",
      "Train Epoch: 1 [7680/25000 (31%)]\tLoss: 1.843757 testing loss: tensor(1.7360)\n",
      "Train Epoch: 1 [8320/25000 (33%)]\tLoss: 1.969782 testing loss: tensor(1.7339)\n",
      "Train Epoch: 1 [8960/25000 (36%)]\tLoss: 1.856179 testing loss: tensor(1.7325)\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 1.912218 testing loss: tensor(1.7317)\n",
      "Train Epoch: 1 [10240/25000 (41%)]\tLoss: 1.844946 testing loss: tensor(1.7309)\n",
      "Train Epoch: 1 [10880/25000 (43%)]\tLoss: 1.956998 testing loss: tensor(1.7278)\n",
      "Train Epoch: 1 [11520/25000 (46%)]\tLoss: 1.880742 testing loss: tensor(1.7274)\n",
      "Train Epoch: 1 [12160/25000 (48%)]\tLoss: 1.935220 testing loss: tensor(1.7257)\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 2.004170 testing loss: tensor(1.7266)\n",
      "Train Epoch: 1 [13440/25000 (54%)]\tLoss: 1.869344 testing loss: tensor(1.7251)\n",
      "Train Epoch: 1 [14080/25000 (56%)]\tLoss: 1.949215 testing loss: tensor(1.7226)\n",
      "Train Epoch: 1 [14720/25000 (59%)]\tLoss: 1.798088 testing loss: tensor(1.7222)\n",
      "Train Epoch: 1 [15360/25000 (61%)]\tLoss: 1.747992 testing loss: tensor(1.7198)\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 1.959561 testing loss: tensor(1.7188)\n",
      "Train Epoch: 1 [16640/25000 (66%)]\tLoss: 1.897254 testing loss: tensor(1.7196)\n",
      "Train Epoch: 1 [17280/25000 (69%)]\tLoss: 1.879204 testing loss: tensor(1.7200)\n",
      "Train Epoch: 1 [17920/25000 (71%)]\tLoss: 1.892770 testing loss: tensor(1.7189)\n",
      "Train Epoch: 1 [18560/25000 (74%)]\tLoss: 1.850127 testing loss: tensor(1.7209)\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 1.888938 testing loss: tensor(1.7173)\n",
      "Train Epoch: 1 [19840/25000 (79%)]\tLoss: 1.933611 testing loss: tensor(1.7171)\n",
      "Train Epoch: 1 [20480/25000 (82%)]\tLoss: 1.748408 testing loss: tensor(1.7161)\n",
      "Train Epoch: 1 [21120/25000 (84%)]\tLoss: 1.885284 testing loss: tensor(1.7160)\n",
      "Train Epoch: 1 [21760/25000 (87%)]\tLoss: 2.002883 testing loss: tensor(1.7147)\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 1.854657 testing loss: tensor(1.7112)\n",
      "Train Epoch: 1 [23040/25000 (92%)]\tLoss: 1.836660 testing loss: tensor(1.7160)\n",
      "Train Epoch: 1 [23680/25000 (94%)]\tLoss: 1.840915 testing loss: tensor(1.7182)\n",
      "Train Epoch: 1 [24320/25000 (97%)]\tLoss: 1.944092 testing loss: tensor(1.7125)\n",
      "Train Epoch: 1 [7800/25000 (99%)]\tLoss: 2.011868 testing loss: tensor(1.7104)\n",
      "penalty: 0.0006675422191619873\n",
      "NN 2 : tensor(1.7104)\n",
      "CS 2 : 1.77668\n",
      "DP 2 : 1.74812\n",
      "heuristic 2 : 1.74784\n",
      "DP: 1.7499999999999998\n",
      "tensor([5.2391e-01, 4.7609e-01, 8.8823e-07])\n",
      "tensor([0.5214, 0.4786, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: beta random initializing\n",
      "do nothing\n",
      "NN 1 : tensor(1.7766)\n",
      "CS 1 : 1.77668\n",
      "DP 1 : 1.74812\n",
      "heuristic 1 : 1.74784\n",
      "DP: 1.7499999999999998\n",
      "tensor([0.2312, 0.4948, 0.2740])\n",
      "tensor([0.3861, 0.6139, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 1.905567 testing loss: tensor(1.7788)\n",
      "Train Epoch: 1 [640/25000 (3%)]\tLoss: 1.924502 testing loss: tensor(1.7780)\n",
      "Train Epoch: 1 [1280/25000 (5%)]\tLoss: 1.873931 testing loss: tensor(1.7772)\n",
      "Train Epoch: 1 [1920/25000 (8%)]\tLoss: 1.938227 testing loss: tensor(1.7761)\n",
      "Train Epoch: 1 [2560/25000 (10%)]\tLoss: 2.023172 testing loss: tensor(1.7770)\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 2.000742 testing loss: tensor(1.7751)\n",
      "Train Epoch: 1 [3840/25000 (15%)]\tLoss: 1.973648 testing loss: tensor(1.7760)\n",
      "Train Epoch: 1 [4480/25000 (18%)]\tLoss: 1.831874 testing loss: tensor(1.7770)\n",
      "Train Epoch: 1 [5120/25000 (20%)]\tLoss: 1.893610 testing loss: tensor(1.7773)\n",
      "Train Epoch: 1 [5760/25000 (23%)]\tLoss: 1.919277 testing loss: tensor(1.7767)\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 1.917425 testing loss: tensor(1.7776)\n",
      "Train Epoch: 1 [7040/25000 (28%)]\tLoss: 1.959079 testing loss: tensor(1.7778)\n",
      "Train Epoch: 1 [7680/25000 (31%)]\tLoss: 2.005402 testing loss: tensor(1.7766)\n",
      "Train Epoch: 1 [8320/25000 (33%)]\tLoss: 1.968696 testing loss: tensor(1.7760)\n",
      "Train Epoch: 1 [8960/25000 (36%)]\tLoss: 1.912784 testing loss: tensor(1.7776)\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 1.959808 testing loss: tensor(1.7777)\n",
      "Train Epoch: 1 [10240/25000 (41%)]\tLoss: 1.934569 testing loss: tensor(1.7783)\n",
      "Train Epoch: 1 [10880/25000 (43%)]\tLoss: 1.963442 testing loss: tensor(1.7781)\n",
      "Train Epoch: 1 [11520/25000 (46%)]\tLoss: 1.943326 testing loss: tensor(1.7782)\n",
      "Train Epoch: 1 [12160/25000 (48%)]\tLoss: 1.884514 testing loss: tensor(1.7790)\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 1.971753 testing loss: tensor(1.7792)\n",
      "Train Epoch: 1 [13440/25000 (54%)]\tLoss: 1.820430 testing loss: tensor(1.7792)\n",
      "Train Epoch: 1 [14080/25000 (56%)]\tLoss: 1.742796 testing loss: tensor(1.7792)\n",
      "Train Epoch: 1 [14720/25000 (59%)]\tLoss: 1.930411 testing loss: tensor(1.7779)\n",
      "Train Epoch: 1 [15360/25000 (61%)]\tLoss: 1.893669 testing loss: tensor(1.7759)\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 1.967353 testing loss: tensor(1.7766)\n",
      "Train Epoch: 1 [16640/25000 (66%)]\tLoss: 1.917517 testing loss: tensor(1.7769)\n",
      "Train Epoch: 1 [17280/25000 (69%)]\tLoss: 1.894465 testing loss: tensor(1.7777)\n",
      "Train Epoch: 1 [17920/25000 (71%)]\tLoss: 1.994417 testing loss: tensor(1.7780)\n",
      "Train Epoch: 1 [18560/25000 (74%)]\tLoss: 1.976016 testing loss: tensor(1.7782)\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 2.022126 testing loss: tensor(1.7783)\n",
      "Train Epoch: 1 [19840/25000 (79%)]\tLoss: 1.869571 testing loss: tensor(1.7771)\n",
      "Train Epoch: 1 [20480/25000 (82%)]\tLoss: 1.947977 testing loss: tensor(1.7795)\n",
      "Train Epoch: 1 [21120/25000 (84%)]\tLoss: 1.953783 testing loss: tensor(1.7779)\n",
      "Train Epoch: 1 [21760/25000 (87%)]\tLoss: 1.929881 testing loss: tensor(1.7768)\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 1.923640 testing loss: tensor(1.7777)\n",
      "Train Epoch: 1 [23040/25000 (92%)]\tLoss: 1.938771 testing loss: tensor(1.7763)\n",
      "Train Epoch: 1 [23680/25000 (94%)]\tLoss: 1.934138 testing loss: tensor(1.7775)\n",
      "Train Epoch: 1 [24320/25000 (97%)]\tLoss: 1.856553 testing loss: tensor(1.7778)\n",
      "Train Epoch: 1 [7800/25000 (99%)]\tLoss: 1.896982 testing loss: tensor(1.7773)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.7773)\n",
      "CS 2 : 1.77668\n",
      "DP 2 : 1.74812\n",
      "heuristic 2 : 1.74784\n",
      "DP: 1.7499999999999998\n",
      "tensor([0.3116, 0.4592, 0.2292])\n",
      "tensor([0.4263, 0.5737, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: beta costsharing\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 0.001350\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 0.000132\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 0.000006\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 0.000000\n",
      "NN 1 : tensor(1.7767)\n",
      "CS 1 : 1.77668\n",
      "DP 1 : 1.74812\n",
      "heuristic 1 : 1.74784\n",
      "DP: 1.7499999999999998\n",
      "tensor([0.3333, 0.3333, 0.3333])\n",
      "tensor([0.5000, 0.5000, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 1.897421 testing loss: tensor(1.7778)\n",
      "Train Epoch: 1 [640/25000 (3%)]\tLoss: 1.940027 testing loss: tensor(1.7771)\n",
      "Train Epoch: 1 [1280/25000 (5%)]\tLoss: 1.978769 testing loss: tensor(1.7766)\n",
      "Train Epoch: 1 [1920/25000 (8%)]\tLoss: 1.854735 testing loss: tensor(1.7758)\n",
      "Train Epoch: 1 [2560/25000 (10%)]\tLoss: 1.937872 testing loss: tensor(1.7746)\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 1.930005 testing loss: tensor(1.7747)\n",
      "Train Epoch: 1 [3840/25000 (15%)]\tLoss: 1.830175 testing loss: tensor(1.7756)\n",
      "Train Epoch: 1 [4480/25000 (18%)]\tLoss: 1.918548 testing loss: tensor(1.7752)\n",
      "Train Epoch: 1 [5120/25000 (20%)]\tLoss: 1.999629 testing loss: tensor(1.7758)\n",
      "Train Epoch: 1 [5760/25000 (23%)]\tLoss: 1.955506 testing loss: tensor(1.7758)\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 1.929554 testing loss: tensor(1.7760)\n",
      "Train Epoch: 1 [7040/25000 (28%)]\tLoss: 1.977368 testing loss: tensor(1.7765)\n",
      "Train Epoch: 1 [7680/25000 (31%)]\tLoss: 1.855634 testing loss: tensor(1.7765)\n",
      "Train Epoch: 1 [8320/25000 (33%)]\tLoss: 1.906800 testing loss: tensor(1.7767)\n",
      "Train Epoch: 1 [8960/25000 (36%)]\tLoss: 1.976097 testing loss: tensor(1.7765)\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 1.809954 testing loss: tensor(1.7764)\n",
      "Train Epoch: 1 [10240/25000 (41%)]\tLoss: 2.083132 testing loss: tensor(1.7762)\n",
      "Train Epoch: 1 [10880/25000 (43%)]\tLoss: 1.903329 testing loss: tensor(1.7765)\n",
      "Train Epoch: 1 [11520/25000 (46%)]\tLoss: 1.965959 testing loss: tensor(1.7767)\n",
      "Train Epoch: 1 [12160/25000 (48%)]\tLoss: 2.112478 testing loss: tensor(1.7770)\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 1.971698 testing loss: tensor(1.7761)\n",
      "Train Epoch: 1 [13440/25000 (54%)]\tLoss: 1.942005 testing loss: tensor(1.7761)\n",
      "Train Epoch: 1 [14080/25000 (56%)]\tLoss: 1.918678 testing loss: tensor(1.7760)\n",
      "Train Epoch: 1 [14720/25000 (59%)]\tLoss: 1.888338 testing loss: tensor(1.7778)\n",
      "Train Epoch: 1 [15360/25000 (61%)]\tLoss: 1.968300 testing loss: tensor(1.7777)\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 2.075547 testing loss: tensor(1.7777)\n",
      "Train Epoch: 1 [16640/25000 (66%)]\tLoss: 1.935792 testing loss: tensor(1.7773)\n",
      "Train Epoch: 1 [17280/25000 (69%)]\tLoss: 1.987911 testing loss: tensor(1.7784)\n",
      "Train Epoch: 1 [17920/25000 (71%)]\tLoss: 1.882485 testing loss: tensor(1.7795)\n",
      "Train Epoch: 1 [18560/25000 (74%)]\tLoss: 1.916013 testing loss: tensor(1.7788)\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 1.959353 testing loss: tensor(1.7780)\n",
      "Train Epoch: 1 [19840/25000 (79%)]\tLoss: 1.896957 testing loss: tensor(1.7772)\n",
      "Train Epoch: 1 [20480/25000 (82%)]\tLoss: 1.989667 testing loss: tensor(1.7767)\n",
      "Train Epoch: 1 [21120/25000 (84%)]\tLoss: 1.967996 testing loss: tensor(1.7760)\n",
      "Train Epoch: 1 [21760/25000 (87%)]\tLoss: 1.903612 testing loss: tensor(1.7756)\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 1.940217 testing loss: tensor(1.7766)\n",
      "Train Epoch: 1 [23040/25000 (92%)]\tLoss: 1.882948 testing loss: tensor(1.7780)\n",
      "Train Epoch: 1 [23680/25000 (94%)]\tLoss: 2.084357 testing loss: tensor(1.7786)\n",
      "Train Epoch: 1 [24320/25000 (97%)]\tLoss: 2.035697 testing loss: tensor(1.7785)\n",
      "Train Epoch: 1 [7800/25000 (99%)]\tLoss: 1.804407 testing loss: tensor(1.7790)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.7790)\n",
      "CS 2 : 1.77668\n",
      "DP 2 : 1.74812\n",
      "heuristic 2 : 1.74784\n",
      "DP: 1.7499999999999998\n",
      "tensor([0.2848, 0.4465, 0.2686])\n",
      "tensor([0.3964, 0.6036, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: beta heuristic\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 0.010529\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 0.000174\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 0.000086\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 0.000011\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 0.000146\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 0.000019\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 0.000212\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 0.000144\n",
      "NN 1 : tensor(1.7659)\n",
      "CS 1 : 1.77668\n",
      "DP 1 : 1.74812\n",
      "heuristic 1 : 1.74784\n",
      "DP: 1.7499999999999998\n",
      "tensor([0.0141, 0.4848, 0.5011])\n",
      "tensor([0.5135, 0.4865, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 1.945254 testing loss: tensor(1.7619)\n",
      "Train Epoch: 1 [640/25000 (3%)]\tLoss: 1.932584 testing loss: tensor(1.7600)\n",
      "Train Epoch: 1 [1280/25000 (5%)]\tLoss: 1.937364 testing loss: tensor(1.7598)\n",
      "Train Epoch: 1 [1920/25000 (8%)]\tLoss: 1.939710 testing loss: tensor(1.7596)\n",
      "Train Epoch: 1 [2560/25000 (10%)]\tLoss: 2.019625 testing loss: tensor(1.7591)\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 1.879375 testing loss: tensor(1.7592)\n",
      "Train Epoch: 1 [3840/25000 (15%)]\tLoss: 1.865078 testing loss: tensor(1.7576)\n",
      "Train Epoch: 1 [4480/25000 (18%)]\tLoss: 1.892716 testing loss: tensor(1.7549)\n",
      "Train Epoch: 1 [5120/25000 (20%)]\tLoss: 1.896152 testing loss: tensor(1.7527)\n",
      "Train Epoch: 1 [5760/25000 (23%)]\tLoss: 1.921387 testing loss: tensor(1.7509)\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 1.863269 testing loss: tensor(1.7498)\n",
      "Train Epoch: 1 [7040/25000 (28%)]\tLoss: 1.911541 testing loss: tensor(1.7477)\n",
      "Train Epoch: 1 [7680/25000 (31%)]\tLoss: 1.929610 testing loss: tensor(1.7489)\n",
      "Train Epoch: 1 [8320/25000 (33%)]\tLoss: 1.883618 testing loss: tensor(1.7459)\n",
      "Train Epoch: 1 [8960/25000 (36%)]\tLoss: 1.892024 testing loss: tensor(1.7448)\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 1.897580 testing loss: tensor(1.7447)\n",
      "Train Epoch: 1 [10240/25000 (41%)]\tLoss: 1.941883 testing loss: tensor(1.7427)\n",
      "Train Epoch: 1 [10880/25000 (43%)]\tLoss: 1.827281 testing loss: tensor(1.7429)\n",
      "Train Epoch: 1 [11520/25000 (46%)]\tLoss: 1.974481 testing loss: tensor(1.7409)\n",
      "Train Epoch: 1 [12160/25000 (48%)]\tLoss: 1.760909 testing loss: tensor(1.7398)\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 1.874583 testing loss: tensor(1.7388)\n",
      "Train Epoch: 1 [13440/25000 (54%)]\tLoss: 1.907934 testing loss: tensor(1.7359)\n",
      "Train Epoch: 1 [14080/25000 (56%)]\tLoss: 1.924918 testing loss: tensor(1.7362)\n",
      "Train Epoch: 1 [14720/25000 (59%)]\tLoss: 1.798119 testing loss: tensor(1.7362)\n",
      "Train Epoch: 1 [15360/25000 (61%)]\tLoss: 1.946079 testing loss: tensor(1.7353)\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 1.870704 testing loss: tensor(1.7338)\n",
      "Train Epoch: 1 [16640/25000 (66%)]\tLoss: 2.000356 testing loss: tensor(1.7305)\n",
      "Train Epoch: 1 [17280/25000 (69%)]\tLoss: 1.936246 testing loss: tensor(1.7321)\n",
      "Train Epoch: 1 [17920/25000 (71%)]\tLoss: 1.819702 testing loss: tensor(1.7301)\n",
      "Train Epoch: 1 [18560/25000 (74%)]\tLoss: 1.880901 testing loss: tensor(1.7295)\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 1.840669 testing loss: tensor(1.7271)\n",
      "Train Epoch: 1 [19840/25000 (79%)]\tLoss: 1.953675 testing loss: tensor(1.7266)\n",
      "Train Epoch: 1 [20480/25000 (82%)]\tLoss: 1.938317 testing loss: tensor(1.7288)\n",
      "Train Epoch: 1 [21120/25000 (84%)]\tLoss: 1.841628 testing loss: tensor(1.7286)\n",
      "Train Epoch: 1 [21760/25000 (87%)]\tLoss: 1.851609 testing loss: tensor(1.7282)\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 1.878961 testing loss: tensor(1.7272)\n",
      "Train Epoch: 1 [23040/25000 (92%)]\tLoss: 1.848373 testing loss: tensor(1.7222)\n",
      "Train Epoch: 1 [23680/25000 (94%)]\tLoss: 1.850790 testing loss: tensor(1.7218)\n",
      "Train Epoch: 1 [24320/25000 (97%)]\tLoss: 1.845870 testing loss: tensor(1.7232)\n",
      "Train Epoch: 1 [7800/25000 (99%)]\tLoss: 1.932771 testing loss: tensor(1.7233)\n",
      "penalty: 0.001181572675704956\n",
      "NN 2 : tensor(1.7233)\n",
      "CS 2 : 1.77668\n",
      "DP 2 : 1.74812\n",
      "heuristic 2 : 1.74784\n",
      "DP: 1.7499999999999998\n",
      "tensor([1.3534e-05, 4.9055e-01, 5.0944e-01])\n",
      "tensor([0.3793, 0.6207, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.1)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "    \n",
    "for order in stage:\n",
    "    producedata(order)\n",
    "    #running\n",
    "    for order1 in order1name:\n",
    "        print(\"Supervised Aim:\",order,order1)\n",
    "        \n",
    "        # for mapping binary to payments before softmax\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(n, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, n),\n",
    "        )\n",
    "        model.apply(init_weights)\n",
    "        # optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr1)\n",
    "        \n",
    "        runningLossNN = []\n",
    "        runningLossCS = []\n",
    "        runningLossDP = []\n",
    "        runningLossHeuristic = []\n",
    "        #model=torch.load(\"save/pytorchNN=5dp1\");\n",
    "        #model.eval()\n",
    "        ##order1name=[\"costsharing\",\"dp\",\"heuristic\",\"random initializing\"]\n",
    "        for epoch in range(1, supervisionEpochs + 1):\n",
    "#             print(\"distributionRatio\",distributionRatio)\n",
    "            if(order1==\"costsharing\"):\n",
    "                supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "            elif(order1==\"dp\"):\n",
    "                supervisionTrain(epoch, dpSupervisionRule)\n",
    "            elif(order1==\"heuristic\"):\n",
    "                supervisionTrain(epoch, heuristicSupervisionRule)\n",
    "            elif(order1==\"random initializing\"):\n",
    "                print(\"do nothing\");\n",
    "\n",
    "        test()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr2)\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(epoch)\n",
    "            test()\n",
    "        losslistname.append(order+\" \"+order1);\n",
    "        losslist.append(losslisttemp);\n",
    "        losslisttemp=[];\n",
    "        savepath=\"save/pytorchNN=5all-beta\"+order+str(order1)\n",
    "        torch.save(model, savepath);\n",
    "        print(\"end\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colorlist=[\"#D2691E\",'#4169E1',\"#9ACD32\",\"#B22222\",\"#FF00FF\",\"#708090\"]\n",
    "for i in range(len(losslist)):\n",
    "    plt.plot(losslist[i], '.-', color=colorlist[i], alpha=0.8, label=losslistname[i])\n",
    "\n",
    "# 显示标签，如果不加这句，即使加了label='一些数字'的参数，最终还是不会显示标签\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('Trianing Times')\n",
    "plt.ylabel('Delay')\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('pytorchNN=3_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
