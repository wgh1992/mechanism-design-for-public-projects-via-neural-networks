{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40058530825361593\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from random import randint\n",
    "import random\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "from pylab import *\n",
    "from scipy.stats import beta\n",
    "import math\n",
    "n = 5\n",
    "epochs = 1\n",
    "supervisionEpochs = 1\n",
    "lr1 = 0.01\n",
    "lr2 = 0.0005\n",
    "log_interval = 5\n",
    "trainSize = 50000#100000\n",
    "percentage_train_test= 0.5\n",
    "penaltyLambda = 10\n",
    "doublePeakHighMean = 0.8\n",
    "doublePeakLowMean = 0.2\n",
    "doublePeakStd = 0.04\n",
    "uniformlow=0\n",
    "uniformhigh=1.0\n",
    "normalloc = 0.2\n",
    "normalscale = 0.1\n",
    "\n",
    "cauchyloc = 1/n\n",
    "cauchyscalen = 0.004\n",
    "\n",
    "exponentialhigh = 15 #Symbol(\"b\", real=True)\n",
    "exponentiallow  = 15 #Symbol(\"a\", real=True)\n",
    "\n",
    "\n",
    "beta_a = 0.3\n",
    "beta_b  = 0.2\n",
    "kumaraswamy_a = beta_a \n",
    "kumaraswamy_b = (1.0+(beta_a-1.0)*math.pow( (beta_a+beta_b-2.0)/(beta_a-1.0), beta_a) )/beta_a \n",
    "print(kumaraswamy_b)\n",
    "\n",
    "\n",
    "independentnormalloc1=[(float(ii)+1)/(2*n+1) for ii in range(n,0,-1)]\n",
    "independentnormalscale1=[0.05 for ii in range(n)]\n",
    "\n",
    "independentnormalloc2=[(float(ii)+1)/(2*n+1) for ii in range(1,n+1,1)]\n",
    "independentnormalscale2=[0.05 for ii in range(n)]\n",
    "stage=[\"twopeak\"]\n",
    "# \"twopeak\",\"normal\",\"uniform\",\"independent1\",\"independent2\",\"cauchy\",\"beta\",\"U-exponential\",\"arcsine\"\n",
    "order1name=[\"dp\",\"random initializing\",\"costsharing\",\"heuristic\"]\n",
    "# \"costsharing\",\"dp\",\"heuristic\",\"random initializing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio1 = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase1 = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "d3 = D.normal.Normal(normalloc, normalscale)\n",
    "distributionRatio3 = d3.cdf(1) - d3.cdf(0)\n",
    "distributionBase3 = d3.cdf(0)\n",
    "\n",
    "d4 = D.uniform.Uniform(uniformlow,uniformhigh)\n",
    "distributionRatio4 = d4.cdf(1) - d4.cdf(0)\n",
    "distributionBase4 = d4.cdf(0)\n",
    "\n",
    "d5 = [D.normal.Normal(independentnormalloc1[ii], independentnormalscale1[ii]) for ii in range(n)]\n",
    "d6 = [D.normal.Normal(independentnormalloc2[ii], independentnormalscale2[ii]) for ii in range(n)]\n",
    "\n",
    "d7 = D.cauchy.Cauchy(cauchyloc,cauchyscalen)\n",
    "\n",
    "d81 = D.exponential.Exponential(exponentiallow)\n",
    "d82 = D.exponential.Exponential(exponentialhigh)\n",
    "\n",
    "d9 = D.beta.Beta(beta_a,beta_b)\n",
    "#sample_d9=d9.rsample(torch.Size([100000]))\n",
    "\n",
    "#d9_sample=np.linspace(0.0001, 0.9999, 10000)\n",
    "#d9_pdf=torch.exp(d9.log_prob(torch.tensor(d9_sample,dtype=torch.float32)))\n",
    "#d9_delta=d9_sample[1]-d9_sample[0]\n",
    "#d9_sum_pdf=torch.sum(d9_pdf*d9_delta)\n",
    "\n",
    "d10 = D.beta.Beta(0.5,0.5)\n",
    "\n",
    "def cdf(x,y, i=None):\n",
    "    if(y==\"twopeak\"):\n",
    "        return (d1.cdf(x) + d2.cdf(x) - distributionBase1) / 2 / distributionRatio1\n",
    "    elif(y==\"normal\"):\n",
    "        return (d3.cdf(x)-distributionBase3)/distributionRatio3;\n",
    "    elif(y==\"uniform\"):\n",
    "        return (d4.cdf(x)-distributionBase4)/distributionRatio4;\n",
    "    elif(y==\"independent1\"):\n",
    "        return d5[i].cdf(x);\n",
    "    elif(y==\"independent2\"):\n",
    "        return d6[i].cdf(x);\n",
    "    elif(y==\"cauchy\"):\n",
    "        return d7.cdf(x);\n",
    "    elif(y==\"beta\"):\n",
    "#         sum_cdf=0.0;\n",
    "#         if(x<0.0001):\n",
    "#             x=0.00011;\n",
    "#         if(x>0.9999):\n",
    "#             x=0.99989;\n",
    "#         for i in range(len(d9_pdf)):\n",
    "#             if(d9_sample[i]<x):\n",
    "#                 sum_cdf+=d9_pdf[i]*d9_delta;\n",
    "#             else:\n",
    "#                 sum_cdf+=(d9_pdf[i]+d9_pdf[i-1])/ 2 *(x-d9_sample[i-1])\n",
    "#                 break;\n",
    "#         return sum_cdf/d9_sum_pdf\n",
    "#         cdf_v=torch.sum((sample_d9<(x)), dtype=torch.float32)/100000\n",
    "#         return cdf_v\n",
    "#    F(x|a,b)=1–(1–x^a)^b\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(x,kumaraswamy_a),kumaraswamy_b);\n",
    "        except:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(torch.tensor(x,dtype=torch.float32),kumaraswamy_a),kumaraswamy_b);\n",
    "    elif(y==\"arcsine\"):\n",
    "        #\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            res=2.0/math.pi * torch.asin(torch.sqrt(x))\n",
    "            #print(x)\n",
    "            return res# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "        except:\n",
    "            return 2.0/math.pi * torch.asin(torch.sqrt(torch.tensor(x,dtype=torch.float32)))# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "    elif(y==\"U-exponential\"):\n",
    "        return (d81.cdf(x) + (1.0 - d82.cdf(1.0-x)))  / 2 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    if torch.sum(bits).item() == 0:\n",
    "        return torch.ones(n)\n",
    "    bits = bits.type(torch.float32)\n",
    "    negBits = torch.ones(n) - bits\n",
    "    payments = model(bits)\n",
    "    payments = payments - 1000 * negBits\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    payments = payments + negBits\n",
    "    return payments\n",
    "\n",
    "def tpToBits(tp, bits=torch.ones(n).type(torch.uint8)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = (tp >= payments).type(torch.uint8)\n",
    "    if torch.equal(newBits, bits):\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits(tp, newBits)#bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    return n - torch.sum(tpToBits(tp).type(torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    global  dp,decision\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money, yes]\n",
    "        offer = float(offerIndex) / dpPrecision\n",
    "        if tp[i] >= offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits[i] = 0\n",
    "            payments[i] = 1\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "#     print()\n",
    "#     print(tp)\n",
    "#     print(bits)\n",
    "#     print(payments)\n",
    "#     print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def heuristicSupervisionRule(tp):\n",
    "    global  dp_H,decision_H\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    tempold=-1;\n",
    "    for turns in range(n,0,-1):\n",
    "        money = dpPrecision\n",
    "        j=0\n",
    "        tempo=sum(bits)\n",
    "        #print(\"bits\",tempo)\n",
    "        for i in range(n):\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            \n",
    "            offerIndex = decision_H[tempo ,tempo - i, money]\n",
    "            offer = float(offerIndex) / dpPrecision\n",
    "            while(j<n):\n",
    "                if(bits[j]!=0):\n",
    "                    break;\n",
    "                j+=1;\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            if tp[j] >= offer:\n",
    "                #print(money,j,tp[j],offer)\n",
    "                money -= offerIndex\n",
    "                bits[j] = 1\n",
    "                payments[j] = offer\n",
    "            else:\n",
    "                bits[j] = 0;\n",
    "                payments[j] = 1.0;\n",
    "            j+=1;\n",
    "        #print(\"money\",money)\n",
    "        if(money==0 and tempold==tempo):\n",
    "            break;\n",
    "        tempold=tempo;\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    for k in range(n, -1, -1):\n",
    "        if k == 0:\n",
    "            break\n",
    "        bits = [1 if tp[ii] >= 1.0 / k else 0 for ii in range(n)]\n",
    "        if sum(bits) == k:\n",
    "            break\n",
    "    if k == 0:\n",
    "        payments = [1 for ii in range(n)]\n",
    "    else:\n",
    "        payments = [1.0 / k if bits[ii] == 1 else 1 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    return float(n - torch.sum(costSharingSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def dpDelay(tp):\n",
    "    return float(n - torch.sum(dpSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def heuristicDelay(tp):\n",
    "    return float(n - torch.sum(heuristicSupervisionRule(tp)[0]).item())\n",
    "\n",
    "\n",
    "# templire=0;\n",
    "# num=1\n",
    "# for i in range(num):\n",
    "#     temp=torch.tensor(samplesJoint[i])\n",
    "#     #print(temp)\n",
    "#     #print(temp);\n",
    "#     #print(heuristicSupervisionRule(temp))\n",
    "#     #print(dpSupervisionRule(temp))\n",
    "#     #print(dpDelay(temp))\n",
    "#     res=dpDelay(temp)\n",
    "#     templire+=res\n",
    "#     #print(\"Delay\",res)\n",
    "#     #print()\n",
    "# print(templire/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordAndReport(name, source, loss):\n",
    "    source.append(loss)\n",
    "    realLength = len(source)\n",
    "    #print(f\"{name} ({realLength}): {loss}\")\n",
    "    print(name,realLength,\":\" ,loss)\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    \n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda /100000\n",
    "        \n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % (log_interval*5) == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "losslist=[];\n",
    "losslistname=[];\n",
    "losslisttemp=[];\n",
    "def test_batch():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lenLoss= 0\n",
    "        nnLoss = 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "            for tp in tp_batch:\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        nnLoss/=lenLoss\n",
    "    return nnLoss\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                Delay1 = tpToTotalDelay(tp1)\n",
    "                Delay0 = tpToTotalDelay(tp0)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                    loss = loss + (1 - cdf(offer,order)) * Delay1 + cdf(offer,order) * Delay0\n",
    "                else:\n",
    "                    loss = loss + (1 - cdf(offer,order,i)) * Delay1 + cdf(offer,order,i) * Delay0\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "    \n",
    "allBits = [torch.tensor(bits, dtype=torch.int16) for bits in itertools.product([0, 1], repeat=n)]\n",
    "\n",
    "def test():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        costSharingLoss = 0\n",
    "        dpLoss = 0\n",
    "        nnLoss = 0\n",
    "        heuristicLoss=0\n",
    "        lenLoss= 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "                heuristicLoss+= heuristicDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        costSharingLoss /= lenLoss\n",
    "        dpLoss /= lenLoss\n",
    "        nnLoss /= lenLoss\n",
    "        heuristicLoss /= lenLoss\n",
    "        #print(lenLoss)\n",
    "        losslisttemp.append(nnLoss);\n",
    "        recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "        recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "        recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "        recordAndReport(\"heuristic\", runningLossHeuristic, heuristicLoss)\n",
    "        print(\"DP:\",dp[n, dpPrecision,0])\n",
    "        #for i in range(n, 0, -1):\n",
    "        #    print(\"Heuristic:\",i,5*(1-dp_H[i, i, dpPrecision]))\n",
    "        for i in range(n, 0, -1):\n",
    "            print(\n",
    "                    tpToPayments(\n",
    "                            torch.tensor([0 if ii >= i else 1 for ii in range(n)], dtype=torch.float32)\n",
    "                    )\n",
    "                )\n",
    "        \n",
    "dpPrecision = 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def producedata(order):\n",
    "    global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "    dpPrecision = 100 \n",
    "    if(order==\"twopeak\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = np.random.normal(\n",
    "            loc=doublePeakLowMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=doublePeakLowMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samples2 = np.random.normal(\n",
    "            loc=doublePeakHighMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = np.random.normal(\n",
    "                        loc=doublePeakHighMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "    elif(order==\"normal\"):\n",
    "        print(\"loc\",normalloc, \"scale\",normalscale)\n",
    "        samples1 = np.random.normal(\n",
    "            loc=normalloc, scale=normalscale, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=normalloc, scale=normalscale\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"uniform\"):  \n",
    "        print(\"uniformlow\",uniformlow, \"uniformhigh\",uniformhigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        uniformlow, uniformhigh\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent1\"):\n",
    "        print(\"loc\",independentnormalloc1,\"scale\",independentnormalscale1)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent2\"):\n",
    "        print(\"loc\",independentnormalloc2, \"scale\",independentnormalscale2)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"cauchy\"):\n",
    "        print(\"cauchyloc\",cauchyloc, \"cauchyscale\",cauchyscalen)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"beta\"):\n",
    "        print(\"beta_a\",beta_a, \"beta_b\",beta_b)\n",
    "        print(\"kumaraswamy_a\",kumaraswamy_a, \"kumaraswamy_b\",kumaraswamy_b)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"arcsine\"):\n",
    "        print(\"betalow\",0.5, \"betahigh\",0.5)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"U-exponential\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = d81.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d81.rsample(torch.Size([1])).numpy()\n",
    "                    \n",
    "        samples2 = d82.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        \n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = d82.rsample(torch.Size([1])).numpy()\n",
    "        samples2 = 1.0 - samples2\n",
    "        samplesJoint = signals * samples1 - (signals - 1.0) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "        \n",
    "    plt.hist(samplesJoint,bins=500)\n",
    "    plt.show()\n",
    "    \n",
    "    tp_dataset = TensorDataset(tp_tensor[: int(trainSize * percentage_train_test)])\n",
    "    tp_dataset_testing = TensorDataset(tp_tensor[int(trainSize * (1.0-percentage_train_test)) :])\n",
    "    tp_dataloader = DataLoader(tp_dataset, batch_size=128, shuffle=True)\n",
    "    tp_dataloader_testing = DataLoader(tp_dataset_testing, batch_size=256, shuffle=False)\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "    decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for yes in range(n + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp[0, 0, yes] = 0\n",
    "            else:\n",
    "                dp[0, money, yes] = yes# + 1.0\n",
    "    for ppl in range(1,  n + 1):\n",
    "        for yes in range(n + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                minSoFar = 1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1 - cdf(offer,order)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order) * (1 + dp[ppl - 1, money, yes])\n",
    "                    else:\n",
    "                        res = (1 - cdf(offer,order,n-ppl)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order,n-ppl) * (1 + dp[ppl - 1, money, yes])\n",
    "                    if minSoFar > res.item():\n",
    "                        minSoFar = res.item()\n",
    "                        decision[ppl, money, yes] = offerIndex\n",
    "                dp[ppl, money, yes] = minSoFar\n",
    "    \n",
    "    print(\"dp\",dp[n, dpPrecision, 0])\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp_H = np.zeros([n + 1 , n + 1, dpPrecision + 1])\n",
    "    decision_H = np.zeros([n + 1 , n + 1, dpPrecision + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for i in range(1,n+1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp_H[i, 0, 0] = 1\n",
    "            else:\n",
    "                offer = money / dpPrecision\n",
    "                dp_H[i, 0, money] = 0#cdf(offer)# + 1.0\n",
    "    for i in range(1,n+1):\n",
    "        for ppl in range(1, i + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                maxSoFar = -1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1-cdf(offer,order)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    else:\n",
    "                        res = (1-cdf(offer,order,ppl-1)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    if maxSoFar < res.item():\n",
    "                        maxSoFar = res.item()\n",
    "                        decision_H[i, ppl, money] = offerIndex\n",
    "                dp_H[i, ppl, money] = maxSoFar\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loc 0.2 scale 0.04\n",
      "loc 0.8 scale 0.04\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dp 1.1005079746246338\n",
      "Supervised Aim: twopeak dp\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 0.037090\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 0.000444\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 0.000008\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 0.000001\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 0.000001\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 0.000004\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 0.000000\n",
      "NN 1 : tensor(1.0944)\n",
      "CS 1 : 2.209\n",
      "DP 1 : 1.09804\n",
      "heuristic 1 : 1.12588\n",
      "DP: 1.1005079746246338\n",
      "tensor([0.6700, 0.0899, 0.0902, 0.0800, 0.0698])\n",
      "tensor([0.7230, 0.1033, 0.0931, 0.0807, 1.0000])\n",
      "tensor([0.7781, 0.1214, 0.1005, 1.0000, 1.0000])\n",
      "tensor([0.8572, 0.1428, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 1.200195 testing loss: tensor(1.0931)\n",
      "Train Epoch: 1 [640/25000 (3%)]\tLoss: 1.077244 testing loss: tensor(1.0923)\n",
      "Train Epoch: 1 [1280/25000 (5%)]\tLoss: 1.355007 testing loss: tensor(1.0922)\n",
      "Train Epoch: 1 [1920/25000 (8%)]\tLoss: 1.063038 testing loss: tensor(1.0917)\n",
      "Train Epoch: 1 [2560/25000 (10%)]\tLoss: 0.955181 testing loss: tensor(1.0913)\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 1.014282 testing loss: tensor(1.0912)\n",
      "Train Epoch: 1 [3840/25000 (15%)]\tLoss: 1.065057 testing loss: tensor(1.0913)\n",
      "Train Epoch: 1 [4480/25000 (18%)]\tLoss: 1.118208 testing loss: tensor(1.0906)\n",
      "Train Epoch: 1 [5120/25000 (20%)]\tLoss: 1.188813 testing loss: tensor(1.0914)\n",
      "Train Epoch: 1 [5760/25000 (23%)]\tLoss: 0.888820 testing loss: tensor(1.0910)\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 1.019768 testing loss: tensor(1.0907)\n",
      "Train Epoch: 1 [7040/25000 (28%)]\tLoss: 1.131041 testing loss: tensor(1.0906)\n",
      "Train Epoch: 1 [7680/25000 (31%)]\tLoss: 1.000371 testing loss: tensor(1.0900)\n",
      "Train Epoch: 1 [8320/25000 (33%)]\tLoss: 1.074166 testing loss: tensor(1.0913)\n",
      "Train Epoch: 1 [8960/25000 (36%)]\tLoss: 1.190292 testing loss: tensor(1.0902)\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 1.027911 testing loss: tensor(1.0907)\n",
      "Train Epoch: 1 [10240/25000 (41%)]\tLoss: 1.038065 testing loss: tensor(1.0919)\n",
      "Train Epoch: 1 [10880/25000 (43%)]\tLoss: 1.098324 testing loss: tensor(1.0908)\n",
      "Train Epoch: 1 [11520/25000 (46%)]\tLoss: 0.964394 testing loss: tensor(1.0905)\n",
      "Train Epoch: 1 [12160/25000 (48%)]\tLoss: 1.102462 testing loss: tensor(1.0906)\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 1.131453 testing loss: tensor(1.0908)\n",
      "Train Epoch: 1 [13440/25000 (54%)]\tLoss: 1.228992 testing loss: tensor(1.0903)\n",
      "Train Epoch: 1 [14080/25000 (56%)]\tLoss: 1.176210 testing loss: tensor(1.0906)\n",
      "Train Epoch: 1 [14720/25000 (59%)]\tLoss: 1.112925 testing loss: tensor(1.0904)\n",
      "Train Epoch: 1 [15360/25000 (61%)]\tLoss: 1.027668 testing loss: tensor(1.0902)\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 1.139789 testing loss: tensor(1.0905)\n",
      "Train Epoch: 1 [16640/25000 (66%)]\tLoss: 1.044623 testing loss: tensor(1.0901)\n",
      "Train Epoch: 1 [17280/25000 (69%)]\tLoss: 0.980518 testing loss: tensor(1.0906)\n",
      "Train Epoch: 1 [17920/25000 (71%)]\tLoss: 1.052541 testing loss: tensor(1.0898)\n",
      "Train Epoch: 1 [18560/25000 (74%)]\tLoss: 1.012109 testing loss: tensor(1.0894)\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 1.324678 testing loss: tensor(1.0907)\n",
      "Train Epoch: 1 [19840/25000 (79%)]\tLoss: 1.284613 testing loss: tensor(1.0900)\n",
      "Train Epoch: 1 [20480/25000 (82%)]\tLoss: 1.086905 testing loss: tensor(1.0905)\n",
      "Train Epoch: 1 [21120/25000 (84%)]\tLoss: 0.946926 testing loss: tensor(1.0898)\n",
      "Train Epoch: 1 [21760/25000 (87%)]\tLoss: 0.950314 testing loss: tensor(1.0899)\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 0.969232 testing loss: tensor(1.0898)\n",
      "Train Epoch: 1 [23040/25000 (92%)]\tLoss: 1.163042 testing loss: tensor(1.0905)\n",
      "Train Epoch: 1 [23680/25000 (94%)]\tLoss: 1.293658 testing loss: tensor(1.0902)\n",
      "Train Epoch: 1 [24320/25000 (97%)]\tLoss: 1.016915 testing loss: tensor(1.0904)\n",
      "Train Epoch: 1 [7800/25000 (99%)]\tLoss: 1.214834 testing loss: tensor(1.0906)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.0906)\n",
      "CS 2 : 2.209\n",
      "DP 2 : 1.09804\n",
      "heuristic 2 : 1.12588\n",
      "DP: 1.1005079746246338\n",
      "tensor([0.6628, 0.0830, 0.0857, 0.0843, 0.0842])\n",
      "tensor([0.7032, 0.0995, 0.0997, 0.0977, 1.0000])\n",
      "tensor([0.7728, 0.1149, 0.1123, 1.0000, 1.0000])\n",
      "tensor([0.8627, 0.1373, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: twopeak random initializing\n",
      "do nothing\n",
      "NN 1 : tensor(2.1430)\n",
      "CS 1 : 2.209\n",
      "DP 1 : 1.09804\n",
      "heuristic 1 : 1.12588\n",
      "DP: 1.1005079746246338\n",
      "tensor([0.2257, 0.0755, 0.1346, 0.2585, 0.3056])\n",
      "tensor([0.3215, 0.1123, 0.2069, 0.3592, 1.0000])\n",
      "tensor([0.4616, 0.2014, 0.3370, 1.0000, 1.0000])\n",
      "tensor([0.6946, 0.3054, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 2.175177 testing loss: tensor(2.0673)\n",
      "Train Epoch: 1 [640/25000 (3%)]\tLoss: 1.566362 testing loss: tensor(1.5568)\n",
      "Train Epoch: 1 [1280/25000 (5%)]\tLoss: 1.640507 testing loss: tensor(1.4653)\n",
      "Train Epoch: 1 [1920/25000 (8%)]\tLoss: 1.526057 testing loss: tensor(1.3479)\n",
      "Train Epoch: 1 [2560/25000 (10%)]\tLoss: 1.331325 testing loss: tensor(1.3180)\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 1.412199 testing loss: tensor(1.2955)\n",
      "Train Epoch: 1 [3840/25000 (15%)]\tLoss: 1.305753 testing loss: tensor(1.2898)\n",
      "Train Epoch: 1 [4480/25000 (18%)]\tLoss: 1.399934 testing loss: tensor(1.2724)\n",
      "Train Epoch: 1 [5120/25000 (20%)]\tLoss: 1.254944 testing loss: tensor(1.2736)\n",
      "Train Epoch: 1 [5760/25000 (23%)]\tLoss: 1.224714 testing loss: tensor(1.2647)\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 1.286376 testing loss: tensor(1.2613)\n",
      "Train Epoch: 1 [7040/25000 (28%)]\tLoss: 1.250929 testing loss: tensor(1.2584)\n",
      "Train Epoch: 1 [7680/25000 (31%)]\tLoss: 1.210066 testing loss: tensor(1.2554)\n",
      "Train Epoch: 1 [8320/25000 (33%)]\tLoss: 0.982158 testing loss: tensor(1.2548)\n",
      "Train Epoch: 1 [8960/25000 (36%)]\tLoss: 1.264151 testing loss: tensor(1.2535)\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 1.379558 testing loss: tensor(1.2522)\n",
      "Train Epoch: 1 [10240/25000 (41%)]\tLoss: 1.459908 testing loss: tensor(1.2522)\n",
      "Train Epoch: 1 [10880/25000 (43%)]\tLoss: 1.320283 testing loss: tensor(1.2503)\n",
      "Train Epoch: 1 [11520/25000 (46%)]\tLoss: 1.300982 testing loss: tensor(1.2494)\n",
      "Train Epoch: 1 [12160/25000 (48%)]\tLoss: 1.002533 testing loss: tensor(1.2482)\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 1.231381 testing loss: tensor(1.2473)\n",
      "Train Epoch: 1 [13440/25000 (54%)]\tLoss: 1.431537 testing loss: tensor(1.2466)\n",
      "Train Epoch: 1 [14080/25000 (56%)]\tLoss: 1.271996 testing loss: tensor(1.2467)\n",
      "Train Epoch: 1 [14720/25000 (59%)]\tLoss: 1.417039 testing loss: tensor(1.2466)\n",
      "Train Epoch: 1 [15360/25000 (61%)]\tLoss: 1.049659 testing loss: tensor(1.2462)\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 1.340224 testing loss: tensor(1.2463)\n",
      "Train Epoch: 1 [16640/25000 (66%)]\tLoss: 1.159685 testing loss: tensor(1.2468)\n",
      "Train Epoch: 1 [17280/25000 (69%)]\tLoss: 1.215198 testing loss: tensor(1.2457)\n",
      "Train Epoch: 1 [17920/25000 (71%)]\tLoss: 1.250634 testing loss: tensor(1.2458)\n",
      "Train Epoch: 1 [18560/25000 (74%)]\tLoss: 1.282648 testing loss: tensor(1.2459)\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 1.421081 testing loss: tensor(1.2456)\n",
      "Train Epoch: 1 [19840/25000 (79%)]\tLoss: 1.352266 testing loss: tensor(1.2463)\n",
      "Train Epoch: 1 [20480/25000 (82%)]\tLoss: 1.308133 testing loss: tensor(1.2457)\n",
      "Train Epoch: 1 [21120/25000 (84%)]\tLoss: 1.225346 testing loss: tensor(1.2460)\n",
      "Train Epoch: 1 [21760/25000 (87%)]\tLoss: 1.308692 testing loss: tensor(1.2458)\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 1.173669 testing loss: tensor(1.2450)\n",
      "Train Epoch: 1 [23040/25000 (92%)]\tLoss: 1.240266 testing loss: tensor(1.2452)\n",
      "Train Epoch: 1 [23680/25000 (94%)]\tLoss: 1.112926 testing loss: tensor(1.2460)\n",
      "Train Epoch: 1 [24320/25000 (97%)]\tLoss: 1.224950 testing loss: tensor(1.2458)\n",
      "Train Epoch: 1 [7800/25000 (99%)]\tLoss: 1.306031 testing loss: tensor(1.2461)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.2461)\n",
      "CS 2 : 2.209\n",
      "DP 2 : 1.09804\n",
      "heuristic 2 : 1.12588\n",
      "DP: 1.1005079746246338\n",
      "tensor([0.0898, 0.0880, 0.0635, 0.0894, 0.6692])\n",
      "tensor([0.4709, 0.0887, 0.0924, 0.3480, 1.0000])\n",
      "tensor([0.7182, 0.1379, 0.1439, 1.0000, 1.0000])\n",
      "tensor([0.7326, 0.2674, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: twopeak costsharing\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 0.002303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 0.000036\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 0.000001\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 0.000000\n",
      "NN 1 : tensor(2.2086)\n",
      "CS 1 : 2.209\n",
      "DP 1 : 1.09804\n",
      "heuristic 1 : 1.12588\n",
      "DP: 1.1005079746246338\n",
      "tensor([0.2001, 0.2000, 0.2000, 0.2000, 0.1999])\n",
      "tensor([0.2497, 0.2506, 0.2495, 0.2502, 1.0000])\n",
      "tensor([0.3339, 0.3336, 0.3326, 1.0000, 1.0000])\n",
      "tensor([0.5003, 0.4997, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 2.160499 testing loss: tensor(2.2092)\n",
      "Train Epoch: 1 [640/25000 (3%)]\tLoss: 2.157694 testing loss: tensor(2.2066)\n",
      "Train Epoch: 1 [1280/25000 (5%)]\tLoss: 2.292700 testing loss: tensor(2.2075)\n",
      "Train Epoch: 1 [1920/25000 (8%)]\tLoss: 2.126471 testing loss: tensor(2.2063)\n",
      "Train Epoch: 1 [2560/25000 (10%)]\tLoss: 2.404402 testing loss: tensor(2.2060)\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 2.165998 testing loss: tensor(2.2066)\n",
      "Train Epoch: 1 [3840/25000 (15%)]\tLoss: 2.241948 testing loss: tensor(2.2051)\n",
      "Train Epoch: 1 [4480/25000 (18%)]\tLoss: 2.213365 testing loss: tensor(2.2054)\n",
      "Train Epoch: 1 [5120/25000 (20%)]\tLoss: 2.187159 testing loss: tensor(2.2071)\n",
      "Train Epoch: 1 [5760/25000 (23%)]\tLoss: 2.006897 testing loss: tensor(2.2061)\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 2.246827 testing loss: tensor(2.2046)\n",
      "Train Epoch: 1 [7040/25000 (28%)]\tLoss: 1.929496 testing loss: tensor(2.2031)\n",
      "Train Epoch: 1 [7680/25000 (31%)]\tLoss: 2.226649 testing loss: tensor(2.2028)\n",
      "Train Epoch: 1 [8320/25000 (33%)]\tLoss: 2.156789 testing loss: tensor(2.2035)\n",
      "Train Epoch: 1 [8960/25000 (36%)]\tLoss: 2.311224 testing loss: tensor(2.2031)\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 2.402117 testing loss: tensor(2.2007)\n",
      "Train Epoch: 1 [10240/25000 (41%)]\tLoss: 2.314843 testing loss: tensor(2.1992)\n",
      "Train Epoch: 1 [10880/25000 (43%)]\tLoss: 2.037012 testing loss: tensor(2.2000)\n",
      "Train Epoch: 1 [11520/25000 (46%)]\tLoss: 2.081674 testing loss: tensor(2.1982)\n",
      "Train Epoch: 1 [12160/25000 (48%)]\tLoss: 1.990292 testing loss: tensor(2.1960)\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 2.098055 testing loss: tensor(2.1932)\n",
      "Train Epoch: 1 [13440/25000 (54%)]\tLoss: 2.470405 testing loss: tensor(2.1889)\n",
      "Train Epoch: 1 [14080/25000 (56%)]\tLoss: 2.131922 testing loss: tensor(2.1792)\n",
      "Train Epoch: 1 [14720/25000 (59%)]\tLoss: 2.105514 testing loss: tensor(2.1608)\n",
      "Train Epoch: 1 [15360/25000 (61%)]\tLoss: 2.097855 testing loss: tensor(2.1225)\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 2.155580 testing loss: tensor(2.0212)\n",
      "Train Epoch: 1 [16640/25000 (66%)]\tLoss: 1.940230 testing loss: tensor(1.7831)\n",
      "Train Epoch: 1 [17280/25000 (69%)]\tLoss: 1.541497 testing loss: tensor(1.6425)\n",
      "Train Epoch: 1 [17920/25000 (71%)]\tLoss: 1.602321 testing loss: tensor(1.5908)\n",
      "Train Epoch: 1 [18560/25000 (74%)]\tLoss: 1.453916 testing loss: tensor(1.5790)\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 1.587719 testing loss: tensor(1.5396)\n",
      "Train Epoch: 1 [19840/25000 (79%)]\tLoss: 1.855904 testing loss: tensor(1.5077)\n",
      "Train Epoch: 1 [20480/25000 (82%)]\tLoss: 1.613679 testing loss: tensor(1.4668)\n",
      "Train Epoch: 1 [21120/25000 (84%)]\tLoss: 1.246387 testing loss: tensor(1.4155)\n",
      "Train Epoch: 1 [21760/25000 (87%)]\tLoss: 1.187426 testing loss: tensor(1.3938)\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 1.306096 testing loss: tensor(1.3554)\n",
      "Train Epoch: 1 [23040/25000 (92%)]\tLoss: 1.258194 testing loss: tensor(1.3277)\n",
      "Train Epoch: 1 [23680/25000 (94%)]\tLoss: 1.347451 testing loss: tensor(1.3136)\n",
      "Train Epoch: 1 [24320/25000 (97%)]\tLoss: 1.205272 testing loss: tensor(1.3038)\n",
      "Train Epoch: 1 [7800/25000 (99%)]\tLoss: 1.337526 testing loss: tensor(1.2980)\n",
      "penalty: 0.016776438802480698\n",
      "NN 2 : tensor(1.2980)\n",
      "CS 2 : 2.209\n",
      "DP 2 : 1.09804\n",
      "heuristic 2 : 1.12588\n",
      "DP: 1.1005079746246338\n",
      "tensor([0.0472, 0.0464, 0.0449, 0.7068, 0.1548])\n",
      "tensor([0.0977, 0.0952, 0.0956, 0.7115, 1.0000])\n",
      "tensor([0.3318, 0.3288, 0.3395, 1.0000, 1.0000])\n",
      "tensor([0.4972, 0.5028, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: twopeak heuristic\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 0.072996\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 0.002351\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 0.000063\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 0.001056\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 0.000005\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 0.000013\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 0.000012\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 0.000145\n",
      "NN 1 : tensor(1.1059)\n",
      "CS 1 : 2.209\n",
      "DP 1 : 1.09804\n",
      "heuristic 1 : 1.12588\n",
      "DP: 1.1005079746246338\n",
      "tensor([0.0804, 0.0807, 0.6775, 0.0811, 0.0804])\n",
      "tensor([0.1131, 0.0862, 0.4726, 0.3281, 1.0000])\n",
      "tensor([0.1372, 0.1689, 0.6938, 1.0000, 1.0000])\n",
      "tensor([0.3821, 0.6179, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 1.175362 testing loss: tensor(1.1055)\n",
      "Train Epoch: 1 [640/25000 (3%)]\tLoss: 1.163487 testing loss: tensor(1.1085)\n",
      "Train Epoch: 1 [1280/25000 (5%)]\tLoss: 1.245269 testing loss: tensor(1.1133)\n",
      "Train Epoch: 1 [1920/25000 (8%)]\tLoss: 1.243291 testing loss: tensor(1.1163)\n",
      "Train Epoch: 1 [2560/25000 (10%)]\tLoss: 1.043035 testing loss: tensor(1.1176)\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 0.978454 testing loss: tensor(1.1148)\n",
      "Train Epoch: 1 [3840/25000 (15%)]\tLoss: 1.143911 testing loss: tensor(1.1121)\n",
      "Train Epoch: 1 [4480/25000 (18%)]\tLoss: 1.020517 testing loss: tensor(1.1101)\n",
      "Train Epoch: 1 [5120/25000 (20%)]\tLoss: 1.205740 testing loss: tensor(1.1087)\n",
      "Train Epoch: 1 [5760/25000 (23%)]\tLoss: 0.936289 testing loss: tensor(1.1083)\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 0.915398 testing loss: tensor(1.1090)\n",
      "Train Epoch: 1 [7040/25000 (28%)]\tLoss: 1.121257 testing loss: tensor(1.1077)\n",
      "Train Epoch: 1 [7680/25000 (31%)]\tLoss: 1.001958 testing loss: tensor(1.1057)\n",
      "Train Epoch: 1 [8320/25000 (33%)]\tLoss: 0.995020 testing loss: tensor(1.1043)\n",
      "Train Epoch: 1 [8960/25000 (36%)]\tLoss: 0.962618 testing loss: tensor(1.1044)\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 1.026321 testing loss: tensor(1.1043)\n",
      "Train Epoch: 1 [10240/25000 (41%)]\tLoss: 1.085962 testing loss: tensor(1.1047)\n",
      "Train Epoch: 1 [10880/25000 (43%)]\tLoss: 1.243058 testing loss: tensor(1.1033)\n",
      "Train Epoch: 1 [11520/25000 (46%)]\tLoss: 1.079530 testing loss: tensor(1.1038)\n",
      "Train Epoch: 1 [12160/25000 (48%)]\tLoss: 1.011010 testing loss: tensor(1.1037)\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 1.028396 testing loss: tensor(1.1039)\n",
      "Train Epoch: 1 [13440/25000 (54%)]\tLoss: 0.977351 testing loss: tensor(1.1046)\n",
      "Train Epoch: 1 [14080/25000 (56%)]\tLoss: 1.123498 testing loss: tensor(1.1032)\n",
      "Train Epoch: 1 [14720/25000 (59%)]\tLoss: 1.080797 testing loss: tensor(1.1044)\n",
      "Train Epoch: 1 [15360/25000 (61%)]\tLoss: 0.936738 testing loss: tensor(1.1036)\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 1.247970 testing loss: tensor(1.1041)\n",
      "Train Epoch: 1 [16640/25000 (66%)]\tLoss: 1.140736 testing loss: tensor(1.1030)\n",
      "Train Epoch: 1 [17280/25000 (69%)]\tLoss: 1.065449 testing loss: tensor(1.1032)\n",
      "Train Epoch: 1 [17920/25000 (71%)]\tLoss: 1.159387 testing loss: tensor(1.1032)\n",
      "Train Epoch: 1 [18560/25000 (74%)]\tLoss: 1.269616 testing loss: tensor(1.1032)\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 1.026547 testing loss: tensor(1.1032)\n",
      "Train Epoch: 1 [19840/25000 (79%)]\tLoss: 1.023166 testing loss: tensor(1.1026)\n",
      "Train Epoch: 1 [20480/25000 (82%)]\tLoss: 0.927661 testing loss: tensor(1.1030)\n",
      "Train Epoch: 1 [21120/25000 (84%)]\tLoss: 1.139681 testing loss: tensor(1.1036)\n",
      "Train Epoch: 1 [21760/25000 (87%)]\tLoss: 1.088586 testing loss: tensor(1.1040)\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 1.254016 testing loss: tensor(1.1027)\n",
      "Train Epoch: 1 [23040/25000 (92%)]\tLoss: 1.016613 testing loss: tensor(1.1044)\n",
      "Train Epoch: 1 [23680/25000 (94%)]\tLoss: 1.067864 testing loss: tensor(1.1033)\n",
      "Train Epoch: 1 [24320/25000 (97%)]\tLoss: 1.017293 testing loss: tensor(1.1024)\n",
      "Train Epoch: 1 [7800/25000 (99%)]\tLoss: 1.128124 testing loss: tensor(1.1044)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.1044)\n",
      "CS 2 : 2.209\n",
      "DP 2 : 1.09804\n",
      "heuristic 2 : 1.12588\n",
      "DP: 1.1005079746246338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0849, 0.0836, 0.6652, 0.0814, 0.0849])\n",
      "tensor([0.0942, 0.0966, 0.6733, 0.1358, 1.0000])\n",
      "tensor([0.1201, 0.1502, 0.7297, 1.0000, 1.0000])\n",
      "tensor([0.4098, 0.5902, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.1)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "    \n",
    "for order in stage:\n",
    "    producedata(order)\n",
    "    #running\n",
    "    for order1 in order1name:\n",
    "        print(\"Supervised Aim:\",order,order1)\n",
    "        \n",
    "        # for mapping binary to payments before softmax\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(n, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, n),\n",
    "        )\n",
    "        model.apply(init_weights)\n",
    "        # optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr1)\n",
    "        \n",
    "        runningLossNN = []\n",
    "        runningLossCS = []\n",
    "        runningLossDP = []\n",
    "        runningLossHeuristic = []\n",
    "        #model=torch.load(\"save/pytorchNN=5dp1\");\n",
    "        #model.eval()\n",
    "        ##order1name=[\"costsharing\",\"dp\",\"heuristic\",\"random initializing\"]\n",
    "        for epoch in range(1, supervisionEpochs + 1):\n",
    "#             print(\"distributionRatio\",distributionRatio)\n",
    "            if(order1==\"costsharing\"):\n",
    "                supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "            elif(order1==\"dp\"):\n",
    "                supervisionTrain(epoch, dpSupervisionRule)\n",
    "            elif(order1==\"heuristic\"):\n",
    "                supervisionTrain(epoch, heuristicSupervisionRule)\n",
    "            elif(order1==\"random initializing\"):\n",
    "                print(\"do nothing\");\n",
    "\n",
    "        test()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr2)\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(epoch)\n",
    "            test()\n",
    "        losslistname.append(order+\" \"+order1);\n",
    "        losslist.append(losslisttemp);\n",
    "        losslisttemp=[];\n",
    "        savepath=\"save/pytorchNN=5all-beta\"+order+str(order1)\n",
    "        torch.save(model, savepath);\n",
    "        print(\"end\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colorlist=[\"#D2691E\",'#4169E1',\"#9ACD32\",\"#B22222\",\"#FF00FF\",\"#708090\"]\n",
    "for i in range(len(losslist)):\n",
    "    plt.plot(losslist[i], '.-', color=colorlist[i], alpha=0.8, label=losslistname[i])\n",
    "\n",
    "# 显示标签，如果不加这句，即使加了label='一些数字'的参数，最终还是不会显示标签\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('Trianing Times')\n",
    "plt.ylabel('Delay')\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('pytorchNN=5_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
