{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from random import randint\n",
    "import random\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "from pylab import *\n",
    "n = 7\n",
    "epochs = 4\n",
    "supervisionEpochs = 3\n",
    "lr = 0.001\n",
    "log_interval = 20\n",
    "trainSize = 40000#100000\n",
    "percentage_train_test= 0.25\n",
    "penaltyLambda = 10\n",
    "doublePeakHighMean = 0.9\n",
    "doublePeakLowMean = 0.1\n",
    "doublePeakStd = 0.1\n",
    "uniformlow=0\n",
    "uniformhigh=1.0\n",
    "normalloc = 0.2\n",
    "normalscale = 0.1\n",
    "\n",
    "cauchyloc = 1/n\n",
    "cauchyscalen = 0.004\n",
    "\n",
    "independentnormalloc1=[(float(ii)+1)/(2*n+1) for ii in range(n,0,-1)]\n",
    "independentnormalscale1=[0.05 for ii in range(n)]\n",
    "\n",
    "independentnormalloc2=[(float(ii)+1)/(2*n+1) for ii in range(1,n+1,1)]\n",
    "independentnormalscale2=[0.05 for ii in range(n)]\n",
    "stage=[\"twopeak\"]\n",
    "# \"twopeak\",\"normal\",\"uniform\",\"independent1\",\"independent2\",\"cauchy\"\n",
    "order1name=[\"costsharing\",\"dp\",\"random initializing\"]\n",
    "# \"costsharing\",\"dp\",\"heuristic\",\"random initializing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "d3 = D.normal.Normal(normalloc, normalscale)\n",
    "d4 = D.uniform.Uniform(uniformlow,uniformhigh)\n",
    "\n",
    "d5 = [D.normal.Normal(independentnormalloc1[ii], independentnormalscale1[ii]) for ii in range(n)]\n",
    "d6 = [D.normal.Normal(independentnormalloc2[ii], independentnormalscale2[ii]) for ii in range(n)]\n",
    "\n",
    "d7 = D.cauchy.Cauchy(cauchyloc,cauchyscalen)\n",
    "def cdf(x,y, i=None):\n",
    "    if(y==\"twopeak\"):\n",
    "        return (d1.cdf(x) + d2.cdf(x) - distributionBase) / 2 / distributionRatio\n",
    "    elif(y==\"normal\"):\n",
    "        return d3.cdf(x);\n",
    "    elif(y==\"uniform\"):\n",
    "        return d4.cdf(x);\n",
    "    elif(y==\"independent1\"):\n",
    "        return d5[i].cdf(x);\n",
    "    elif(y==\"independent2\"):\n",
    "        return d6[i].cdf(x);\n",
    "    elif(y==\"cauchy\"):\n",
    "        return d7.cdf(x);\n",
    "#print(cdf(0.1,\"independent\",1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    if torch.sum(bits).item() == 0:\n",
    "        return torch.ones(n)\n",
    "    bits = bits.type(torch.float32)\n",
    "    negBits = torch.ones(n) - bits\n",
    "    payments = model(bits)\n",
    "    payments = payments - 1000 * negBits\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    payments = payments + negBits\n",
    "    return payments\n",
    "\n",
    "def tpToBits(tp, bits=torch.ones(n).type(torch.uint8)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = (tp >= payments).type(torch.uint8)\n",
    "    if torch.equal(newBits, bits):\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits(tp, newBits)#bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    return n - torch.sum(tpToBits(tp).type(torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    global  dp,decision\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money, yes]\n",
    "        offer = float(offerIndex) / dpPrecision\n",
    "        if tp[i] >= offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits[i] = 0\n",
    "            payments[i] = 1\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "#     print()\n",
    "#     print(tp)\n",
    "#     print(bits)\n",
    "#     print(payments)\n",
    "#     print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def heuristicSupervisionRule(tp):\n",
    "    global  dp_H,decision_H\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    tempold=-1;\n",
    "    for turns in range(n,0,-1):\n",
    "        money = dpPrecision\n",
    "        j=0\n",
    "        tempo=sum(bits)\n",
    "        #print(\"bits\",tempo)\n",
    "        for i in range(n):\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            \n",
    "            offerIndex = decision_H[tempo ,tempo - i, money]\n",
    "            offer = float(offerIndex) / dpPrecision\n",
    "            while(j<n):\n",
    "                if(bits[j]!=0):\n",
    "                    break;\n",
    "                j+=1;\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            if tp[j] >= offer:\n",
    "                #print(money,j,tp[j],offer)\n",
    "                money -= offerIndex\n",
    "                bits[j] = 1\n",
    "                payments[j] = offer\n",
    "            else:\n",
    "                bits[j] = 0;\n",
    "                payments[j] = 1.0;\n",
    "            j+=1;\n",
    "        #print(\"money\",money)\n",
    "        if(money==0 and tempold==tempo):\n",
    "            break;\n",
    "        tempold=tempo;\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    for k in range(n, -1, -1):\n",
    "        if k == 0:\n",
    "            break\n",
    "        bits = [1 if tp[ii] >= 1.0 / k else 0 for ii in range(n)]\n",
    "        if sum(bits) == k:\n",
    "            break\n",
    "    if k == 0:\n",
    "        payments = [1 for ii in range(n)]\n",
    "    else:\n",
    "        payments = [1.0 / k if bits[ii] == 1 else 1 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    return float(n - torch.sum(costSharingSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def dpDelay(tp):\n",
    "    return float(n - torch.sum(dpSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def heuristicDelay(tp):\n",
    "    return float(n - torch.sum(heuristicSupervisionRule(tp)[0]).item())\n",
    "\n",
    "\n",
    "# templire=0;\n",
    "# num=1\n",
    "# for i in range(num):\n",
    "#     temp=torch.tensor(samplesJoint[i])\n",
    "#     #print(temp)\n",
    "#     #print(temp);\n",
    "#     #print(heuristicSupervisionRule(temp))\n",
    "#     #print(dpSupervisionRule(temp))\n",
    "#     #print(dpDelay(temp))\n",
    "#     res=dpDelay(temp)\n",
    "#     templire+=res\n",
    "#     #print(\"delay\",res)\n",
    "#     #print()\n",
    "# print(templire/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordAndReport(name, source, loss):\n",
    "    source.append(loss)\n",
    "    realLength = len(source)\n",
    "    #print(f\"{name} ({realLength}): {loss}\")\n",
    "    print(name,realLength,\":\" ,loss)\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    \n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda /100000\n",
    "        \n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "losslist=[];\n",
    "losslistname=[];\n",
    "losslisttemp=[];\n",
    "def test_batch():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lenLoss= 0\n",
    "        nnLoss = 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "            for tp in tp_batch:\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        nnLoss/=lenLoss\n",
    "    return nnLoss\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                delay1 = tpToTotalDelay(tp1)\n",
    "                delay0 = tpToTotalDelay(tp0)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                    loss = loss + (1 - cdf(offer,order)) * delay1 + cdf(offer,order) * delay0\n",
    "                else:\n",
    "                    loss = loss + (1 - cdf(offer,order,i)) * delay1 + cdf(offer,order,i) * delay0\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "    \n",
    "allBits = [torch.tensor(bits, dtype=torch.int16) for bits in itertools.product([0, 1], repeat=n)]\n",
    "\n",
    "def test():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        costSharingLoss = 0\n",
    "        dpLoss = 0\n",
    "        nnLoss = 0\n",
    "        heuristicLoss=0\n",
    "        lenLoss= 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "                heuristicLoss+= heuristicDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        costSharingLoss /= lenLoss\n",
    "        dpLoss /= lenLoss\n",
    "        nnLoss /= lenLoss\n",
    "        heuristicLoss /= lenLoss\n",
    "        #print(lenLoss)\n",
    "        losslisttemp.append(nnLoss);\n",
    "        recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "        recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "        recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "        recordAndReport(\"heuristic\", runningLossHeuristic, heuristicLoss)\n",
    "        print(\"DP:\",dp[n, dpPrecision,0])\n",
    "        #for i in range(n, 0, -1):\n",
    "        #    print(\"Heuristic:\",i,5*(1-dp_H[i, i, dpPrecision]))\n",
    "        for i in range(n, 0, -1):\n",
    "            print(\n",
    "                    tpToPayments(\n",
    "                            torch.tensor([0 if ii >= i else 1 for ii in range(n)], dtype=torch.float32)\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "dpPrecision = 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def producedata(order):\n",
    "    global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "    dpPrecision = 100 \n",
    "    if(order==\"twopeak\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = np.random.normal(\n",
    "            loc=doublePeakLowMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=doublePeakLowMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samples2 = np.random.normal(\n",
    "            loc=doublePeakHighMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = np.random.normal(\n",
    "                        loc=doublePeakHighMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "    elif(order==\"normal\"):\n",
    "        print(\"loc\",normalloc, \"scale\",normalscale)\n",
    "        samples1 = np.random.normal(\n",
    "            loc=normalloc, scale=normalscale, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=normalloc, scale=normalscale\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"uniform\"):  \n",
    "        print(\"uniformlow\",uniformlow, \"uniformhigh\",uniformhigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        uniformlow, uniformhigh\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent1\"):\n",
    "        print(\"loc\",independentnormalloc1,\"scale\",independentnormalscale1)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent2\"):\n",
    "        print(\"loc\",independentnormalloc2, \"scale\",independentnormalscale2)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"cauchy\"):\n",
    "        print(\"cauchyloc\",cauchyloc, \"cauchyscale\",cauchyscalen)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        \n",
    "    tp_dataset = TensorDataset(tp_tensor[: int(trainSize * 0.3)])\n",
    "    tp_dataset_testing = TensorDataset(tp_tensor[int(trainSize * 0.7) :])\n",
    "    tp_dataloader = DataLoader(tp_dataset, batch_size=128, shuffle=True)\n",
    "    tp_dataloader_testing = DataLoader(tp_dataset_testing, batch_size=256, shuffle=False)\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "    decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for yes in range(n + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp[0, 0, yes] = 0\n",
    "            else:\n",
    "                dp[0, money, yes] = yes# + 1.0\n",
    "    for ppl in range(1,  n + 1):\n",
    "        for yes in range(n + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                minSoFar = 1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1 - cdf(offer,order)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order) * (1 + dp[ppl - 1, money, yes])\n",
    "                    else:\n",
    "                        res = (1 - cdf(offer,order,n-ppl)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order,n-ppl) * (1 + dp[ppl - 1, money, yes])\n",
    "                    if minSoFar > res.item():\n",
    "                        minSoFar = res.item()\n",
    "                        decision[ppl, money, yes] = offerIndex\n",
    "                dp[ppl, money, yes] = minSoFar\n",
    "    \n",
    "    print(\"dp\",dp[n, dpPrecision, 0])\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp_H = np.zeros([n + 1 , n + 1, dpPrecision + 1])\n",
    "    decision_H = np.zeros([n + 1 , n + 1, dpPrecision + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for i in range(1,n+1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp_H[i, 0, 0] = 1\n",
    "            else:\n",
    "                offer = money / dpPrecision\n",
    "                dp_H[i, 0, money] = 0#cdf(offer)# + 1.0\n",
    "    for i in range(1,n+1):\n",
    "        for ppl in range(1, i + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                maxSoFar = -1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1-cdf(offer,order)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    else:\n",
    "                        res = (1-cdf(offer,order,ppl-1)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    if maxSoFar < res.item():\n",
    "                        maxSoFar = res.item()\n",
    "                        decision_H[i, ppl, money] = offerIndex\n",
    "                dp_H[i, ppl, money] = maxSoFar\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loc 0.1 scale 0.1\n",
      "loc 0.9 scale 0.1\n",
      "dp 1.7726764678955078\n",
      "Supervised Aim: twopeak costsharing\n",
      "Train Epoch: 1 [0/12000 (0%)]\tLoss: 0.000900\n",
      "Train Epoch: 1 [2560/12000 (21%)]\tLoss: 0.000028\n",
      "Train Epoch: 1 [5120/12000 (43%)]\tLoss: 0.000004\n",
      "Train Epoch: 1 [7680/12000 (64%)]\tLoss: 0.000003\n",
      "Train Epoch: 1 [10240/12000 (85%)]\tLoss: 0.000001\n",
      "Train Epoch: 2 [0/12000 (0%)]\tLoss: 0.000001\n",
      "Train Epoch: 2 [2560/12000 (21%)]\tLoss: 0.000001\n",
      "Train Epoch: 2 [5120/12000 (43%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [7680/12000 (64%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [10240/12000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [0/12000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [2560/12000 (21%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [5120/12000 (43%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [7680/12000 (64%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [10240/12000 (85%)]\tLoss: 0.000000\n",
      "NN 1 : tensor(2.9447)\n",
      "CS 1 : 2.9445\n",
      "DP 1 : 1.7524166666666667\n",
      "heuristic 1 : 1.7480833333333334\n",
      "DP: 1.7726764678955078\n",
      "tensor([0.1429, 0.1428, 0.1428, 0.1428, 0.1431, 0.1429, 0.1427])\n",
      "tensor([0.1665, 0.1667, 0.1668, 0.1669, 0.1664, 0.1667, 1.0000])\n",
      "tensor([0.2003, 0.1997, 0.1994, 0.2007, 0.1999, 1.0000, 1.0000])\n",
      "tensor([0.2502, 0.2499, 0.2502, 0.2497, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.3333, 0.3333, 0.3334, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.5001, 0.4999, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/12000 (0%)]\tLoss: 2.942070 testing loss: tensor(2.9459)\n",
      "Train Epoch: 1 [2560/12000 (21%)]\tLoss: 2.006501 testing loss: tensor(2.0899)\n",
      "Train Epoch: 1 [5120/12000 (43%)]\tLoss: 1.800601 testing loss: tensor(1.9683)\n",
      "Train Epoch: 1 [7680/12000 (64%)]\tLoss: 2.074739 testing loss: tensor(1.9048)\n",
      "Train Epoch: 1 [10240/12000 (85%)]\tLoss: 1.815928 testing loss: tensor(1.8792)\n",
      "penalty: 0.03214705362915993\n",
      "NN 2 : tensor(1.8415)\n",
      "CS 2 : 2.9445\n",
      "DP 2 : 1.7524166666666667\n",
      "heuristic 2 : 1.7480833333333334\n",
      "DP: 1.7726764678955078\n",
      "tensor([0.0080, 0.2174, 0.0111, 0.0124, 0.7322, 0.0101, 0.0087])\n",
      "tensor([0.0100, 0.2186, 0.0153, 0.0159, 0.7277, 0.0124, 1.0000])\n",
      "tensor([0.0124, 0.2215, 0.0177, 0.0179, 0.7305, 1.0000, 1.0000])\n",
      "tensor([0.0636, 0.7411, 0.0918, 0.1035, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.0974, 0.7833, 0.1193, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.1424, 0.8576, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 2 [0/12000 (0%)]\tLoss: 1.927671 testing loss: tensor(1.8487)\n",
      "Train Epoch: 2 [2560/12000 (21%)]\tLoss: 1.567392 testing loss: tensor(1.7931)\n",
      "Train Epoch: 2 [5120/12000 (43%)]\tLoss: 1.739664 testing loss: tensor(1.7876)\n",
      "Train Epoch: 2 [7680/12000 (64%)]\tLoss: 1.511711 testing loss: tensor(1.7759)\n",
      "Train Epoch: 2 [10240/12000 (85%)]\tLoss: 1.735511 testing loss: tensor(1.7557)\n",
      "penalty: 0.006579814478754997\n",
      "NN 3 : tensor(1.7451)\n",
      "CS 3 : 2.9445\n",
      "DP 3 : 1.7524166666666667\n",
      "heuristic 3 : 1.7480833333333334\n",
      "DP: 1.7726764678955078\n",
      "tensor([0.0313, 0.1379, 0.0336, 0.0266, 0.7288, 0.0268, 0.0150])\n",
      "tensor([0.0309, 0.1470, 0.0367, 0.0289, 0.7304, 0.0262, 1.0000])\n",
      "tensor([0.0362, 0.1582, 0.0390, 0.0310, 0.7356, 1.0000, 1.0000])\n",
      "tensor([0.0830, 0.7375, 0.0860, 0.0935, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.1203, 0.7625, 0.1172, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.1982, 0.8018, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 3 [0/12000 (0%)]\tLoss: 1.629579 testing loss: tensor(1.7479)\n",
      "Train Epoch: 3 [2560/12000 (21%)]\tLoss: 1.715163 testing loss: tensor(1.7215)\n",
      "Train Epoch: 3 [5120/12000 (43%)]\tLoss: 1.562628 testing loss: tensor(1.7142)\n",
      "Train Epoch: 3 [7680/12000 (64%)]\tLoss: 1.581540 testing loss: tensor(1.7021)\n",
      "Train Epoch: 3 [10240/12000 (85%)]\tLoss: 1.736541 testing loss: tensor(1.6992)\n",
      "penalty: 0.004496127367019653\n",
      "NN 4 : tensor(1.7046)\n",
      "CS 4 : 2.9445\n",
      "DP 4 : 1.7524166666666667\n",
      "heuristic 4 : 1.7480833333333334\n",
      "DP: 1.7726764678955078\n",
      "tensor([0.0488, 0.0520, 0.0508, 0.0481, 0.7165, 0.0440, 0.0398])\n",
      "tensor([0.0528, 0.0656, 0.0563, 0.0506, 0.7281, 0.0467, 1.0000])\n",
      "tensor([0.0609, 0.0820, 0.0608, 0.0548, 0.7415, 1.0000, 1.0000])\n",
      "tensor([0.0823, 0.7586, 0.0787, 0.0803, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.1068, 0.7946, 0.0987, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.1854, 0.8146, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 4 [0/12000 (0%)]\tLoss: 1.580041 testing loss: tensor(1.7053)\n",
      "Train Epoch: 4 [2560/12000 (21%)]\tLoss: 1.773010 testing loss: tensor(1.7036)\n",
      "Train Epoch: 4 [5120/12000 (43%)]\tLoss: 1.775272 testing loss: tensor(1.7072)\n",
      "Train Epoch: 4 [7680/12000 (64%)]\tLoss: 1.729325 testing loss: tensor(1.7063)\n",
      "Train Epoch: 4 [10240/12000 (85%)]\tLoss: 1.615493 testing loss: tensor(1.7020)\n",
      "penalty: 0.004881322383880615\n",
      "NN 5 : tensor(1.7042)\n",
      "CS 5 : 2.9445\n",
      "DP 5 : 1.7524166666666667\n",
      "heuristic 5 : 1.7480833333333334\n",
      "DP: 1.7726764678955078\n",
      "tensor([0.0505, 0.0517, 0.0474, 0.0473, 0.7113, 0.0460, 0.0458])\n",
      "tensor([0.0589, 0.0672, 0.0546, 0.0522, 0.7155, 0.0517, 1.0000])\n",
      "tensor([0.0689, 0.0833, 0.0605, 0.0582, 0.7291, 1.0000, 1.0000])\n",
      "tensor([0.0905, 0.7448, 0.0770, 0.0876, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.1131, 0.7873, 0.0996, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.1918, 0.8082, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: twopeak dp\n",
      "Train Epoch: 1 [0/12000 (0%)]\tLoss: 0.052840\n",
      "Train Epoch: 1 [2560/12000 (21%)]\tLoss: 0.003684\n",
      "Train Epoch: 1 [5120/12000 (43%)]\tLoss: 0.002637\n",
      "Train Epoch: 1 [7680/12000 (64%)]\tLoss: 0.001787\n",
      "Train Epoch: 1 [10240/12000 (85%)]\tLoss: 0.000730\n",
      "Train Epoch: 2 [0/12000 (0%)]\tLoss: 0.000632\n",
      "Train Epoch: 2 [2560/12000 (21%)]\tLoss: 0.000306\n",
      "Train Epoch: 2 [5120/12000 (43%)]\tLoss: 0.000199\n",
      "Train Epoch: 2 [7680/12000 (64%)]\tLoss: 0.000136\n",
      "Train Epoch: 2 [10240/12000 (85%)]\tLoss: 0.000102\n",
      "Train Epoch: 3 [0/12000 (0%)]\tLoss: 0.000076\n",
      "Train Epoch: 3 [2560/12000 (21%)]\tLoss: 0.000066\n",
      "Train Epoch: 3 [5120/12000 (43%)]\tLoss: 0.000030\n",
      "Train Epoch: 3 [7680/12000 (64%)]\tLoss: 0.000023\n",
      "Train Epoch: 3 [10240/12000 (85%)]\tLoss: 0.000023\n",
      "NN 1 : tensor(1.7268)\n",
      "CS 1 : 2.9445\n",
      "DP 1 : 1.7524166666666667\n",
      "heuristic 1 : 1.7480833333333334\n",
      "DP: 1.7726764678955078\n",
      "tensor([0.7404, 0.0692, 0.0587, 0.0603, 0.0484, 0.0210, 0.0020])\n",
      "tensor([0.7329, 0.0859, 0.0530, 0.0664, 0.0440, 0.0178, 1.0000])\n",
      "tensor([0.7335, 0.0900, 0.0592, 0.0665, 0.0507, 1.0000, 1.0000])\n",
      "tensor([0.7568, 0.0956, 0.0756, 0.0720, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.7859, 0.1172, 0.0970, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.8605, 0.1395, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/12000 (0%)]\tLoss: 1.707264 testing loss: tensor(1.7479)\n",
      "Train Epoch: 1 [2560/12000 (21%)]\tLoss: 1.772594 testing loss: tensor(1.7247)\n",
      "Train Epoch: 1 [5120/12000 (43%)]\tLoss: 1.652419 testing loss: tensor(1.6894)\n",
      "Train Epoch: 1 [7680/12000 (64%)]\tLoss: 1.571697 testing loss: tensor(1.6883)\n",
      "Train Epoch: 1 [10240/12000 (85%)]\tLoss: 1.645603 testing loss: tensor(1.6794)\n",
      "penalty: 0.0007560253143310547\n",
      "NN 2 : tensor(1.6874)\n",
      "CS 2 : 2.9445\n",
      "DP 2 : 1.7524166666666667\n",
      "heuristic 2 : 1.7480833333333334\n",
      "DP: 1.7726764678955078\n",
      "tensor([0.7098, 0.0516, 0.0491, 0.0455, 0.0498, 0.0453, 0.0489])\n",
      "tensor([0.7131, 0.0592, 0.0564, 0.0553, 0.0598, 0.0561, 1.0000])\n",
      "tensor([0.7361, 0.0680, 0.0661, 0.0646, 0.0652, 1.0000, 1.0000])\n",
      "tensor([0.7582, 0.0841, 0.0770, 0.0807, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.8022, 0.1055, 0.0923, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.8673, 0.1327, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 2 [0/12000 (0%)]\tLoss: 1.816325 testing loss: tensor(1.6888)\n",
      "Train Epoch: 2 [2560/12000 (21%)]\tLoss: 1.768580 testing loss: tensor(1.6807)\n",
      "Train Epoch: 2 [5120/12000 (43%)]\tLoss: 1.723467 testing loss: tensor(1.6796)\n",
      "Train Epoch: 2 [7680/12000 (64%)]\tLoss: 1.727950 testing loss: tensor(1.6796)\n",
      "Train Epoch: 2 [10240/12000 (85%)]\tLoss: 1.525655 testing loss: tensor(1.6742)\n",
      "penalty: 0.0\n",
      "NN 3 : tensor(1.6784)\n",
      "CS 3 : 2.9445\n",
      "DP 3 : 1.7524166666666667\n",
      "heuristic 3 : 1.7480833333333334\n",
      "DP: 1.7726764678955078\n",
      "tensor([0.7164, 0.0491, 0.0459, 0.0479, 0.0474, 0.0476, 0.0458])\n",
      "tensor([0.7241, 0.0570, 0.0508, 0.0573, 0.0549, 0.0560, 1.0000])\n",
      "tensor([0.7394, 0.0694, 0.0608, 0.0692, 0.0612, 1.0000, 1.0000])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7544, 0.0861, 0.0726, 0.0870, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.8059, 0.1087, 0.0854, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.8575, 0.1425, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 3 [0/12000 (0%)]\tLoss: 1.705477 testing loss: tensor(1.6788)\n",
      "Train Epoch: 3 [2560/12000 (21%)]\tLoss: 1.638121 testing loss: tensor(1.6738)\n",
      "Train Epoch: 3 [5120/12000 (43%)]\tLoss: 1.635375 testing loss: tensor(1.6801)\n",
      "Train Epoch: 3 [7680/12000 (64%)]\tLoss: 1.697415 testing loss: tensor(1.6753)\n",
      "Train Epoch: 3 [10240/12000 (85%)]\tLoss: 1.755456 testing loss: tensor(1.6783)\n",
      "penalty: 0.012192800641059875\n",
      "NN 4 : tensor(1.6798)\n",
      "CS 4 : 2.9445\n",
      "DP 4 : 1.7524166666666667\n",
      "heuristic 4 : 1.7480833333333334\n",
      "DP: 1.7726764678955078\n",
      "tensor([0.7151, 0.0451, 0.0486, 0.0446, 0.0518, 0.0461, 0.0487])\n",
      "tensor([0.7250, 0.0545, 0.0546, 0.0529, 0.0592, 0.0538, 1.0000])\n",
      "tensor([0.7341, 0.0668, 0.0670, 0.0660, 0.0662, 1.0000, 1.0000])\n",
      "tensor([0.7533, 0.0830, 0.0782, 0.0855, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.8044, 0.1037, 0.0919, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.8575, 0.1425, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 4 [0/12000 (0%)]\tLoss: 1.673447 testing loss: tensor(1.6791)\n",
      "Train Epoch: 4 [2560/12000 (21%)]\tLoss: 1.834212 testing loss: tensor(1.6712)\n",
      "Train Epoch: 4 [5120/12000 (43%)]\tLoss: 1.675262 testing loss: tensor(1.6750)\n",
      "Train Epoch: 4 [7680/12000 (64%)]\tLoss: 1.630660 testing loss: tensor(1.6813)\n",
      "Train Epoch: 4 [10240/12000 (85%)]\tLoss: 1.736055 testing loss: tensor(1.6776)\n",
      "penalty: 0.0\n",
      "NN 5 : tensor(1.6745)\n",
      "CS 5 : 2.9445\n",
      "DP 5 : 1.7524166666666667\n",
      "heuristic 5 : 1.7480833333333334\n",
      "DP: 1.7726764678955078\n",
      "tensor([0.7164, 0.0452, 0.0471, 0.0482, 0.0453, 0.0472, 0.0505])\n",
      "tensor([0.7259, 0.0566, 0.0541, 0.0565, 0.0516, 0.0553, 1.0000])\n",
      "tensor([0.7342, 0.0688, 0.0663, 0.0706, 0.0601, 1.0000, 1.0000])\n",
      "tensor([0.7508, 0.0819, 0.0783, 0.0891, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.8059, 0.1031, 0.0910, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.8575, 0.1425, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: twopeak random initializing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "NN 1 : tensor(2.9207)\n",
      "CS 1 : 2.9445\n",
      "DP 1 : 1.7524166666666667\n",
      "heuristic 1 : 1.7480833333333334\n",
      "DP: 1.7726764678955078\n",
      "tensor([0.1106, 0.1756, 0.1316, 0.1964, 0.1297, 0.1380, 0.1182])\n",
      "tensor([0.1118, 0.1962, 0.1527, 0.2407, 0.1515, 0.1470, 1.0000])\n",
      "tensor([0.1380, 0.2315, 0.1946, 0.2552, 0.1807, 1.0000, 1.0000])\n",
      "tensor([0.1744, 0.2854, 0.2419, 0.2983, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.2607, 0.3989, 0.3405, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.4127, 0.5873, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/12000 (0%)]\tLoss: 2.872844 testing loss: tensor(2.8937)\n",
      "Train Epoch: 1 [2560/12000 (21%)]\tLoss: 2.169047 testing loss: tensor(2.0733)\n",
      "Train Epoch: 1 [5120/12000 (43%)]\tLoss: 1.940668 testing loss: tensor(1.9751)\n",
      "Train Epoch: 1 [7680/12000 (64%)]\tLoss: 1.756428 testing loss: tensor(1.9069)\n",
      "Train Epoch: 1 [10240/12000 (85%)]\tLoss: 1.937183 testing loss: tensor(1.8771)\n",
      "penalty: 0.03611535206437111\n",
      "NN 2 : tensor(1.8563)\n",
      "CS 2 : 2.9445\n",
      "DP 2 : 1.7524166666666667\n",
      "heuristic 2 : 1.7480833333333334\n",
      "DP: 1.7726764678955078\n",
      "tensor([0.0127, 0.4266, 0.0130, 0.5089, 0.0091, 0.0162, 0.0134])\n",
      "tensor([0.0149, 0.4292, 0.0154, 0.5091, 0.0126, 0.0187, 1.0000])\n",
      "tensor([0.0187, 0.4330, 0.0219, 0.5095, 0.0170, 1.0000, 1.0000])\n",
      "tensor([0.0210, 0.4394, 0.0271, 0.5124, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.0814, 0.8164, 0.1022, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.1354, 0.8646, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 2 [0/12000 (0%)]\tLoss: 1.784117 testing loss: tensor(1.8533)\n",
      "Train Epoch: 2 [2560/12000 (21%)]\tLoss: 2.034093 testing loss: tensor(1.8342)\n",
      "Train Epoch: 2 [5120/12000 (43%)]\tLoss: 1.625528 testing loss: tensor(1.8203)\n",
      "Train Epoch: 2 [7680/12000 (64%)]\tLoss: 1.882287 testing loss: tensor(1.7938)\n",
      "Train Epoch: 2 [10240/12000 (85%)]\tLoss: 1.804684 testing loss: tensor(1.7912)\n",
      "penalty: 0.022130591794848442\n",
      "NN 3 : tensor(1.7754)\n",
      "CS 3 : 2.9445\n",
      "DP 3 : 1.7524166666666667\n",
      "heuristic 3 : 1.7480833333333334\n",
      "DP: 1.7726764678955078\n",
      "tensor([0.0123, 0.2026, 0.0129, 0.7430, 0.0054, 0.0113, 0.0126])\n",
      "tensor([0.0154, 0.2038, 0.0158, 0.7431, 0.0080, 0.0139, 1.0000])\n",
      "tensor([0.0161, 0.2068, 0.0189, 0.7483, 0.0100, 1.0000, 1.0000])\n",
      "tensor([0.0172, 0.2093, 0.0208, 0.7528, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.0930, 0.7791, 0.1279, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.1858, 0.8142, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 3 [0/12000 (0%)]\tLoss: 1.867664 testing loss: tensor(1.7767)\n",
      "Train Epoch: 3 [2560/12000 (21%)]\tLoss: 1.767830 testing loss: tensor(1.7708)\n",
      "Train Epoch: 3 [5120/12000 (43%)]\tLoss: 1.579561 testing loss: tensor(1.7499)\n",
      "Train Epoch: 3 [7680/12000 (64%)]\tLoss: 1.702834 testing loss: tensor(1.7144)\n",
      "Train Epoch: 3 [10240/12000 (85%)]\tLoss: 1.606855 testing loss: tensor(1.7042)\n",
      "penalty: 0.0011660940945148468\n",
      "NN 4 : tensor(1.6953)\n",
      "CS 4 : 2.9445\n",
      "DP 4 : 1.7524166666666667\n",
      "heuristic 4 : 1.7480833333333334\n",
      "DP: 1.7726764678955078\n",
      "tensor([0.0483, 0.0576, 0.0450, 0.7379, 0.0160, 0.0468, 0.0484])\n",
      "tensor([0.0547, 0.0712, 0.0528, 0.7451, 0.0217, 0.0546, 1.0000])\n",
      "tensor([0.0635, 0.0922, 0.0598, 0.7572, 0.0273, 1.0000, 1.0000])\n",
      "tensor([0.0697, 0.0965, 0.0707, 0.7631, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.1022, 0.7689, 0.1289, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.1934, 0.8066, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 4 [0/12000 (0%)]\tLoss: 1.624623 testing loss: tensor(1.6957)\n",
      "Train Epoch: 4 [2560/12000 (21%)]\tLoss: 1.638726 testing loss: tensor(1.6936)\n",
      "Train Epoch: 4 [5120/12000 (43%)]\tLoss: 1.669637 testing loss: tensor(1.6951)\n",
      "Train Epoch: 4 [7680/12000 (64%)]\tLoss: 1.671161 testing loss: tensor(1.6892)\n",
      "Train Epoch: 4 [10240/12000 (85%)]\tLoss: 1.528256 testing loss: tensor(1.6908)\n",
      "penalty: 4.470348358154297e-05\n",
      "NN 5 : tensor(1.6932)\n",
      "CS 5 : 2.9445\n",
      "DP 5 : 1.7524166666666667\n",
      "heuristic 5 : 1.7480833333333334\n",
      "DP: 1.7726764678955078\n",
      "tensor([0.0472, 0.0449, 0.0447, 0.7316, 0.0387, 0.0457, 0.0472])\n",
      "tensor([0.0546, 0.0564, 0.0536, 0.7314, 0.0496, 0.0544, 1.0000])\n",
      "tensor([0.0694, 0.0742, 0.0589, 0.7412, 0.0564, 1.0000, 1.0000])\n",
      "tensor([0.0856, 0.0826, 0.0758, 0.7560, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.1192, 0.7704, 0.1104, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.1990, 0.8010, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "    \n",
    "for order in stage:\n",
    "    producedata(order)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "    #running\n",
    "    for order1 in order1name:\n",
    "        print(\"Supervised Aim:\",order,order1)\n",
    "        \n",
    "        # for mapping binary to payments before softmax\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(n, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, n),\n",
    "        )\n",
    "        model.apply(init_weights)\n",
    "        # optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        runningLossNN = []\n",
    "        runningLossCS = []\n",
    "        runningLossDP = []\n",
    "        runningLossHeuristic = []\n",
    "        #model=torch.load(\"save/pytorchNN=5dp1\");\n",
    "        #model.eval()\n",
    "        ##order1name=[\"costsharing\",\"dp\",\"heuristic\",\"random initializing\"]\n",
    "        for epoch in range(1, supervisionEpochs + 1):\n",
    "#             print(\"distributionRatio\",distributionRatio)\n",
    "            if(order1==\"costsharing\"):\n",
    "                supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "            elif(order1==\"dp\"):\n",
    "                supervisionTrain(epoch, dpSupervisionRule)\n",
    "            elif(order1==\"heuristic\"):\n",
    "                supervisionTrain(epoch, heuristicSupervisionRule)\n",
    "            elif(order1==\"random initializing\"):\n",
    "                print(\"do nothing\");\n",
    "\n",
    "        test()\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(epoch)\n",
    "            test()\n",
    "        losslistname.append(order+\" \"+order1);\n",
    "        losslist.append(losslisttemp);\n",
    "        losslisttemp=[];\n",
    "        savepath=\"save/pytorchNN=5all-phoenix\"+order+str(order1)\n",
    "        torch.save(model, savepath);\n",
    "        print(\"end\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colorlist=[\"#D2691E\",'#4169E1',\"#9ACD32\",\"#B22222\",\"#FF00FF\",\"#708090\"]\n",
    "for i in range(len(losslist)):\n",
    "    plt.plot(losslist[i], 'ro-', color=colorlist[i], alpha=0.8, label=losslistname[i])\n",
    "\n",
    "# 显示标签，如果不加这句，即使加了label='一些数字'的参数，最终还是不会显示标签\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel('trianing times')\n",
    "plt.ylabel('delay')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
