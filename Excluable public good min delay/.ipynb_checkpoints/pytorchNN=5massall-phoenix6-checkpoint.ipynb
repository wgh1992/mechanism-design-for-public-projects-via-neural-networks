{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from random import randint\n",
    "import random\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "from pylab import *\n",
    "n = 7\n",
    "epochs = 4\n",
    "supervisionEpochs = 3\n",
    "lr = 0.001\n",
    "log_interval = 20\n",
    "trainSize = 40000#100000\n",
    "percentage_train_test= 0.25\n",
    "penaltyLambda = 10\n",
    "doublePeakHighMean = 0.9\n",
    "doublePeakLowMean = 0.1\n",
    "doublePeakStd = 0.1\n",
    "uniformlow=0\n",
    "uniformhigh=1.0\n",
    "normalloc = 0.2\n",
    "normalscale = 0.1\n",
    "\n",
    "cauchyloc = 1/n\n",
    "cauchyscalen = 0.004\n",
    "\n",
    "independentnormalloc1=[(float(ii)+1)/(2*n+1) for ii in range(n,0,-1)]\n",
    "independentnormalscale1=[0.05 for ii in range(n)]\n",
    "\n",
    "independentnormalloc2=[(float(ii)+1)/(2*n+1) for ii in range(1,n+1,1)]\n",
    "independentnormalscale2=[0.05 for ii in range(n)]\n",
    "stage=[\"twopeak\"]\n",
    "# \"twopeak\",\"normal\",\"uniform\",\"independent1\",\"independent2\",\"cauchy\"\n",
    "order1name=[\"costsharing\",\"dp\",\"random initializing\"]\n",
    "# \"costsharing\",\"dp\",\"heuristic\",\"random initializing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "d3 = D.normal.Normal(normalloc, normalscale)\n",
    "d4 = D.uniform.Uniform(uniformlow,uniformhigh)\n",
    "\n",
    "d5 = [D.normal.Normal(independentnormalloc1[ii], independentnormalscale1[ii]) for ii in range(n)]\n",
    "d6 = [D.normal.Normal(independentnormalloc2[ii], independentnormalscale2[ii]) for ii in range(n)]\n",
    "\n",
    "d7 = D.cauchy.Cauchy(cauchyloc,cauchyscalen)\n",
    "def cdf(x,y, i=None):\n",
    "    if(y==\"twopeak\"):\n",
    "        return (d1.cdf(x) + d2.cdf(x) - distributionBase) / 2 / distributionRatio\n",
    "    elif(y==\"normal\"):\n",
    "        return d3.cdf(x);\n",
    "    elif(y==\"uniform\"):\n",
    "        return d4.cdf(x);\n",
    "    elif(y==\"independent1\"):\n",
    "        return d5[i].cdf(x);\n",
    "    elif(y==\"independent2\"):\n",
    "        return d6[i].cdf(x);\n",
    "    elif(y==\"cauchy\"):\n",
    "        return d7.cdf(x);\n",
    "#print(cdf(0.1,\"independent\",1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    if torch.sum(bits).item() == 0:\n",
    "        return torch.ones(n)\n",
    "    bits = bits.type(torch.float32)\n",
    "    negBits = torch.ones(n) - bits\n",
    "    payments = model(bits)\n",
    "    payments = payments - 1000 * negBits\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    payments = payments + negBits\n",
    "    return payments\n",
    "\n",
    "def tpToBits(tp, bits=torch.ones(n).type(torch.uint8)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = (tp >= payments).type(torch.uint8)\n",
    "    if torch.equal(newBits, bits):\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits(tp, newBits)#bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    return n - torch.sum(tpToBits(tp).type(torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    global  dp,decision\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money, yes]\n",
    "        offer = float(offerIndex) / dpPrecision\n",
    "        if tp[i] >= offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits[i] = 0\n",
    "            payments[i] = 1\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "#     print()\n",
    "#     print(tp)\n",
    "#     print(bits)\n",
    "#     print(payments)\n",
    "#     print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def heuristicSupervisionRule(tp):\n",
    "    global  dp_H,decision_H\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    tempold=-1;\n",
    "    for turns in range(n,0,-1):\n",
    "        money = dpPrecision\n",
    "        j=0\n",
    "        tempo=sum(bits)\n",
    "        #print(\"bits\",tempo)\n",
    "        for i in range(n):\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            \n",
    "            offerIndex = decision_H[tempo ,tempo - i, money]\n",
    "            offer = float(offerIndex) / dpPrecision\n",
    "            while(j<n):\n",
    "                if(bits[j]!=0):\n",
    "                    break;\n",
    "                j+=1;\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            if tp[j] >= offer:\n",
    "                #print(money,j,tp[j],offer)\n",
    "                money -= offerIndex\n",
    "                bits[j] = 1\n",
    "                payments[j] = offer\n",
    "            else:\n",
    "                bits[j] = 0;\n",
    "                payments[j] = 1.0;\n",
    "            j+=1;\n",
    "        #print(\"money\",money)\n",
    "        if(money==0 and tempold==tempo):\n",
    "            break;\n",
    "        tempold=tempo;\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    for k in range(n, -1, -1):\n",
    "        if k == 0:\n",
    "            break\n",
    "        bits = [1 if tp[ii] >= 1.0 / k else 0 for ii in range(n)]\n",
    "        if sum(bits) == k:\n",
    "            break\n",
    "    if k == 0:\n",
    "        payments = [1 for ii in range(n)]\n",
    "    else:\n",
    "        payments = [1.0 / k if bits[ii] == 1 else 1 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    return float(n - torch.sum(costSharingSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def dpDelay(tp):\n",
    "    return float(n - torch.sum(dpSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def heuristicDelay(tp):\n",
    "    return float(n - torch.sum(heuristicSupervisionRule(tp)[0]).item())\n",
    "\n",
    "\n",
    "# templire=0;\n",
    "# num=1\n",
    "# for i in range(num):\n",
    "#     temp=torch.tensor(samplesJoint[i])\n",
    "#     #print(temp)\n",
    "#     #print(temp);\n",
    "#     #print(heuristicSupervisionRule(temp))\n",
    "#     #print(dpSupervisionRule(temp))\n",
    "#     #print(dpDelay(temp))\n",
    "#     res=dpDelay(temp)\n",
    "#     templire+=res\n",
    "#     #print(\"delay\",res)\n",
    "#     #print()\n",
    "# print(templire/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordAndReport(name, source, loss):\n",
    "    source.append(loss)\n",
    "    realLength = len(source)\n",
    "    #print(f\"{name} ({realLength}): {loss}\")\n",
    "    print(name,realLength,\":\" ,loss)\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    \n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda /100000\n",
    "        \n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "losslist=[];\n",
    "losslistname=[];\n",
    "losslisttemp=[];\n",
    "def test_batch():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lenLoss= 0\n",
    "        nnLoss = 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "            for tp in tp_batch:\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        nnLoss/=lenLoss\n",
    "    return nnLoss\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                delay1 = tpToTotalDelay(tp1)\n",
    "                delay0 = tpToTotalDelay(tp0)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                    loss = loss + (1 - cdf(offer,order)) * delay1 + cdf(offer,order) * delay0\n",
    "                else:\n",
    "                    loss = loss + (1 - cdf(offer,order,i)) * delay1 + cdf(offer,order,i) * delay0\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "    \n",
    "allBits = [torch.tensor(bits, dtype=torch.int16) for bits in itertools.product([0, 1], repeat=n)]\n",
    "\n",
    "def test():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        costSharingLoss = 0\n",
    "        dpLoss = 0\n",
    "        nnLoss = 0\n",
    "        heuristicLoss=0\n",
    "        lenLoss= 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "                heuristicLoss+= heuristicDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        costSharingLoss /= lenLoss\n",
    "        dpLoss /= lenLoss\n",
    "        nnLoss /= lenLoss\n",
    "        heuristicLoss /= lenLoss\n",
    "        #print(lenLoss)\n",
    "        losslisttemp.append(nnLoss);\n",
    "        recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "        recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "        recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "        recordAndReport(\"heuristic\", runningLossHeuristic, heuristicLoss)\n",
    "        print(\"DP:\",dp[n, dpPrecision,0])\n",
    "        #for i in range(n, 0, -1):\n",
    "        #    print(\"Heuristic:\",i,5*(1-dp_H[i, i, dpPrecision]))\n",
    "        for i in range(n, 0, -1):\n",
    "            print(\n",
    "                    tpToPayments(\n",
    "                            torch.tensor([0 if ii >= i else 1 for ii in range(n)], dtype=torch.float32)\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "dpPrecision = 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def producedata(order):\n",
    "    global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "    dpPrecision = 100 \n",
    "    if(order==\"twopeak\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = np.random.normal(\n",
    "            loc=doublePeakLowMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=doublePeakLowMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samples2 = np.random.normal(\n",
    "            loc=doublePeakHighMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = np.random.normal(\n",
    "                        loc=doublePeakHighMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "    elif(order==\"normal\"):\n",
    "        print(\"loc\",normalloc, \"scale\",normalscale)\n",
    "        samples1 = np.random.normal(\n",
    "            loc=normalloc, scale=normalscale, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=normalloc, scale=normalscale\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"uniform\"):  \n",
    "        print(\"uniformlow\",uniformlow, \"uniformhigh\",uniformhigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        uniformlow, uniformhigh\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent1\"):\n",
    "        print(\"loc\",independentnormalloc1,\"scale\",independentnormalscale1)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent2\"):\n",
    "        print(\"loc\",independentnormalloc2, \"scale\",independentnormalscale2)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"cauchy\"):\n",
    "        print(\"cauchyloc\",cauchyloc, \"cauchyscale\",cauchyscalen)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        \n",
    "    tp_dataset = TensorDataset(tp_tensor[: int(trainSize * 0.3)])\n",
    "    tp_dataset_testing = TensorDataset(tp_tensor[int(trainSize * 0.7) :])\n",
    "    tp_dataloader = DataLoader(tp_dataset, batch_size=128, shuffle=True)\n",
    "    tp_dataloader_testing = DataLoader(tp_dataset_testing, batch_size=256, shuffle=False)\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "    decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for yes in range(n + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp[0, 0, yes] = 0\n",
    "            else:\n",
    "                dp[0, money, yes] = yes# + 1.0\n",
    "    for ppl in range(1,  n + 1):\n",
    "        for yes in range(n + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                minSoFar = 1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1 - cdf(offer,order)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order) * (1 + dp[ppl - 1, money, yes])\n",
    "                    else:\n",
    "                        res = (1 - cdf(offer,order,n-ppl)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order,n-ppl) * (1 + dp[ppl - 1, money, yes])\n",
    "                    if minSoFar > res.item():\n",
    "                        minSoFar = res.item()\n",
    "                        decision[ppl, money, yes] = offerIndex\n",
    "                dp[ppl, money, yes] = minSoFar\n",
    "    \n",
    "    print(\"dp\",dp[n, dpPrecision, 0])\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp_H = np.zeros([n + 1 , n + 1, dpPrecision + 1])\n",
    "    decision_H = np.zeros([n + 1 , n + 1, dpPrecision + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for i in range(1,n+1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp_H[i, 0, 0] = 1\n",
    "            else:\n",
    "                offer = money / dpPrecision\n",
    "                dp_H[i, 0, money] = 0#cdf(offer)# + 1.0\n",
    "    for i in range(1,n+1):\n",
    "        for ppl in range(1, i + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                maxSoFar = -1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1-cdf(offer,order)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    else:\n",
    "                        res = (1-cdf(offer,order,ppl-1)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    if maxSoFar < res.item():\n",
    "                        maxSoFar = res.item()\n",
    "                        decision_H[i, ppl, money] = offerIndex\n",
    "                dp_H[i, ppl, money] = maxSoFar\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loc 0.1 scale 0.1\n",
      "loc 0.9 scale 0.1\n",
      "dp 1.7726764678955078\n",
      "Supervised Aim: twopeak costsharing\n",
      "Train Epoch: 1 [0/12000 (0%)]\tLoss: 0.002444\n",
      "Train Epoch: 1 [2560/12000 (21%)]\tLoss: 0.000091\n",
      "Train Epoch: 1 [5120/12000 (43%)]\tLoss: 0.000010\n",
      "Train Epoch: 1 [7680/12000 (64%)]\tLoss: 0.000008\n",
      "Train Epoch: 1 [10240/12000 (85%)]\tLoss: 0.000005\n",
      "Train Epoch: 2 [0/12000 (0%)]\tLoss: 0.000003\n",
      "Train Epoch: 2 [2560/12000 (21%)]\tLoss: 0.000002\n",
      "Train Epoch: 2 [5120/12000 (43%)]\tLoss: 0.000002\n",
      "Train Epoch: 2 [7680/12000 (64%)]\tLoss: 0.000001\n",
      "Train Epoch: 2 [10240/12000 (85%)]\tLoss: 0.000001\n",
      "Train Epoch: 3 [0/12000 (0%)]\tLoss: 0.000001\n",
      "Train Epoch: 3 [2560/12000 (21%)]\tLoss: 0.000001\n",
      "Train Epoch: 3 [5120/12000 (43%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [7680/12000 (64%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [10240/12000 (85%)]\tLoss: 0.000000\n",
      "NN 1 : tensor(3.0072)\n",
      "CS 1 : 3.0063333333333335\n",
      "DP 1 : 1.7939166666666666\n",
      "heuristic 1 : 1.7888333333333333\n",
      "DP: 1.7726764678955078\n",
      "tensor([0.1427, 0.1430, 0.1431, 0.1423, 0.1429, 0.1429, 0.1430])\n",
      "tensor([0.1673, 0.1661, 0.1675, 0.1664, 0.1667, 0.1661, 1.0000])\n",
      "tensor([0.2003, 0.1984, 0.2006, 0.2008, 0.1999, 1.0000, 1.0000])\n",
      "tensor([0.2494, 0.2508, 0.2501, 0.2497, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.3337, 0.3345, 0.3318, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.4998, 0.5002, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/12000 (0%)]\tLoss: 2.969993 testing loss: tensor(2.9993)\n",
      "Train Epoch: 1 [2560/12000 (21%)]\tLoss: 2.039197 testing loss: tensor(2.0001)\n",
      "Train Epoch: 1 [5120/12000 (43%)]\tLoss: 1.917199 testing loss: tensor(1.9017)\n",
      "Train Epoch: 1 [7680/12000 (64%)]\tLoss: 1.703092 testing loss: tensor(1.8841)\n",
      "Train Epoch: 1 [10240/12000 (85%)]\tLoss: 2.103970 testing loss: tensor(1.8745)\n",
      "penalty: 0.08266147971153259\n",
      "NN 2 : tensor(1.8623)\n",
      "CS 2 : 3.0063333333333335\n",
      "DP 2 : 1.7939166666666666\n",
      "heuristic 2 : 1.7888333333333333\n",
      "DP: 1.7726764678955078\n",
      "tensor([0.0365, 0.0349, 0.7403, 0.0710, 0.0535, 0.0352, 0.0286])\n",
      "tensor([0.0365, 0.0349, 0.7484, 0.0874, 0.0578, 0.0351, 1.0000])\n",
      "tensor([0.0427, 0.0394, 0.7544, 0.0988, 0.0646, 1.0000, 1.0000])\n",
      "tensor([0.0684, 0.0688, 0.7642, 0.0986, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.1025, 0.1048, 0.7928, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.4884, 0.5116, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 2 [0/12000 (0%)]\tLoss: 1.669767 testing loss: tensor(1.8596)\n",
      "Train Epoch: 2 [2560/12000 (21%)]\tLoss: 1.782546 testing loss: tensor(1.8182)\n",
      "Train Epoch: 2 [5120/12000 (43%)]\tLoss: 1.696114 testing loss: tensor(1.7880)\n",
      "Train Epoch: 2 [7680/12000 (64%)]\tLoss: 1.454176 testing loss: tensor(1.7929)\n",
      "Train Epoch: 2 [10240/12000 (85%)]\tLoss: 1.747060 testing loss: tensor(1.7818)\n",
      "penalty: 0.07394677400588989\n",
      "NN 3 : tensor(1.7732)\n",
      "CS 3 : 3.0063333333333335\n",
      "DP 3 : 1.7939166666666666\n",
      "heuristic 3 : 1.7888333333333333\n",
      "DP: 1.7726764678955078\n",
      "tensor([0.0447, 0.0397, 0.7395, 0.0464, 0.0432, 0.0467, 0.0397])\n",
      "tensor([0.0510, 0.0444, 0.7463, 0.0578, 0.0482, 0.0524, 1.0000])\n",
      "tensor([0.0626, 0.0549, 0.7543, 0.0702, 0.0580, 1.0000, 1.0000])\n",
      "tensor([0.0799, 0.0743, 0.7718, 0.0740, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.1134, 0.0950, 0.7915, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.7281, 0.2719, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 3 [0/12000 (0%)]\tLoss: 1.681895 testing loss: tensor(1.7797)\n",
      "Train Epoch: 3 [2560/12000 (21%)]\tLoss: 1.754518 testing loss: tensor(1.7651)\n",
      "Train Epoch: 3 [5120/12000 (43%)]\tLoss: 1.662939 testing loss: tensor(1.7667)\n",
      "Train Epoch: 3 [7680/12000 (64%)]\tLoss: 1.793287 testing loss: tensor(1.7664)\n",
      "Train Epoch: 3 [10240/12000 (85%)]\tLoss: 1.839179 testing loss: tensor(1.7647)\n",
      "penalty: 0.004762668162584305\n",
      "NN 4 : tensor(1.7651)\n",
      "CS 4 : 3.0063333333333335\n",
      "DP 4 : 1.7939166666666666\n",
      "heuristic 4 : 1.7888333333333333\n",
      "DP: 1.7726764678955078\n",
      "tensor([0.0464, 0.0465, 0.7266, 0.0460, 0.0436, 0.0448, 0.0461])\n",
      "tensor([0.0528, 0.0532, 0.7361, 0.0588, 0.0488, 0.0504, 1.0000])\n",
      "tensor([0.0632, 0.0653, 0.7412, 0.0715, 0.0588, 1.0000, 1.0000])\n",
      "tensor([0.0828, 0.0847, 0.7584, 0.0740, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.1164, 0.1078, 0.7759, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.7761, 0.2239, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 4 [0/12000 (0%)]\tLoss: 1.639058 testing loss: tensor(1.7647)\n",
      "Train Epoch: 4 [2560/12000 (21%)]\tLoss: 1.830947 testing loss: tensor(1.7596)\n",
      "Train Epoch: 4 [5120/12000 (43%)]\tLoss: 1.688367 testing loss: tensor(1.7582)\n",
      "Train Epoch: 4 [7680/12000 (64%)]\tLoss: 1.931622 testing loss: tensor(1.7637)\n",
      "Train Epoch: 4 [10240/12000 (85%)]\tLoss: 1.761115 testing loss: tensor(1.7621)\n",
      "penalty: 0.0005332194268703461\n",
      "NN 5 : tensor(1.7643)\n",
      "CS 5 : 3.0063333333333335\n",
      "DP 5 : 1.7939166666666666\n",
      "heuristic 5 : 1.7888333333333333\n",
      "DP: 1.7726764678955078\n",
      "tensor([0.0455, 0.0440, 0.7315, 0.0431, 0.0459, 0.0456, 0.0444])\n",
      "tensor([0.0525, 0.0510, 0.7384, 0.0553, 0.0512, 0.0516, 1.0000])\n",
      "tensor([0.0636, 0.0637, 0.7433, 0.0673, 0.0621, 1.0000, 1.0000])\n",
      "tensor([0.0827, 0.0820, 0.7642, 0.0710, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.1164, 0.1065, 0.7771, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.7884, 0.2116, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: twopeak dp\n",
      "Train Epoch: 1 [0/12000 (0%)]\tLoss: 0.044531\n",
      "Train Epoch: 1 [2560/12000 (21%)]\tLoss: 0.002811\n",
      "Train Epoch: 1 [5120/12000 (43%)]\tLoss: 0.002606\n",
      "Train Epoch: 1 [7680/12000 (64%)]\tLoss: 0.001319\n",
      "Train Epoch: 1 [10240/12000 (85%)]\tLoss: 0.000935\n",
      "Train Epoch: 2 [0/12000 (0%)]\tLoss: 0.000702\n",
      "Train Epoch: 2 [2560/12000 (21%)]\tLoss: 0.000374\n",
      "Train Epoch: 2 [5120/12000 (43%)]\tLoss: 0.000324\n",
      "Train Epoch: 2 [7680/12000 (64%)]\tLoss: 0.000095\n",
      "Train Epoch: 2 [10240/12000 (85%)]\tLoss: 0.000094\n",
      "Train Epoch: 3 [0/12000 (0%)]\tLoss: 0.000072\n",
      "Train Epoch: 3 [2560/12000 (21%)]\tLoss: 0.000065\n",
      "Train Epoch: 3 [5120/12000 (43%)]\tLoss: 0.000053\n",
      "Train Epoch: 3 [7680/12000 (64%)]\tLoss: 0.000048\n",
      "Train Epoch: 3 [10240/12000 (85%)]\tLoss: 0.000033\n",
      "NN 1 : tensor(1.7662)\n",
      "CS 1 : 3.0063333333333335\n",
      "DP 1 : 1.7939166666666666\n",
      "heuristic 1 : 1.7888333333333333\n",
      "DP: 1.7726764678955078\n",
      "tensor([0.7390, 0.0684, 0.0583, 0.0593, 0.0486, 0.0230, 0.0034])\n",
      "tensor([0.6873, 0.0991, 0.0697, 0.0569, 0.0546, 0.0323, 1.0000])\n",
      "tensor([0.6952, 0.1114, 0.0672, 0.0637, 0.0625, 1.0000, 1.0000])\n",
      "tensor([0.7340, 0.1276, 0.0778, 0.0606, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.7944, 0.1179, 0.0877, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.8753, 0.1247, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/12000 (0%)]\tLoss: 1.840077 testing loss: tensor(1.8025)\n",
      "Train Epoch: 1 [2560/12000 (21%)]\tLoss: 1.665723 testing loss: tensor(1.7357)\n",
      "Train Epoch: 1 [5120/12000 (43%)]\tLoss: 1.546426 testing loss: tensor(1.7220)\n",
      "Train Epoch: 1 [7680/12000 (64%)]\tLoss: 1.785127 testing loss: tensor(1.7274)\n",
      "Train Epoch: 1 [10240/12000 (85%)]\tLoss: 1.627888 testing loss: tensor(1.7205)\n",
      "penalty: 0.0002710595726966858\n",
      "NN 2 : tensor(1.7192)\n",
      "CS 2 : 3.0063333333333335\n",
      "DP 2 : 1.7939166666666666\n",
      "heuristic 2 : 1.7888333333333333\n",
      "DP: 1.7726764678955078\n",
      "tensor([0.7188, 0.0491, 0.0448, 0.0454, 0.0481, 0.0490, 0.0447])\n",
      "tensor([0.7213, 0.0563, 0.0536, 0.0553, 0.0574, 0.0562, 1.0000])\n",
      "tensor([0.7393, 0.0657, 0.0628, 0.0614, 0.0708, 1.0000, 1.0000])\n",
      "tensor([0.7751, 0.0801, 0.0748, 0.0700, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.8115, 0.0920, 0.0964, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.8913, 0.1087, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 2 [0/12000 (0%)]\tLoss: 1.545541 testing loss: tensor(1.7252)\n",
      "Train Epoch: 2 [2560/12000 (21%)]\tLoss: 1.492690 testing loss: tensor(1.7168)\n",
      "Train Epoch: 2 [5120/12000 (43%)]\tLoss: 1.507996 testing loss: tensor(1.7322)\n",
      "Train Epoch: 2 [7680/12000 (64%)]\tLoss: 1.532635 testing loss: tensor(1.7161)\n",
      "Train Epoch: 2 [10240/12000 (85%)]\tLoss: 1.771214 testing loss: tensor(1.7107)\n",
      "penalty: 0.0005645789206027985\n",
      "NN 3 : tensor(1.7172)\n",
      "CS 3 : 3.0063333333333335\n",
      "DP 3 : 1.7939166666666666\n",
      "heuristic 3 : 1.7888333333333333\n",
      "DP: 1.7726764678955078\n",
      "tensor([0.7050, 0.0485, 0.0473, 0.0527, 0.0495, 0.0469, 0.0501])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7129, 0.0556, 0.0557, 0.0634, 0.0595, 0.0530, 1.0000])\n",
      "tensor([0.7274, 0.0631, 0.0661, 0.0705, 0.0729, 1.0000, 1.0000])\n",
      "tensor([0.7491, 0.0791, 0.0819, 0.0899, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.7960, 0.0907, 0.1133, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.8872, 0.1128, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 3 [0/12000 (0%)]\tLoss: 1.726754 testing loss: tensor(1.7175)\n",
      "Train Epoch: 3 [2560/12000 (21%)]\tLoss: 1.544321 testing loss: tensor(1.7283)\n",
      "Train Epoch: 3 [5120/12000 (43%)]\tLoss: 1.967091 testing loss: tensor(1.7221)\n",
      "Train Epoch: 3 [7680/12000 (64%)]\tLoss: 1.745477 testing loss: tensor(1.7095)\n",
      "Train Epoch: 3 [10240/12000 (85%)]\tLoss: 1.584972 testing loss: tensor(1.7219)\n",
      "penalty: 0.0004933476448059082\n",
      "NN 4 : tensor(1.7223)\n",
      "CS 4 : 3.0063333333333335\n",
      "DP 4 : 1.7939166666666666\n",
      "heuristic 4 : 1.7888333333333333\n",
      "DP: 1.7726764678955078\n",
      "tensor([0.7377, 0.0445, 0.0446, 0.0443, 0.0449, 0.0393, 0.0446])\n",
      "tensor([0.7383, 0.0537, 0.0543, 0.0533, 0.0556, 0.0448, 1.0000])\n",
      "tensor([0.7510, 0.0593, 0.0616, 0.0600, 0.0681, 1.0000, 1.0000])\n",
      "tensor([0.7739, 0.0765, 0.0737, 0.0759, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.8134, 0.0855, 0.1010, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.8916, 0.1084, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 4 [0/12000 (0%)]\tLoss: 1.657318 testing loss: tensor(1.7213)\n",
      "Train Epoch: 4 [2560/12000 (21%)]\tLoss: 1.841127 testing loss: tensor(1.7118)\n",
      "Train Epoch: 4 [5120/12000 (43%)]\tLoss: 1.673742 testing loss: tensor(1.7103)\n",
      "Train Epoch: 4 [7680/12000 (64%)]\tLoss: 1.816401 testing loss: tensor(1.7149)\n",
      "Train Epoch: 4 [10240/12000 (85%)]\tLoss: 1.577923 testing loss: tensor(1.7129)\n",
      "penalty: 0.0\n",
      "NN 5 : tensor(1.7131)\n",
      "CS 5 : 3.0063333333333335\n",
      "DP 5 : 1.7939166666666666\n",
      "heuristic 5 : 1.7888333333333333\n",
      "DP: 1.7726764678955078\n",
      "tensor([0.7106, 0.0471, 0.0479, 0.0471, 0.0495, 0.0489, 0.0489])\n",
      "tensor([0.7186, 0.0557, 0.0555, 0.0560, 0.0600, 0.0543, 1.0000])\n",
      "tensor([0.7318, 0.0636, 0.0663, 0.0651, 0.0732, 1.0000, 1.0000])\n",
      "tensor([0.7576, 0.0809, 0.0804, 0.0810, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.7935, 0.0939, 0.1126, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.8809, 0.1191, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: twopeak random initializing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "NN 1 : tensor(2.9124)\n",
      "CS 1 : 3.0063333333333335\n",
      "DP 1 : 1.7939166666666666\n",
      "heuristic 1 : 1.7888333333333333\n",
      "DP: 1.7726764678955078\n",
      "tensor([0.1463, 0.1052, 0.2094, 0.1001, 0.1249, 0.2159, 0.0983])\n",
      "tensor([0.1723, 0.1246, 0.2274, 0.1209, 0.1458, 0.2090, 1.0000])\n",
      "tensor([0.2371, 0.1412, 0.2862, 0.1453, 0.1901, 1.0000, 1.0000])\n",
      "tensor([0.2866, 0.1742, 0.3670, 0.1721, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.3353, 0.2164, 0.4484, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.6088, 0.3912, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/12000 (0%)]\tLoss: 3.022270 testing loss: tensor(2.8327)\n",
      "Train Epoch: 1 [2560/12000 (21%)]\tLoss: 2.016578 testing loss: tensor(2.0804)\n",
      "Train Epoch: 1 [5120/12000 (43%)]\tLoss: 1.786787 testing loss: tensor(1.9518)\n",
      "Train Epoch: 1 [7680/12000 (64%)]\tLoss: 1.822934 testing loss: tensor(1.8352)\n",
      "Train Epoch: 1 [10240/12000 (85%)]\tLoss: 1.813513 testing loss: tensor(1.7822)\n",
      "penalty: 0.07016585767269135\n",
      "NN 2 : tensor(1.7617)\n",
      "CS 2 : 3.0063333333333335\n",
      "DP 2 : 1.7939166666666666\n",
      "heuristic 2 : 1.7888333333333333\n",
      "DP: 1.7726764678955078\n",
      "tensor([0.0161, 0.0396, 0.0670, 0.0532, 0.0480, 0.7329, 0.0431])\n",
      "tensor([0.0197, 0.0425, 0.0828, 0.0637, 0.0521, 0.7392, 1.0000])\n",
      "tensor([0.0859, 0.0497, 0.7366, 0.0759, 0.0519, 1.0000, 1.0000])\n",
      "tensor([0.1013, 0.0686, 0.7439, 0.0863, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.1106, 0.1027, 0.7867, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.8581, 0.1419, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 2 [0/12000 (0%)]\tLoss: 2.004391 testing loss: tensor(1.7600)\n",
      "Train Epoch: 2 [2560/12000 (21%)]\tLoss: 1.711261 testing loss: tensor(1.7303)\n",
      "Train Epoch: 2 [5120/12000 (43%)]\tLoss: 1.625117 testing loss: tensor(1.7234)\n",
      "Train Epoch: 2 [7680/12000 (64%)]\tLoss: 1.767433 testing loss: tensor(1.7376)\n",
      "Train Epoch: 2 [10240/12000 (85%)]\tLoss: 1.899079 testing loss: tensor(1.7241)\n",
      "penalty: 0.00038478896021842957\n",
      "NN 3 : tensor(1.7293)\n",
      "CS 3 : 3.0063333333333335\n",
      "DP 3 : 1.7939166666666666\n",
      "heuristic 3 : 1.7888333333333333\n",
      "DP: 1.7726764678955078\n",
      "tensor([0.0308, 0.0473, 0.0537, 0.0449, 0.0470, 0.7317, 0.0446])\n",
      "tensor([0.0359, 0.0529, 0.0687, 0.0553, 0.0527, 0.7345, 1.0000])\n",
      "tensor([0.0833, 0.0577, 0.7429, 0.0570, 0.0590, 1.0000, 1.0000])\n",
      "tensor([0.0973, 0.0850, 0.7522, 0.0655, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.1083, 0.1129, 0.7788, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.8307, 0.1693, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 3 [0/12000 (0%)]\tLoss: 1.669780 testing loss: tensor(1.7284)\n",
      "Train Epoch: 3 [2560/12000 (21%)]\tLoss: 1.439529 testing loss: tensor(1.7302)\n",
      "Train Epoch: 3 [5120/12000 (43%)]\tLoss: 1.891727 testing loss: tensor(1.7265)\n",
      "Train Epoch: 3 [7680/12000 (64%)]\tLoss: 1.851051 testing loss: tensor(1.7197)\n",
      "Train Epoch: 3 [10240/12000 (85%)]\tLoss: 1.754544 testing loss: tensor(1.7200)\n",
      "penalty: 0.0074504949152469635\n",
      "NN 4 : tensor(1.7200)\n",
      "CS 4 : 3.0063333333333335\n",
      "DP 4 : 1.7939166666666666\n",
      "heuristic 4 : 1.7888333333333333\n",
      "DP: 1.7726764678955078\n",
      "tensor([0.0376, 0.0452, 0.0475, 0.0427, 0.0461, 0.7345, 0.0464])\n",
      "tensor([0.0429, 0.0520, 0.0591, 0.0540, 0.0528, 0.7393, 1.0000])\n",
      "tensor([0.0713, 0.0638, 0.7285, 0.0668, 0.0696, 1.0000, 1.0000])\n",
      "tensor([0.0869, 0.0898, 0.7451, 0.0782, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.1072, 0.1189, 0.7739, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.8324, 0.1676, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 4 [0/12000 (0%)]\tLoss: 1.681886 testing loss: tensor(1.7192)\n",
      "Train Epoch: 4 [2560/12000 (21%)]\tLoss: 1.547762 testing loss: tensor(1.7207)\n",
      "Train Epoch: 4 [5120/12000 (43%)]\tLoss: 1.773517 testing loss: tensor(1.7231)\n",
      "Train Epoch: 4 [7680/12000 (64%)]\tLoss: 1.516825 testing loss: tensor(1.7241)\n",
      "Train Epoch: 4 [10240/12000 (85%)]\tLoss: 1.715614 testing loss: tensor(1.7150)\n",
      "penalty: 0.0004909634590148926\n",
      "NN 5 : tensor(1.7198)\n",
      "CS 5 : 3.0063333333333335\n",
      "DP 5 : 1.7939166666666666\n",
      "heuristic 5 : 1.7888333333333333\n",
      "DP: 1.7726764678955078\n",
      "tensor([0.0436, 0.0433, 0.0465, 0.0438, 0.0464, 0.7317, 0.0447])\n",
      "tensor([0.0493, 0.0499, 0.0573, 0.0541, 0.0517, 0.7378, 1.0000])\n",
      "tensor([0.0684, 0.0589, 0.7463, 0.0641, 0.0624, 1.0000, 1.0000])\n",
      "tensor([0.0833, 0.0836, 0.7563, 0.0767, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.1102, 0.1160, 0.7738, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.8260, 0.1740, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.1)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "    \n",
    "for order in stage:\n",
    "    producedata(order)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "    #running\n",
    "    for order1 in order1name:\n",
    "        print(\"Supervised Aim:\",order,order1)\n",
    "        \n",
    "        # for mapping binary to payments before softmax\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(n, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, n),\n",
    "        )\n",
    "        model.apply(init_weights)\n",
    "        # optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        runningLossNN = []\n",
    "        runningLossCS = []\n",
    "        runningLossDP = []\n",
    "        runningLossHeuristic = []\n",
    "        #model=torch.load(\"save/pytorchNN=5dp1\");\n",
    "        #model.eval()\n",
    "        ##order1name=[\"costsharing\",\"dp\",\"heuristic\",\"random initializing\"]\n",
    "        for epoch in range(1, supervisionEpochs + 1):\n",
    "#             print(\"distributionRatio\",distributionRatio)\n",
    "            if(order1==\"costsharing\"):\n",
    "                supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "            elif(order1==\"dp\"):\n",
    "                supervisionTrain(epoch, dpSupervisionRule)\n",
    "            elif(order1==\"heuristic\"):\n",
    "                supervisionTrain(epoch, heuristicSupervisionRule)\n",
    "            elif(order1==\"random initializing\"):\n",
    "                print(\"do nothing\");\n",
    "\n",
    "        test()\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(epoch)\n",
    "            test()\n",
    "        losslistname.append(order+\" \"+order1);\n",
    "        losslist.append(losslisttemp);\n",
    "        losslisttemp=[];\n",
    "        savepath=\"save/pytorchNN=5all-phoenix\"+order+str(order1)\n",
    "        torch.save(model, savepath);\n",
    "        print(\"end\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colorlist=[\"#D2691E\",'#4169E1',\"#9ACD32\",\"#B22222\",\"#FF00FF\",\"#708090\"]\n",
    "for i in range(len(losslist)):\n",
    "    plt.plot(losslist[i], 'ro-', color=colorlist[i], alpha=0.8, label=losslistname[i])\n",
    "\n",
    "# 显示标签，如果不加这句，即使加了label='一些数字'的参数，最终还是不会显示标签\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel('trianing times')\n",
    "plt.ylabel('delay')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
