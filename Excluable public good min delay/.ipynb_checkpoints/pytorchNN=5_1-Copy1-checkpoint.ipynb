{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40058530825361593\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from random import randint\n",
    "import random\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "from pylab import *\n",
    "from scipy.stats import beta\n",
    "import math\n",
    "n = 5\n",
    "epochs = 1\n",
    "supervisionEpochs = 1\n",
    "lr1 = 0.01\n",
    "lr2 = 0.0005\n",
    "log_interval = 5\n",
    "trainSize = 50000#100000\n",
    "percentage_train_test= 0.5\n",
    "penaltyLambda = 10\n",
    "doublePeakHighMean = 0.6\n",
    "doublePeakLowMean = 0.2\n",
    "doublePeakStd = 0.1\n",
    "uniformlow=0\n",
    "uniformhigh=1.0\n",
    "normalloc = 0.2\n",
    "normalscale = 0.1\n",
    "\n",
    "cauchyloc = 1/n\n",
    "cauchyscalen = 0.004\n",
    "\n",
    "exponentialhigh = 15 #Symbol(\"b\", real=True)\n",
    "exponentiallow  = 15 #Symbol(\"a\", real=True)\n",
    "\n",
    "\n",
    "beta_a = 0.3\n",
    "beta_b  = 0.2\n",
    "kumaraswamy_a = beta_a \n",
    "kumaraswamy_b = (1.0+(beta_a-1.0)*math.pow( (beta_a+beta_b-2.0)/(beta_a-1.0), beta_a) )/beta_a \n",
    "print(kumaraswamy_b)\n",
    "\n",
    "\n",
    "independentnormalloc1=[(float(ii)+1)/(2*n+1) for ii in range(n,0,-1)]\n",
    "independentnormalscale1=[0.05 for ii in range(n)]\n",
    "\n",
    "independentnormalloc2=[(float(ii)+1)/(2*n+1) for ii in range(1,n+1,1)]\n",
    "independentnormalscale2=[0.05 for ii in range(n)]\n",
    "stage=[\"beta\"]\n",
    "# \"twopeak\",\"normal\",\"uniform\",\"independent1\",\"independent2\",\"cauchy\",\"beta\",\"U-exponential\",\"arcsine\"\n",
    "order1name=[\"dp\",\"random initializing\",\"costsharing\",\"heuristic\"]\n",
    "# \"costsharing\",\"dp\",\"heuristic\",\"random initializing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio1 = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase1 = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "d3 = D.normal.Normal(normalloc, normalscale)\n",
    "distributionRatio3 = d3.cdf(1) - d3.cdf(0)\n",
    "distributionBase3 = d3.cdf(0)\n",
    "\n",
    "d4 = D.uniform.Uniform(uniformlow,uniformhigh)\n",
    "distributionRatio4 = d4.cdf(1) - d4.cdf(0)\n",
    "distributionBase4 = d4.cdf(0)\n",
    "\n",
    "d5 = [D.normal.Normal(independentnormalloc1[ii], independentnormalscale1[ii]) for ii in range(n)]\n",
    "d6 = [D.normal.Normal(independentnormalloc2[ii], independentnormalscale2[ii]) for ii in range(n)]\n",
    "\n",
    "d7 = D.cauchy.Cauchy(cauchyloc,cauchyscalen)\n",
    "\n",
    "d81 = D.exponential.Exponential(exponentiallow)\n",
    "d82 = D.exponential.Exponential(exponentialhigh)\n",
    "\n",
    "d9 = D.beta.Beta(beta_a,beta_b)\n",
    "#sample_d9=d9.rsample(torch.Size([100000]))\n",
    "\n",
    "#d9_sample=np.linspace(0.0001, 0.9999, 10000)\n",
    "#d9_pdf=torch.exp(d9.log_prob(torch.tensor(d9_sample,dtype=torch.float32)))\n",
    "#d9_delta=d9_sample[1]-d9_sample[0]\n",
    "#d9_sum_pdf=torch.sum(d9_pdf*d9_delta)\n",
    "\n",
    "d10 = D.beta.Beta(0.5,0.5)\n",
    "\n",
    "def cdf(x,y, i=None):\n",
    "    if(y==\"twopeak\"):\n",
    "        return (d1.cdf(x) + d2.cdf(x) - distributionBase1) / 2 / distributionRatio1\n",
    "    elif(y==\"normal\"):\n",
    "        return (d3.cdf(x)-distributionBase3)/distributionRatio3;\n",
    "    elif(y==\"uniform\"):\n",
    "        return (d4.cdf(x)-distributionBase4)/distributionRatio4;\n",
    "    elif(y==\"independent1\"):\n",
    "        return d5[i].cdf(x);\n",
    "    elif(y==\"independent2\"):\n",
    "        return d6[i].cdf(x);\n",
    "    elif(y==\"cauchy\"):\n",
    "        return d7.cdf(x);\n",
    "    elif(y==\"beta\"):\n",
    "#         sum_cdf=0.0;\n",
    "#         if(x<0.0001):\n",
    "#             x=0.00011;\n",
    "#         if(x>0.9999):\n",
    "#             x=0.99989;\n",
    "#         for i in range(len(d9_pdf)):\n",
    "#             if(d9_sample[i]<x):\n",
    "#                 sum_cdf+=d9_pdf[i]*d9_delta;\n",
    "#             else:\n",
    "#                 sum_cdf+=(d9_pdf[i]+d9_pdf[i-1])/ 2 *(x-d9_sample[i-1])\n",
    "#                 break;\n",
    "#         return sum_cdf/d9_sum_pdf\n",
    "#         cdf_v=torch.sum((sample_d9<(x)), dtype=torch.float32)/100000\n",
    "#         return cdf_v\n",
    "#    F(x|a,b)=1–(1–x^a)^b\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(x,kumaraswamy_a),kumaraswamy_b);\n",
    "        except:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(torch.tensor(x,dtype=torch.float32),kumaraswamy_a),kumaraswamy_b);\n",
    "    elif(y==\"arcsine\"):\n",
    "        #\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            res=2.0/math.pi * torch.asin(torch.sqrt(x))\n",
    "            #print(x)\n",
    "            return res# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "        except:\n",
    "            return 2.0/math.pi * torch.asin(torch.sqrt(torch.tensor(x,dtype=torch.float32)))# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "    elif(y==\"U-exponential\"):\n",
    "        return (d81.cdf(x) + (1.0 - d82.cdf(1.0-x)))  / 2 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    if torch.sum(bits).item() == 0:\n",
    "        return torch.ones(n)\n",
    "    bits = bits.type(torch.float32)\n",
    "    negBits = torch.ones(n) - bits\n",
    "    payments = model(bits)\n",
    "    payments = payments - 1000 * negBits\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    payments = payments + negBits\n",
    "    return payments\n",
    "\n",
    "def tpToBits(tp, bits=torch.ones(n).type(torch.uint8)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = (tp >= payments).type(torch.uint8)\n",
    "    if torch.equal(newBits, bits):\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits(tp, newBits)#bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    return n - torch.sum(tpToBits(tp).type(torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    global  dp,decision\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money, yes]\n",
    "        offer = float(offerIndex) / dpPrecision\n",
    "        if tp[i] >= offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits[i] = 0\n",
    "            payments[i] = 1\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "#     print()\n",
    "#     print(tp)\n",
    "#     print(bits)\n",
    "#     print(payments)\n",
    "#     print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def heuristicSupervisionRule(tp):\n",
    "    global  dp_H,decision_H\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    tempold=-1;\n",
    "    for turns in range(n,0,-1):\n",
    "        money = dpPrecision\n",
    "        j=0\n",
    "        tempo=sum(bits)\n",
    "        #print(\"bits\",tempo)\n",
    "        for i in range(n):\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            \n",
    "            offerIndex = decision_H[tempo ,tempo - i, money]\n",
    "            offer = float(offerIndex) / dpPrecision\n",
    "            while(j<n):\n",
    "                if(bits[j]!=0):\n",
    "                    break;\n",
    "                j+=1;\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            if tp[j] >= offer:\n",
    "                #print(money,j,tp[j],offer)\n",
    "                money -= offerIndex\n",
    "                bits[j] = 1\n",
    "                payments[j] = offer\n",
    "            else:\n",
    "                bits[j] = 0;\n",
    "                payments[j] = 1.0;\n",
    "            j+=1;\n",
    "        #print(\"money\",money)\n",
    "        if(money==0 and tempold==tempo):\n",
    "            break;\n",
    "        tempold=tempo;\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    for k in range(n, -1, -1):\n",
    "        if k == 0:\n",
    "            break\n",
    "        bits = [1 if tp[ii] >= 1.0 / k else 0 for ii in range(n)]\n",
    "        if sum(bits) == k:\n",
    "            break\n",
    "    if k == 0:\n",
    "        payments = [1 for ii in range(n)]\n",
    "    else:\n",
    "        payments = [1.0 / k if bits[ii] == 1 else 1 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    return float(n - torch.sum(costSharingSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def dpDelay(tp):\n",
    "    return float(n - torch.sum(dpSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def heuristicDelay(tp):\n",
    "    return float(n - torch.sum(heuristicSupervisionRule(tp)[0]).item())\n",
    "\n",
    "\n",
    "# templire=0;\n",
    "# num=1\n",
    "# for i in range(num):\n",
    "#     temp=torch.tensor(samplesJoint[i])\n",
    "#     #print(temp)\n",
    "#     #print(temp);\n",
    "#     #print(heuristicSupervisionRule(temp))\n",
    "#     #print(dpSupervisionRule(temp))\n",
    "#     #print(dpDelay(temp))\n",
    "#     res=dpDelay(temp)\n",
    "#     templire+=res\n",
    "#     #print(\"Delay\",res)\n",
    "#     #print()\n",
    "# print(templire/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordAndReport(name, source, loss):\n",
    "    source.append(loss)\n",
    "    realLength = len(source)\n",
    "    #print(f\"{name} ({realLength}): {loss}\")\n",
    "    print(name,realLength,\":\" ,loss)\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    \n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda /100000\n",
    "        \n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % (log_interval*5) == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "losslist=[];\n",
    "losslistname=[];\n",
    "losslisttemp=[];\n",
    "def test_batch():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lenLoss= 0\n",
    "        nnLoss = 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "            for tp in tp_batch:\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        nnLoss/=lenLoss\n",
    "    return nnLoss\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                Delay1 = tpToTotalDelay(tp1)\n",
    "                Delay0 = tpToTotalDelay(tp0)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                    loss = loss + (1 - cdf(offer,order)) * Delay1 + cdf(offer,order) * Delay0\n",
    "                else:\n",
    "                    loss = loss + (1 - cdf(offer,order,i)) * Delay1 + cdf(offer,order,i) * Delay0\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "    \n",
    "allBits = [torch.tensor(bits, dtype=torch.int16) for bits in itertools.product([0, 1], repeat=n)]\n",
    "\n",
    "def test():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        costSharingLoss = 0\n",
    "        dpLoss = 0\n",
    "        nnLoss = 0\n",
    "        heuristicLoss=0\n",
    "        lenLoss= 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "                heuristicLoss+= heuristicDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        costSharingLoss /= lenLoss\n",
    "        dpLoss /= lenLoss\n",
    "        nnLoss /= lenLoss\n",
    "        heuristicLoss /= lenLoss\n",
    "        #print(lenLoss)\n",
    "        losslisttemp.append(nnLoss);\n",
    "        recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "        recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "        recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "        recordAndReport(\"heuristic\", runningLossHeuristic, heuristicLoss)\n",
    "        print(\"DP:\",dp[n, dpPrecision,0])\n",
    "        #for i in range(n, 0, -1):\n",
    "        #    print(\"Heuristic:\",i,5*(1-dp_H[i, i, dpPrecision]))\n",
    "        for i in range(n, 0, -1):\n",
    "            print(\n",
    "                    tpToPayments(\n",
    "                            torch.tensor([0 if ii >= i else 1 for ii in range(n)], dtype=torch.float32)\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "dpPrecision = 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def producedata(order):\n",
    "    global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "    dpPrecision = 100 \n",
    "    if(order==\"twopeak\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = np.random.normal(\n",
    "            loc=doublePeakLowMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=doublePeakLowMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samples2 = np.random.normal(\n",
    "            loc=doublePeakHighMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = np.random.normal(\n",
    "                        loc=doublePeakHighMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "    elif(order==\"normal\"):\n",
    "        print(\"loc\",normalloc, \"scale\",normalscale)\n",
    "        samples1 = np.random.normal(\n",
    "            loc=normalloc, scale=normalscale, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=normalloc, scale=normalscale\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"uniform\"):  \n",
    "        print(\"uniformlow\",uniformlow, \"uniformhigh\",uniformhigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        uniformlow, uniformhigh\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent1\"):\n",
    "        print(\"loc\",independentnormalloc1,\"scale\",independentnormalscale1)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent2\"):\n",
    "        print(\"loc\",independentnormalloc2, \"scale\",independentnormalscale2)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"cauchy\"):\n",
    "        print(\"cauchyloc\",cauchyloc, \"cauchyscale\",cauchyscalen)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"beta\"):\n",
    "        print(\"beta_a\",beta_a, \"beta_b\",beta_b)\n",
    "        print(\"kumaraswamy_a\",kumaraswamy_a, \"kumaraswamy_b\",kumaraswamy_b)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"arcsine\"):\n",
    "        print(\"betalow\",0.5, \"betahigh\",0.5)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"U-exponential\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = d81.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d81.rsample(torch.Size([1])).numpy()\n",
    "                    \n",
    "        samples2 = d82.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        \n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = d82.rsample(torch.Size([1])).numpy()\n",
    "        samples2 = 1.0 - samples2\n",
    "        samplesJoint = signals * samples1 - (signals - 1.0) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "        \n",
    "    plt.hist(samplesJoint,bins=500)\n",
    "    plt.show()\n",
    "    \n",
    "    tp_dataset = TensorDataset(tp_tensor[: int(trainSize * percentage_train_test)])\n",
    "    tp_dataset_testing = TensorDataset(tp_tensor[int(trainSize * (1.0-percentage_train_test)) :])\n",
    "    tp_dataloader = DataLoader(tp_dataset, batch_size=128, shuffle=True)\n",
    "    tp_dataloader_testing = DataLoader(tp_dataset_testing, batch_size=256, shuffle=False)\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "    decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for yes in range(n + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp[0, 0, yes] = 0\n",
    "            else:\n",
    "                dp[0, money, yes] = yes# + 1.0\n",
    "    for ppl in range(1,  n + 1):\n",
    "        for yes in range(n + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                minSoFar = 1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1 - cdf(offer,order)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order) * (1 + dp[ppl - 1, money, yes])\n",
    "                    else:\n",
    "                        res = (1 - cdf(offer,order,n-ppl)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order,n-ppl) * (1 + dp[ppl - 1, money, yes])\n",
    "                    if minSoFar > res.item():\n",
    "                        minSoFar = res.item()\n",
    "                        decision[ppl, money, yes] = offerIndex\n",
    "                dp[ppl, money, yes] = minSoFar\n",
    "    \n",
    "    print(\"dp\",dp[n, dpPrecision, 0])\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp_H = np.zeros([n + 1 , n + 1, dpPrecision + 1])\n",
    "    decision_H = np.zeros([n + 1 , n + 1, dpPrecision + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for i in range(1,n+1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp_H[i, 0, 0] = 1\n",
    "            else:\n",
    "                offer = money / dpPrecision\n",
    "                dp_H[i, 0, money] = 0#cdf(offer)# + 1.0\n",
    "    for i in range(1,n+1):\n",
    "        for ppl in range(1, i + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                maxSoFar = -1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1-cdf(offer,order)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    else:\n",
    "                        res = (1-cdf(offer,order,ppl-1)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    if maxSoFar < res.item():\n",
    "                        maxSoFar = res.item()\n",
    "                        decision_H[i, ppl, money] = offerIndex\n",
    "                dp_H[i, ppl, money] = maxSoFar\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loc 0.2 scale 0.1\n",
      "loc 0.6 scale 0.1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dp 2.129004955291748\n",
      "Supervised Aim: twopeak dp\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 0.021693\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 0.001019\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 0.000259\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 0.000047\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 0.000006\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 0.000005\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 0.000001\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 0.000001\n",
      "NN 1 : tensor(2.1030)\n",
      "CS 1 : 2.05756\n",
      "DP 1 : 2.15916\n",
      "heuristic 1 : 2.20352\n",
      "DP: 2.129004955291748\n",
      "tensor([0.4601, 0.1799, 0.1698, 0.1397, 0.0504])\n",
      "tensor([0.4949, 0.1861, 0.1610, 0.1579, 1.0000])\n",
      "tensor([0.5919, 0.2159, 0.1922, 1.0000, 1.0000])\n",
      "tensor([0.7586, 0.2414, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 2.041241 testing loss: tensor(2.0963)\n",
      "Train Epoch: 1 [640/25000 (3%)]\tLoss: 2.239076 testing loss: tensor(2.0694)\n",
      "Train Epoch: 1 [1280/25000 (5%)]\tLoss: 2.142658 testing loss: tensor(2.0498)\n",
      "Train Epoch: 1 [1920/25000 (8%)]\tLoss: 2.129854 testing loss: tensor(2.0365)\n",
      "Train Epoch: 1 [2560/25000 (10%)]\tLoss: 1.906607 testing loss: tensor(2.0317)\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 2.118650 testing loss: tensor(2.0292)\n",
      "Train Epoch: 1 [3840/25000 (15%)]\tLoss: 2.184644 testing loss: tensor(2.0240)\n",
      "Train Epoch: 1 [4480/25000 (18%)]\tLoss: 1.865281 testing loss: tensor(2.0170)\n",
      "Train Epoch: 1 [5120/25000 (20%)]\tLoss: 1.970199 testing loss: tensor(2.0157)\n",
      "Train Epoch: 1 [5760/25000 (23%)]\tLoss: 1.933716 testing loss: tensor(2.0152)\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 1.892488 testing loss: tensor(2.0104)\n",
      "Train Epoch: 1 [7040/25000 (28%)]\tLoss: 1.987020 testing loss: tensor(2.0055)\n",
      "Train Epoch: 1 [7680/25000 (31%)]\tLoss: 2.107478 testing loss: tensor(1.9990)\n",
      "Train Epoch: 1 [8320/25000 (33%)]\tLoss: 2.051233 testing loss: tensor(1.9950)\n",
      "Train Epoch: 1 [8960/25000 (36%)]\tLoss: 1.765774 testing loss: tensor(1.9948)\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 1.892873 testing loss: tensor(1.9937)\n",
      "Train Epoch: 1 [10240/25000 (41%)]\tLoss: 1.889834 testing loss: tensor(1.9918)\n",
      "Train Epoch: 1 [10880/25000 (43%)]\tLoss: 2.091972 testing loss: tensor(1.9905)\n",
      "Train Epoch: 1 [11520/25000 (46%)]\tLoss: 1.802592 testing loss: tensor(1.9889)\n",
      "Train Epoch: 1 [12160/25000 (48%)]\tLoss: 1.825320 testing loss: tensor(1.9888)\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 1.913958 testing loss: tensor(1.9872)\n",
      "Train Epoch: 1 [13440/25000 (54%)]\tLoss: 1.915244 testing loss: tensor(1.9854)\n",
      "Train Epoch: 1 [14080/25000 (56%)]\tLoss: 2.201492 testing loss: tensor(1.9821)\n",
      "Train Epoch: 1 [14720/25000 (59%)]\tLoss: 1.929710 testing loss: tensor(1.9818)\n",
      "Train Epoch: 1 [15360/25000 (61%)]\tLoss: 2.072131 testing loss: tensor(1.9779)\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 1.935993 testing loss: tensor(1.9780)\n",
      "Train Epoch: 1 [16640/25000 (66%)]\tLoss: 1.792344 testing loss: tensor(1.9792)\n",
      "Train Epoch: 1 [17280/25000 (69%)]\tLoss: 1.816301 testing loss: tensor(1.9771)\n",
      "Train Epoch: 1 [17920/25000 (71%)]\tLoss: 1.988973 testing loss: tensor(1.9778)\n",
      "Train Epoch: 1 [18560/25000 (74%)]\tLoss: 1.997719 testing loss: tensor(1.9783)\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 2.033891 testing loss: tensor(1.9795)\n",
      "Train Epoch: 1 [19840/25000 (79%)]\tLoss: 2.029063 testing loss: tensor(1.9773)\n",
      "Train Epoch: 1 [20480/25000 (82%)]\tLoss: 2.077587 testing loss: tensor(1.9755)\n",
      "Train Epoch: 1 [21120/25000 (84%)]\tLoss: 1.883254 testing loss: tensor(1.9762)\n",
      "Train Epoch: 1 [21760/25000 (87%)]\tLoss: 2.103994 testing loss: tensor(1.9738)\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 1.750396 testing loss: tensor(1.9738)\n",
      "Train Epoch: 1 [23040/25000 (92%)]\tLoss: 1.904209 testing loss: tensor(1.9754)\n",
      "Train Epoch: 1 [23680/25000 (94%)]\tLoss: 1.979535 testing loss: tensor(1.9746)\n",
      "Train Epoch: 1 [24320/25000 (97%)]\tLoss: 2.058837 testing loss: tensor(1.9745)\n",
      "Train Epoch: 1 [7800/25000 (99%)]\tLoss: 2.193828 testing loss: tensor(1.9742)\n",
      "penalty: 0.002612590789794922\n",
      "NN 2 : tensor(1.9742)\n",
      "CS 2 : 2.05756\n",
      "DP 2 : 2.15916\n",
      "heuristic 2 : 2.20352\n",
      "DP: 2.129004955291748\n",
      "tensor([0.4268, 0.1453, 0.1368, 0.1462, 0.1449])\n",
      "tensor([0.4471, 0.1905, 0.1763, 0.1862, 1.0000])\n",
      "tensor([0.4604, 0.2765, 0.2631, 1.0000, 1.0000])\n",
      "tensor([0.5277, 0.4723, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: twopeak random initializing\n",
      "do nothing\n",
      "NN 1 : tensor(2.0755)\n",
      "CS 1 : 2.05756\n",
      "DP 1 : 2.15916\n",
      "heuristic 1 : 2.20352\n",
      "DP: 2.129004955291748\n",
      "tensor([0.1553, 0.2402, 0.2050, 0.1777, 0.2218])\n",
      "tensor([0.1954, 0.2932, 0.2830, 0.2283, 1.0000])\n",
      "tensor([0.2782, 0.3657, 0.3561, 1.0000, 1.0000])\n",
      "tensor([0.4338, 0.5662, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 2.114987 testing loss: tensor(2.0615)\n",
      "Train Epoch: 1 [640/25000 (3%)]\tLoss: 2.076919 testing loss: tensor(2.0620)\n",
      "Train Epoch: 1 [1280/25000 (5%)]\tLoss: 2.236953 testing loss: tensor(2.0548)\n",
      "Train Epoch: 1 [1920/25000 (8%)]\tLoss: 2.155635 testing loss: tensor(2.0615)\n",
      "Train Epoch: 1 [2560/25000 (10%)]\tLoss: 2.019164 testing loss: tensor(2.0573)\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 1.914006 testing loss: tensor(2.0577)\n",
      "Train Epoch: 1 [3840/25000 (15%)]\tLoss: 1.840565 testing loss: tensor(2.0535)\n",
      "Train Epoch: 1 [4480/25000 (18%)]\tLoss: 1.994657 testing loss: tensor(2.0556)\n",
      "Train Epoch: 1 [5120/25000 (20%)]\tLoss: 1.934229 testing loss: tensor(2.0530)\n",
      "Train Epoch: 1 [5760/25000 (23%)]\tLoss: 2.102895 testing loss: tensor(2.0548)\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 2.077487 testing loss: tensor(2.0528)\n",
      "Train Epoch: 1 [7040/25000 (28%)]\tLoss: 2.042992 testing loss: tensor(2.0514)\n",
      "Train Epoch: 1 [7680/25000 (31%)]\tLoss: 2.108291 testing loss: tensor(2.0491)\n",
      "Train Epoch: 1 [8320/25000 (33%)]\tLoss: 2.067463 testing loss: tensor(2.0440)\n",
      "Train Epoch: 1 [8960/25000 (36%)]\tLoss: 2.074543 testing loss: tensor(2.0428)\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 2.168369 testing loss: tensor(2.0408)\n",
      "Train Epoch: 1 [10240/25000 (41%)]\tLoss: 2.088205 testing loss: tensor(2.0377)\n",
      "Train Epoch: 1 [10880/25000 (43%)]\tLoss: 2.115094 testing loss: tensor(2.0367)\n",
      "Train Epoch: 1 [11520/25000 (46%)]\tLoss: 1.903634 testing loss: tensor(2.0303)\n",
      "Train Epoch: 1 [12160/25000 (48%)]\tLoss: 1.993865 testing loss: tensor(2.0278)\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 2.007917 testing loss: tensor(2.0174)\n",
      "Train Epoch: 1 [13440/25000 (54%)]\tLoss: 2.160900 testing loss: tensor(2.0332)\n",
      "Train Epoch: 1 [14080/25000 (56%)]\tLoss: 1.986372 testing loss: tensor(2.0160)\n",
      "Train Epoch: 1 [14720/25000 (59%)]\tLoss: 2.019602 testing loss: tensor(2.0215)\n",
      "Train Epoch: 1 [15360/25000 (61%)]\tLoss: 2.120249 testing loss: tensor(2.0118)\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 2.124308 testing loss: tensor(2.0126)\n",
      "Train Epoch: 1 [16640/25000 (66%)]\tLoss: 2.160205 testing loss: tensor(2.0087)\n",
      "Train Epoch: 1 [17280/25000 (69%)]\tLoss: 2.030940 testing loss: tensor(2.0087)\n",
      "Train Epoch: 1 [17920/25000 (71%)]\tLoss: 2.055043 testing loss: tensor(2.0110)\n",
      "Train Epoch: 1 [18560/25000 (74%)]\tLoss: 2.072927 testing loss: tensor(2.0070)\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 1.796284 testing loss: tensor(2.0042)\n",
      "Train Epoch: 1 [19840/25000 (79%)]\tLoss: 2.072098 testing loss: tensor(2.0036)\n",
      "Train Epoch: 1 [20480/25000 (82%)]\tLoss: 2.073053 testing loss: tensor(2.0014)\n",
      "Train Epoch: 1 [21120/25000 (84%)]\tLoss: 1.883494 testing loss: tensor(2.0017)\n",
      "Train Epoch: 1 [21760/25000 (87%)]\tLoss: 2.053351 testing loss: tensor(1.9998)\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 2.058250 testing loss: tensor(1.9971)\n",
      "Train Epoch: 1 [23040/25000 (92%)]\tLoss: 1.979922 testing loss: tensor(1.9929)\n",
      "Train Epoch: 1 [23680/25000 (94%)]\tLoss: 2.051235 testing loss: tensor(2.0012)\n",
      "Train Epoch: 1 [24320/25000 (97%)]\tLoss: 2.244616 testing loss: tensor(2.0080)\n",
      "Train Epoch: 1 [7800/25000 (99%)]\tLoss: 2.079083 testing loss: tensor(2.0046)\n",
      "penalty: 0.016558676958084106\n",
      "NN 2 : tensor(2.0046)\n",
      "CS 2 : 2.05756\n",
      "DP 2 : 2.15916\n",
      "heuristic 2 : 2.20352\n",
      "DP: 2.129004955291748\n",
      "tensor([0.1875, 0.2015, 0.2554, 0.1777, 0.1779])\n",
      "tensor([0.2130, 0.2032, 0.3284, 0.2554, 1.0000])\n",
      "tensor([0.4417, 0.2100, 0.3483, 1.0000, 1.0000])\n",
      "tensor([0.4973, 0.5027, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: twopeak costsharing\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 0.001463\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 0.000019\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 0.000000\n",
      "NN 1 : tensor(2.0573)\n",
      "CS 1 : 2.05756\n",
      "DP 1 : 2.15916\n",
      "heuristic 1 : 2.20352\n",
      "DP: 2.129004955291748\n",
      "tensor([0.2000, 0.2002, 0.1999, 0.1999, 0.2000])\n",
      "tensor([0.2500, 0.2500, 0.2496, 0.2504, 1.0000])\n",
      "tensor([0.3334, 0.3334, 0.3332, 1.0000, 1.0000])\n",
      "tensor([0.4998, 0.5002, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 1.753847 testing loss: tensor(2.0573)\n",
      "Train Epoch: 1 [640/25000 (3%)]\tLoss: 1.869365 testing loss: tensor(2.0580)\n",
      "Train Epoch: 1 [1280/25000 (5%)]\tLoss: 2.085793 testing loss: tensor(2.0572)\n",
      "Train Epoch: 1 [1920/25000 (8%)]\tLoss: 2.052246 testing loss: tensor(2.0566)\n",
      "Train Epoch: 1 [2560/25000 (10%)]\tLoss: 2.072863 testing loss: tensor(2.0578)\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 2.153352 testing loss: tensor(2.0587)\n",
      "Train Epoch: 1 [3840/25000 (15%)]\tLoss: 2.064365 testing loss: tensor(2.0600)\n",
      "Train Epoch: 1 [4480/25000 (18%)]\tLoss: 2.342099 testing loss: tensor(2.0616)\n",
      "Train Epoch: 1 [5120/25000 (20%)]\tLoss: 1.864062 testing loss: tensor(2.0628)\n",
      "Train Epoch: 1 [5760/25000 (23%)]\tLoss: 2.298677 testing loss: tensor(2.0615)\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 2.162316 testing loss: tensor(2.0577)\n",
      "Train Epoch: 1 [7040/25000 (28%)]\tLoss: 2.201639 testing loss: tensor(2.0562)\n",
      "Train Epoch: 1 [7680/25000 (31%)]\tLoss: 1.918156 testing loss: tensor(2.0574)\n",
      "Train Epoch: 1 [8320/25000 (33%)]\tLoss: 2.087064 testing loss: tensor(2.0572)\n",
      "Train Epoch: 1 [8960/25000 (36%)]\tLoss: 2.060297 testing loss: tensor(2.0580)\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 1.986800 testing loss: tensor(2.0567)\n",
      "Train Epoch: 1 [10240/25000 (41%)]\tLoss: 2.201749 testing loss: tensor(2.0579)\n",
      "Train Epoch: 1 [10880/25000 (43%)]\tLoss: 2.277591 testing loss: tensor(2.0584)\n",
      "Train Epoch: 1 [11520/25000 (46%)]\tLoss: 2.311688 testing loss: tensor(2.0586)\n",
      "Train Epoch: 1 [12160/25000 (48%)]\tLoss: 2.148595 testing loss: tensor(2.0569)\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 1.877879 testing loss: tensor(2.0588)\n",
      "Train Epoch: 1 [13440/25000 (54%)]\tLoss: 2.200241 testing loss: tensor(2.0587)\n",
      "Train Epoch: 1 [14080/25000 (56%)]\tLoss: 1.823666 testing loss: tensor(2.0586)\n",
      "Train Epoch: 1 [14720/25000 (59%)]\tLoss: 2.060941 testing loss: tensor(2.0593)\n",
      "Train Epoch: 1 [15360/25000 (61%)]\tLoss: 2.131607 testing loss: tensor(2.0582)\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 2.141053 testing loss: tensor(2.0580)\n",
      "Train Epoch: 1 [16640/25000 (66%)]\tLoss: 2.141108 testing loss: tensor(2.0577)\n",
      "Train Epoch: 1 [17280/25000 (69%)]\tLoss: 1.952387 testing loss: tensor(2.0581)\n",
      "Train Epoch: 1 [17920/25000 (71%)]\tLoss: 2.119007 testing loss: tensor(2.0566)\n",
      "Train Epoch: 1 [18560/25000 (74%)]\tLoss: 2.157347 testing loss: tensor(2.0553)\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 2.001980 testing loss: tensor(2.0554)\n",
      "Train Epoch: 1 [19840/25000 (79%)]\tLoss: 1.799621 testing loss: tensor(2.0568)\n",
      "Train Epoch: 1 [20480/25000 (82%)]\tLoss: 2.228043 testing loss: tensor(2.0574)\n",
      "Train Epoch: 1 [21120/25000 (84%)]\tLoss: 1.954903 testing loss: tensor(2.0584)\n",
      "Train Epoch: 1 [21760/25000 (87%)]\tLoss: 1.980100 testing loss: tensor(2.0577)\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 1.799105 testing loss: tensor(2.0595)\n",
      "Train Epoch: 1 [23040/25000 (92%)]\tLoss: 1.890791 testing loss: tensor(2.0595)\n",
      "Train Epoch: 1 [23680/25000 (94%)]\tLoss: 2.071322 testing loss: tensor(2.0609)\n",
      "Train Epoch: 1 [24320/25000 (97%)]\tLoss: 2.075228 testing loss: tensor(2.0614)\n",
      "Train Epoch: 1 [7800/25000 (99%)]\tLoss: 2.280715 testing loss: tensor(2.0613)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(2.0613)\n",
      "CS 2 : 2.05756\n",
      "DP 2 : 2.15916\n",
      "heuristic 2 : 2.20352\n",
      "DP: 2.129004955291748\n",
      "tensor([0.2025, 0.1995, 0.2008, 0.1962, 0.2010])\n",
      "tensor([0.2536, 0.2456, 0.2528, 0.2481, 1.0000])\n",
      "tensor([0.3380, 0.3283, 0.3337, 1.0000, 1.0000])\n",
      "tensor([0.5021, 0.4979, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: twopeak heuristic\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 0.009373\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 0.001291\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 0.000308\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 0.000101\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 0.000024\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 0.000006\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 0.000005\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 0.000003\n",
      "NN 1 : tensor(2.0048)\n",
      "CS 1 : 2.05756\n",
      "DP 1 : 2.15916\n",
      "heuristic 1 : 2.20352\n",
      "DP: 2.129004955291748\n",
      "tensor([0.1287, 0.1300, 0.1404, 0.1407, 0.4602])\n",
      "tensor([0.1805, 0.1701, 0.1708, 0.4787, 1.0000])\n",
      "tensor([0.1059, 0.4394, 0.4547, 1.0000, 1.0000])\n",
      "tensor([0.5020, 0.4980, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 1.976806 testing loss: tensor(1.9956)\n",
      "Train Epoch: 1 [640/25000 (3%)]\tLoss: 1.986018 testing loss: tensor(1.9854)\n",
      "Train Epoch: 1 [1280/25000 (5%)]\tLoss: 2.023058 testing loss: tensor(1.9788)\n",
      "Train Epoch: 1 [1920/25000 (8%)]\tLoss: 1.969300 testing loss: tensor(1.9768)\n",
      "Train Epoch: 1 [2560/25000 (10%)]\tLoss: 1.672560 testing loss: tensor(1.9745)\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 1.999645 testing loss: tensor(1.9715)\n",
      "Train Epoch: 1 [3840/25000 (15%)]\tLoss: 2.064234 testing loss: tensor(1.9752)\n",
      "Train Epoch: 1 [4480/25000 (18%)]\tLoss: 1.851154 testing loss: tensor(1.9712)\n",
      "Train Epoch: 1 [5120/25000 (20%)]\tLoss: 1.887783 testing loss: tensor(1.9690)\n",
      "Train Epoch: 1 [5760/25000 (23%)]\tLoss: 1.996226 testing loss: tensor(1.9650)\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 1.999544 testing loss: tensor(1.9669)\n",
      "Train Epoch: 1 [7040/25000 (28%)]\tLoss: 2.098460 testing loss: tensor(1.9668)\n",
      "Train Epoch: 1 [7680/25000 (31%)]\tLoss: 1.812651 testing loss: tensor(1.9608)\n",
      "Train Epoch: 1 [8320/25000 (33%)]\tLoss: 1.981765 testing loss: tensor(1.9659)\n",
      "Train Epoch: 1 [8960/25000 (36%)]\tLoss: 1.912758 testing loss: tensor(1.9651)\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 1.928102 testing loss: tensor(1.9652)\n",
      "Train Epoch: 1 [10240/25000 (41%)]\tLoss: 1.969166 testing loss: tensor(1.9666)\n",
      "Train Epoch: 1 [10880/25000 (43%)]\tLoss: 2.091316 testing loss: tensor(1.9638)\n",
      "Train Epoch: 1 [11520/25000 (46%)]\tLoss: 2.139697 testing loss: tensor(1.9640)\n",
      "Train Epoch: 1 [12160/25000 (48%)]\tLoss: 2.046144 testing loss: tensor(1.9607)\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 2.093630 testing loss: tensor(1.9612)\n",
      "Train Epoch: 1 [13440/25000 (54%)]\tLoss: 2.029789 testing loss: tensor(1.9602)\n",
      "Train Epoch: 1 [14080/25000 (56%)]\tLoss: 2.001629 testing loss: tensor(1.9596)\n",
      "Train Epoch: 1 [14720/25000 (59%)]\tLoss: 1.926600 testing loss: tensor(1.9597)\n",
      "Train Epoch: 1 [15360/25000 (61%)]\tLoss: 2.040937 testing loss: tensor(1.9587)\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 1.877249 testing loss: tensor(1.9598)\n",
      "Train Epoch: 1 [16640/25000 (66%)]\tLoss: 1.932131 testing loss: tensor(1.9604)\n",
      "Train Epoch: 1 [17280/25000 (69%)]\tLoss: 1.974730 testing loss: tensor(1.9651)\n",
      "Train Epoch: 1 [17920/25000 (71%)]\tLoss: 2.143833 testing loss: tensor(1.9648)\n",
      "Train Epoch: 1 [18560/25000 (74%)]\tLoss: 1.937760 testing loss: tensor(1.9633)\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 2.053487 testing loss: tensor(1.9626)\n",
      "Train Epoch: 1 [19840/25000 (79%)]\tLoss: 1.873055 testing loss: tensor(1.9585)\n",
      "Train Epoch: 1 [20480/25000 (82%)]\tLoss: 1.831020 testing loss: tensor(1.9582)\n",
      "Train Epoch: 1 [21120/25000 (84%)]\tLoss: 2.154991 testing loss: tensor(1.9608)\n",
      "Train Epoch: 1 [21760/25000 (87%)]\tLoss: 2.069951 testing loss: tensor(1.9599)\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 1.819907 testing loss: tensor(1.9592)\n",
      "Train Epoch: 1 [23040/25000 (92%)]\tLoss: 2.025964 testing loss: tensor(1.9590)\n",
      "Train Epoch: 1 [23680/25000 (94%)]\tLoss: 1.911368 testing loss: tensor(1.9610)\n",
      "Train Epoch: 1 [24320/25000 (97%)]\tLoss: 1.980677 testing loss: tensor(1.9613)\n",
      "Train Epoch: 1 [7800/25000 (99%)]\tLoss: 2.247876 testing loss: tensor(1.9618)\n",
      "penalty: 0.009172439575195312\n",
      "NN 2 : tensor(1.9618)\n",
      "CS 2 : 2.05756\n",
      "DP 2 : 2.15916\n",
      "heuristic 2 : 2.20352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP: 2.129004955291748\n",
      "tensor([0.1413, 0.1445, 0.1450, 0.1441, 0.4250])\n",
      "tensor([0.1782, 0.1928, 0.1848, 0.4442, 1.0000])\n",
      "tensor([0.1714, 0.4206, 0.4080, 1.0000, 1.0000])\n",
      "tensor([0.4960, 0.5040, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.1)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "    \n",
    "for order in stage:\n",
    "    producedata(order)\n",
    "    #running\n",
    "    for order1 in order1name:\n",
    "        print(\"Supervised Aim:\",order,order1)\n",
    "        \n",
    "        # for mapping binary to payments before softmax\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(n, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, n),\n",
    "        )\n",
    "        model.apply(init_weights)\n",
    "        # optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr1)\n",
    "        \n",
    "        runningLossNN = []\n",
    "        runningLossCS = []\n",
    "        runningLossDP = []\n",
    "        runningLossHeuristic = []\n",
    "        #model=torch.load(\"save/pytorchNN=5dp1\");\n",
    "        #model.eval()\n",
    "        ##order1name=[\"costsharing\",\"dp\",\"heuristic\",\"random initializing\"]\n",
    "        for epoch in range(1, supervisionEpochs + 1):\n",
    "#             print(\"distributionRatio\",distributionRatio)\n",
    "            if(order1==\"costsharing\"):\n",
    "                supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "            elif(order1==\"dp\"):\n",
    "                supervisionTrain(epoch, dpSupervisionRule)\n",
    "            elif(order1==\"heuristic\"):\n",
    "                supervisionTrain(epoch, heuristicSupervisionRule)\n",
    "            elif(order1==\"random initializing\"):\n",
    "                print(\"do nothing\");\n",
    "\n",
    "        test()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr2)\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(epoch)\n",
    "            test()\n",
    "        losslistname.append(order+\" \"+order1);\n",
    "        losslist.append(losslisttemp);\n",
    "        losslisttemp=[];\n",
    "        savepath=\"save/pytorchNN=5all-beta\"+order+str(order1)\n",
    "        torch.save(model, savepath);\n",
    "        print(\"end\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colorlist=[\"#D2691E\",'#4169E1',\"#9ACD32\",\"#B22222\",\"#FF00FF\",\"#708090\"]\n",
    "for i in range(len(losslist)):\n",
    "    plt.plot(losslist[i], '.-', color=colorlist[i], alpha=0.8, label=losslistname[i])\n",
    "\n",
    "# 显示标签，如果不加这句，即使加了label='一些数字'的参数，最终还是不会显示标签\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('Trianing Times')\n",
    "plt.ylabel('Delay')\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('pytorchNN=5_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
