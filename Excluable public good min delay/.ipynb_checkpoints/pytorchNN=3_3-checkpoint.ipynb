{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40058530825361593\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from random import randint\n",
    "import random\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "from pylab import *\n",
    "from scipy.stats import beta\n",
    "import math\n",
    "n = 3\n",
    "epochs = 1\n",
    "supervisionEpochs = 1\n",
    "lr1 = 0.01\n",
    "lr2 = 0.0005\n",
    "log_interval = 5\n",
    "trainSize = 50000#100000\n",
    "percentage_train_test= 0.5\n",
    "penaltyLambda = 10\n",
    "doublePeakHighMean = 0.6\n",
    "doublePeakLowMean = 0.2\n",
    "doublePeakStd = 0.1\n",
    "uniformlow=0\n",
    "uniformhigh=1.0\n",
    "normalloc = 0.2\n",
    "normalscale = 0.1\n",
    "\n",
    "cauchyloc = 1/n\n",
    "cauchyscalen = 0.004\n",
    "\n",
    "exponentialhigh = 15 #Symbol(\"b\", real=True)\n",
    "exponentiallow  = 15 #Symbol(\"a\", real=True)\n",
    "\n",
    "\n",
    "beta_a = 0.3\n",
    "beta_b  = 0.2\n",
    "kumaraswamy_a = beta_a \n",
    "kumaraswamy_b = (1.0+(beta_a-1.0)*math.pow( (beta_a+beta_b-2.0)/(beta_a-1.0), beta_a) )/beta_a \n",
    "print(kumaraswamy_b)\n",
    "\n",
    "\n",
    "independentnormalloc1=[(float(ii)+1)/(2*n+1) for ii in range(n,0,-1)]\n",
    "independentnormalscale1=[0.05 for ii in range(n)]\n",
    "\n",
    "independentnormalloc2=[(float(ii)+1)/(2*n+1) for ii in range(1,n+1,1)]\n",
    "independentnormalscale2=[0.05 for ii in range(n)]\n",
    "stage=[\"U-exponential\"]\n",
    "# \"twopeak\",\"normal\",\"uniform\",\"independent1\",\"independent2\",\"cauchy\",\"beta\",\"U-exponential\",\"arcsine\"\n",
    "order1name=[\"dp\",\"random initializing\",\"costsharing\",\"heuristic\"]\n",
    "# \"costsharing\",\"dp\",\"heuristic\",\"random initializing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio1 = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase1 = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "d3 = D.normal.Normal(normalloc, normalscale)\n",
    "distributionRatio3 = d3.cdf(1) - d3.cdf(0)\n",
    "distributionBase3 = d3.cdf(0)\n",
    "\n",
    "d4 = D.uniform.Uniform(uniformlow,uniformhigh)\n",
    "distributionRatio4 = d4.cdf(1) - d4.cdf(0)\n",
    "distributionBase4 = d4.cdf(0)\n",
    "\n",
    "d5 = [D.normal.Normal(independentnormalloc1[ii], independentnormalscale1[ii]) for ii in range(n)]\n",
    "d6 = [D.normal.Normal(independentnormalloc2[ii], independentnormalscale2[ii]) for ii in range(n)]\n",
    "\n",
    "d7 = D.cauchy.Cauchy(cauchyloc,cauchyscalen)\n",
    "\n",
    "d81 = D.exponential.Exponential(exponentiallow)\n",
    "d82 = D.exponential.Exponential(exponentialhigh)\n",
    "\n",
    "d9 = D.beta.Beta(beta_a,beta_b)\n",
    "#sample_d9=d9.rsample(torch.Size([100000]))\n",
    "\n",
    "#d9_sample=np.linspace(0.0001, 0.9999, 10000)\n",
    "#d9_pdf=torch.exp(d9.log_prob(torch.tensor(d9_sample,dtype=torch.float32)))\n",
    "#d9_delta=d9_sample[1]-d9_sample[0]\n",
    "#d9_sum_pdf=torch.sum(d9_pdf*d9_delta)\n",
    "\n",
    "d10 = D.beta.Beta(0.5,0.5)\n",
    "\n",
    "def cdf(x,y, i=None):\n",
    "    if(y==\"twopeak\"):\n",
    "        return (d1.cdf(x) + d2.cdf(x) - distributionBase1) / 2 / distributionRatio1\n",
    "    elif(y==\"normal\"):\n",
    "        return (d3.cdf(x)-distributionBase3)/distributionRatio3;\n",
    "    elif(y==\"uniform\"):\n",
    "        return (d4.cdf(x)-distributionBase4)/distributionRatio4;\n",
    "    elif(y==\"independent1\"):\n",
    "        return d5[i].cdf(x);\n",
    "    elif(y==\"independent2\"):\n",
    "        return d6[i].cdf(x);\n",
    "    elif(y==\"cauchy\"):\n",
    "        return d7.cdf(x);\n",
    "    elif(y==\"beta\"):\n",
    "#         sum_cdf=0.0;\n",
    "#         if(x<0.0001):\n",
    "#             x=0.00011;\n",
    "#         if(x>0.9999):\n",
    "#             x=0.99989;\n",
    "#         for i in range(len(d9_pdf)):\n",
    "#             if(d9_sample[i]<x):\n",
    "#                 sum_cdf+=d9_pdf[i]*d9_delta;\n",
    "#             else:\n",
    "#                 sum_cdf+=(d9_pdf[i]+d9_pdf[i-1])/ 2 *(x-d9_sample[i-1])\n",
    "#                 break;\n",
    "#         return sum_cdf/d9_sum_pdf\n",
    "#         cdf_v=torch.sum((sample_d9<(x)), dtype=torch.float32)/100000\n",
    "#         return cdf_v\n",
    "#    F(x|a,b)=1–(1–x^a)^b\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(x,kumaraswamy_a),kumaraswamy_b);\n",
    "        except:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(torch.tensor(x,dtype=torch.float32),kumaraswamy_a),kumaraswamy_b);\n",
    "    elif(y==\"arcsine\"):\n",
    "        #\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            res=2.0/math.pi * torch.asin(torch.sqrt(x))\n",
    "            #print(x)\n",
    "            return res# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "        except:\n",
    "            return 2.0/math.pi * torch.asin(torch.sqrt(torch.tensor(x,dtype=torch.float32)))# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "    elif(y==\"U-exponential\"):\n",
    "        return (d81.cdf(x) + (1.0 - d82.cdf(1.0-x)))  / 2 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    if torch.sum(bits).item() == 0:\n",
    "        return torch.ones(n)\n",
    "    bits = bits.type(torch.float32)\n",
    "    negBits = torch.ones(n) - bits\n",
    "    payments = model(bits)\n",
    "    payments = payments - 1000 * negBits\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    payments = payments + negBits\n",
    "    return payments\n",
    "\n",
    "def tpToBits(tp, bits=torch.ones(n).type(torch.uint8)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = (tp >= payments).type(torch.uint8)\n",
    "    if torch.equal(newBits, bits):\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits(tp, newBits)#bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    return n - torch.sum(tpToBits(tp).type(torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    global  dp,decision\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money, yes]\n",
    "        offer = float(offerIndex) / dpPrecision\n",
    "        if tp[i] >= offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits[i] = 0\n",
    "            payments[i] = 1\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "#     print()\n",
    "#     print(tp)\n",
    "#     print(bits)\n",
    "#     print(payments)\n",
    "#     print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def heuristicSupervisionRule(tp):\n",
    "    global  dp_H,decision_H\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    tempold=-1;\n",
    "    for turns in range(n,0,-1):\n",
    "        money = dpPrecision\n",
    "        j=0\n",
    "        tempo=sum(bits)\n",
    "        #print(\"bits\",tempo)\n",
    "        for i in range(n):\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            \n",
    "            offerIndex = decision_H[tempo ,tempo - i, money]\n",
    "            offer = float(offerIndex) / dpPrecision\n",
    "            while(j<n):\n",
    "                if(bits[j]!=0):\n",
    "                    break;\n",
    "                j+=1;\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            if tp[j] >= offer:\n",
    "                #print(money,j,tp[j],offer)\n",
    "                money -= offerIndex\n",
    "                bits[j] = 1\n",
    "                payments[j] = offer\n",
    "            else:\n",
    "                bits[j] = 0;\n",
    "                payments[j] = 1.0;\n",
    "            j+=1;\n",
    "        #print(\"money\",money)\n",
    "        if(money==0 and tempold==tempo):\n",
    "            break;\n",
    "        tempold=tempo;\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    for k in range(n, -1, -1):\n",
    "        if k == 0:\n",
    "            break\n",
    "        bits = [1 if tp[ii] >= 1.0 / k else 0 for ii in range(n)]\n",
    "        if sum(bits) == k:\n",
    "            break\n",
    "    if k == 0:\n",
    "        payments = [1 for ii in range(n)]\n",
    "    else:\n",
    "        payments = [1.0 / k if bits[ii] == 1 else 1 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    return float(n - torch.sum(costSharingSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def dpDelay(tp):\n",
    "    return float(n - torch.sum(dpSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def heuristicDelay(tp):\n",
    "    return float(n - torch.sum(heuristicSupervisionRule(tp)[0]).item())\n",
    "\n",
    "\n",
    "# templire=0;\n",
    "# num=1\n",
    "# for i in range(num):\n",
    "#     temp=torch.tensor(samplesJoint[i])\n",
    "#     #print(temp)\n",
    "#     #print(temp);\n",
    "#     #print(heuristicSupervisionRule(temp))\n",
    "#     #print(dpSupervisionRule(temp))\n",
    "#     #print(dpDelay(temp))\n",
    "#     res=dpDelay(temp)\n",
    "#     templire+=res\n",
    "#     #print(\"Delay\",res)\n",
    "#     #print()\n",
    "# print(templire/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordAndReport(name, source, loss):\n",
    "    source.append(loss)\n",
    "    realLength = len(source)\n",
    "    #print(f\"{name} ({realLength}): {loss}\")\n",
    "    print(name,realLength,\":\" ,loss)\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    \n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda /100000\n",
    "        \n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % (log_interval*5) == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "losslist=[];\n",
    "losslistname=[];\n",
    "losslisttemp=[];\n",
    "def test_batch():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lenLoss= 0\n",
    "        nnLoss = 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "            for tp in tp_batch:\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        nnLoss/=lenLoss\n",
    "    return nnLoss\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                Delay1 = tpToTotalDelay(tp1)\n",
    "                Delay0 = tpToTotalDelay(tp0)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                    loss = loss + (1 - cdf(offer,order)) * Delay1 + cdf(offer,order) * Delay0\n",
    "                else:\n",
    "                    loss = loss + (1 - cdf(offer,order,i)) * Delay1 + cdf(offer,order,i) * Delay0\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "    \n",
    "allBits = [torch.tensor(bits, dtype=torch.int16) for bits in itertools.product([0, 1], repeat=n)]\n",
    "\n",
    "def test():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        costSharingLoss = 0\n",
    "        dpLoss = 0\n",
    "        nnLoss = 0\n",
    "        heuristicLoss=0\n",
    "        lenLoss= 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "                heuristicLoss+= heuristicDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        costSharingLoss /= lenLoss\n",
    "        dpLoss /= lenLoss\n",
    "        nnLoss /= lenLoss\n",
    "        heuristicLoss /= lenLoss\n",
    "        #print(lenLoss)\n",
    "        losslisttemp.append(nnLoss);\n",
    "        recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "        recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "        recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "        recordAndReport(\"heuristic\", runningLossHeuristic, heuristicLoss)\n",
    "        print(\"DP:\",dp[n, dpPrecision,0])\n",
    "        #for i in range(n, 0, -1):\n",
    "        #    print(\"Heuristic:\",i,5*(1-dp_H[i, i, dpPrecision]))\n",
    "        for i in range(n, 0, -1):\n",
    "            print(\n",
    "                    tpToPayments(\n",
    "                            torch.tensor([0 if ii >= i else 1 for ii in range(n)], dtype=torch.float32)\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "dpPrecision = 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def producedata(order):\n",
    "    global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "    dpPrecision = 100 \n",
    "    if(order==\"twopeak\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = np.random.normal(\n",
    "            loc=doublePeakLowMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=doublePeakLowMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samples2 = np.random.normal(\n",
    "            loc=doublePeakHighMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = np.random.normal(\n",
    "                        loc=doublePeakHighMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "    elif(order==\"normal\"):\n",
    "        print(\"loc\",normalloc, \"scale\",normalscale)\n",
    "        samples1 = np.random.normal(\n",
    "            loc=normalloc, scale=normalscale, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=normalloc, scale=normalscale\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"uniform\"):  \n",
    "        print(\"uniformlow\",uniformlow, \"uniformhigh\",uniformhigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        uniformlow, uniformhigh\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent1\"):\n",
    "        print(\"loc\",independentnormalloc1,\"scale\",independentnormalscale1)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent2\"):\n",
    "        print(\"loc\",independentnormalloc2, \"scale\",independentnormalscale2)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"cauchy\"):\n",
    "        print(\"cauchyloc\",cauchyloc, \"cauchyscale\",cauchyscalen)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"beta\"):\n",
    "        print(\"beta_a\",beta_a, \"beta_b\",beta_b)\n",
    "        print(\"kumaraswamy_a\",kumaraswamy_a, \"kumaraswamy_b\",kumaraswamy_b)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"arcsine\"):\n",
    "        print(\"betalow\",0.5, \"betahigh\",0.5)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"U-exponential\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = d81.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d81.rsample(torch.Size([1])).numpy()\n",
    "                    \n",
    "        samples2 = d82.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        \n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = d82.rsample(torch.Size([1])).numpy()\n",
    "        samples2 = 1.0 - samples2\n",
    "        samplesJoint = signals * samples1 - (signals - 1.0) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "        \n",
    "    plt.hist(samplesJoint,bins=500)\n",
    "    plt.show()\n",
    "    \n",
    "    tp_dataset = TensorDataset(tp_tensor[: int(trainSize * percentage_train_test)])\n",
    "    tp_dataset_testing = TensorDataset(tp_tensor[int(trainSize * (1.0-percentage_train_test)) :])\n",
    "    tp_dataloader = DataLoader(tp_dataset, batch_size=128, shuffle=True)\n",
    "    tp_dataloader_testing = DataLoader(tp_dataset_testing, batch_size=256, shuffle=False)\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "    decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for yes in range(n + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp[0, 0, yes] = 0\n",
    "            else:\n",
    "                dp[0, money, yes] = yes# + 1.0\n",
    "    for ppl in range(1,  n + 1):\n",
    "        for yes in range(n + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                minSoFar = 1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1 - cdf(offer,order)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order) * (1 + dp[ppl - 1, money, yes])\n",
    "                    else:\n",
    "                        res = (1 - cdf(offer,order,n-ppl)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order,n-ppl) * (1 + dp[ppl - 1, money, yes])\n",
    "                    if minSoFar > res.item():\n",
    "                        minSoFar = res.item()\n",
    "                        decision[ppl, money, yes] = offerIndex\n",
    "                dp[ppl, money, yes] = minSoFar\n",
    "    \n",
    "    print(\"dp\",dp[n, dpPrecision, 0])\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp_H = np.zeros([n + 1 , n + 1, dpPrecision + 1])\n",
    "    decision_H = np.zeros([n + 1 , n + 1, dpPrecision + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for i in range(1,n+1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp_H[i, 0, 0] = 1\n",
    "            else:\n",
    "                offer = money / dpPrecision\n",
    "                dp_H[i, 0, money] = 0#cdf(offer)# + 1.0\n",
    "    for i in range(1,n+1):\n",
    "        for ppl in range(1, i + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                maxSoFar = -1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1-cdf(offer,order)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    else:\n",
    "                        res = (1-cdf(offer,order,ppl-1)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    if maxSoFar < res.item():\n",
    "                        maxSoFar = res.item()\n",
    "                        decision_H[i, ppl, money] = offerIndex\n",
    "                dp_H[i, ppl, money] = maxSoFar\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loc 0.2 scale 0.1\n",
      "loc 0.6 scale 0.1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dp 1.75\n",
      "Supervised Aim: U-exponential dp\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 0.029628\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 0.000421\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 0.000164\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 0.000016\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 0.000007\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 0.000004\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 0.000003\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 0.000186\n",
      "NN 1 : tensor(1.7640)\n",
      "CS 1 : 1.87356\n",
      "DP 1 : 1.75084\n",
      "heuristic 1 : 2.00504\n",
      "DP: 1.75\n",
      "tensor([0.4725, 0.5191, 0.0084])\n",
      "tensor([0.4713, 0.5287, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 1.761305 testing loss: tensor(1.7631)\n",
      "Train Epoch: 1 [640/25000 (3%)]\tLoss: 1.795808 testing loss: tensor(1.7577)\n",
      "Train Epoch: 1 [1280/25000 (5%)]\tLoss: 1.696913 testing loss: tensor(1.7568)\n",
      "Train Epoch: 1 [1920/25000 (8%)]\tLoss: 1.815310 testing loss: tensor(1.7537)\n",
      "Train Epoch: 1 [2560/25000 (10%)]\tLoss: 1.817717 testing loss: tensor(1.7528)\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 1.846019 testing loss: tensor(1.7525)\n",
      "Train Epoch: 1 [3840/25000 (15%)]\tLoss: 1.728466 testing loss: tensor(1.7528)\n",
      "Train Epoch: 1 [4480/25000 (18%)]\tLoss: 1.719480 testing loss: tensor(1.7520)\n",
      "Train Epoch: 1 [5120/25000 (20%)]\tLoss: 1.831113 testing loss: tensor(1.7520)\n",
      "Train Epoch: 1 [5760/25000 (23%)]\tLoss: 1.615688 testing loss: tensor(1.7508)\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 1.829079 testing loss: tensor(1.7520)\n",
      "Train Epoch: 1 [7040/25000 (28%)]\tLoss: 1.790274 testing loss: tensor(1.7513)\n",
      "Train Epoch: 1 [7680/25000 (31%)]\tLoss: 1.787076 testing loss: tensor(1.7499)\n",
      "Train Epoch: 1 [8320/25000 (33%)]\tLoss: 1.663243 testing loss: tensor(1.7489)\n",
      "Train Epoch: 1 [8960/25000 (36%)]\tLoss: 1.883022 testing loss: tensor(1.7497)\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 1.739806 testing loss: tensor(1.7497)\n",
      "Train Epoch: 1 [10240/25000 (41%)]\tLoss: 1.847839 testing loss: tensor(1.7496)\n",
      "Train Epoch: 1 [10880/25000 (43%)]\tLoss: 1.698259 testing loss: tensor(1.7498)\n",
      "Train Epoch: 1 [11520/25000 (46%)]\tLoss: 1.811121 testing loss: tensor(1.7495)\n",
      "Train Epoch: 1 [12160/25000 (48%)]\tLoss: 1.699053 testing loss: tensor(1.7498)\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 1.731559 testing loss: tensor(1.7506)\n",
      "Train Epoch: 1 [13440/25000 (54%)]\tLoss: 1.785331 testing loss: tensor(1.7502)\n",
      "Train Epoch: 1 [14080/25000 (56%)]\tLoss: 1.924088 testing loss: tensor(1.7504)\n",
      "Train Epoch: 1 [14720/25000 (59%)]\tLoss: 1.714770 testing loss: tensor(1.7499)\n",
      "Train Epoch: 1 [15360/25000 (61%)]\tLoss: 1.725836 testing loss: tensor(1.7496)\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 1.875518 testing loss: tensor(1.7514)\n",
      "Train Epoch: 1 [16640/25000 (66%)]\tLoss: 1.846542 testing loss: tensor(1.7506)\n",
      "Train Epoch: 1 [17280/25000 (69%)]\tLoss: 1.803715 testing loss: tensor(1.7489)\n",
      "Train Epoch: 1 [17920/25000 (71%)]\tLoss: 1.721419 testing loss: tensor(1.7484)\n",
      "Train Epoch: 1 [18560/25000 (74%)]\tLoss: 1.807346 testing loss: tensor(1.7482)\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 1.733222 testing loss: tensor(1.7481)\n",
      "Train Epoch: 1 [19840/25000 (79%)]\tLoss: 1.881442 testing loss: tensor(1.7478)\n",
      "Train Epoch: 1 [20480/25000 (82%)]\tLoss: 1.838450 testing loss: tensor(1.7483)\n",
      "Train Epoch: 1 [21120/25000 (84%)]\tLoss: 1.812878 testing loss: tensor(1.7484)\n",
      "Train Epoch: 1 [21760/25000 (87%)]\tLoss: 1.755947 testing loss: tensor(1.7493)\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 1.690333 testing loss: tensor(1.7497)\n",
      "Train Epoch: 1 [23040/25000 (92%)]\tLoss: 1.836886 testing loss: tensor(1.7490)\n",
      "Train Epoch: 1 [23680/25000 (94%)]\tLoss: 1.758096 testing loss: tensor(1.7477)\n",
      "Train Epoch: 1 [24320/25000 (97%)]\tLoss: 1.773524 testing loss: tensor(1.7487)\n",
      "Train Epoch: 1 [7800/25000 (99%)]\tLoss: 1.567120 testing loss: tensor(1.7490)\n",
      "penalty: 3.0517578125e-05\n",
      "NN 2 : tensor(1.7490)\n",
      "CS 2 : 1.87356\n",
      "DP 2 : 1.75084\n",
      "heuristic 2 : 2.00504\n",
      "DP: 1.75\n",
      "tensor([3.5397e-01, 6.4591e-01, 1.2484e-04])\n",
      "tensor([0.3487, 0.6513, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: U-exponential random initializing\n",
      "do nothing\n",
      "NN 1 : tensor(1.8700)\n",
      "CS 1 : 1.87356\n",
      "DP 1 : 1.75084\n",
      "heuristic 1 : 2.00504\n",
      "DP: 1.75\n",
      "tensor([0.3730, 0.4126, 0.2144])\n",
      "tensor([0.4903, 0.5097, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 1.898301 testing loss: tensor(1.8675)\n",
      "Train Epoch: 1 [640/25000 (3%)]\tLoss: 1.838648 testing loss: tensor(1.8513)\n",
      "Train Epoch: 1 [1280/25000 (5%)]\tLoss: 1.811143 testing loss: tensor(1.8240)\n",
      "Train Epoch: 1 [1920/25000 (8%)]\tLoss: 1.854812 testing loss: tensor(1.8128)\n",
      "Train Epoch: 1 [2560/25000 (10%)]\tLoss: 1.737031 testing loss: tensor(1.8014)\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 1.921515 testing loss: tensor(1.7924)\n",
      "Train Epoch: 1 [3840/25000 (15%)]\tLoss: 1.764344 testing loss: tensor(1.7840)\n",
      "Train Epoch: 1 [4480/25000 (18%)]\tLoss: 1.797988 testing loss: tensor(1.7765)\n",
      "Train Epoch: 1 [5120/25000 (20%)]\tLoss: 1.673506 testing loss: tensor(1.7728)\n",
      "Train Epoch: 1 [5760/25000 (23%)]\tLoss: 1.732028 testing loss: tensor(1.7707)\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 1.775064 testing loss: tensor(1.7656)\n",
      "Train Epoch: 1 [7040/25000 (28%)]\tLoss: 1.788118 testing loss: tensor(1.7661)\n",
      "Train Epoch: 1 [7680/25000 (31%)]\tLoss: 1.870809 testing loss: tensor(1.7640)\n",
      "Train Epoch: 1 [8320/25000 (33%)]\tLoss: 1.766176 testing loss: tensor(1.7634)\n",
      "Train Epoch: 1 [8960/25000 (36%)]\tLoss: 1.877578 testing loss: tensor(1.7647)\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 1.745279 testing loss: tensor(1.7618)\n",
      "Train Epoch: 1 [10240/25000 (41%)]\tLoss: 1.794078 testing loss: tensor(1.7589)\n",
      "Train Epoch: 1 [10880/25000 (43%)]\tLoss: 1.854310 testing loss: tensor(1.7563)\n",
      "Train Epoch: 1 [11520/25000 (46%)]\tLoss: 1.605073 testing loss: tensor(1.7564)\n",
      "Train Epoch: 1 [12160/25000 (48%)]\tLoss: 1.743421 testing loss: tensor(1.7586)\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 1.743193 testing loss: tensor(1.7584)\n",
      "Train Epoch: 1 [13440/25000 (54%)]\tLoss: 1.739565 testing loss: tensor(1.7569)\n",
      "Train Epoch: 1 [14080/25000 (56%)]\tLoss: 1.631766 testing loss: tensor(1.7565)\n",
      "Train Epoch: 1 [14720/25000 (59%)]\tLoss: 1.714460 testing loss: tensor(1.7570)\n",
      "Train Epoch: 1 [15360/25000 (61%)]\tLoss: 1.797165 testing loss: tensor(1.7544)\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 1.702013 testing loss: tensor(1.7532)\n",
      "Train Epoch: 1 [16640/25000 (66%)]\tLoss: 1.807371 testing loss: tensor(1.7522)\n",
      "Train Epoch: 1 [17280/25000 (69%)]\tLoss: 1.895519 testing loss: tensor(1.7520)\n",
      "Train Epoch: 1 [17920/25000 (71%)]\tLoss: 1.654881 testing loss: tensor(1.7514)\n",
      "Train Epoch: 1 [18560/25000 (74%)]\tLoss: 1.715814 testing loss: tensor(1.7512)\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 1.709713 testing loss: tensor(1.7520)\n",
      "Train Epoch: 1 [19840/25000 (79%)]\tLoss: 1.744061 testing loss: tensor(1.7526)\n",
      "Train Epoch: 1 [20480/25000 (82%)]\tLoss: 1.762294 testing loss: tensor(1.7496)\n",
      "Train Epoch: 1 [21120/25000 (84%)]\tLoss: 1.814907 testing loss: tensor(1.7502)\n",
      "Train Epoch: 1 [21760/25000 (87%)]\tLoss: 1.712936 testing loss: tensor(1.7505)\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 1.711572 testing loss: tensor(1.7510)\n",
      "Train Epoch: 1 [23040/25000 (92%)]\tLoss: 1.837919 testing loss: tensor(1.7516)\n",
      "Train Epoch: 1 [23680/25000 (94%)]\tLoss: 1.647360 testing loss: tensor(1.7537)\n",
      "Train Epoch: 1 [24320/25000 (97%)]\tLoss: 1.774174 testing loss: tensor(1.7535)\n",
      "Train Epoch: 1 [7800/25000 (99%)]\tLoss: 1.679461 testing loss: tensor(1.7533)\n",
      "penalty: 0.0002009570598602295\n",
      "NN 2 : tensor(1.7533)\n",
      "CS 2 : 1.87356\n",
      "DP 2 : 1.75084\n",
      "heuristic 2 : 2.00504\n",
      "DP: 1.75\n",
      "tensor([2.6229e-01, 7.3736e-01, 3.4554e-04])\n",
      "tensor([0.2665, 0.7335, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: U-exponential costsharing\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 0.000956\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 0.000004\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 0.000000\n",
      "NN 1 : tensor(1.8736)\n",
      "CS 1 : 1.87356\n",
      "DP 1 : 1.75084\n",
      "heuristic 1 : 2.00504\n",
      "DP: 1.75\n",
      "tensor([0.3333, 0.3333, 0.3333])\n",
      "tensor([0.5000, 0.5000, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 1.902843 testing loss: tensor(1.8736)\n",
      "Train Epoch: 1 [640/25000 (3%)]\tLoss: 1.875403 testing loss: tensor(1.8736)\n",
      "Train Epoch: 1 [1280/25000 (5%)]\tLoss: 1.842781 testing loss: tensor(1.8732)\n",
      "Train Epoch: 1 [1920/25000 (8%)]\tLoss: 1.884376 testing loss: tensor(1.8729)\n",
      "Train Epoch: 1 [2560/25000 (10%)]\tLoss: 1.881346 testing loss: tensor(1.8722)\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 1.963458 testing loss: tensor(1.8709)\n",
      "Train Epoch: 1 [3840/25000 (15%)]\tLoss: 1.878569 testing loss: tensor(1.8692)\n",
      "Train Epoch: 1 [4480/25000 (18%)]\tLoss: 1.814924 testing loss: tensor(1.8672)\n",
      "Train Epoch: 1 [5120/25000 (20%)]\tLoss: 1.900679 testing loss: tensor(1.8653)\n",
      "Train Epoch: 1 [5760/25000 (23%)]\tLoss: 1.925096 testing loss: tensor(1.8612)\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 1.936640 testing loss: tensor(1.8538)\n",
      "Train Epoch: 1 [7040/25000 (28%)]\tLoss: 1.970709 testing loss: tensor(1.8454)\n",
      "Train Epoch: 1 [7680/25000 (31%)]\tLoss: 1.828910 testing loss: tensor(1.8368)\n",
      "Train Epoch: 1 [8320/25000 (33%)]\tLoss: 1.656823 testing loss: tensor(1.8266)\n",
      "Train Epoch: 1 [8960/25000 (36%)]\tLoss: 1.782034 testing loss: tensor(1.8190)\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 1.905980 testing loss: tensor(1.8167)\n",
      "Train Epoch: 1 [10240/25000 (41%)]\tLoss: 1.800889 testing loss: tensor(1.8145)\n",
      "Train Epoch: 1 [10880/25000 (43%)]\tLoss: 1.815711 testing loss: tensor(1.8112)\n",
      "Train Epoch: 1 [11520/25000 (46%)]\tLoss: 1.851717 testing loss: tensor(1.8076)\n",
      "Train Epoch: 1 [12160/25000 (48%)]\tLoss: 1.878421 testing loss: tensor(1.8076)\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 1.778206 testing loss: tensor(1.8042)\n",
      "Train Epoch: 1 [13440/25000 (54%)]\tLoss: 1.707058 testing loss: tensor(1.8029)\n",
      "Train Epoch: 1 [14080/25000 (56%)]\tLoss: 1.708095 testing loss: tensor(1.8039)\n",
      "Train Epoch: 1 [14720/25000 (59%)]\tLoss: 1.795511 testing loss: tensor(1.8035)\n",
      "Train Epoch: 1 [15360/25000 (61%)]\tLoss: 1.753497 testing loss: tensor(1.7996)\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 1.786574 testing loss: tensor(1.7994)\n",
      "Train Epoch: 1 [16640/25000 (66%)]\tLoss: 1.790522 testing loss: tensor(1.7984)\n",
      "Train Epoch: 1 [17280/25000 (69%)]\tLoss: 1.907612 testing loss: tensor(1.7987)\n",
      "Train Epoch: 1 [17920/25000 (71%)]\tLoss: 1.821848 testing loss: tensor(1.7983)\n",
      "Train Epoch: 1 [18560/25000 (74%)]\tLoss: 1.925995 testing loss: tensor(1.7981)\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 1.833206 testing loss: tensor(1.7975)\n",
      "Train Epoch: 1 [19840/25000 (79%)]\tLoss: 1.761712 testing loss: tensor(1.7972)\n",
      "Train Epoch: 1 [20480/25000 (82%)]\tLoss: 1.936816 testing loss: tensor(1.7968)\n",
      "Train Epoch: 1 [21120/25000 (84%)]\tLoss: 1.779901 testing loss: tensor(1.7975)\n",
      "Train Epoch: 1 [21760/25000 (87%)]\tLoss: 1.948398 testing loss: tensor(1.7968)\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 1.862613 testing loss: tensor(1.7970)\n",
      "Train Epoch: 1 [23040/25000 (92%)]\tLoss: 1.833265 testing loss: tensor(1.7976)\n",
      "Train Epoch: 1 [23680/25000 (94%)]\tLoss: 1.855254 testing loss: tensor(1.7974)\n",
      "Train Epoch: 1 [24320/25000 (97%)]\tLoss: 1.725047 testing loss: tensor(1.7976)\n",
      "Train Epoch: 1 [7800/25000 (99%)]\tLoss: 1.937690 testing loss: tensor(1.7988)\n",
      "penalty: 0.010898947715759277\n",
      "NN 2 : tensor(1.7988)\n",
      "CS 2 : 1.87356\n",
      "DP 2 : 1.75084\n",
      "heuristic 2 : 2.00504\n",
      "DP: 1.75\n",
      "tensor([0.0416, 0.8630, 0.0953])\n",
      "tensor([0.1400, 0.8600, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: U-exponential heuristic\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 0.021317\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 0.000046\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 0.000003\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 0.000005\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 0.000003\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 0.000008\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 0.000003\n",
      "NN 1 : tensor(1.9374)\n",
      "CS 1 : 1.87356\n",
      "DP 1 : 1.75084\n",
      "heuristic 1 : 2.00504\n",
      "DP: 1.75\n",
      "tensor([0.0028, 0.4888, 0.5084])\n",
      "tensor([0.4914, 0.5086, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 1.910301 testing loss: tensor(1.9265)\n",
      "Train Epoch: 1 [640/25000 (3%)]\tLoss: 1.798262 testing loss: tensor(1.8843)\n",
      "Train Epoch: 1 [1280/25000 (5%)]\tLoss: 1.811873 testing loss: tensor(1.8386)\n",
      "Train Epoch: 1 [1920/25000 (8%)]\tLoss: 1.770400 testing loss: tensor(1.8083)\n",
      "Train Epoch: 1 [2560/25000 (10%)]\tLoss: 1.866476 testing loss: tensor(1.8035)\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 1.935341 testing loss: tensor(1.8012)\n",
      "Train Epoch: 1 [3840/25000 (15%)]\tLoss: 1.845200 testing loss: tensor(1.7968)\n",
      "Train Epoch: 1 [4480/25000 (18%)]\tLoss: 1.804398 testing loss: tensor(1.7913)\n",
      "Train Epoch: 1 [5120/25000 (20%)]\tLoss: 1.757506 testing loss: tensor(1.7851)\n",
      "Train Epoch: 1 [5760/25000 (23%)]\tLoss: 1.761093 testing loss: tensor(1.7805)\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 1.737833 testing loss: tensor(1.7756)\n",
      "Train Epoch: 1 [7040/25000 (28%)]\tLoss: 1.728519 testing loss: tensor(1.7725)\n",
      "Train Epoch: 1 [7680/25000 (31%)]\tLoss: 1.799719 testing loss: tensor(1.7717)\n",
      "Train Epoch: 1 [8320/25000 (33%)]\tLoss: 1.655718 testing loss: tensor(1.7682)\n",
      "Train Epoch: 1 [8960/25000 (36%)]\tLoss: 1.820241 testing loss: tensor(1.7685)\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 1.696153 testing loss: tensor(1.7667)\n",
      "Train Epoch: 1 [10240/25000 (41%)]\tLoss: 1.927515 testing loss: tensor(1.7668)\n",
      "Train Epoch: 1 [10880/25000 (43%)]\tLoss: 1.764321 testing loss: tensor(1.7646)\n",
      "Train Epoch: 1 [11520/25000 (46%)]\tLoss: 1.834472 testing loss: tensor(1.7633)\n",
      "Train Epoch: 1 [12160/25000 (48%)]\tLoss: 1.778222 testing loss: tensor(1.7649)\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 1.685301 testing loss: tensor(1.7643)\n",
      "Train Epoch: 1 [13440/25000 (54%)]\tLoss: 1.777867 testing loss: tensor(1.7636)\n",
      "Train Epoch: 1 [14080/25000 (56%)]\tLoss: 1.831087 testing loss: tensor(1.7629)\n",
      "Train Epoch: 1 [14720/25000 (59%)]\tLoss: 1.736634 testing loss: tensor(1.7624)\n",
      "Train Epoch: 1 [15360/25000 (61%)]\tLoss: 1.737597 testing loss: tensor(1.7618)\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 1.753331 testing loss: tensor(1.7618)\n",
      "Train Epoch: 1 [16640/25000 (66%)]\tLoss: 1.725031 testing loss: tensor(1.7606)\n",
      "Train Epoch: 1 [17280/25000 (69%)]\tLoss: 1.887213 testing loss: tensor(1.7609)\n",
      "Train Epoch: 1 [17920/25000 (71%)]\tLoss: 1.622099 testing loss: tensor(1.7601)\n",
      "Train Epoch: 1 [18560/25000 (74%)]\tLoss: 1.700626 testing loss: tensor(1.7596)\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 1.800140 testing loss: tensor(1.7590)\n",
      "Train Epoch: 1 [19840/25000 (79%)]\tLoss: 1.642210 testing loss: tensor(1.7576)\n",
      "Train Epoch: 1 [20480/25000 (82%)]\tLoss: 1.746218 testing loss: tensor(1.7574)\n",
      "Train Epoch: 1 [21120/25000 (84%)]\tLoss: 1.796911 testing loss: tensor(1.7567)\n",
      "Train Epoch: 1 [21760/25000 (87%)]\tLoss: 1.789431 testing loss: tensor(1.7568)\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 1.651911 testing loss: tensor(1.7572)\n",
      "Train Epoch: 1 [23040/25000 (92%)]\tLoss: 1.828046 testing loss: tensor(1.7570)\n",
      "Train Epoch: 1 [23680/25000 (94%)]\tLoss: 1.829885 testing loss: tensor(1.7563)\n",
      "Train Epoch: 1 [24320/25000 (97%)]\tLoss: 1.724223 testing loss: tensor(1.7568)\n",
      "Train Epoch: 1 [7800/25000 (99%)]\tLoss: 1.892199 testing loss: tensor(1.7556)\n",
      "penalty: 0.0004207193851470947\n",
      "NN 2 : tensor(1.7556)\n",
      "CS 2 : 1.87356\n",
      "DP 2 : 1.75084\n",
      "heuristic 2 : 2.00504\n",
      "DP: 1.75\n",
      "tensor([0.0010, 0.7613, 0.2377])\n",
      "tensor([0.2222, 0.7778, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.1)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "    \n",
    "for order in stage:\n",
    "    producedata(order)\n",
    "    #running\n",
    "    for order1 in order1name:\n",
    "        print(\"Supervised Aim:\",order,order1)\n",
    "        \n",
    "        # for mapping binary to payments before softmax\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(n, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, n),\n",
    "        )\n",
    "        model.apply(init_weights)\n",
    "        # optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr1)\n",
    "        \n",
    "        runningLossNN = []\n",
    "        runningLossCS = []\n",
    "        runningLossDP = []\n",
    "        runningLossHeuristic = []\n",
    "        #model=torch.load(\"save/pytorchNN=5dp1\");\n",
    "        #model.eval()\n",
    "        ##order1name=[\"costsharing\",\"dp\",\"heuristic\",\"random initializing\"]\n",
    "        for epoch in range(1, supervisionEpochs + 1):\n",
    "#             print(\"distributionRatio\",distributionRatio)\n",
    "            if(order1==\"costsharing\"):\n",
    "                supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "            elif(order1==\"dp\"):\n",
    "                supervisionTrain(epoch, dpSupervisionRule)\n",
    "            elif(order1==\"heuristic\"):\n",
    "                supervisionTrain(epoch, heuristicSupervisionRule)\n",
    "            elif(order1==\"random initializing\"):\n",
    "                print(\"do nothing\");\n",
    "\n",
    "        test()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr2)\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(epoch)\n",
    "            test()\n",
    "        losslistname.append(order+\" \"+order1);\n",
    "        losslist.append(losslisttemp);\n",
    "        losslisttemp=[];\n",
    "        savepath=\"save/pytorchNN=5all-beta\"+order+str(order1)\n",
    "        torch.save(model, savepath);\n",
    "        print(\"end\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colorlist=[\"#D2691E\",'#4169E1',\"#9ACD32\",\"#B22222\",\"#FF00FF\",\"#708090\"]\n",
    "for i in range(len(losslist)):\n",
    "    plt.plot(losslist[i], '.-', color=colorlist[i], alpha=0.8, label=losslistname[i])\n",
    "\n",
    "# 显示标签，如果不加这句，即使加了label='一些数字'的参数，最终还是不会显示标签\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('Trianing Times')\n",
    "plt.ylabel('Delay')\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('pytorchNN=3_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
