{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40058530825361593\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from random import randint\n",
    "import random\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "from pylab import *\n",
    "from scipy.stats import beta\n",
    "import math\n",
    "n = 10\n",
    "epochs = 1\n",
    "supervisionEpochs = 1\n",
    "lr1 = 0.01\n",
    "lr2 = 0.001\n",
    "log_interval = 10\n",
    "trainSize = 80000#100000\n",
    "percentage_train_test= 0.75\n",
    "penaltyLambda = 100\n",
    "doublePeakHighMean = 0.85\n",
    "doublePeakLowMean = 0.15\n",
    "doublePeakStd = 0.1\n",
    "uniformlow=0\n",
    "uniformhigh=1.0\n",
    "normalloc = 0.2\n",
    "normalscale = 0.1\n",
    "\n",
    "cauchyloc = 1/n\n",
    "cauchyscalen = 0.004\n",
    "\n",
    "exponentialhigh = 15 #Symbol(\"b\", real=True)\n",
    "exponentiallow  = 15 #Symbol(\"a\", real=True)\n",
    "\n",
    "\n",
    "beta_a = 0.3\n",
    "beta_b  = 0.2\n",
    "kumaraswamy_a = beta_a \n",
    "kumaraswamy_b = (1.0+(beta_a-1.0)*math.pow( (beta_a+beta_b-2.0)/(beta_a-1.0), beta_a) )/beta_a \n",
    "print(kumaraswamy_b)\n",
    "\n",
    "\n",
    "independentnormalloc1=[(float(ii)+1)/(2*n+1) for ii in range(n,0,-1)]\n",
    "independentnormalscale1=[0.05 for ii in range(n)]\n",
    "\n",
    "independentnormalloc2=[(float(ii)+1)/(2*n+1) for ii in range(1,n+1,1)]\n",
    "independentnormalscale2=[0.05 for ii in range(n)]\n",
    "stage=[\"twopeak\"]\n",
    "# \"twopeak\",\"normal\",\"uniform\",\"independent1\",\"independent2\",\"cauchy\",\"beta\",\"U-exponential\",\"arcsine\"\n",
    "order1name=[\"dp\",\"random initializing\",\"costsharing\",\"heuristic\"]\n",
    "# \"costsharing\",\"dp\",\"heuristic\",\"random initializing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio1 = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase1 = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "d3 = D.normal.Normal(normalloc, normalscale)\n",
    "distributionRatio3 = d3.cdf(1) - d3.cdf(0)\n",
    "distributionBase3 = d3.cdf(0)\n",
    "\n",
    "d4 = D.uniform.Uniform(uniformlow,uniformhigh)\n",
    "distributionRatio4 = d4.cdf(1) - d4.cdf(0)\n",
    "distributionBase4 = d4.cdf(0)\n",
    "\n",
    "d5 = [D.normal.Normal(independentnormalloc1[ii], independentnormalscale1[ii]) for ii in range(n)]\n",
    "d6 = [D.normal.Normal(independentnormalloc2[ii], independentnormalscale2[ii]) for ii in range(n)]\n",
    "\n",
    "d7 = D.cauchy.Cauchy(cauchyloc,cauchyscalen)\n",
    "\n",
    "d81 = D.exponential.Exponential(exponentiallow)\n",
    "d82 = D.exponential.Exponential(exponentialhigh)\n",
    "\n",
    "d9 = D.beta.Beta(beta_a,beta_b)\n",
    "#sample_d9=d9.rsample(torch.Size([100000]))\n",
    "\n",
    "#d9_sample=np.linspace(0.0001, 0.9999, 10000)\n",
    "#d9_pdf=torch.exp(d9.log_prob(torch.tensor(d9_sample,dtype=torch.float32)))\n",
    "#d9_delta=d9_sample[1]-d9_sample[0]\n",
    "#d9_sum_pdf=torch.sum(d9_pdf*d9_delta)\n",
    "\n",
    "d10 = D.beta.Beta(0.5,0.5)\n",
    "\n",
    "def cdf(x,y, i=None):\n",
    "    if(y==\"twopeak\"):\n",
    "        return (d1.cdf(x) + d2.cdf(x) - distributionBase1) / 2 / distributionRatio1\n",
    "    elif(y==\"normal\"):\n",
    "        return (d3.cdf(x)-distributionBase3)/distributionRatio3;\n",
    "    elif(y==\"uniform\"):\n",
    "        return (d4.cdf(x)-distributionBase4)/distributionRatio4;\n",
    "    elif(y==\"independent1\"):\n",
    "        return d5[i].cdf(x);\n",
    "    elif(y==\"independent2\"):\n",
    "        return d6[i].cdf(x);\n",
    "    elif(y==\"cauchy\"):\n",
    "        return d7.cdf(x);\n",
    "    elif(y==\"beta\"):\n",
    "#         sum_cdf=0.0;\n",
    "#         if(x<0.0001):\n",
    "#             x=0.00011;\n",
    "#         if(x>0.9999):\n",
    "#             x=0.99989;\n",
    "#         for i in range(len(d9_pdf)):\n",
    "#             if(d9_sample[i]<x):\n",
    "#                 sum_cdf+=d9_pdf[i]*d9_delta;\n",
    "#             else:\n",
    "#                 sum_cdf+=(d9_pdf[i]+d9_pdf[i-1])/ 2 *(x-d9_sample[i-1])\n",
    "#                 break;\n",
    "#         return sum_cdf/d9_sum_pdf\n",
    "#         cdf_v=torch.sum((sample_d9<(x)), dtype=torch.float32)/100000\n",
    "#         return cdf_v\n",
    "#    F(x|a,b)=1–(1–x^a)^b\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(x,kumaraswamy_a),kumaraswamy_b);\n",
    "        except:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(torch.tensor(x,dtype=torch.float32),kumaraswamy_a),kumaraswamy_b);\n",
    "    elif(y==\"arcsine\"):\n",
    "        #\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            res=2.0/math.pi * torch.asin(torch.sqrt(x))\n",
    "            #print(x)\n",
    "            return res# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "        except:\n",
    "            return 2.0/math.pi * torch.asin(torch.sqrt(torch.tensor(x,dtype=torch.float32)))# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "    elif(y==\"U-exponential\"):\n",
    "        return (d81.cdf(x) + (1.0 - d82.cdf(1.0-x)))  / 2 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    if torch.sum(bits).item() == 0:\n",
    "        return torch.ones(n)\n",
    "    bits = bits.type(torch.float32)\n",
    "    negBits = torch.ones(n) - bits\n",
    "    payments = model(bits)\n",
    "    payments = payments - 1000 * negBits\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    payments = payments + negBits\n",
    "    return payments\n",
    "\n",
    "def tpToBits(tp, bits=torch.ones(n).type(torch.uint8)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = (tp >= payments).type(torch.uint8)\n",
    "    if torch.equal(newBits, bits):\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits(tp, newBits)#bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    return n - torch.sum(tpToBits(tp).type(torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    global  dp,decision\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money, yes]\n",
    "        offer = float(offerIndex) / dpPrecision\n",
    "        if tp[i] >= offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits[i] = 0\n",
    "            payments[i] = 1\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "#     print()\n",
    "#     print(tp)\n",
    "#     print(bits)\n",
    "#     print(payments)\n",
    "#     print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def heuristicSupervisionRule(tp):\n",
    "    global  dp_H,decision_H\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    tempold=-1;\n",
    "    for turns in range(n,0,-1):\n",
    "        money = dpPrecision\n",
    "        j=0\n",
    "        tempo=sum(bits)\n",
    "        #print(\"bits\",tempo)\n",
    "        for i in range(n):\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            \n",
    "            offerIndex = decision_H[tempo ,tempo - i, money]\n",
    "            offer = float(offerIndex) / dpPrecision\n",
    "            while(j<n):\n",
    "                if(bits[j]!=0):\n",
    "                    break;\n",
    "                j+=1;\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            if tp[j] >= offer:\n",
    "                #print(money,j,tp[j],offer)\n",
    "                money -= offerIndex\n",
    "                bits[j] = 1\n",
    "                payments[j] = offer\n",
    "            else:\n",
    "                bits[j] = 0;\n",
    "                payments[j] = 1.0;\n",
    "            j+=1;\n",
    "        #print(\"money\",money)\n",
    "        if(money==0 and tempold==tempo):\n",
    "            break;\n",
    "        tempold=tempo;\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    for k in range(n, -1, -1):\n",
    "        if k == 0:\n",
    "            break\n",
    "        bits = [1 if tp[ii] >= 1.0 / k else 0 for ii in range(n)]\n",
    "        if sum(bits) == k:\n",
    "            break\n",
    "    if k == 0:\n",
    "        payments = [1 for ii in range(n)]\n",
    "    else:\n",
    "        payments = [1.0 / k if bits[ii] == 1 else 1 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    return float(n - torch.sum(costSharingSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def dpDelay(tp):\n",
    "    return float(n - torch.sum(dpSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def heuristicDelay(tp):\n",
    "    return float(n - torch.sum(heuristicSupervisionRule(tp)[0]).item())\n",
    "\n",
    "\n",
    "# templire=0;\n",
    "# num=1\n",
    "# for i in range(num):\n",
    "#     temp=torch.tensor(samplesJoint[i])\n",
    "#     #print(temp)\n",
    "#     #print(temp);\n",
    "#     #print(heuristicSupervisionRule(temp))\n",
    "#     #print(dpSupervisionRule(temp))\n",
    "#     #print(dpDelay(temp))\n",
    "#     res=dpDelay(temp)\n",
    "#     templire+=res\n",
    "#     #print(\"Delay\",res)\n",
    "#     #print()\n",
    "# print(templire/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordAndReport(name, source, loss):\n",
    "    source.append(loss)\n",
    "    realLength = len(source)\n",
    "    #print(f\"{name} ({realLength}): {loss}\")\n",
    "    print(name,realLength,\":\" ,loss)\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    \n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda /100000\n",
    "        \n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % (log_interval*5) == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "losslist=[];\n",
    "losslistname=[];\n",
    "losslisttemp=[];\n",
    "def test_batch():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lenLoss= 0\n",
    "        nnLoss = 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "            for tp in tp_batch:\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        nnLoss/=lenLoss\n",
    "    return nnLoss\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                Delay1 = tpToTotalDelay(tp1)\n",
    "                Delay0 = tpToTotalDelay(tp0)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                    loss = loss + (1 - cdf(offer,order)) * Delay1 + cdf(offer,order) * Delay0\n",
    "                else:\n",
    "                    loss = loss + (1 - cdf(offer,order,i)) * Delay1 + cdf(offer,order,i) * Delay0\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "    \n",
    "allBits = [torch.tensor(bits, dtype=torch.int16) for bits in itertools.product([0, 1], repeat=n)]\n",
    "\n",
    "def test():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        costSharingLoss = 0\n",
    "        dpLoss = 0\n",
    "        nnLoss = 0\n",
    "        heuristicLoss=0\n",
    "        lenLoss= 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "                heuristicLoss+= heuristicDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        costSharingLoss /= lenLoss\n",
    "        dpLoss /= lenLoss\n",
    "        nnLoss /= lenLoss\n",
    "        heuristicLoss /= lenLoss\n",
    "        #print(lenLoss)\n",
    "        losslisttemp.append(nnLoss);\n",
    "        recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "        recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "        recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "        recordAndReport(\"heuristic\", runningLossHeuristic, heuristicLoss)\n",
    "        print(\"DP:\",dp[n, dpPrecision,0])\n",
    "        #for i in range(n, 0, -1):\n",
    "        #    print(\"Heuristic:\",i,5*(1-dp_H[i, i, dpPrecision]))\n",
    "        for i in range(n, 0, -1):\n",
    "            print(\n",
    "                    tpToPayments(\n",
    "                            torch.tensor([0 if ii >= i else 1 for ii in range(n)], dtype=torch.float32)\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "dpPrecision = 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def producedata(order):\n",
    "    global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "    dpPrecision = 100 \n",
    "    if(order==\"twopeak\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = np.random.normal(\n",
    "            loc=doublePeakLowMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=doublePeakLowMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samples2 = np.random.normal(\n",
    "            loc=doublePeakHighMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = np.random.normal(\n",
    "                        loc=doublePeakHighMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "    elif(order==\"normal\"):\n",
    "        print(\"loc\",normalloc, \"scale\",normalscale)\n",
    "        samples1 = np.random.normal(\n",
    "            loc=normalloc, scale=normalscale, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=normalloc, scale=normalscale\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"uniform\"):  \n",
    "        print(\"uniformlow\",uniformlow, \"uniformhigh\",uniformhigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        uniformlow, uniformhigh\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent1\"):\n",
    "        print(\"loc\",independentnormalloc1,\"scale\",independentnormalscale1)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent2\"):\n",
    "        print(\"loc\",independentnormalloc2, \"scale\",independentnormalscale2)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"cauchy\"):\n",
    "        print(\"cauchyloc\",cauchyloc, \"cauchyscale\",cauchyscalen)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"beta\"):\n",
    "        print(\"beta_a\",beta_a, \"beta_b\",beta_b)\n",
    "        print(\"kumaraswamy_a\",kumaraswamy_a, \"kumaraswamy_b\",kumaraswamy_b)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"arcsine\"):\n",
    "        print(\"betalow\",0.5, \"betahigh\",0.5)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"U-exponential\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = d81.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d81.rsample(torch.Size([1])).numpy()\n",
    "                    \n",
    "        samples2 = d82.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        \n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = d82.rsample(torch.Size([1])).numpy()\n",
    "        samples2 = 1.0 - samples2\n",
    "        samplesJoint = signals * samples1 - (signals - 1.0) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "        \n",
    "    plt.hist(samplesJoint,bins=500)\n",
    "    plt.show()\n",
    "    \n",
    "    tp_dataset = TensorDataset(tp_tensor[: int(trainSize * percentage_train_test)])\n",
    "    tp_dataset_testing = TensorDataset(tp_tensor[int(trainSize * (1.0-percentage_train_test)) :])\n",
    "    tp_dataloader = DataLoader(tp_dataset, batch_size=128, shuffle=True)\n",
    "    tp_dataloader_testing = DataLoader(tp_dataset_testing, batch_size=256, shuffle=False)\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "    decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for yes in range(n + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp[0, 0, yes] = 0\n",
    "            else:\n",
    "                dp[0, money, yes] = yes# + 1.0\n",
    "    for ppl in range(1,  n + 1):\n",
    "        for yes in range(n + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                minSoFar = 1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1 - cdf(offer,order)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order) * (1 + dp[ppl - 1, money, yes])\n",
    "                    else:\n",
    "                        res = (1 - cdf(offer,order,n-ppl)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order,n-ppl) * (1 + dp[ppl - 1, money, yes])\n",
    "                    if minSoFar > res.item():\n",
    "                        minSoFar = res.item()\n",
    "                        decision[ppl, money, yes] = offerIndex\n",
    "                dp[ppl, money, yes] = minSoFar\n",
    "    \n",
    "    print(\"dp\",dp[n, dpPrecision, 0])\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp_H = np.zeros([n + 1 , n + 1, dpPrecision + 1])\n",
    "    decision_H = np.zeros([n + 1 , n + 1, dpPrecision + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for i in range(1,n+1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp_H[i, 0, 0] = 1\n",
    "            else:\n",
    "                offer = money / dpPrecision\n",
    "                dp_H[i, 0, money] = 0#cdf(offer)# + 1.0\n",
    "    for i in range(1,n+1):\n",
    "        for ppl in range(1, i + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                maxSoFar = -1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1-cdf(offer,order)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    else:\n",
    "                        res = (1-cdf(offer,order,ppl-1)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    if maxSoFar < res.item():\n",
    "                        maxSoFar = res.item()\n",
    "                        decision_H[i, ppl, money] = offerIndex\n",
    "                dp_H[i, ppl, money] = maxSoFar\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loc 0.15 scale 0.1\n",
      "loc 0.85 scale 0.1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dp 1.4498810768127441\n",
      "Supervised Aim: twopeak dp\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.030408\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.000140\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.000044\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.000040\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.000031\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.000017\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.000033\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.000027\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.000017\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.000029\n",
      "NN 1 : tensor(1.4346)\n",
      "CS 1 : 1.7364666666666666\n",
      "DP 1 : 1.4480166666666667\n",
      "heuristic 1 : 1.8130666666666666\n",
      "DP: 1.4498810768127441\n",
      "tensor([0.6506, 0.0504, 0.0494, 0.0497, 0.0492, 0.0420, 0.0427, 0.0418, 0.0219,\n",
      "        0.0023])\n",
      "tensor([0.6504, 0.0507, 0.0501, 0.0501, 0.0502, 0.0418, 0.0425, 0.0419, 0.0223,\n",
      "        1.0000])\n",
      "tensor([0.6517, 0.0570, 0.0526, 0.0530, 0.0518, 0.0433, 0.0452, 0.0454, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.6754, 0.0593, 0.0560, 0.0574, 0.0560, 0.0461, 0.0497, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.6885, 0.0674, 0.0629, 0.0655, 0.0631, 0.0526, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.7101, 0.0828, 0.0700, 0.0706, 0.0664, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.7364, 0.0904, 0.0863, 0.0870, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.7959, 0.1021, 0.1020, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.8610, 0.1390, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 1.522579 testing loss: tensor(1.4387)\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 1.524366 testing loss: tensor(1.4426)\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 1.206467 testing loss: tensor(1.4313)\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 1.539000 testing loss: tensor(1.4285)\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 1.228362 testing loss: tensor(1.4266)\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.439896 testing loss: tensor(1.4268)\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.512169 testing loss: tensor(1.4264)\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.512889 testing loss: tensor(1.4257)\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.420119 testing loss: tensor(1.4246)\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.347842 testing loss: tensor(1.4256)\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.369856 testing loss: tensor(1.4238)\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 1.348565 testing loss: tensor(1.4244)\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 1.603280 testing loss: tensor(1.4246)\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 1.259299 testing loss: tensor(1.4250)\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 1.499425 testing loss: tensor(1.4229)\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.344981 testing loss: tensor(1.4220)\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 1.212098 testing loss: tensor(1.4206)\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 1.393642 testing loss: tensor(1.4224)\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 1.484937 testing loss: tensor(1.4197)\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 1.542742 testing loss: tensor(1.4199)\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.436989 testing loss: tensor(1.4186)\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 1.284581 testing loss: tensor(1.4192)\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 1.412308 testing loss: tensor(1.4184)\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 1.393536 testing loss: tensor(1.4180)\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 1.260231 testing loss: tensor(1.4176)\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.725525 testing loss: tensor(1.4172)\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 1.444397 testing loss: tensor(1.4166)\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 1.376480 testing loss: tensor(1.4161)\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 1.508884 testing loss: tensor(1.4165)\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 1.510823 testing loss: tensor(1.4156)\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.559735 testing loss: tensor(1.4160)\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 1.211116 testing loss: tensor(1.4158)\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 1.377318 testing loss: tensor(1.4152)\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 1.305266 testing loss: tensor(1.4144)\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 1.504095 testing loss: tensor(1.4153)\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 1.349253 testing loss: tensor(1.4145)\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 1.666662 testing loss: tensor(1.4138)\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 1.484691 testing loss: tensor(1.4128)\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 1.142671 testing loss: tensor(1.4144)\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 1.389216 testing loss: tensor(1.4144)\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.395083 testing loss: tensor(1.4133)\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 1.286981 testing loss: tensor(1.4137)\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 1.310577 testing loss: tensor(1.4138)\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 1.322764 testing loss: tensor(1.4126)\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 1.349107 testing loss: tensor(1.4138)\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.340700 testing loss: tensor(1.4127)\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 1.421073 testing loss: tensor(1.4129)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.4135)\n",
      "CS 2 : 1.7364666666666666\n",
      "DP 2 : 1.4480166666666667\n",
      "heuristic 2 : 1.8130666666666666\n",
      "DP: 1.4498810768127441\n",
      "tensor([0.6515, 0.0463, 0.0429, 0.0417, 0.0400, 0.0392, 0.0395, 0.0405, 0.0385,\n",
      "        0.0200])\n",
      "tensor([0.6604, 0.0484, 0.0441, 0.0434, 0.0417, 0.0403, 0.0407, 0.0415, 0.0395,\n",
      "        1.0000])\n",
      "tensor([0.6757, 0.0548, 0.0475, 0.0467, 0.0443, 0.0424, 0.0436, 0.0450, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.6931, 0.0614, 0.0508, 0.0514, 0.0486, 0.0460, 0.0487, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.7134, 0.0662, 0.0574, 0.0572, 0.0547, 0.0512, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.7322, 0.0836, 0.0645, 0.0618, 0.0579, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.7686, 0.0869, 0.0748, 0.0696, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.8251, 0.0922, 0.0826, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.8744, 0.1256, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: twopeak random initializing\n",
      "do nothing\n",
      "NN 1 : tensor(1.8706)\n",
      "CS 1 : 1.7364666666666666\n",
      "DP 1 : 1.4480166666666667\n",
      "heuristic 1 : 1.8130666666666666\n",
      "DP: 1.4498810768127441\n",
      "tensor([0.0716, 0.0799, 0.1694, 0.1128, 0.0937, 0.0553, 0.0722, 0.1257, 0.0747,\n",
      "        0.1447])\n",
      "tensor([0.0957, 0.0901, 0.2011, 0.1158, 0.1017, 0.0712, 0.0815, 0.1412, 0.1017,\n",
      "        1.0000])\n",
      "tensor([0.1162, 0.1064, 0.2144, 0.1423, 0.0968, 0.0745, 0.0892, 0.1603, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.1314, 0.1127, 0.2536, 0.1887, 0.1090, 0.0970, 0.1076, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.1524, 0.1281, 0.2649, 0.2046, 0.1274, 0.1225, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.1715, 0.1412, 0.3061, 0.2276, 0.1536, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.2373, 0.1893, 0.3313, 0.2421, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.2929, 0.2874, 0.4197, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.5295, 0.4705, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 1.906341 testing loss: tensor(1.8359)\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 1.621825 testing loss: tensor(1.7524)\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 1.817262 testing loss: tensor(1.7466)\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 1.771858 testing loss: tensor(1.7429)\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 1.632474 testing loss: tensor(1.7426)\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.637689 testing loss: tensor(1.7431)\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.763969 testing loss: tensor(1.7418)\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.643296 testing loss: tensor(1.7428)\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.837938 testing loss: tensor(1.7401)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.732286 testing loss: tensor(1.7438)\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.740256 testing loss: tensor(1.7391)\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 1.661216 testing loss: tensor(1.7415)\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 1.924168 testing loss: tensor(1.7402)\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 1.723010 testing loss: tensor(1.7409)\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 1.717871 testing loss: tensor(1.7452)\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.834373 testing loss: tensor(1.7448)\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 1.653609 testing loss: tensor(1.7405)\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 1.653601 testing loss: tensor(1.7425)\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 1.719774 testing loss: tensor(1.7424)\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 1.738782 testing loss: tensor(1.7422)\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.727636 testing loss: tensor(1.7445)\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 1.835597 testing loss: tensor(1.7442)\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 1.781594 testing loss: tensor(1.7402)\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 1.546573 testing loss: tensor(1.7414)\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 1.725257 testing loss: tensor(1.7401)\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.447921 testing loss: tensor(1.7431)\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 1.562520 testing loss: tensor(1.7408)\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 1.852030 testing loss: tensor(1.7400)\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 1.900138 testing loss: tensor(1.7395)\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 1.791512 testing loss: tensor(1.7425)\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.558615 testing loss: tensor(1.7443)\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 1.613472 testing loss: tensor(1.7406)\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 1.487956 testing loss: tensor(1.7418)\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 1.698817 testing loss: tensor(1.7410)\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 1.891102 testing loss: tensor(1.7431)\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 1.895383 testing loss: tensor(1.7441)\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 1.788925 testing loss: tensor(1.7391)\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 1.706584 testing loss: tensor(1.7381)\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 1.833432 testing loss: tensor(1.7427)\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 1.677278 testing loss: tensor(1.7397)\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.527202 testing loss: tensor(1.7401)\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 1.908494 testing loss: tensor(1.7412)\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 1.696750 testing loss: tensor(1.7438)\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 1.730211 testing loss: tensor(1.7418)\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 1.773254 testing loss: tensor(1.7411)\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.752831 testing loss: tensor(1.7379)\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 1.593734 testing loss: tensor(1.7408)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.7394)\n",
      "CS 2 : 1.7364666666666666\n",
      "DP 2 : 1.4480166666666667\n",
      "heuristic 2 : 1.8130666666666666\n",
      "DP: 1.4498810768127441\n",
      "tensor([0.0975, 0.1038, 0.0982, 0.1019, 0.1024, 0.0991, 0.0929, 0.0969, 0.1002,\n",
      "        0.1071])\n",
      "tensor([0.1096, 0.1142, 0.1104, 0.1114, 0.1148, 0.1127, 0.1046, 0.1081, 0.1143,\n",
      "        1.0000])\n",
      "tensor([0.1267, 0.1319, 0.1245, 0.1221, 0.1282, 0.1253, 0.1177, 0.1236, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.1427, 0.1483, 0.1419, 0.1439, 0.1463, 0.1409, 0.1359, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.1697, 0.1667, 0.1635, 0.1628, 0.1704, 0.1668, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.2046, 0.2050, 0.1899, 0.1839, 0.2167, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.2739, 0.2711, 0.2238, 0.2312, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.3403, 0.3575, 0.3022, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.4931, 0.5069, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: twopeak costsharing\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.001499\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.000001\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.000000\n",
      "NN 1 : tensor(1.7364)\n",
      "CS 1 : 1.7364666666666666\n",
      "DP 1 : 1.4480166666666667\n",
      "heuristic 1 : 1.8130666666666666\n",
      "DP: 1.4498810768127441\n",
      "tensor([0.1000, 0.1001, 0.0999, 0.1001, 0.1000, 0.1000, 0.0998, 0.1000, 0.1001,\n",
      "        0.1001])\n",
      "tensor([0.1109, 0.1111, 0.1107, 0.1113, 0.1111, 0.1112, 0.1110, 0.1114, 0.1112,\n",
      "        1.0000])\n",
      "tensor([0.1247, 0.1253, 0.1252, 0.1249, 0.1249, 0.1248, 0.1251, 0.1251, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.1423, 0.1430, 0.1428, 0.1430, 0.1428, 0.1432, 0.1429, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.1660, 0.1671, 0.1668, 0.1667, 0.1667, 0.1667, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.1996, 0.2004, 0.1996, 0.2002, 0.2002, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.2498, 0.2505, 0.2489, 0.2508, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.3327, 0.3344, 0.3329, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.4993, 0.5007, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 1.597497 testing loss: tensor(1.7374)\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 1.563106 testing loss: tensor(1.7376)\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 1.702872 testing loss: tensor(1.7387)\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 1.836344 testing loss: tensor(1.7376)\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 1.687959 testing loss: tensor(1.7395)\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.858413 testing loss: tensor(1.7400)\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.750129 testing loss: tensor(1.7383)\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.896770 testing loss: tensor(1.7385)\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.761519 testing loss: tensor(1.7380)\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.688241 testing loss: tensor(1.7362)\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.868133 testing loss: tensor(1.7365)\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 1.652146 testing loss: tensor(1.7371)\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 1.739900 testing loss: tensor(1.7380)\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 1.829036 testing loss: tensor(1.7389)\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 1.616640 testing loss: tensor(1.7388)\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.022055 testing loss: tensor(1.7395)\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 1.800101 testing loss: tensor(1.7380)\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 2.023297 testing loss: tensor(1.7388)\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 1.982263 testing loss: tensor(1.7380)\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 1.962596 testing loss: tensor(1.7372)\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.768381 testing loss: tensor(1.7399)\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 1.708089 testing loss: tensor(1.7403)\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 1.740768 testing loss: tensor(1.7382)\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 1.671140 testing loss: tensor(1.7387)\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 1.465191 testing loss: tensor(1.7399)\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.851937 testing loss: tensor(1.7384)\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 1.499189 testing loss: tensor(1.7362)\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 1.803511 testing loss: tensor(1.7372)\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 1.755644 testing loss: tensor(1.7375)\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 1.874220 testing loss: tensor(1.7366)\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.752181 testing loss: tensor(1.7385)\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 1.792975 testing loss: tensor(1.7366)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 1.844689 testing loss: tensor(1.7366)\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 1.611318 testing loss: tensor(1.7384)\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 1.687130 testing loss: tensor(1.7379)\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 1.765476 testing loss: tensor(1.7375)\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 1.757486 testing loss: tensor(1.7392)\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 1.694367 testing loss: tensor(1.7412)\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 1.577730 testing loss: tensor(1.7400)\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 1.651375 testing loss: tensor(1.7394)\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.735874 testing loss: tensor(1.7407)\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 1.821573 testing loss: tensor(1.7403)\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 1.690954 testing loss: tensor(1.7388)\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 1.632986 testing loss: tensor(1.7407)\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 2.005056 testing loss: tensor(1.7374)\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.853528 testing loss: tensor(1.7368)\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 1.698364 testing loss: tensor(1.7367)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.7367)\n",
      "CS 2 : 1.7364666666666666\n",
      "DP 2 : 1.4480166666666667\n",
      "heuristic 2 : 1.8130666666666666\n",
      "DP: 1.4498810768127441\n",
      "tensor([0.0987, 0.1007, 0.1005, 0.0976, 0.0993, 0.0990, 0.1018, 0.0997, 0.1018,\n",
      "        0.1009])\n",
      "tensor([0.1102, 0.1119, 0.1121, 0.1084, 0.1105, 0.1097, 0.1136, 0.1110, 0.1127,\n",
      "        1.0000])\n",
      "tensor([0.1239, 0.1265, 0.1258, 0.1230, 0.1248, 0.1242, 0.1272, 0.1247, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.1412, 0.1444, 0.1439, 0.1406, 0.1423, 0.1419, 0.1457, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.1653, 0.1688, 0.1685, 0.1643, 0.1668, 0.1664, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.1976, 0.2029, 0.2018, 0.1986, 0.1991, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.2423, 0.2513, 0.2563, 0.2500, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.3231, 0.3380, 0.3389, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.4903, 0.5097, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: twopeak heuristic\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.036805\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.005394\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.004237\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.004604\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.003366\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.003285\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.003209\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.002882\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.002764\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.002332\n",
      "NN 1 : tensor(1.5430)\n",
      "CS 1 : 1.7364666666666666\n",
      "DP 1 : 1.4480166666666667\n",
      "heuristic 1 : 1.8130666666666666\n",
      "DP: 1.4498810768127441\n",
      "tensor([0.0353, 0.0444, 0.0360, 0.0329, 0.0463, 0.0349, 0.0329, 0.6625, 0.0381,\n",
      "        0.0366])\n",
      "tensor([0.0376, 0.0384, 0.0457, 0.0757, 0.0767, 0.0367, 0.6231, 0.0526, 0.0134,\n",
      "        1.0000])\n",
      "tensor([0.0589, 0.0587, 0.0599, 0.1072, 0.1253, 0.1166, 0.1629, 0.3105, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.0525, 0.0505, 0.0505, 0.0700, 0.5818, 0.0507, 0.1440, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.0547, 0.0545, 0.0500, 0.0971, 0.1672, 0.5766, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.0860, 0.0893, 0.0857, 0.1667, 0.5723, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.0954, 0.1055, 0.1161, 0.6831, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.1361, 0.1444, 0.7195, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.4805, 0.5195, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 14.101128 testing loss: tensor(1.5895)\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 3.276129 testing loss: tensor(2.0601)\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.039302 testing loss: tensor(1.9068)\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.028084 testing loss: tensor(1.8887)\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 1.539134 testing loss: tensor(1.8826)\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.826915 testing loss: tensor(1.8752)\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.245446 testing loss: tensor(1.8659)\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.003528 testing loss: tensor(1.8545)\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.740640 testing loss: tensor(1.8414)\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.783701 testing loss: tensor(1.8318)\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.910518 testing loss: tensor(1.8262)\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 1.766264 testing loss: tensor(1.8199)\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 1.737109 testing loss: tensor(1.8127)\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 1.913275 testing loss: tensor(1.8092)\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 1.882567 testing loss: tensor(1.8020)\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.694944 testing loss: tensor(1.7951)\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 1.704229 testing loss: tensor(1.7902)\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 2.144948 testing loss: tensor(1.7842)\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 1.870205 testing loss: tensor(1.7800)\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 1.635612 testing loss: tensor(1.7758)\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.850735 testing loss: tensor(1.7720)\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 1.529028 testing loss: tensor(1.7720)\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 1.657414 testing loss: tensor(1.7688)\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 1.757453 testing loss: tensor(1.7652)\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 1.822342 testing loss: tensor(1.7622)\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.873766 testing loss: tensor(1.7607)\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 1.790295 testing loss: tensor(1.7582)\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 2.037818 testing loss: tensor(1.7570)\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 1.654778 testing loss: tensor(1.7552)\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 1.725514 testing loss: tensor(1.7539)\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.673490 testing loss: tensor(1.7525)\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 1.598593 testing loss: tensor(1.7519)\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 1.723937 testing loss: tensor(1.7510)\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 1.667892 testing loss: tensor(1.7507)\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 1.663629 testing loss: tensor(1.7507)\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 1.775670 testing loss: tensor(1.7507)\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 1.996977 testing loss: tensor(1.7502)\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 1.801918 testing loss: tensor(1.7505)\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 1.597426 testing loss: tensor(1.7508)\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 1.499607 testing loss: tensor(1.7504)\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.740400 testing loss: tensor(1.7503)\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 1.780841 testing loss: tensor(1.7502)\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 1.878881 testing loss: tensor(1.7498)\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 1.955276 testing loss: tensor(1.7497)\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 1.612493 testing loss: tensor(1.7485)\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.745367 testing loss: tensor(1.7480)\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 1.871896 testing loss: tensor(1.7476)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.7478)\n",
      "CS 2 : 1.7364666666666666\n",
      "DP 2 : 1.4480166666666667\n",
      "heuristic 2 : 1.8130666666666666\n",
      "DP: 1.4498810768127441\n",
      "tensor([0.0994, 0.0984, 0.1006, 0.0950, 0.1067, 0.1061, 0.0970, 0.1038, 0.0931,\n",
      "        0.1000])\n",
      "tensor([0.0999, 0.1004, 0.1071, 0.1060, 0.1077, 0.1077, 0.1149, 0.1147, 0.1418,\n",
      "        1.0000])\n",
      "tensor([0.1140, 0.1117, 0.1113, 0.1172, 0.1248, 0.1251, 0.1266, 0.1694, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.1243, 0.1186, 0.1280, 0.1301, 0.1255, 0.1512, 0.2222, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.1301, 0.1335, 0.1341, 0.1598, 0.1551, 0.2874, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.1757, 0.1846, 0.1668, 0.1996, 0.2733, 1.0000, 1.0000, 1.0000, 1.0000,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        1.0000])\n",
      "tensor([0.1933, 0.2112, 0.1889, 0.4067, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.3157, 0.3215, 0.3628, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([0.4983, 0.5017, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.1)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "    \n",
    "for order in stage:\n",
    "    producedata(order)\n",
    "    #running\n",
    "    for order1 in order1name:\n",
    "        print(\"Supervised Aim:\",order,order1)\n",
    "        \n",
    "        # for mapping binary to payments before softmax\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(n, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, n),\n",
    "        )\n",
    "        model.apply(init_weights)\n",
    "        # optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr1)\n",
    "        \n",
    "        runningLossNN = []\n",
    "        runningLossCS = []\n",
    "        runningLossDP = []\n",
    "        runningLossHeuristic = []\n",
    "        #model=torch.load(\"save/pytorchNN=5dp1\");\n",
    "        #model.eval()\n",
    "        ##order1name=[\"costsharing\",\"dp\",\"heuristic\",\"random initializing\"]\n",
    "        for epoch in range(1, supervisionEpochs + 1):\n",
    "#             print(\"distributionRatio\",distributionRatio)\n",
    "            if(order1==\"costsharing\"):\n",
    "                supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "            elif(order1==\"dp\"):\n",
    "                supervisionTrain(epoch, dpSupervisionRule)\n",
    "            elif(order1==\"heuristic\"):\n",
    "                supervisionTrain(epoch, heuristicSupervisionRule)\n",
    "            elif(order1==\"random initializing\"):\n",
    "                print(\"do nothing\");\n",
    "\n",
    "        test()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr2)\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(epoch)\n",
    "            test()\n",
    "        losslistname.append(order+\" \"+order1);\n",
    "        losslist.append(losslisttemp);\n",
    "        losslisttemp=[];\n",
    "        savepath=\"save/pytorchNN=5all-beta\"+order+str(order1)\n",
    "        torch.save(model, savepath);\n",
    "        print(\"end\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colorlist=[\"#D2691E\",'#4169E1',\"#9ACD32\",\"#B22222\",\"#FF00FF\",\"#708090\"]\n",
    "for i in range(len(losslist)):\n",
    "    plt.plot(losslist[i], '.-', color=colorlist[i], alpha=0.8, label=losslistname[i])\n",
    "\n",
    "# 显示标签，如果不加这句，即使加了label='一些数字'的参数，最终还是不会显示标签\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('Trianing Times')\n",
    "plt.ylabel('Delay')\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('pytorchNN=5_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
