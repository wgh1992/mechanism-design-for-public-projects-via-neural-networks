{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "from pylab import *\n",
    "import random\n",
    "n = 5\n",
    "epochs = 4\n",
    "supervisionEpochs = 3\n",
    "lr = 0.001\n",
    "log_interval = 10\n",
    "trainSize = 40000#100000\n",
    "percentage_train_test= 0.25\n",
    "penaltyLambda = 10\n",
    "doublePeakHighMean = 0.9\n",
    "doublePeakLowMean = 0.1\n",
    "doublePeakStd = 0.1\n",
    "uniformlow=0\n",
    "uniformhigh=1.0\n",
    "normalloc = 0.2\n",
    "normalscale = 0.1\n",
    "\n",
    "cauchyloc = 1/n\n",
    "cauchyscalen = 0.004\n",
    "\n",
    "independentnormalloc1=[(float(ii)+1)/(2*n+1) for ii in range(n,0,-1)]\n",
    "independentnormalscale1=[0.05 for ii in range(n)]\n",
    "\n",
    "independentnormalloc2=[(float(ii)+1)/(2*n+1) for ii in range(1,n+1,1)]\n",
    "independentnormalscale2=[0.05 for ii in range(n)]\n",
    "stage=[\"twopeak\"]\n",
    "# \"twopeak\",\"normal\",\"uniform\",\"independent1\",\"independent2\",\"cauchy\"\n",
    "order1name=[\"costsharing\",\"dp\",\"random initializing\"]\n",
    "# \"costsharing\",\"dp\",\"heuristic\",\"random initializing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "d3 = D.normal.Normal(normalloc, normalscale)\n",
    "d4 = D.uniform.Uniform(uniformlow,uniformhigh)\n",
    "\n",
    "d5 = [D.normal.Normal(independentnormalloc1[ii], independentnormalscale1[ii]) for ii in range(n)]\n",
    "d6 = [D.normal.Normal(independentnormalloc2[ii], independentnormalscale2[ii]) for ii in range(n)]\n",
    "\n",
    "d7 = D.cauchy.Cauchy(cauchyloc,cauchyscalen)\n",
    "def cdf(x,y, i=None):\n",
    "    if(y==\"twopeak\"):\n",
    "        return (d1.cdf(x) + d2.cdf(x) - distributionBase) / 2 / distributionRatio\n",
    "    elif(y==\"normal\"):\n",
    "        return d3.cdf(x);\n",
    "    elif(y==\"uniform\"):\n",
    "        return d4.cdf(x);\n",
    "    elif(y==\"independent1\"):\n",
    "        return d5[i].cdf(x);\n",
    "    elif(y==\"independent2\"):\n",
    "        return d6[i].cdf(x);\n",
    "    elif(y==\"cauchy\"):\n",
    "        return d7.cdf(x);\n",
    "#print(cdf(0.1,\"independent\",1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    if torch.sum(bits).item() == 0:\n",
    "        return torch.ones(n)\n",
    "    bits = bits.type(torch.float32)\n",
    "    negBits = torch.ones(n) - bits\n",
    "    payments = model(bits)\n",
    "    payments = payments - 1000 * negBits\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    payments = payments + negBits\n",
    "    return payments\n",
    "\n",
    "def tpToBits(tp, bits=torch.ones(n).type(torch.uint8)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = (tp >= payments).type(torch.uint8)\n",
    "    if torch.equal(newBits, bits):\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits(tp, newBits)#bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    return n - torch.sum(tpToBits(tp).type(torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    global  dp,decision\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money, yes]\n",
    "        offer = float(offerIndex) / dpPrecision\n",
    "        if tp[i] >= offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits[i] = 0\n",
    "            payments[i] = 1\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "#     print()\n",
    "#     print(tp)\n",
    "#     print(bits)\n",
    "#     print(payments)\n",
    "#     print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def heuristicSupervisionRule(tp):\n",
    "    global  dp_H,decision_H\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    tempold=-1;\n",
    "    for turns in range(n,0,-1):\n",
    "        money = dpPrecision\n",
    "        j=0\n",
    "        tempo=sum(bits)\n",
    "        #print(\"bits\",tempo)\n",
    "        for i in range(n):\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            \n",
    "            offerIndex = decision_H[tempo ,tempo - i, money]\n",
    "            offer = float(offerIndex) / dpPrecision\n",
    "            while(j<n):\n",
    "                if(bits[j]!=0):\n",
    "                    break;\n",
    "                j+=1;\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            if tp[j] >= offer:\n",
    "                #print(money,j,tp[j],offer)\n",
    "                money -= offerIndex\n",
    "                bits[j] = 1\n",
    "                payments[j] = offer\n",
    "            else:\n",
    "                bits[j] = 0;\n",
    "                payments[j] = 1.0;\n",
    "            j+=1;\n",
    "        #print(\"money\",money)\n",
    "        if(money==0 and tempold==tempo):\n",
    "            break;\n",
    "        tempold=tempo;\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    for k in range(n, -1, -1):\n",
    "        if k == 0:\n",
    "            break\n",
    "        bits = [1 if tp[ii] >= 1.0 / k else 0 for ii in range(n)]\n",
    "        if sum(bits) == k:\n",
    "            break\n",
    "    if k == 0:\n",
    "        payments = [1 for ii in range(n)]\n",
    "    else:\n",
    "        payments = [1.0 / k if bits[ii] == 1 else 1 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    return float(n - torch.sum(costSharingSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def dpDelay(tp):\n",
    "    return float(n - torch.sum(dpSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def heuristicDelay(tp):\n",
    "    return float(n - torch.sum(heuristicSupervisionRule(tp)[0]).item())\n",
    "\n",
    "\n",
    "# templire=0;\n",
    "# num=1\n",
    "# for i in range(num):\n",
    "#     temp=torch.tensor(samplesJoint[i])\n",
    "#     #print(temp)\n",
    "#     #print(temp);\n",
    "#     #print(heuristicSupervisionRule(temp))\n",
    "#     #print(dpSupervisionRule(temp))\n",
    "#     #print(dpDelay(temp))\n",
    "#     res=dpDelay(temp)\n",
    "#     templire+=res\n",
    "#     #print(\"delay\",res)\n",
    "#     #print()\n",
    "# print(templire/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordAndReport(name, source, loss):\n",
    "    source.append(loss)\n",
    "    realLength = len(source)\n",
    "    #print(f\"{name} ({realLength}): {loss}\")\n",
    "    print(name,realLength,\":\" ,loss)\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    \n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda /100000\n",
    "        \n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "losslist=[];\n",
    "losslistname=[];\n",
    "losslisttemp=[];\n",
    "def test_batch():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lenLoss= 0\n",
    "        nnLoss = 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "            for tp in tp_batch:\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        nnLoss/=lenLoss\n",
    "    return nnLoss\n",
    "\n",
    "def train(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                x1=random.randint(0,n-1);\n",
    "                x2=random.randint(0,n-1);\n",
    "                while(x2==x1):\n",
    "                    x2=random.randint(0,n-1);\n",
    "                    \n",
    "                tp11 = tp.clone()\n",
    "                tp11[x1] = 1\n",
    "                tp11[x2] = 1\n",
    "                tp10 = tp.clone()\n",
    "                tp10[x1] = 1\n",
    "                tp10[x2] = 0\n",
    "                tp01 = tp.clone()\n",
    "                tp01[x1] = 0\n",
    "                tp01[x2] = 1\n",
    "                tp00 = tp.clone()\n",
    "                tp00[x1] = 0\n",
    "                tp00[x2] = 0\n",
    "                \n",
    "                offer1 = tpToPayments(tp11)[x1]\n",
    "                offer2 = tpToPayments(tp11)[x2]\n",
    "                offer3 = tpToPayments(tp10)[x1]\n",
    "                offer4 = tpToPayments(tp01)[x2]\n",
    "                \n",
    "                #print(tp)\n",
    "                #print(offer1,offer2)\n",
    "                \n",
    "                delay11 = tpToTotalDelay(tp11)\n",
    "                delay01 = tpToTotalDelay(tp01)\n",
    "                delay10 = tpToTotalDelay(tp10)\n",
    "                delay00 = tpToTotalDelay(tp00)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "#                     print ((1.0-cdf(offer1,order)) * (1.0-cdf(offer2,order))+                                                   \n",
    "#                            (1.0-cdf(offer3,order)) * cdf(offer2,order)+\n",
    "#                            cdf(offer1,order) * (1.0-cdf(offer4,order))+\n",
    "#                            (cdf(offer3,order) * cdf(offer4,order) -\n",
    "#                                      (cdf(offer3,order)-cdf(offer1,order))*(cdf(offer4,order)-cdf(offer2,order)) )\n",
    "#                           )\n",
    "                    loss = loss +(      (1.0-cdf(offer1,order)) * (1.0-cdf(offer2,order)) * delay11 +\n",
    "                                  (1.0-cdf(offer3,order)) * cdf(offer2,order) * delay10 +\n",
    "                                  cdf(offer1,order) * (1.0-cdf(offer4,order)) * delay01 +\n",
    "                                    (cdf(offer3,order) * cdf(offer4,order) -\n",
    "                                     (cdf(offer3,order)-cdf(offer1,order))*(cdf(offer4,order)-cdf(offer2,order)) )* delay00 \n",
    "                                                       )\n",
    "                else:\n",
    "                    loss = loss +(      (1.0-cdf(offer1,order,x1)) * (1.0-cdf(offer2,order),x2) * delay11 +\n",
    "                                  (1.0-cdf(offer3,order,x1)) * cdf(offer2,order,x2) * delay10 +\n",
    "                                  cdf(offer1,order,x1) * (1.0-cdf(offer4,order,x2)) * delay01 +\n",
    "                                    (cdf(offer3,order,x1) * cdf(offer4,order,x2) -\n",
    "                                     (cdf(offer3,order,x1)-cdf(offer1,order,x1))*(cdf(offer4,order,x2)-cdf(offer2,order,x2)) )* delay00 \n",
    "                                                       )\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "    \n",
    "allBits = [torch.tensor(bits, dtype=torch.int) for bits in itertools.product([0, 1], repeat=n)]\n",
    "\n",
    "def test():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        costSharingLoss = 0\n",
    "        dpLoss = 0\n",
    "        nnLoss = 0\n",
    "        heuristicLoss=0\n",
    "        lenLoss= 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "                heuristicLoss+= heuristicDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        costSharingLoss /= lenLoss\n",
    "        dpLoss /= lenLoss\n",
    "        nnLoss /= lenLoss\n",
    "        heuristicLoss /= lenLoss\n",
    "        #print(lenLoss)\n",
    "        losslisttemp.append(nnLoss);\n",
    "        recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "        recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "        recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "        recordAndReport(\"heuristic\", runningLossHeuristic, heuristicLoss)\n",
    "        print(\"DP:\",dp[n, dpPrecision,0])\n",
    "        #for i in range(n, 0, -1):\n",
    "        #    print(\"Heuristic:\",i,5*(1-dp_H[i, i, dpPrecision]))\n",
    "        for i in range(n, 0, -1):\n",
    "            print(\n",
    "                    tpToPayments(\n",
    "                            torch.tensor([0 if ii >= i else 1 for ii in range(n)], dtype=torch.float32)\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "dpPrecision = 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def producedata(order):\n",
    "    global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "    dpPrecision = 100 \n",
    "    if(order==\"twopeak\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = np.random.normal(\n",
    "            loc=doublePeakLowMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=doublePeakLowMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samples2 = np.random.normal(\n",
    "            loc=doublePeakHighMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = np.random.normal(\n",
    "                        loc=doublePeakHighMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "    elif(order==\"normal\"):\n",
    "        print(\"loc\",normalloc, \"scale\",normalscale)\n",
    "        samples1 = np.random.normal(\n",
    "            loc=normalloc, scale=normalscale, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=normalloc, scale=normalscale\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"uniform\"):  \n",
    "        print(\"uniformlow\",uniformlow, \"uniformhigh\",uniformhigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        uniformlow, uniformhigh\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent1\"):\n",
    "        print(\"loc\",independentnormalloc1,\"scale\",independentnormalscale1)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent2\"):\n",
    "        print(\"loc\",independentnormalloc2, \"scale\",independentnormalscale2)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"cauchy\"):\n",
    "        print(\"cauchyloc\",cauchyloc, \"cauchyscale\",cauchyscalen)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        \n",
    "    tp_dataset = TensorDataset(tp_tensor[: int(trainSize * percentage_train_test)])\n",
    "    tp_dataset_testing = TensorDataset(tp_tensor[int(trainSize * (1.0-percentage_train_test)) :])\n",
    "    tp_dataloader = DataLoader(tp_dataset, batch_size=128, shuffle=True)\n",
    "    tp_dataloader_testing = DataLoader(tp_dataset_testing, batch_size=256, shuffle=False)\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "    decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for yes in range(n + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp[0, 0, yes] = 0\n",
    "            else:\n",
    "                dp[0, money, yes] = yes# + 1.0\n",
    "    for ppl in range(1,  n + 1):\n",
    "        for yes in range(n + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                minSoFar = 1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1 - cdf(offer,order)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order) * (1 + dp[ppl - 1, money, yes])\n",
    "                    else:\n",
    "                        res = (1 - cdf(offer,order,n-ppl)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order,n-ppl) * (1 + dp[ppl - 1, money, yes])\n",
    "                    if minSoFar > res.item():\n",
    "                        minSoFar = res.item()\n",
    "                        decision[ppl, money, yes] = offerIndex\n",
    "                dp[ppl, money, yes] = minSoFar\n",
    "    \n",
    "    print(\"dp\",dp[n, dpPrecision, 0])\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp_H = np.zeros([n + 1 , n + 1, dpPrecision + 1])\n",
    "    decision_H = np.zeros([n + 1 , n + 1, dpPrecision + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for i in range(1,n+1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp_H[i, 0, 0] = 1\n",
    "            else:\n",
    "                offer = money / dpPrecision\n",
    "                dp_H[i, 0, money] = 0#cdf(offer)# + 1.0\n",
    "    for i in range(1,n+1):\n",
    "        for ppl in range(1, i + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                maxSoFar = -1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1-cdf(offer,order)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    else:\n",
    "                        res = (1-cdf(offer,order,ppl-1)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    if maxSoFar < res.item():\n",
    "                        maxSoFar = res.item()\n",
    "                        decision_H[i, ppl, money] = offerIndex\n",
    "                dp_H[i, ppl, money] = maxSoFar\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loc 0.1 scale 0.1\n",
      "loc 0.9 scale 0.1\n",
      "dp 1.8651759624481201\n",
      "Supervised Aim: twopeak costsharing\n",
      "Train Epoch: 1 [0/5000 (0%)]\tLoss: 0.000477\n",
      "Train Epoch: 1 [1280/5000 (25%)]\tLoss: 0.000061\n",
      "Train Epoch: 1 [2560/5000 (50%)]\tLoss: 0.000017\n",
      "Train Epoch: 1 [3840/5000 (75%)]\tLoss: 0.000005\n",
      "Train Epoch: 2 [0/5000 (0%)]\tLoss: 0.000001\n",
      "Train Epoch: 2 [1280/5000 (25%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [2560/5000 (50%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [3840/5000 (75%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [0/5000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [1280/5000 (25%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [2560/5000 (50%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [3840/5000 (75%)]\tLoss: 0.000000\n",
      "NN 1 : tensor(2.5290)\n",
      "CS 1 : 2.529\n",
      "DP 1 : 1.8646\n",
      "heuristic 1 : 1.8514\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500, 1.0000])\n",
      "tensor([0.3333, 0.3334, 0.3333, 1.0000, 1.0000])\n",
      "tensor([0.4999, 0.5001, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/5000 (0%)]\tLoss: 2.471624 testing loss: tensor(2.5270)\n",
      "Train Epoch: 1 [1280/5000 (25%)]\tLoss: 1.920930 testing loss: tensor(2.1242)\n",
      "Train Epoch: 1 [2560/5000 (50%)]\tLoss: 2.016692 testing loss: tensor(1.9818)\n",
      "Train Epoch: 1 [3840/5000 (75%)]\tLoss: 1.848875 testing loss: tensor(1.9542)\n",
      "penalty: 0.004358947277069092\n",
      "NN 2 : tensor(1.9344)\n",
      "CS 2 : 2.529\n",
      "DP 2 : 1.8646\n",
      "heuristic 2 : 1.8514\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.4796, 0.0218, 0.0181, 0.0249, 0.4555])\n",
      "tensor([0.7573, 0.0841, 0.0686, 0.0901, 1.0000])\n",
      "tensor([0.7732, 0.1196, 0.1073, 1.0000, 1.0000])\n",
      "tensor([0.8470, 0.1530, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 2 [0/5000 (0%)]\tLoss: 1.807361 testing loss: tensor(1.9318)\n",
      "Train Epoch: 2 [1280/5000 (25%)]\tLoss: 2.031011 testing loss: tensor(1.9286)\n",
      "Train Epoch: 2 [2560/5000 (50%)]\tLoss: 1.732404 testing loss: tensor(1.9216)\n",
      "Train Epoch: 2 [3840/5000 (75%)]\tLoss: 2.076255 testing loss: tensor(1.9200)\n",
      "penalty: 0.004466444253921509\n",
      "NN 3 : tensor(1.9096)\n",
      "CS 3 : 2.529\n",
      "DP 3 : 1.8646\n",
      "heuristic 3 : 1.8514\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.4554, 0.0134, 0.0139, 0.0162, 0.5012])\n",
      "tensor([0.7358, 0.0885, 0.0705, 0.1052, 1.0000])\n",
      "tensor([0.7480, 0.1276, 0.1244, 1.0000, 1.0000])\n",
      "tensor([0.8086, 0.1914, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 3 [0/5000 (0%)]\tLoss: 1.798252 testing loss: tensor(1.9092)\n",
      "Train Epoch: 3 [1280/5000 (25%)]\tLoss: 1.847638 testing loss: tensor(1.8974)\n",
      "Train Epoch: 3 [2560/5000 (50%)]\tLoss: 1.851517 testing loss: tensor(1.8856)\n",
      "Train Epoch: 3 [3840/5000 (75%)]\tLoss: 1.874020 testing loss: tensor(1.8750)\n",
      "penalty: 0.030988454818725586\n",
      "NN 4 : tensor(1.8710)\n",
      "CS 4 : 2.529\n",
      "DP 4 : 1.8646\n",
      "heuristic 4 : 1.8514\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.4327, 0.0061, 0.0081, 0.0071, 0.5460])\n",
      "tensor([0.7509, 0.0737, 0.0860, 0.0895, 1.0000])\n",
      "tensor([0.7738, 0.1047, 0.1216, 1.0000, 1.0000])\n",
      "tensor([0.8000, 0.2000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 4 [0/5000 (0%)]\tLoss: 1.936816 testing loss: tensor(1.8748)\n",
      "Train Epoch: 4 [1280/5000 (25%)]\tLoss: 1.827394 testing loss: tensor(1.8708)\n",
      "Train Epoch: 4 [2560/5000 (50%)]\tLoss: 1.774694 testing loss: tensor(1.8614)\n",
      "Train Epoch: 4 [3840/5000 (75%)]\tLoss: 1.735749 testing loss: tensor(1.8662)\n",
      "penalty: 0.008673995733261108\n",
      "NN 5 : tensor(1.8612)\n",
      "CS 5 : 2.529\n",
      "DP 5 : 1.8646\n",
      "heuristic 5 : 1.8514\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.4040, 0.0048, 0.0058, 0.0046, 0.5808])\n",
      "tensor([0.7464, 0.0802, 0.0868, 0.0866, 1.0000])\n",
      "tensor([0.7726, 0.1080, 0.1194, 1.0000, 1.0000])\n",
      "tensor([0.7857, 0.2143, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: twopeak dp\n",
      "Train Epoch: 1 [0/5000 (0%)]\tLoss: 0.050001\n",
      "Train Epoch: 1 [1280/5000 (25%)]\tLoss: 0.013578\n",
      "Train Epoch: 1 [2560/5000 (50%)]\tLoss: 0.002879\n",
      "Train Epoch: 1 [3840/5000 (75%)]\tLoss: 0.003881\n",
      "Train Epoch: 2 [0/5000 (0%)]\tLoss: 0.002318\n",
      "Train Epoch: 2 [1280/5000 (25%)]\tLoss: 0.001456\n",
      "Train Epoch: 2 [2560/5000 (50%)]\tLoss: 0.001003\n",
      "Train Epoch: 2 [3840/5000 (75%)]\tLoss: 0.000562\n",
      "Train Epoch: 3 [0/5000 (0%)]\tLoss: 0.000533\n",
      "Train Epoch: 3 [1280/5000 (25%)]\tLoss: 0.000406\n",
      "Train Epoch: 3 [2560/5000 (50%)]\tLoss: 0.000192\n",
      "Train Epoch: 3 [3840/5000 (75%)]\tLoss: 0.000163\n",
      "NN 1 : tensor(1.8398)\n",
      "CS 1 : 2.529\n",
      "DP 1 : 1.8646\n",
      "heuristic 1 : 1.8514\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.7625, 0.0962, 0.0809, 0.0563, 0.0039])\n",
      "tensor([0.7129, 0.1177, 0.1026, 0.0667, 1.0000])\n",
      "tensor([0.7334, 0.1374, 0.1292, 1.0000, 1.0000])\n",
      "tensor([0.8304, 0.1696, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/5000 (0%)]\tLoss: 1.718077 testing loss: tensor(1.8384)\n",
      "Train Epoch: 1 [1280/5000 (25%)]\tLoss: 1.927730 testing loss: tensor(1.8084)\n",
      "Train Epoch: 1 [2560/5000 (50%)]\tLoss: 1.746624 testing loss: tensor(1.7958)\n",
      "Train Epoch: 1 [3840/5000 (75%)]\tLoss: 1.577003 testing loss: tensor(1.7680)\n",
      "penalty: 0.020496033132076263\n",
      "NN 2 : tensor(1.7844)\n",
      "CS 2 : 2.529\n",
      "DP 2 : 1.8646\n",
      "heuristic 2 : 1.8514\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.7154, 0.0746, 0.0693, 0.0707, 0.0699])\n",
      "tensor([0.7229, 0.0998, 0.0895, 0.0878, 1.0000])\n",
      "tensor([0.7483, 0.1263, 0.1254, 1.0000, 1.0000])\n",
      "tensor([0.8090, 0.1910, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 2 [0/5000 (0%)]\tLoss: 1.677611 testing loss: tensor(1.8088)\n",
      "Train Epoch: 2 [1280/5000 (25%)]\tLoss: 1.680807 testing loss: tensor(1.7808)\n",
      "Train Epoch: 2 [2560/5000 (50%)]\tLoss: 1.747792 testing loss: tensor(1.7734)\n",
      "Train Epoch: 2 [3840/5000 (75%)]\tLoss: 1.724947 testing loss: tensor(1.7746)\n",
      "penalty: 0.023867130279541016\n",
      "NN 3 : tensor(1.7644)\n",
      "CS 3 : 2.529\n",
      "DP 3 : 1.8646\n",
      "heuristic 3 : 1.8514\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.7392, 0.0716, 0.0645, 0.0621, 0.0626])\n",
      "tensor([0.7519, 0.0909, 0.0807, 0.0766, 1.0000])\n",
      "tensor([0.7626, 0.1146, 0.1227, 1.0000, 1.0000])\n",
      "tensor([0.7924, 0.2076, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 3 [0/5000 (0%)]\tLoss: 1.716996 testing loss: tensor(1.7616)\n",
      "Train Epoch: 3 [1280/5000 (25%)]\tLoss: 1.829605 testing loss: tensor(1.7706)\n",
      "Train Epoch: 3 [2560/5000 (50%)]\tLoss: 1.807084 testing loss: tensor(1.7698)\n",
      "Train Epoch: 3 [3840/5000 (75%)]\tLoss: 1.845350 testing loss: tensor(1.7694)\n",
      "penalty: 0.011384844779968262\n",
      "NN 4 : tensor(1.7702)\n",
      "CS 4 : 2.529\n",
      "DP 4 : 1.8646\n",
      "heuristic 4 : 1.8514\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.7443, 0.0684, 0.0618, 0.0619, 0.0636])\n",
      "tensor([0.7485, 0.0915, 0.0802, 0.0797, 1.0000])\n",
      "tensor([0.7588, 0.1263, 0.1148, 1.0000, 1.0000])\n",
      "tensor([0.7740, 0.2260, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 4 [0/5000 (0%)]\tLoss: 1.654474 testing loss: tensor(1.7716)\n",
      "Train Epoch: 4 [1280/5000 (25%)]\tLoss: 1.613602 testing loss: tensor(1.7638)\n",
      "Train Epoch: 4 [2560/5000 (50%)]\tLoss: 1.689656 testing loss: tensor(1.7710)\n",
      "Train Epoch: 4 [3840/5000 (75%)]\tLoss: 1.681909 testing loss: tensor(1.7742)\n",
      "penalty: 0.007009625434875488\n",
      "NN 5 : tensor(1.7642)\n",
      "CS 5 : 2.529\n",
      "DP 5 : 1.8646\n",
      "heuristic 5 : 1.8514\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.7260, 0.0683, 0.0673, 0.0676, 0.0708])\n",
      "tensor([0.7419, 0.0881, 0.0869, 0.0831, 1.0000])\n",
      "tensor([0.7547, 0.1222, 0.1230, 1.0000, 1.0000])\n",
      "tensor([0.7736, 0.2264, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: twopeak random initializing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "NN 1 : tensor(2.4938)\n",
      "CS 1 : 2.529\n",
      "DP 1 : 1.8646\n",
      "heuristic 1 : 1.8514\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.1611, 0.1395, 0.1791, 0.2438, 0.2766])\n",
      "tensor([0.2267, 0.2046, 0.2347, 0.3339, 1.0000])\n",
      "tensor([0.3281, 0.3302, 0.3417, 1.0000, 1.0000])\n",
      "tensor([0.5001, 0.4999, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/5000 (0%)]\tLoss: 2.565124 testing loss: tensor(2.4536)\n",
      "Train Epoch: 1 [1280/5000 (25%)]\tLoss: 1.891553 testing loss: tensor(2.0486)\n",
      "Train Epoch: 1 [2560/5000 (50%)]\tLoss: 1.827603 testing loss: tensor(1.9522)\n",
      "Train Epoch: 1 [3840/5000 (75%)]\tLoss: 1.870402 testing loss: tensor(1.9476)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.9210)\n",
      "CS 2 : 2.529\n",
      "DP 2 : 1.8646\n",
      "heuristic 2 : 1.8514\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0343, 0.0282, 0.0351, 0.3538, 0.5486])\n",
      "tensor([0.0953, 0.0846, 0.0994, 0.7207, 1.0000])\n",
      "tensor([0.3392, 0.2777, 0.3831, 1.0000, 1.0000])\n",
      "tensor([0.5328, 0.4672, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 2 [0/5000 (0%)]\tLoss: 2.040604 testing loss: tensor(1.9170)\n",
      "Train Epoch: 2 [1280/5000 (25%)]\tLoss: 1.984365 testing loss: tensor(1.9032)\n",
      "Train Epoch: 2 [2560/5000 (50%)]\tLoss: 1.799535 testing loss: tensor(1.8830)\n",
      "Train Epoch: 2 [3840/5000 (75%)]\tLoss: 1.797286 testing loss: tensor(1.8832)\n",
      "penalty: 0.010693728923797607\n",
      "NN 3 : tensor(1.8620)\n",
      "CS 3 : 2.529\n",
      "DP 3 : 1.8646\n",
      "heuristic 3 : 1.8514\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0267, 0.0194, 0.0247, 0.2026, 0.7267])\n",
      "tensor([0.1000, 0.0621, 0.0917, 0.7462, 1.0000])\n",
      "tensor([0.3593, 0.1751, 0.4656, 1.0000, 1.0000])\n",
      "tensor([0.6058, 0.3942, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 3 [0/5000 (0%)]\tLoss: 1.934129 testing loss: tensor(1.8642)\n",
      "Train Epoch: 3 [1280/5000 (25%)]\tLoss: 1.830063 testing loss: tensor(1.8516)\n",
      "Train Epoch: 3 [2560/5000 (50%)]\tLoss: 1.799232 testing loss: tensor(1.8304)\n",
      "Train Epoch: 3 [3840/5000 (75%)]\tLoss: 1.875067 testing loss: tensor(1.8024)\n",
      "penalty: 0.04989069700241089\n",
      "NN 4 : tensor(1.7806)\n",
      "CS 4 : 2.529\n",
      "DP 4 : 1.8646\n",
      "heuristic 4 : 1.8514\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0439, 0.0531, 0.0442, 0.1164, 0.7425])\n",
      "tensor([0.0742, 0.0679, 0.1061, 0.7518, 1.0000])\n",
      "tensor([0.1589, 0.0692, 0.7719, 1.0000, 1.0000])\n",
      "tensor([0.6045, 0.3955, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 4 [0/5000 (0%)]\tLoss: 1.775074 testing loss: tensor(1.7792)\n",
      "Train Epoch: 4 [1280/5000 (25%)]\tLoss: 1.847282 testing loss: tensor(1.7672)\n",
      "Train Epoch: 4 [2560/5000 (50%)]\tLoss: 1.710942 testing loss: tensor(1.7524)\n",
      "Train Epoch: 4 [3840/5000 (75%)]\tLoss: 1.728622 testing loss: tensor(1.7570)\n",
      "penalty: 0.0\n",
      "NN 5 : tensor(1.7526)\n",
      "CS 5 : 2.529\n",
      "DP 5 : 1.8646\n",
      "heuristic 5 : 1.8514\n",
      "DP: 1.8651759624481201\n",
      "tensor([0.0627, 0.0670, 0.0628, 0.0606, 0.7470])\n",
      "tensor([0.0788, 0.0891, 0.0960, 0.7361, 1.0000])\n",
      "tensor([0.1395, 0.1054, 0.7550, 1.0000, 1.0000])\n",
      "tensor([0.5231, 0.4769, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "    \n",
    "for order in stage:\n",
    "    producedata(order)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "    #running\n",
    "    for order1 in order1name:\n",
    "        print(\"Supervised Aim:\",order,order1)\n",
    "        \n",
    "        # for mapping binary to payments before softmax\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(n, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, n),\n",
    "        )\n",
    "        model.apply(init_weights)\n",
    "        # optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        runningLossNN = []\n",
    "        runningLossCS = []\n",
    "        runningLossDP = []\n",
    "        runningLossHeuristic = []\n",
    "        #model=torch.load(\"save/pytorchNN=5dp1\");\n",
    "        #model.eval()\n",
    "        ##order1name=[\"costsharing\",\"dp\",\"heuristic\",\"random initializing\"]\n",
    "        for epoch in range(1, supervisionEpochs + 1):\n",
    "#             print(\"distributionRatio\",distributionRatio)\n",
    "            if(order1==\"costsharing\"):\n",
    "                supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "            elif(order1==\"dp\"):\n",
    "                supervisionTrain(epoch, dpSupervisionRule)\n",
    "            elif(order1==\"heuristic\"):\n",
    "                supervisionTrain(epoch, heuristicSupervisionRule)\n",
    "            elif(order1==\"random initializing\"):\n",
    "                print(\"do nothing\");\n",
    "\n",
    "        test()\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(epoch)\n",
    "            test()\n",
    "        losslistname.append(order+\" \"+order1);\n",
    "        losslist.append(losslisttemp);\n",
    "        losslisttemp=[];\n",
    "        savepath=\"save/pytorchNN=5all-phoenix\"+order+str(order1)\n",
    "        torch.save(model, savepath);\n",
    "        print(\"end\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colorlist=[\"#D2691E\",'#4169E1',\"#9ACD32\",\"#B22222\",\"#FF00FF\",\"#708090\"]\n",
    "for i in range(len(losslist)):\n",
    "    plt.plot(losslist[i], 'ro-', color=colorlist[i], alpha=0.8, label=losslistname[i])\n",
    "\n",
    "# 显示标签，如果不加这句，即使加了label='一些数字'的参数，最终还是不会显示标签\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel('trianing times')\n",
    "plt.ylabel('delay')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
