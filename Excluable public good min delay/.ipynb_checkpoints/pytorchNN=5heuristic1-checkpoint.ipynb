{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1587)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "n = 5\n",
    "epochs = 5\n",
    "supervisionEpochs = 3\n",
    "lr = 0.001\n",
    "log_interval = 20\n",
    "trainSize = 100000\n",
    "penaltyLambda = 10\n",
    "doublePeakHighMean = 0.9\n",
    "doublePeakLowMean = 0.1\n",
    "doublePeakStd = 0.1\n",
    "\n",
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "\n",
    "def cdf(x, i=None):\n",
    "    return (d1.cdf(x) + d2.cdf(x) - distributionBase) / 2 / distributionRatio\n",
    "\n",
    "# def cdf(x, i=None):\n",
    "#     if x < 0.1:\n",
    "#         return 0\n",
    "#     if x <= 0.2:\n",
    "#         return 0.5 * (x - 0.1) / 0.1\n",
    "#     if x < 0.8:\n",
    "#         return 0.5\n",
    "#     if x < 0.9:\n",
    "#         return 0.5 + 0.5 * (x - 0.8) / 0.1\n",
    "#     return 1\n",
    "\n",
    "\n",
    "print(distributionBase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals = np.random.randint(2, size=(trainSize, n))\n",
    "# samples1 = np.random.uniform(low=0.1, high=0.2, size=(trainSize, n))\n",
    "# samples2 = np.random.uniform(low=0.8, high=0.9, size=(trainSize, n))\n",
    "samples1 = np.random.normal(\n",
    "    loc=doublePeakLowMean, scale=doublePeakStd, size=(trainSize, n)\n",
    ")\n",
    "for i in range(trainSize):\n",
    "    for j in range(n):\n",
    "        while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "            samples1[i, j] = np.random.normal(\n",
    "                loc=doublePeakLowMean, scale=doublePeakStd\n",
    "            )\n",
    "samples2 = np.random.normal(\n",
    "    loc=doublePeakHighMean, scale=doublePeakStd, size=(trainSize, n)\n",
    ")\n",
    "for i in range(trainSize):\n",
    "    for j in range(n):\n",
    "        while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "            samples2[i, j] = np.random.normal(\n",
    "                loc=doublePeakHighMean, scale=doublePeakStd\n",
    "            )\n",
    "samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "# tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "tp_dataset = TensorDataset(tp_tensor[: round(trainSize * 0.3)])\n",
    "tp_dataset_testing = TensorDataset(tp_tensor[round(trainSize * 0.7) :])\n",
    "tp_dataloader = DataLoader(tp_dataset, batch_size=128, shuffle=True)\n",
    "tp_dataloader_testing = DataLoader(tp_dataset_testing, batch_size=256, shuffle=False)\n",
    "\n",
    "\n",
    "# for mapping binary to payments before softmax\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(n, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, n),\n",
    ")\n",
    "\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    if sum(bits).item() == 0:\n",
    "        return torch.ones(n)\n",
    "    bits = bits.type(torch.float32)\n",
    "    negBits = torch.ones(n) - bits\n",
    "    payments = model(bits)\n",
    "    payments = payments - 1000 * negBits\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    payments = payments + negBits\n",
    "    return payments\n",
    "\n",
    "\n",
    "def tpToBits(tp, bits=torch.ones(n).type(torch.uint8)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = (tp >= payments).type(torch.uint8)\n",
    "    \n",
    "    if torch.equal(newBits, bits):\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits(tp, newBits)#bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    return n - sum(tpToBits(tp).type(torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "tensor(0.5000)\n"
     ]
    }
   ],
   "source": [
    "print(1_000_000)\n",
    "print(cdf(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpPrecision = 100\n",
    "# howManyPpl left, money left, yes already\n",
    "dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "# ppl = 0 left\n",
    "for yes in range(n + 1):\n",
    "    for money in range(dpPrecision + 1):\n",
    "        if money == 0:\n",
    "            dp[0, 0, yes] = 0\n",
    "        else:\n",
    "            dp[0, money, yes] = yes# + 1.0\n",
    "for ppl in range(1, n + 1):\n",
    "    for yes in range(n + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            minSoFar = 1_000_000\n",
    "            for offerIndex in range(money + 1):\n",
    "                offer = offerIndex / dpPrecision\n",
    "                res = (1 - cdf(offer)) * dp[\n",
    "                    ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                ] + cdf(offer) * (1 + dp[ppl - 1, money, yes])\n",
    "                if minSoFar > res:\n",
    "                    minSoFar = res\n",
    "                    decision[ppl, money, yes] = offerIndex\n",
    "            dp[ppl, money, yes] = minSoFar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8651759624481201\n"
     ]
    }
   ],
   "source": [
    "print(dp[n, dpPrecision, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      "0.0\n",
      "5.0\n",
      "\n",
      " 2\n",
      "0.2500000596046448\n",
      "3.749999701976776\n",
      "\n",
      " 3\n",
      "0.2585742771625519\n",
      "3.7071286141872406\n",
      "\n",
      " 4\n",
      "0.27404484152793884\n",
      "3.629775792360306\n",
      "\n",
      " 5\n",
      "0.28367453813552856\n",
      "3.581627309322357\n"
     ]
    }
   ],
   "source": [
    "# howManyPpl left, money left, yes already\n",
    "dp_H = np.zeros([n + 1 , n + 1, dpPrecision + 1])\n",
    "decision_H = np.zeros([n + 1 , n + 1, dpPrecision + 1], dtype=np.uint8)\n",
    "# ppl = 0 left\n",
    "for i in range(1,n+1):\n",
    "    for money in range(dpPrecision + 1):\n",
    "        if money == 0:\n",
    "            dp_H[i, 0, 0] = 1\n",
    "        else:\n",
    "            offer = money / dpPrecision\n",
    "            dp_H[i, 0, money] = 0#cdf(offer)# + 1.0\n",
    "for i in range(1,n+1):\n",
    "    for ppl in range(1, i + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            maxSoFar = -1_000_000\n",
    "            for offerIndex in range(money + 1):\n",
    "                offer = offerIndex / dpPrecision\n",
    "                res = (1-cdf(offer)) * dp_H[\n",
    "                     i, ppl - 1, money - offerIndex\n",
    "                    ]\n",
    "                if maxSoFar < res:\n",
    "                    maxSoFar = res\n",
    "                    decision_H[i, ppl, money] = offerIndex\n",
    "            dp_H[i, ppl, money] = maxSoFar\n",
    "    print(\"\\n\",i)\n",
    "    print(dp_H[i, i, dpPrecision])\n",
    "    print(5*(1-dp_H[i, i, dpPrecision]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.90842331 0.28666622 0.05423724 0.91099693 0.09186071]\n",
      " [0.16985994 0.10930618 0.22175992 0.11748958 0.00288149]\n",
      " [0.133514   0.20053333 0.04271561 0.1486343  0.84952736]\n",
      " ...\n",
      " [0.92434466 0.99357661 0.12371934 0.88285648 0.21044419]\n",
      " [0.00493168 0.64243843 0.05881323 0.81564478 0.93310955]\n",
      " [0.87020499 0.96793927 0.0381335  0.89604189 0.91983237]]\n"
     ]
    }
   ],
   "source": [
    "print(samplesJoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_dp(temp):\n",
    "    #print(temp)\n",
    "    remain=dpPrecision\n",
    "    yes=0;\n",
    "    ans =0;\n",
    "    o_list=[];\n",
    "    remain_list=[];\n",
    "    for ppl in range(n,0,-1):\n",
    "        o=decision[ppl, remain, yes]\n",
    "        #print(o,remain)\n",
    "        o_list.append(o)\n",
    "        remain_list.append(remain);\n",
    "        if(o<temp[n-ppl]):\n",
    "            remain-=int(o);\n",
    "            yes+=1;\n",
    "        elif (remain>0):\n",
    "            ans+=1;\n",
    "    if(remain<=0):\n",
    "        return ans,o_list;\n",
    "    else:\n",
    "        return n,o_list;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8693\n"
     ]
    }
   ],
   "source": [
    "ans_list=[];\n",
    "for i in range(10000):\n",
    "    temp=samplesJoint[i]*dpPrecision\n",
    "    #print(temp)\n",
    "    ans_list.append(plan_dp(temp)[0]);\n",
    "    #print(\"\\n\",temp)\n",
    "    #print(plan_dp(temp))\n",
    "print(sum(ans_list)/len(ans_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9084, 0.2867, 0.0542, 0.9110, 0.0919], dtype=torch.float64)\n",
      "bits 5\n",
      "100 0 0.908423312895584 0.05\n",
      "95 1 0.28666622212604653 0.06\n",
      "89 3 0.910996928805712 0.1\n",
      "money 79\n",
      "bits 3\n",
      "100 0 0.908423312895584 0.79\n",
      "21 1 0.28666622212604653 0.1\n",
      "11 3 0.910996928805712 0.11\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-9a132ca6bd74>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;31m#print(temp)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheuristicSupervisionRule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-9a132ca6bd74>\u001b[0m in \u001b[0;36mheuristicSupervisionRule\u001b[1;34m(tp)\u001b[0m\n\u001b[0;32m     49\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[0mj\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0moffer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmoney\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m                 \u001b[0mmoney\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mofferIndex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0 for ii in range(n)]\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money, yes]\n",
    "        offer = offerIndex / dpPrecision\n",
    "        if tp[i] >= offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits[i] = 0\n",
    "            payments[i] = 1\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def heuristicSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0 for ii in range(n)]\n",
    "\n",
    "    for turns in range(n,0,-1):\n",
    "        money = dpPrecision\n",
    "        j=0\n",
    "        tempo=sum(bits)\n",
    "        print(\"bits\",tempo)\n",
    "        for i in range(n):\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            \n",
    "            offerIndex = decision_H[tempo ,tempo - i, money]\n",
    "            offer = offerIndex / dpPrecision\n",
    "            while(j<n):\n",
    "                if(bits[j]!=0):\n",
    "                    break;\n",
    "                j+=1;\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            if tp[j] >= offer:\n",
    "                print(money,j,tp[j],offer)\n",
    "                money -= offerIndex\n",
    "                bits[j] = 1\n",
    "                payments[j] = offer\n",
    "            else:\n",
    "                bits[j] = 0;\n",
    "                payments[j] = 1;\n",
    "            j+=1;\n",
    "        print(\"money\",money)\n",
    "        if(money==0):\n",
    "            break;\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1 for ii in range(n)]\n",
    "\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "for i in range(10):\n",
    "    temp=torch.tensor(samplesJoint[i])\n",
    "    #print(temp)\n",
    "    print(temp);\n",
    "    print(heuristicSupervisionRule(temp))\n",
    "    print()\n",
    "    \n",
    "def dpDelay(tp):\n",
    "    bits, payments = dpSupervisionRule(tp)\n",
    "    totalPayment = torch.dot(bits.type(torch.float32), payments).item()\n",
    "    if totalPayment < 1:\n",
    "        return n\n",
    "    else:\n",
    "        return n - sum(bits).item()\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    for k in range(n, -1, -1):\n",
    "        if k == 0:\n",
    "            break\n",
    "        bits = [1 if tp[ii] >= 1 / k else 0 for ii in range(n)]\n",
    "        if sum(bits) == k:\n",
    "            break\n",
    "    if k == 0:\n",
    "        payments = [1 for ii in range(n)]\n",
    "    else:\n",
    "        payments = [1 / k if bits[ii] == 1 else 1 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    return n - sum(costSharingSupervisionRule(tp)[0]).item()\n",
    "\n",
    "def heuristicDelay(tp):\n",
    "    return n - sum(heuristicSupervisionRule(tp)[0]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0]\n",
      "tensor([0.2012, 0.1844, 0.1921, 0.2156, 0.2067], grad_fn=<AddBackward0>)\n",
      "[0, 0, 0, 0, 1]\n",
      "tensor([0.2547, 0.2307, 0.2413, 0.2733, 1.0000], grad_fn=<AddBackward0>)\n",
      "[0, 0, 0, 1, 1]\n",
      "tensor([0.3559, 0.3122, 0.3319, 1.0000, 1.0000], grad_fn=<AddBackward0>)\n",
      "[0, 0, 1, 1, 1]\n",
      "tensor([0.5401, 0.4599, 1.0000, 1.0000, 1.0000], grad_fn=<AddBackward0>)\n",
      "[0, 1, 1, 1, 1]\n",
      "tensor([1., 1., 1., 1., 1.], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(n, 0, -1):\n",
    "            print([1 if ii >= i else 0 for ii in range(n)])\n",
    "            print(\n",
    "                    tpToPayments(\n",
    "                            torch.tensor([0 if ii >= i else 1 for ii in range(n)], dtype=torch.float32)\n",
    "                    )\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 1]), tensor([0, 0, 0, 1, 0]), tensor([0, 0, 0, 1, 1]), tensor([0, 0, 1, 0, 0]), tensor([0, 0, 1, 0, 1]), tensor([0, 0, 1, 1, 0]), tensor([0, 0, 1, 1, 1]), tensor([0, 1, 0, 0, 0]), tensor([0, 1, 0, 0, 1]), tensor([0, 1, 0, 1, 0]), tensor([0, 1, 0, 1, 1]), tensor([0, 1, 1, 0, 0]), tensor([0, 1, 1, 0, 1]), tensor([0, 1, 1, 1, 0]), tensor([0, 1, 1, 1, 1]), tensor([1, 0, 0, 0, 0]), tensor([1, 0, 0, 0, 1]), tensor([1, 0, 0, 1, 0]), tensor([1, 0, 0, 1, 1]), tensor([1, 0, 1, 0, 0]), tensor([1, 0, 1, 0, 1]), tensor([1, 0, 1, 1, 0]), tensor([1, 0, 1, 1, 1]), tensor([1, 1, 0, 0, 0]), tensor([1, 1, 0, 0, 1]), tensor([1, 1, 0, 1, 0]), tensor([1, 1, 0, 1, 1]), tensor([1, 1, 1, 0, 0]), tensor([1, 1, 1, 0, 1]), tensor([1, 1, 1, 1, 0]), tensor([1, 1, 1, 1, 1])]\n",
      "\n",
      "tensor([0.2051, 0.8375, 0.0813, 0.2272, 0.1308])\n",
      "tensor(0.5401, grad_fn=<SelectBackward>)\n",
      "tensor([0.5401, 0.4599, 1.0000, 1.0000, 1.0000], grad_fn=<AddBackward0>)\n",
      "tensor(3.)\n",
      "tensor(5.)\n"
     ]
    }
   ],
   "source": [
    "allBits = [torch.tensor(bits) for bits in itertools.product([0, 1], repeat=n)]\n",
    "print(allBits)\n",
    "\n",
    "for batch_idx, (tp_batch,) in enumerate(tp_dataloader_testing):\n",
    "    penalty = 0\n",
    "    for bitsMoreOnes in allBits:\n",
    "        for i in range(n):\n",
    "            if bitsMoreOnes[i] == 1:\n",
    "                bitsLessOnes = bitsMoreOnes.clone()\n",
    "                bitsLessOnes[i] = 0\n",
    "                penalty = penalty + sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "    loss = penalty * penaltyLambda\n",
    "    for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                delay1 = tpToTotalDelay(tp1)\n",
    "                delay0 = tpToTotalDelay(tp0)\n",
    "                #loss = loss + (1 - cdf(offer)) * delay1 + cdf(offer) * delay0\n",
    "                loss = loss + (1 - cdf(offer)) * delay1 + cdf(offer) * delay0\n",
    "    print()\n",
    "    print(tp)\n",
    "    tp1 = tp.clone()\n",
    "    tp1[0] = 1\n",
    "    tp0 = tp.clone()\n",
    "    tp0[0] = 0\n",
    "    offer = tpToPayments(tp1)[0]\n",
    "    print(offer)\n",
    "    print(tpToPayments(tp1))\n",
    "    print(delay1)\n",
    "    print(delay0)\n",
    "    break;\n",
    "#print(loss)\n",
    "#print(penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "allBits = [torch.tensor(bits) for bits in itertools.product([0, 1], repeat=n)]\n",
    "\n",
    "runningLossNN = []\n",
    "runningLossCS = []\n",
    "runningLossDP = []\n",
    "runningLossHeuristic = []\n",
    "\n",
    "def recordAndReport(name, source, loss, n=100):\n",
    "    source.append(loss)\n",
    "    realLength = min(n, len(source))\n",
    "    avgLoss = sum(source[-n:]) / realLength\n",
    "    print(f\"{name} ({realLength}): {avgLoss}\")\n",
    "\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        loss = 0\n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            # print()\n",
    "            # print(\"supervision\")\n",
    "            # print(tp)\n",
    "            # print(bits)\n",
    "            # print()\n",
    "            # print(payments)\n",
    "            # print(bitsToPayments(bits))\n",
    "            # print()\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print()\n",
    "            print(tp)\n",
    "            print(bits)\n",
    "            print(payments)\n",
    "            print(bitsToPayments(bits))\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        # costSharingLoss = 0\n",
    "        # dpLoss = 0\n",
    "        for tp in tp_batch:\n",
    "            # costSharingLoss += costSharingDelay(tp)\n",
    "            # dpLoss += dpDelay(tp)\n",
    "            # print()\n",
    "            # print(\"---\")\n",
    "            # print(tp)\n",
    "            # print(costSharingSupervisionRule(tp))\n",
    "            # print(dpSupervisionRule(tp))\n",
    "            # print(costSharingDelay(tp), dpDelay(tp))\n",
    "            # print()\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                delay1 = tpToTotalDelay(tp1)\n",
    "                delay0 = tpToTotalDelay(tp0)\n",
    "                loss = loss + (1 - cdf(offer)) * delay1 + cdf(offer) * delay0\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        # costSharingLoss /= len(tp_batch)\n",
    "        # dpLoss /= len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "            # recordAndReport(\"NN\", runningLossNN, loss.item())\n",
    "            # recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "            # recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "            # print(dp[n, dpPrecision, 0])\n",
    "            print(\"penalty:\",penalty.item())\n",
    "            # print(distributionRatio)\n",
    "            # for i in range(n, 0, -1):\n",
    "            #     print(\n",
    "            #         tpToPayments(\n",
    "            #             torch.tensor([1] * i + [0] * (n - i), dtype=torch.float32)\n",
    "            #         )\n",
    "            #     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8413)\n",
      "\n",
      "tensor([0.7765, 0.8274, 0.1956, 0.8534, 0.9885])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.0500, 0.0600, 0.0600, 0.0600, 0.7700])\n",
      "tensor([0.2029, 0.1838, 0.1847, 0.2180, 0.2105], grad_fn=<AddBackward0>)\n",
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 0.059809\n",
      "\n",
      "tensor([0.0077, 0.8629, 0.9748, 0.2353, 0.9314])\n",
      "tensor([0., 1., 1., 1., 1.])\n",
      "tensor([1.0000, 0.7900, 0.1000, 0.1100, 0.0000])\n",
      "tensor([1.0000, 0.2300, 0.1546, 0.3309, 0.2845], grad_fn=<AddBackward0>)\n",
      "Train Epoch: 1 [2560/30000 (9%)]\tLoss: 0.050502\n",
      "\n",
      "tensor([0.2352, 0.1597, 0.9460, 0.7777, 0.1940])\n",
      "tensor([0., 0., 0., 0., 0.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [5120/30000 (17%)]\tLoss: 0.040441\n",
      "\n",
      "tensor([0.0797, 0.8176, 0.0754, 0.2632, 0.1091])\n",
      "tensor([0., 0., 0., 0., 0.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [7680/30000 (26%)]\tLoss: 0.036822\n",
      "\n",
      "tensor([0.0778, 0.9392, 0.0042, 0.0689, 0.1370])\n",
      "tensor([0., 0., 0., 0., 0.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [10240/30000 (34%)]\tLoss: 0.031851\n",
      "\n",
      "tensor([0.0092, 0.1310, 0.0699, 0.4405, 0.2049])\n",
      "tensor([0., 0., 0., 0., 0.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [12800/30000 (43%)]\tLoss: 0.030405\n",
      "\n",
      "tensor([0.8879, 0.2061, 0.7906, 0.9397, 0.8435])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.0500, 0.0600, 0.0600, 0.0600, 0.7700])\n",
      "tensor([0.0345, 0.0774, 0.0838, 0.2840, 0.5203], grad_fn=<AddBackward0>)\n",
      "Train Epoch: 1 [15360/30000 (51%)]\tLoss: 0.030678\n",
      "\n",
      "tensor([0.8019, 0.2705, 0.9245, 0.8702, 0.1342])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.0800, 0.0700, 0.0700, 0.7800, 0.0000])\n",
      "tensor([0.0337, 0.0774, 0.0756, 0.2869, 0.5264], grad_fn=<AddBackward0>)\n",
      "Train Epoch: 1 [17920/30000 (60%)]\tLoss: 0.029182\n",
      "\n",
      "tensor([0.0969, 0.1568, 0.7672, 0.8386, 0.8023])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.0500, 0.0600, 0.0600, 0.0600, 0.7700])\n",
      "tensor([0.0340, 0.0626, 0.0768, 0.3383, 0.4882], grad_fn=<AddBackward0>)\n",
      "Train Epoch: 1 [20480/30000 (68%)]\tLoss: 0.029185\n",
      "\n",
      "tensor([0.1248, 0.8923, 0.0416, 0.3329, 0.0594])\n",
      "tensor([0., 0., 0., 0., 0.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [23040/30000 (77%)]\tLoss: 0.033345\n",
      "\n",
      "tensor([0.8220, 0.0738, 0.2132, 0.8831, 0.1259])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.0800, 0.0700, 0.0700, 0.7800, 0.0000])\n",
      "tensor([0.0498, 0.0626, 0.0641, 0.3066, 0.5169], grad_fn=<AddBackward0>)\n",
      "Train Epoch: 1 [25600/30000 (85%)]\tLoss: 0.032761\n",
      "\n",
      "tensor([0.1314, 0.0656, 0.8448, 0.9948, 0.2038])\n",
      "tensor([0., 0., 0., 0., 0.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [28160/30000 (94%)]\tLoss: 0.040135\n",
      "tensor(0.8413)\n",
      "\n",
      "tensor([0.7907, 0.9533, 0.1391, 0.9185, 0.8802])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.0500, 0.0600, 0.0600, 0.0600, 0.7700])\n",
      "tensor([0.0466, 0.0662, 0.0643, 0.2558, 0.5672], grad_fn=<AddBackward0>)\n",
      "Train Epoch: 2 [0/30000 (0%)]\tLoss: 0.023886\n",
      "\n",
      "tensor([0.9473, 0.2595, 0.3690, 0.1630, 0.2198])\n",
      "tensor([1., 1., 1., 0., 1.])\n",
      "tensor([0.7900, 0.1000, 0.1100, 1.0000, 0.0000])\n",
      "tensor([0.5867, 0.0986, 0.1105, 1.0000, 0.2042], grad_fn=<AddBackward0>)\n",
      "Train Epoch: 2 [2560/30000 (9%)]\tLoss: 0.033198\n",
      "\n",
      "tensor([0.7946, 0.7710, 0.8224, 0.2541, 0.9751])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.0500, 0.0600, 0.0600, 0.0600, 0.7700])\n",
      "tensor([0.0668, 0.0668, 0.0707, 0.3298, 0.4660], grad_fn=<AddBackward0>)\n",
      "Train Epoch: 2 [5120/30000 (17%)]\tLoss: 0.033714\n",
      "\n",
      "tensor([0.0839, 0.7691, 0.0745, 0.8756, 0.0691])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.0800, 0.0700, 0.0700, 0.7800, 0.0000])\n",
      "tensor([0.0654, 0.0624, 0.0677, 0.3680, 0.4365], grad_fn=<AddBackward0>)\n",
      "Train Epoch: 2 [7680/30000 (26%)]\tLoss: 0.028726\n",
      "\n",
      "tensor([0.0654, 0.1569, 0.8309, 0.0718, 0.0541])\n",
      "tensor([0., 0., 0., 0., 0.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 2 [10240/30000 (34%)]\tLoss: 0.031552\n",
      "\n",
      "tensor([0.2527, 0.8962, 0.1031, 0.8271, 0.0238])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.0800, 0.0700, 0.0700, 0.7800, 0.0000])\n",
      "tensor([0.0602, 0.0645, 0.0646, 0.3331, 0.4776], grad_fn=<AddBackward0>)\n",
      "Train Epoch: 2 [12800/30000 (43%)]\tLoss: 0.032729\n",
      "\n",
      "tensor([0.9041, 0.1697, 0.9516, 0.1002, 0.9640])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.0500, 0.0600, 0.0600, 0.0600, 0.7700])\n",
      "tensor([0.0648, 0.0693, 0.0730, 0.2923, 0.5006], grad_fn=<AddBackward0>)\n",
      "Train Epoch: 2 [15360/30000 (51%)]\tLoss: 0.033191\n",
      "\n",
      "tensor([0.1642, 0.8936, 0.1172, 0.9790, 0.9600])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.0500, 0.0600, 0.0600, 0.0600, 0.7700])\n",
      "tensor([0.0541, 0.0725, 0.0659, 0.3081, 0.4993], grad_fn=<AddBackward0>)\n",
      "Train Epoch: 2 [17920/30000 (60%)]\tLoss: 0.025712\n",
      "\n",
      "tensor([0.1784, 0.1627, 0.1097, 0.9243, 0.0647])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.0800, 0.0700, 0.0700, 0.7800, 0.0000])\n",
      "tensor([0.0577, 0.0613, 0.0638, 0.2805, 0.5368], grad_fn=<AddBackward0>)\n",
      "Train Epoch: 2 [20480/30000 (68%)]\tLoss: 0.029557\n",
      "\n",
      "tensor([0.7733, 0.8083, 0.6339, 0.2315, 0.9754])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.0500, 0.0600, 0.0600, 0.0600, 0.7700])\n",
      "tensor([0.0630, 0.0616, 0.0643, 0.3256, 0.4854], grad_fn=<AddBackward0>)\n",
      "Train Epoch: 2 [23040/30000 (77%)]\tLoss: 0.027541\n",
      "\n",
      "tensor([0.9055, 0.8341, 0.0644, 0.8711, 0.0587])\n",
      "tensor([1., 1., 0., 1., 1.])\n",
      "tensor([0.7900, 0.1000, 1.0000, 0.1100, 0.0000])\n",
      "tensor([0.1482, 0.0378, 1.0000, 0.7739, 0.0400], grad_fn=<AddBackward0>)\n",
      "Train Epoch: 2 [25600/30000 (85%)]\tLoss: 0.030451\n",
      "\n",
      "tensor([0.1626, 0.6077, 0.9258, 0.2728, 0.9405])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.0500, 0.0600, 0.0600, 0.0600, 0.7700])\n",
      "tensor([0.0577, 0.0586, 0.0564, 0.2973, 0.5300], grad_fn=<AddBackward0>)\n",
      "Train Epoch: 2 [28160/30000 (94%)]\tLoss: 0.023834\n",
      "tensor(0.8413)\n",
      "\n",
      "tensor([0.0968, 0.0439, 0.8770, 0.8784, 0.0655])\n",
      "tensor([1., 0., 1., 1., 1.])\n",
      "tensor([0.0500, 1.0000, 0.1600, 0.7900, 0.0000])\n",
      "tensor([0.0585, 1.0000, 0.1677, 0.7459, 0.0278], grad_fn=<AddBackward0>)\n",
      "Train Epoch: 3 [0/30000 (0%)]\tLoss: 0.028877\n",
      "\n",
      "tensor([0.1614, 0.0401, 0.2606, 0.0221, 0.0041])\n",
      "tensor([0., 0., 0., 0., 0.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 3 [2560/30000 (9%)]\tLoss: 0.026763\n",
      "\n",
      "tensor([0.0552, 0.0161, 0.0601, 0.7315, 0.1694])\n",
      "tensor([0., 0., 0., 0., 0.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 3 [5120/30000 (17%)]\tLoss: 0.032068\n",
      "\n",
      "tensor([0.0788, 0.8519, 0.0715, 0.1151, 0.2225])\n",
      "tensor([0., 1., 0., 1., 1.])\n",
      "tensor([1.0000, 0.7900, 1.0000, 0.1000, 0.1100])\n",
      "tensor([1.0000, 0.7284, 1.0000, 0.1906, 0.0810], grad_fn=<AddBackward0>)\n",
      "Train Epoch: 3 [7680/30000 (26%)]\tLoss: 0.022493\n",
      "\n",
      "tensor([0.0187, 0.9143, 0.9401, 0.9537, 0.0530])\n",
      "tensor([0., 1., 1., 1., 1.])\n",
      "tensor([1.0000, 0.7900, 0.1000, 0.1100, 0.0000])\n",
      "tensor([1.0000, 0.6678, 0.2040, 0.0903, 0.0379], grad_fn=<AddBackward0>)\n",
      "Train Epoch: 3 [10240/30000 (34%)]\tLoss: 0.032615\n",
      "\n",
      "tensor([0.1567, 0.8504, 0.1290, 0.0995, 0.1857])\n",
      "tensor([0., 1., 1., 0., 1.])\n",
      "tensor([1.0000, 0.7900, 0.1000, 1.0000, 0.1100])\n",
      "tensor([1.0000, 0.7367, 0.1695, 1.0000, 0.0938], grad_fn=<AddBackward0>)\n",
      "Train Epoch: 3 [12800/30000 (43%)]\tLoss: 0.025948\n",
      "\n",
      "tensor([0.9002, 0.1066, 0.1345, 0.1723, 0.0321])\n",
      "tensor([1., 1., 1., 0., 1.])\n",
      "tensor([0.7900, 0.1000, 0.1100, 1.0000, 0.0000])\n",
      "tensor([0.5728, 0.0918, 0.0919, 1.0000, 0.2434], grad_fn=<AddBackward0>)\n",
      "Train Epoch: 3 [15360/30000 (51%)]\tLoss: 0.031114\n",
      "\n",
      "tensor([0.0301, 0.0958, 0.8741, 0.2362, 0.8807])\n",
      "tensor([0., 0., 1., 1., 1.])\n",
      "tensor([1.0000, 1.0000, 0.7900, 0.1000, 0.1100])\n",
      "tensor([1.0000, 1.0000, 0.7542, 0.1770, 0.0688], grad_fn=<AddBackward0>)\n",
      "Train Epoch: 3 [17920/30000 (60%)]\tLoss: 0.030793\n",
      "\n",
      "tensor([0.2393, 0.8026, 0.0718, 0.2342, 0.0457])\n",
      "tensor([0., 0., 0., 0., 0.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 3 [20480/30000 (68%)]\tLoss: 0.031176\n",
      "\n",
      "tensor([0.1016, 0.8374, 0.9398, 0.9266, 0.0689])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0.0800, 0.0700, 0.0700, 0.7800, 0.0000])\n",
      "tensor([0.0697, 0.0649, 0.0658, 0.2921, 0.5075], grad_fn=<AddBackward0>)\n",
      "Train Epoch: 3 [23040/30000 (77%)]\tLoss: 0.038655\n",
      "\n",
      "tensor([0.0423, 0.3448, 0.0852, 0.9656, 0.2075])\n",
      "tensor([0., 0., 0., 0., 0.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 3 [25600/30000 (85%)]\tLoss: 0.024648\n",
      "\n",
      "tensor([0.0952, 0.8455, 0.8363, 0.9028, 0.7524])\n",
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0800, 0.0700, 0.0700, 0.7800, 0.0000])\n",
      "tensor([0.0534, 0.0568, 0.0588, 0.3127, 0.5183], grad_fn=<AddBackward0>)\n",
      "Train Epoch: 3 [28160/30000 (94%)]\tLoss: 0.034665\n",
      "30000\n",
      "NN (1): 2.1986000537872314\n",
      "CS (1): 2.5378333333333334\n",
      "DP (1): 1.8716333333333333\n",
      "heuristic (1): 1.5372\n",
      "DP: 1.8651759624481201\n",
      "Heuristic: 5 3.581627309322357\n",
      "Heuristic: 4 3.629775792360306\n",
      "Heuristic: 3 3.7071286141872406\n",
      "Heuristic: 2 3.749999701976776\n",
      "Heuristic: 1 5.0\n",
      "tensor([0.0665, 0.0685, 0.0652, 0.3092, 0.4906])\n",
      "tensor([0.1215, 0.1167, 0.1179, 0.6439, 1.0000])\n",
      "tensor([0.6429, 0.1621, 0.1950, 1.0000, 1.0000])\n",
      "tensor([0.8046, 0.1954, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 2.227330\n",
      "penalty: 9.031968116760254\n",
      "Train Epoch: 1 [2560/30000 (9%)]\tLoss: 1.778499\n",
      "penalty: 0.13160499930381775\n",
      "Train Epoch: 1 [5120/30000 (17%)]\tLoss: 1.849719\n",
      "penalty: 0.004550021141767502\n",
      "Train Epoch: 1 [7680/30000 (26%)]\tLoss: 1.876336\n",
      "penalty: 0.05233722925186157\n",
      "Train Epoch: 1 [10240/30000 (34%)]\tLoss: 1.683511\n",
      "penalty: 0.008070863783359528\n",
      "Train Epoch: 1 [12800/30000 (43%)]\tLoss: 1.708607\n",
      "penalty: 0.011591613292694092\n",
      "Train Epoch: 1 [15360/30000 (51%)]\tLoss: 1.714773\n",
      "penalty: 0.0\n",
      "Train Epoch: 1 [17920/30000 (60%)]\tLoss: 1.723216\n",
      "penalty: 0.0\n",
      "Train Epoch: 1 [20480/30000 (68%)]\tLoss: 1.647277\n",
      "penalty: 0.0\n",
      "Train Epoch: 1 [23040/30000 (77%)]\tLoss: 1.884236\n",
      "penalty: 0.0\n",
      "Train Epoch: 1 [25600/30000 (85%)]\tLoss: 1.763248\n",
      "penalty: 0.0\n",
      "Train Epoch: 1 [28160/30000 (94%)]\tLoss: 1.780913\n",
      "penalty: 0.0019494891166687012\n",
      "30000\n",
      "NN (2): 1.9817166328430176\n",
      "CS (2): 2.5378333333333334\n",
      "DP (2): 1.8716333333333333\n",
      "heuristic (2): 1.5372\n",
      "DP: 1.8651759624481201\n",
      "Heuristic: 5 3.581627309322357\n",
      "Heuristic: 4 3.629775792360306\n",
      "Heuristic: 3 3.7071286141872406\n",
      "Heuristic: 2 3.749999701976776\n",
      "Heuristic: 1 5.0\n",
      "tensor([0.0588, 0.0660, 0.0675, 0.7396, 0.0680])\n",
      "tensor([0.0751, 0.0849, 0.0875, 0.7524, 1.0000])\n",
      "tensor([0.7378, 0.1289, 0.1334, 1.0000, 1.0000])\n",
      "tensor([0.7508, 0.2492, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 2 [0/30000 (0%)]\tLoss: 1.931380\n",
      "penalty: 0.0\n",
      "Train Epoch: 2 [2560/30000 (9%)]\tLoss: 1.667367\n",
      "penalty: 0.0\n",
      "Train Epoch: 2 [5120/30000 (17%)]\tLoss: 1.791837\n",
      "penalty: 0.007417201995849609\n",
      "Train Epoch: 2 [7680/30000 (26%)]\tLoss: 1.756412\n",
      "penalty: 0.007328450679779053\n",
      "Train Epoch: 2 [10240/30000 (34%)]\tLoss: 1.971399\n",
      "penalty: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-442fd3a6df5a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-5bd2930db30c>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;31m# costSharingLoss /= len(tp_batch)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;31m# dpLoss /= len(tp_batch)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ComputerSoftwares\\Anaconda\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \"\"\"\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ComputerSoftwares\\Anaconda\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        costSharingLoss = 0\n",
    "        dpLoss = 0\n",
    "        nnLoss = 0\n",
    "        heuristicLoss=0\n",
    "        lenLoss= 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "                heuristicLoss+= heuristicDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        costSharingLoss /= lenLoss\n",
    "        dpLoss /= lenLoss\n",
    "        nnLoss /= lenLoss\n",
    "        heuristicLoss /= lenLoss\n",
    "        print(lenLoss)\n",
    "        recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "        recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "        recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "        recordAndReport(\"heuristic\", runningLossHeuristic, heuristicLoss)\n",
    "        print(\"DP:\",dp[n, dpPrecision,0])\n",
    "        for i in range(n, 0, -1):\n",
    "            print(\"Heuristic:\",i,5*(1-dp_H[i, i, dpPrecision]))\n",
    "        for i in range(n, 0, -1):\n",
    "            print(\n",
    "                    tpToPayments(\n",
    "                            torch.tensor([0 if ii >= i else 1 for ii in range(n)], dtype=torch.float32)\n",
    "                    )\n",
    "                )\n",
    "\n",
    "#model=torch.load(\"save/pytorchNN=5dp1\");\n",
    "#model.eval()\n",
    "for epoch in range(1, supervisionEpochs + 1):\n",
    "    print(distributionRatio)\n",
    "#    #supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "#    supervisionTrain(epoch, dpSupervisionRule)\n",
    "    supervisionTrain(epoch, heuristicSupervisionRule)\n",
    "test()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save(model, \"save/pytorchNN=5heuristic1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = torch.load(PATH)\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
