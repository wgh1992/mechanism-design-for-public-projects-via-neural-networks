{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from random import randint\n",
    "import random\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "from pylab import *\n",
    "from scipy.stats import beta\n",
    "import math\n",
    "n = 3\n",
    "epochs = 5\n",
    "supervisionEpochs = 8\n",
    "lr = 0.0005\n",
    "log_interval = 20\n",
    "trainSize = 60000#100000\n",
    "percentage_train_test= 0.25\n",
    "penaltyLambda = 10\n",
    "doublePeakHighMean = 0.9\n",
    "doublePeakLowMean = 0.1\n",
    "doublePeakStd = 0.1\n",
    "uniformlow=0\n",
    "uniformhigh=1.0\n",
    "normalloc = 0.2\n",
    "normalscale = 0.1\n",
    "\n",
    "cauchyloc = 1/n\n",
    "cauchyscalen = 0.004\n",
    "\n",
    "exponentialhigh = 15 #Symbol(\"b\", real=True)\n",
    "exponentiallow  = 15 #Symbol(\"a\", real=True)\n",
    "\n",
    "\n",
    "betahigh = 0.1\n",
    "betalow  = 0.1\n",
    "\n",
    "\n",
    "independentnormalloc1=[(float(ii)+1)/(2*n+1) for ii in range(n,0,-1)]\n",
    "independentnormalscale1=[0.05 for ii in range(n)]\n",
    "\n",
    "independentnormalloc2=[(float(ii)+1)/(2*n+1) for ii in range(1,n+1,1)]\n",
    "independentnormalscale2=[0.05 for ii in range(n)]\n",
    "stage=[\"beta\"]\n",
    "# \"twopeak\",\"normal\",\"uniform\",\"independent1\",\"independent2\",\"cauchy\",\"beta\",\"U-exponential\",\"arcsine\"\n",
    "order1name=[\"dp\",\"random initializing\",\"costsharing\"]\n",
    "# \"costsharing\",\"dp\",\"heuristic\",\"random initializing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio1 = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase1 = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "d3 = D.normal.Normal(normalloc, normalscale)\n",
    "distributionRatio3 = d3.cdf(1) - d3.cdf(0)\n",
    "distributionBase3 = d3.cdf(0)\n",
    "\n",
    "d4 = D.uniform.Uniform(uniformlow,uniformhigh)\n",
    "distributionRatio4 = d4.cdf(1) - d4.cdf(0)\n",
    "distributionBase4 = d4.cdf(0)\n",
    "\n",
    "d5 = [D.normal.Normal(independentnormalloc1[ii], independentnormalscale1[ii]) for ii in range(n)]\n",
    "d6 = [D.normal.Normal(independentnormalloc2[ii], independentnormalscale2[ii]) for ii in range(n)]\n",
    "\n",
    "d7 = D.cauchy.Cauchy(cauchyloc,cauchyscalen)\n",
    "\n",
    "d81 = D.exponential.Exponential(exponentiallow)\n",
    "d82 = D.exponential.Exponential(exponentialhigh)\n",
    "\n",
    "d9 = D.beta.Beta(betahigh,betalow)\n",
    "#sample_d9=d9.rsample(torch.Size([100000]))\n",
    "\n",
    "#d9_sample=np.linspace(0.0001, 0.9999, 10000)\n",
    "#d9_pdf=torch.exp(d9.log_prob(torch.tensor(d9_sample,dtype=torch.float32)))\n",
    "#d9_delta=d9_sample[1]-d9_sample[0]\n",
    "#d9_sum_pdf=torch.sum(d9_pdf*d9_delta)\n",
    "\n",
    "d10 = D.beta.Beta(0.5,0.5)\n",
    "\n",
    "def cdf(x,y, i=None):\n",
    "    if(y==\"twopeak\"):\n",
    "        return (d1.cdf(x) + d2.cdf(x) - distributionBase1) / 2 / distributionRatio1\n",
    "    elif(y==\"normal\"):\n",
    "        return (d3.cdf(x)-distributionBase3)/distributionRatio3;\n",
    "    elif(y==\"uniform\"):\n",
    "        return (d4.cdf(x)-distributionBase4)/distributionRatio4;\n",
    "    elif(y==\"independent1\"):\n",
    "        return d5[i].cdf(x);\n",
    "    elif(y==\"independent2\"):\n",
    "        return d6[i].cdf(x);\n",
    "    elif(y==\"cauchy\"):\n",
    "        return d7.cdf(x);\n",
    "    elif(y==\"beta\"):\n",
    "#         sum_cdf=0.0;\n",
    "#         if(x<0.0001):\n",
    "#             x=0.00011;\n",
    "#         if(x>0.9999):\n",
    "#             x=0.99989;\n",
    "#         for i in range(len(d9_pdf)):\n",
    "#             if(d9_sample[i]<x):\n",
    "#                 sum_cdf+=d9_pdf[i]*d9_delta;\n",
    "#             else:\n",
    "#                 sum_cdf+=(d9_pdf[i]+d9_pdf[i-1])/ 2 *(x-d9_sample[i-1])\n",
    "#                 break;\n",
    "#         return sum_cdf/d9_sum_pdf\n",
    "#         cdf_v=torch.sum((sample_d9<(x)), dtype=torch.float32)/100000\n",
    "#         return cdf_v\n",
    "#    F(x|a,b)=1–(1–x^a)^b\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(x,betahigh),betalow);\n",
    "        except:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(torch.tensor(x,dtype=torch.float32),betahigh),betalow);\n",
    "    elif(y==\"arcsine\"):\n",
    "        #\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            res=2.0/math.pi * torch.asin(torch.sqrt(x))\n",
    "            #print(x)\n",
    "            return res# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "        except:\n",
    "            return 2.0/math.pi * torch.asin(torch.sqrt(torch.tensor(x,dtype=torch.float32)))# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "    elif(y==\"U-exponential\"):\n",
    "        return (d81.cdf(x) + (1.0 - d82.cdf(1.0-x)))  / 2 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    if torch.sum(bits).item() == 0:\n",
    "        return torch.ones(n)\n",
    "    bits = bits.type(torch.float32)\n",
    "    negBits = torch.ones(n) - bits\n",
    "    payments = model(bits)\n",
    "    payments = payments - 1000 * negBits\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    payments = payments + negBits\n",
    "    return payments\n",
    "\n",
    "def tpToBits(tp, bits=torch.ones(n).type(torch.uint8)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = (tp >= payments).type(torch.uint8)\n",
    "    if torch.equal(newBits, bits):\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits(tp, newBits)#bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    return n - torch.sum(tpToBits(tp).type(torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    global  dp,decision\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money, yes]\n",
    "        offer = float(offerIndex) / dpPrecision\n",
    "        if tp[i] >= offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits[i] = 0\n",
    "            payments[i] = 1\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "#     print()\n",
    "#     print(tp)\n",
    "#     print(bits)\n",
    "#     print(payments)\n",
    "#     print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def heuristicSupervisionRule(tp):\n",
    "    global  dp_H,decision_H\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    tempold=-1;\n",
    "    for turns in range(n,0,-1):\n",
    "        money = dpPrecision\n",
    "        j=0\n",
    "        tempo=sum(bits)\n",
    "        #print(\"bits\",tempo)\n",
    "        for i in range(n):\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            \n",
    "            offerIndex = decision_H[tempo ,tempo - i, money]\n",
    "            offer = float(offerIndex) / dpPrecision\n",
    "            while(j<n):\n",
    "                if(bits[j]!=0):\n",
    "                    break;\n",
    "                j+=1;\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            if tp[j] >= offer:\n",
    "                #print(money,j,tp[j],offer)\n",
    "                money -= offerIndex\n",
    "                bits[j] = 1\n",
    "                payments[j] = offer\n",
    "            else:\n",
    "                bits[j] = 0;\n",
    "                payments[j] = 1.0;\n",
    "            j+=1;\n",
    "        #print(\"money\",money)\n",
    "        if(money==0 and tempold==tempo):\n",
    "            break;\n",
    "        tempold=tempo;\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    for k in range(n, -1, -1):\n",
    "        if k == 0:\n",
    "            break\n",
    "        bits = [1 if tp[ii] >= 1.0 / k else 0 for ii in range(n)]\n",
    "        if sum(bits) == k:\n",
    "            break\n",
    "    if k == 0:\n",
    "        payments = [1 for ii in range(n)]\n",
    "    else:\n",
    "        payments = [1.0 / k if bits[ii] == 1 else 1 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    return float(n - torch.sum(costSharingSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def dpDelay(tp):\n",
    "    return float(n - torch.sum(dpSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def heuristicDelay(tp):\n",
    "    return float(n - torch.sum(heuristicSupervisionRule(tp)[0]).item())\n",
    "\n",
    "\n",
    "# templire=0;\n",
    "# num=1\n",
    "# for i in range(num):\n",
    "#     temp=torch.tensor(samplesJoint[i])\n",
    "#     #print(temp)\n",
    "#     #print(temp);\n",
    "#     #print(heuristicSupervisionRule(temp))\n",
    "#     #print(dpSupervisionRule(temp))\n",
    "#     #print(dpDelay(temp))\n",
    "#     res=dpDelay(temp)\n",
    "#     templire+=res\n",
    "#     #print(\"delay\",res)\n",
    "#     #print()\n",
    "# print(templire/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordAndReport(name, source, loss):\n",
    "    source.append(loss)\n",
    "    realLength = len(source)\n",
    "    #print(f\"{name} ({realLength}): {loss}\")\n",
    "    print(name,realLength,\":\" ,loss)\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    \n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda /100000\n",
    "        \n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "losslist=[];\n",
    "losslistname=[];\n",
    "losslisttemp=[];\n",
    "def test_batch():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lenLoss= 0\n",
    "        nnLoss = 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "            for tp in tp_batch:\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        nnLoss/=lenLoss\n",
    "    return nnLoss\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                delay1 = tpToTotalDelay(tp1)\n",
    "                delay0 = tpToTotalDelay(tp0)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                    loss = loss + (1 - cdf(offer,order)) * delay1 + cdf(offer,order) * delay0\n",
    "                else:\n",
    "                    loss = loss + (1 - cdf(offer,order,i)) * delay1 + cdf(offer,order,i) * delay0\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "    \n",
    "allBits = [torch.tensor(bits, dtype=torch.int16) for bits in itertools.product([0, 1], repeat=n)]\n",
    "\n",
    "def test():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        costSharingLoss = 0\n",
    "        dpLoss = 0\n",
    "        nnLoss = 0\n",
    "        heuristicLoss=0\n",
    "        lenLoss= 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "                heuristicLoss+= heuristicDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        costSharingLoss /= lenLoss\n",
    "        dpLoss /= lenLoss\n",
    "        nnLoss /= lenLoss\n",
    "        heuristicLoss /= lenLoss\n",
    "        #print(lenLoss)\n",
    "        losslisttemp.append(nnLoss);\n",
    "        recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "        recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "        recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "        recordAndReport(\"heuristic\", runningLossHeuristic, heuristicLoss)\n",
    "        print(\"DP:\",dp[n, dpPrecision,0])\n",
    "        #for i in range(n, 0, -1):\n",
    "        #    print(\"Heuristic:\",i,5*(1-dp_H[i, i, dpPrecision]))\n",
    "        for i in range(n, 0, -1):\n",
    "            print(\n",
    "                    tpToPayments(\n",
    "                            torch.tensor([0 if ii >= i else 1 for ii in range(n)], dtype=torch.float32)\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "dpPrecision = 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def producedata(order):\n",
    "    global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "    dpPrecision = 100 \n",
    "    if(order==\"twopeak\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = np.random.normal(\n",
    "            loc=doublePeakLowMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=doublePeakLowMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samples2 = np.random.normal(\n",
    "            loc=doublePeakHighMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = np.random.normal(\n",
    "                        loc=doublePeakHighMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "    elif(order==\"normal\"):\n",
    "        print(\"loc\",normalloc, \"scale\",normalscale)\n",
    "        samples1 = np.random.normal(\n",
    "            loc=normalloc, scale=normalscale, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=normalloc, scale=normalscale\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"uniform\"):  \n",
    "        print(\"uniformlow\",uniformlow, \"uniformhigh\",uniformhigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        uniformlow, uniformhigh\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent1\"):\n",
    "        print(\"loc\",independentnormalloc1,\"scale\",independentnormalscale1)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent2\"):\n",
    "        print(\"loc\",independentnormalloc2, \"scale\",independentnormalscale2)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"cauchy\"):\n",
    "        print(\"cauchyloc\",cauchyloc, \"cauchyscale\",cauchyscalen)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"beta\"):\n",
    "        print(\"betalow\",betalow, \"betahigh\",betahigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(betahigh,betalow,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(betahigh,betalow,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"arcsine\"):\n",
    "        print(\"betalow\",0.5, \"betahigh\",0.5)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"U-exponential\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = d81.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d81.rsample(torch.Size([1])).numpy()\n",
    "                    \n",
    "        samples2 = d82.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        \n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = d82.rsample(torch.Size([1])).numpy()\n",
    "        samples2 = 1.0 - samples2\n",
    "        samplesJoint = signals * samples1 - (signals - 1.0) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "        \n",
    "    plt.hist(samplesJoint,bins=500)\n",
    "    plt.show()\n",
    "    \n",
    "    tp_dataset = TensorDataset(tp_tensor[: int(trainSize * percentage_train_test)])\n",
    "    tp_dataset_testing = TensorDataset(tp_tensor[int(trainSize * (1.0-percentage_train_test)) :])\n",
    "    tp_dataloader = DataLoader(tp_dataset, batch_size=128, shuffle=True)\n",
    "    tp_dataloader_testing = DataLoader(tp_dataset_testing, batch_size=256, shuffle=False)\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "    decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for yes in range(n + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp[0, 0, yes] = 0\n",
    "            else:\n",
    "                dp[0, money, yes] = yes# + 1.0\n",
    "    for ppl in range(1,  n + 1):\n",
    "        for yes in range(n + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                minSoFar = 1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1 - cdf(offer,order)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order) * (1 + dp[ppl - 1, money, yes])\n",
    "                    else:\n",
    "                        res = (1 - cdf(offer,order,n-ppl)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order,n-ppl) * (1 + dp[ppl - 1, money, yes])\n",
    "                    if minSoFar > res.item():\n",
    "                        minSoFar = res.item()\n",
    "                        decision[ppl, money, yes] = offerIndex\n",
    "                dp[ppl, money, yes] = minSoFar\n",
    "    \n",
    "    print(\"dp\",dp[n, dpPrecision, 0])\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp_H = np.zeros([n + 1 , n + 1, dpPrecision + 1])\n",
    "    decision_H = np.zeros([n + 1 , n + 1, dpPrecision + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for i in range(1,n+1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp_H[i, 0, 0] = 1\n",
    "            else:\n",
    "                offer = money / dpPrecision\n",
    "                dp_H[i, 0, money] = 0#cdf(offer)# + 1.0\n",
    "    for i in range(1,n+1):\n",
    "        for ppl in range(1, i + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                maxSoFar = -1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1-cdf(offer,order)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    else:\n",
    "                        res = (1-cdf(offer,order,ppl-1)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    if maxSoFar < res.item():\n",
    "                        maxSoFar = res.item()\n",
    "                        decision_H[i, ppl, money] = offerIndex\n",
    "                dp_H[i, ppl, money] = maxSoFar\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "betalow 0.1 betahigh 0.1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dp 0.714013934135437\n",
      "Supervised Aim: beta dp\n",
      "Train Epoch: 1 [0/15000 (0%)]\tLoss: 0.010560\n",
      "Train Epoch: 1 [1280/15000 (8%)]\tLoss: 0.005158\n",
      "Train Epoch: 1 [2560/15000 (17%)]\tLoss: 0.002392\n",
      "Train Epoch: 1 [3840/15000 (25%)]\tLoss: 0.001351\n",
      "Train Epoch: 1 [5120/15000 (34%)]\tLoss: 0.000747\n",
      "Train Epoch: 1 [6400/15000 (42%)]\tLoss: 0.000449\n",
      "Train Epoch: 1 [7680/15000 (51%)]\tLoss: 0.000237\n",
      "Train Epoch: 1 [8960/15000 (59%)]\tLoss: 0.000138\n",
      "Train Epoch: 1 [10240/15000 (68%)]\tLoss: 0.000097\n",
      "Train Epoch: 1 [11520/15000 (76%)]\tLoss: 0.000069\n",
      "Train Epoch: 1 [12800/15000 (85%)]\tLoss: 0.000077\n",
      "Train Epoch: 1 [14080/15000 (93%)]\tLoss: 0.000059\n",
      "Train Epoch: 2 [0/15000 (0%)]\tLoss: 0.000041\n",
      "Train Epoch: 2 [1280/15000 (8%)]\tLoss: 0.000040\n",
      "Train Epoch: 2 [2560/15000 (17%)]\tLoss: 0.000043\n",
      "Train Epoch: 2 [3840/15000 (25%)]\tLoss: 0.000035\n",
      "Train Epoch: 2 [5120/15000 (34%)]\tLoss: 0.000028\n",
      "Train Epoch: 2 [6400/15000 (42%)]\tLoss: 0.000031\n",
      "Train Epoch: 2 [7680/15000 (51%)]\tLoss: 0.000021\n",
      "Train Epoch: 2 [8960/15000 (59%)]\tLoss: 0.000017\n",
      "Train Epoch: 2 [10240/15000 (68%)]\tLoss: 0.000020\n",
      "Train Epoch: 2 [11520/15000 (76%)]\tLoss: 0.000014\n",
      "Train Epoch: 2 [12800/15000 (85%)]\tLoss: 0.000012\n",
      "Train Epoch: 2 [14080/15000 (93%)]\tLoss: 0.000016\n",
      "Train Epoch: 3 [0/15000 (0%)]\tLoss: 0.000014\n",
      "Train Epoch: 3 [1280/15000 (8%)]\tLoss: 0.000013\n",
      "Train Epoch: 3 [2560/15000 (17%)]\tLoss: 0.000010\n",
      "Train Epoch: 3 [3840/15000 (25%)]\tLoss: 0.000012\n",
      "Train Epoch: 3 [5120/15000 (34%)]\tLoss: 0.000010\n",
      "Train Epoch: 3 [6400/15000 (42%)]\tLoss: 0.000007\n",
      "Train Epoch: 3 [7680/15000 (51%)]\tLoss: 0.000008\n",
      "Train Epoch: 3 [8960/15000 (59%)]\tLoss: 0.000008\n",
      "Train Epoch: 3 [10240/15000 (68%)]\tLoss: 0.000007\n",
      "Train Epoch: 3 [11520/15000 (76%)]\tLoss: 0.000007\n",
      "Train Epoch: 3 [12800/15000 (85%)]\tLoss: 0.000006\n",
      "Train Epoch: 3 [14080/15000 (93%)]\tLoss: 0.000006\n",
      "Train Epoch: 4 [0/15000 (0%)]\tLoss: 0.000005\n",
      "Train Epoch: 4 [1280/15000 (8%)]\tLoss: 0.000004\n",
      "Train Epoch: 4 [2560/15000 (17%)]\tLoss: 0.000004\n",
      "Train Epoch: 4 [3840/15000 (25%)]\tLoss: 0.000004\n",
      "Train Epoch: 4 [5120/15000 (34%)]\tLoss: 0.000004\n",
      "Train Epoch: 4 [6400/15000 (42%)]\tLoss: 0.000004\n",
      "Train Epoch: 4 [7680/15000 (51%)]\tLoss: 0.000004\n",
      "Train Epoch: 4 [8960/15000 (59%)]\tLoss: 0.000005\n",
      "Train Epoch: 4 [10240/15000 (68%)]\tLoss: 0.000003\n",
      "Train Epoch: 4 [11520/15000 (76%)]\tLoss: 0.000002\n",
      "Train Epoch: 4 [12800/15000 (85%)]\tLoss: 0.000004\n",
      "Train Epoch: 4 [14080/15000 (93%)]\tLoss: 0.000003\n",
      "Train Epoch: 5 [0/15000 (0%)]\tLoss: 0.000003\n",
      "Train Epoch: 5 [1280/15000 (8%)]\tLoss: 0.000003\n",
      "Train Epoch: 5 [2560/15000 (17%)]\tLoss: 0.000004\n",
      "Train Epoch: 5 [3840/15000 (25%)]\tLoss: 0.000002\n",
      "Train Epoch: 5 [5120/15000 (34%)]\tLoss: 0.000003\n",
      "Train Epoch: 5 [6400/15000 (42%)]\tLoss: 0.000003\n",
      "Train Epoch: 5 [7680/15000 (51%)]\tLoss: 0.000003\n",
      "Train Epoch: 5 [8960/15000 (59%)]\tLoss: 0.000002\n",
      "Train Epoch: 5 [10240/15000 (68%)]\tLoss: 0.000003\n",
      "Train Epoch: 5 [11520/15000 (76%)]\tLoss: 0.000003\n",
      "Train Epoch: 5 [12800/15000 (85%)]\tLoss: 0.000002\n",
      "Train Epoch: 5 [14080/15000 (93%)]\tLoss: 0.000002\n",
      "Train Epoch: 6 [0/15000 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 6 [1280/15000 (8%)]\tLoss: 0.000002\n",
      "Train Epoch: 6 [2560/15000 (17%)]\tLoss: 0.000002\n",
      "Train Epoch: 6 [3840/15000 (25%)]\tLoss: 0.000002\n",
      "Train Epoch: 6 [5120/15000 (34%)]\tLoss: 0.000001\n",
      "Train Epoch: 6 [6400/15000 (42%)]\tLoss: 0.000002\n",
      "Train Epoch: 6 [7680/15000 (51%)]\tLoss: 0.000002\n",
      "Train Epoch: 6 [8960/15000 (59%)]\tLoss: 0.000002\n",
      "Train Epoch: 6 [10240/15000 (68%)]\tLoss: 0.000001\n",
      "Train Epoch: 6 [11520/15000 (76%)]\tLoss: 0.000001\n",
      "Train Epoch: 6 [12800/15000 (85%)]\tLoss: 0.000002\n",
      "Train Epoch: 6 [14080/15000 (93%)]\tLoss: 0.000002\n",
      "Train Epoch: 7 [0/15000 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 7 [1280/15000 (8%)]\tLoss: 0.000001\n",
      "Train Epoch: 7 [2560/15000 (17%)]\tLoss: 0.000001\n",
      "Train Epoch: 7 [3840/15000 (25%)]\tLoss: 0.000001\n",
      "Train Epoch: 7 [5120/15000 (34%)]\tLoss: 0.000001\n",
      "Train Epoch: 7 [6400/15000 (42%)]\tLoss: 0.000002\n",
      "Train Epoch: 7 [7680/15000 (51%)]\tLoss: 0.000002\n",
      "Train Epoch: 7 [8960/15000 (59%)]\tLoss: 0.000001\n",
      "Train Epoch: 7 [10240/15000 (68%)]\tLoss: 0.000001\n",
      "Train Epoch: 7 [11520/15000 (76%)]\tLoss: 0.000002\n",
      "Train Epoch: 7 [12800/15000 (85%)]\tLoss: 0.000002\n",
      "Train Epoch: 7 [14080/15000 (93%)]\tLoss: 0.000006\n",
      "Train Epoch: 8 [0/15000 (0%)]\tLoss: 0.000008\n",
      "Train Epoch: 8 [1280/15000 (8%)]\tLoss: 0.000002\n",
      "Train Epoch: 8 [2560/15000 (17%)]\tLoss: 0.000001\n",
      "Train Epoch: 8 [3840/15000 (25%)]\tLoss: 0.000029\n",
      "Train Epoch: 8 [5120/15000 (34%)]\tLoss: 0.000008\n",
      "Train Epoch: 8 [6400/15000 (42%)]\tLoss: 0.000003\n",
      "Train Epoch: 8 [7680/15000 (51%)]\tLoss: 0.000003\n",
      "Train Epoch: 8 [8960/15000 (59%)]\tLoss: 0.000002\n",
      "Train Epoch: 8 [10240/15000 (68%)]\tLoss: 0.000002\n",
      "Train Epoch: 8 [11520/15000 (76%)]\tLoss: 0.000001\n",
      "Train Epoch: 8 [12800/15000 (85%)]\tLoss: 0.000001\n",
      "Train Epoch: 8 [14080/15000 (93%)]\tLoss: 0.000001\n",
      "NN 1 : tensor(1.7771)\n",
      "CS 1 : 1.8062\n",
      "DP 1 : 1.7767333333333333\n",
      "heuristic 1 : 1.7794666666666668\n",
      "DP: 0.714013934135437\n",
      "tensor([0.4986, 0.4985, 0.0029])\n",
      "tensor([0.4992, 0.5008, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 1 [0/15000 (0%)]\tLoss: 1.433769 testing loss: tensor(1.7847)\n",
      "Train Epoch: 1 [1280/15000 (8%)]\tLoss: 1.447543 testing loss: tensor(1.7740)\n",
      "Train Epoch: 1 [2560/15000 (17%)]\tLoss: 1.462620 testing loss: tensor(1.7716)\n",
      "Train Epoch: 1 [3840/15000 (25%)]\tLoss: 1.383070 testing loss: tensor(1.7726)\n",
      "Train Epoch: 1 [5120/15000 (34%)]\tLoss: 1.517838 testing loss: tensor(1.7711)\n",
      "Train Epoch: 1 [6400/15000 (42%)]\tLoss: 1.631230 testing loss: tensor(1.7675)\n",
      "Train Epoch: 1 [7680/15000 (51%)]\tLoss: 1.541042 testing loss: tensor(1.7648)\n",
      "Train Epoch: 1 [8960/15000 (59%)]\tLoss: 1.331320 testing loss: tensor(1.7600)\n",
      "Train Epoch: 1 [10240/15000 (68%)]\tLoss: 1.489319 testing loss: tensor(1.7596)\n",
      "Train Epoch: 1 [11520/15000 (76%)]\tLoss: 1.412523 testing loss: tensor(1.7568)\n",
      "Train Epoch: 1 [12800/15000 (85%)]\tLoss: 1.558362 testing loss: tensor(1.7537)\n",
      "Train Epoch: 1 [14080/15000 (93%)]\tLoss: 1.468865 testing loss: tensor(1.7511)\n",
      "penalty: 0.0067150890827178955\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.1)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "    \n",
    "for order in stage:\n",
    "    producedata(order)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "    #running\n",
    "    for order1 in order1name:\n",
    "        print(\"Supervised Aim:\",order,order1)\n",
    "        \n",
    "        # for mapping binary to payments before softmax\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(n, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, n),\n",
    "        )\n",
    "        model.apply(init_weights)\n",
    "        # optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        runningLossNN = []\n",
    "        runningLossCS = []\n",
    "        runningLossDP = []\n",
    "        runningLossHeuristic = []\n",
    "        #model=torch.load(\"save/pytorchNN=5dp1\");\n",
    "        #model.eval()\n",
    "        ##order1name=[\"costsharing\",\"dp\",\"heuristic\",\"random initializing\"]\n",
    "        for epoch in range(1, supervisionEpochs + 1):\n",
    "#             print(\"distributionRatio\",distributionRatio)\n",
    "            if(order1==\"costsharing\"):\n",
    "                supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "            elif(order1==\"dp\"):\n",
    "                supervisionTrain(epoch, dpSupervisionRule)\n",
    "            elif(order1==\"heuristic\"):\n",
    "                supervisionTrain(epoch, heuristicSupervisionRule)\n",
    "            elif(order1==\"random initializing\"):\n",
    "                print(\"do nothing\");\n",
    "\n",
    "        test()\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(epoch)\n",
    "            test()\n",
    "        losslistname.append(order+\" \"+order1);\n",
    "        losslist.append(losslisttemp);\n",
    "        losslisttemp=[];\n",
    "        savepath=\"save/pytorchNN=5all-beta\"+order+str(order1)\n",
    "        torch.save(model, savepath);\n",
    "        print(\"end\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorlist=[\"#D2691E\",'#4169E1',\"#9ACD32\",\"#B22222\",\"#FF00FF\",\"#708090\"]\n",
    "for i in range(len(losslist)):\n",
    "    plt.plot(losslist[i], 'ro-', color=colorlist[i], alpha=0.8, label=losslistname[i])\n",
    "\n",
    "# 显示标签，如果不加这句，即使加了label='一些数字'的参数，最终还是不会显示标签\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('trianing times')\n",
    "plt.ylabel('delay')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
