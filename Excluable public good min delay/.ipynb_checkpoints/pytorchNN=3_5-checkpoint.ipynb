{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40058530825361593\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from random import randint\n",
    "import random\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "from pylab import *\n",
    "from scipy.stats import beta\n",
    "import math\n",
    "n = 3\n",
    "epochs = 1\n",
    "supervisionEpochs = 1\n",
    "lr1 = 0.01\n",
    "lr2 = 0.0005\n",
    "log_interval = 5\n",
    "trainSize = 50000#100000\n",
    "percentage_train_test= 0.5\n",
    "penaltyLambda = 10\n",
    "doublePeakHighMean = 0.6\n",
    "doublePeakLowMean = 0.2\n",
    "doublePeakStd = 0.1\n",
    "uniformlow=0\n",
    "uniformhigh=1.0\n",
    "normalloc = 0.2\n",
    "normalscale = 0.1\n",
    "\n",
    "cauchyloc = 1/n\n",
    "cauchyscalen = 0.004\n",
    "\n",
    "exponentialhigh = 15 #Symbol(\"b\", real=True)\n",
    "exponentiallow  = 15 #Symbol(\"a\", real=True)\n",
    "\n",
    "\n",
    "beta_a = 0.3\n",
    "beta_b  = 0.2\n",
    "kumaraswamy_a = beta_a \n",
    "kumaraswamy_b = (1.0+(beta_a-1.0)*math.pow( (beta_a+beta_b-2.0)/(beta_a-1.0), beta_a) )/beta_a \n",
    "print(kumaraswamy_b)\n",
    "\n",
    "\n",
    "independentnormalloc1=[(float(ii)+1)/(2*n+1) for ii in range(n,0,-1)]\n",
    "independentnormalscale1=[0.05 for ii in range(n)]\n",
    "\n",
    "independentnormalloc2=[(float(ii)+1)/(2*n+1) for ii in range(1,n+1,1)]\n",
    "independentnormalscale2=[0.05 for ii in range(n)]\n",
    "stage=[\"uniform\"]\n",
    "# \"twopeak\",\"normal\",\"uniform\",\"independent1\",\"independent2\",\"cauchy\",\"beta\",\"U-exponential\",\"arcsine\"\n",
    "order1name=[\"dp\",\"random initializing\",\"costsharing\",\"heuristic\"]\n",
    "# \"costsharing\",\"dp\",\"heuristic\",\"random initializing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio1 = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase1 = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "d3 = D.normal.Normal(normalloc, normalscale)\n",
    "distributionRatio3 = d3.cdf(1) - d3.cdf(0)\n",
    "distributionBase3 = d3.cdf(0)\n",
    "\n",
    "d4 = D.uniform.Uniform(uniformlow,uniformhigh)\n",
    "distributionRatio4 = d4.cdf(1) - d4.cdf(0)\n",
    "distributionBase4 = d4.cdf(0)\n",
    "\n",
    "d5 = [D.normal.Normal(independentnormalloc1[ii], independentnormalscale1[ii]) for ii in range(n)]\n",
    "d6 = [D.normal.Normal(independentnormalloc2[ii], independentnormalscale2[ii]) for ii in range(n)]\n",
    "\n",
    "d7 = D.cauchy.Cauchy(cauchyloc,cauchyscalen)\n",
    "\n",
    "d81 = D.exponential.Exponential(exponentiallow)\n",
    "d82 = D.exponential.Exponential(exponentialhigh)\n",
    "\n",
    "d9 = D.beta.Beta(beta_a,beta_b)\n",
    "#sample_d9=d9.rsample(torch.Size([100000]))\n",
    "\n",
    "#d9_sample=np.linspace(0.0001, 0.9999, 10000)\n",
    "#d9_pdf=torch.exp(d9.log_prob(torch.tensor(d9_sample,dtype=torch.float32)))\n",
    "#d9_delta=d9_sample[1]-d9_sample[0]\n",
    "#d9_sum_pdf=torch.sum(d9_pdf*d9_delta)\n",
    "\n",
    "d10 = D.beta.Beta(0.5,0.5)\n",
    "\n",
    "def cdf(x,y, i=None):\n",
    "    if(y==\"twopeak\"):\n",
    "        return (d1.cdf(x) + d2.cdf(x) - distributionBase1) / 2 / distributionRatio1\n",
    "    elif(y==\"normal\"):\n",
    "        return (d3.cdf(x)-distributionBase3)/distributionRatio3;\n",
    "    elif(y==\"uniform\"):\n",
    "        return (d4.cdf(x)-distributionBase4)/distributionRatio4;\n",
    "    elif(y==\"independent1\"):\n",
    "        return d5[i].cdf(x);\n",
    "    elif(y==\"independent2\"):\n",
    "        return d6[i].cdf(x);\n",
    "    elif(y==\"cauchy\"):\n",
    "        return d7.cdf(x);\n",
    "    elif(y==\"beta\"):\n",
    "#         sum_cdf=0.0;\n",
    "#         if(x<0.0001):\n",
    "#             x=0.00011;\n",
    "#         if(x>0.9999):\n",
    "#             x=0.99989;\n",
    "#         for i in range(len(d9_pdf)):\n",
    "#             if(d9_sample[i]<x):\n",
    "#                 sum_cdf+=d9_pdf[i]*d9_delta;\n",
    "#             else:\n",
    "#                 sum_cdf+=(d9_pdf[i]+d9_pdf[i-1])/ 2 *(x-d9_sample[i-1])\n",
    "#                 break;\n",
    "#         return sum_cdf/d9_sum_pdf\n",
    "#         cdf_v=torch.sum((sample_d9<(x)), dtype=torch.float32)/100000\n",
    "#         return cdf_v\n",
    "#    F(x|a,b)=1–(1–x^a)^b\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(x,kumaraswamy_a),kumaraswamy_b);\n",
    "        except:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(torch.tensor(x,dtype=torch.float32),kumaraswamy_a),kumaraswamy_b);\n",
    "    elif(y==\"arcsine\"):\n",
    "        #\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            res=2.0/math.pi * torch.asin(torch.sqrt(x))\n",
    "            #print(x)\n",
    "            return res# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "        except:\n",
    "            return 2.0/math.pi * torch.asin(torch.sqrt(torch.tensor(x,dtype=torch.float32)))# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "    elif(y==\"U-exponential\"):\n",
    "        return (d81.cdf(x) + (1.0 - d82.cdf(1.0-x)))  / 2 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    if torch.sum(bits).item() == 0:\n",
    "        return torch.ones(n)\n",
    "    bits = bits.type(torch.float32)\n",
    "    negBits = torch.ones(n) - bits\n",
    "    payments = model(bits)\n",
    "    payments = payments - 1000 * negBits\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    payments = payments + negBits\n",
    "    return payments\n",
    "\n",
    "def tpToBits(tp, bits=torch.ones(n).type(torch.uint8)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = (tp >= payments).type(torch.uint8)\n",
    "    if torch.equal(newBits, bits):\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits(tp, newBits)#bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    return n - torch.sum(tpToBits(tp).type(torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    global  dp,decision\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money, yes]\n",
    "        offer = float(offerIndex) / dpPrecision\n",
    "        if tp[i] >= offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits[i] = 0\n",
    "            payments[i] = 1\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "#     print()\n",
    "#     print(tp)\n",
    "#     print(bits)\n",
    "#     print(payments)\n",
    "#     print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def heuristicSupervisionRule(tp):\n",
    "    global  dp_H,decision_H\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    tempold=-1;\n",
    "    for turns in range(n,0,-1):\n",
    "        money = dpPrecision\n",
    "        j=0\n",
    "        tempo=sum(bits)\n",
    "        #print(\"bits\",tempo)\n",
    "        for i in range(n):\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            \n",
    "            offerIndex = decision_H[tempo ,tempo - i, money]\n",
    "            offer = float(offerIndex) / dpPrecision\n",
    "            while(j<n):\n",
    "                if(bits[j]!=0):\n",
    "                    break;\n",
    "                j+=1;\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            if tp[j] >= offer:\n",
    "                #print(money,j,tp[j],offer)\n",
    "                money -= offerIndex\n",
    "                bits[j] = 1\n",
    "                payments[j] = offer\n",
    "            else:\n",
    "                bits[j] = 0;\n",
    "                payments[j] = 1.0;\n",
    "            j+=1;\n",
    "        #print(\"money\",money)\n",
    "        if(money==0 and tempold==tempo):\n",
    "            break;\n",
    "        tempold=tempo;\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    for k in range(n, -1, -1):\n",
    "        if k == 0:\n",
    "            break\n",
    "        bits = [1 if tp[ii] >= 1.0 / k else 0 for ii in range(n)]\n",
    "        if sum(bits) == k:\n",
    "            break\n",
    "    if k == 0:\n",
    "        payments = [1 for ii in range(n)]\n",
    "    else:\n",
    "        payments = [1.0 / k if bits[ii] == 1 else 1 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    return float(n - torch.sum(costSharingSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def dpDelay(tp):\n",
    "    return float(n - torch.sum(dpSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def heuristicDelay(tp):\n",
    "    return float(n - torch.sum(heuristicSupervisionRule(tp)[0]).item())\n",
    "\n",
    "\n",
    "# templire=0;\n",
    "# num=1\n",
    "# for i in range(num):\n",
    "#     temp=torch.tensor(samplesJoint[i])\n",
    "#     #print(temp)\n",
    "#     #print(temp);\n",
    "#     #print(heuristicSupervisionRule(temp))\n",
    "#     #print(dpSupervisionRule(temp))\n",
    "#     #print(dpDelay(temp))\n",
    "#     res=dpDelay(temp)\n",
    "#     templire+=res\n",
    "#     #print(\"Delay\",res)\n",
    "#     #print()\n",
    "# print(templire/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordAndReport(name, source, loss):\n",
    "    source.append(loss)\n",
    "    realLength = len(source)\n",
    "    #print(f\"{name} ({realLength}): {loss}\")\n",
    "    print(name,realLength,\":\" ,loss)\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    \n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda /100000\n",
    "        \n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % (log_interval*5) == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "losslist=[];\n",
    "losslistname=[];\n",
    "losslisttemp=[];\n",
    "def test_batch():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lenLoss= 0\n",
    "        nnLoss = 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "            for tp in tp_batch:\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        nnLoss/=lenLoss\n",
    "    return nnLoss\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                Delay1 = tpToTotalDelay(tp1)\n",
    "                Delay0 = tpToTotalDelay(tp0)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                    loss = loss + (1 - cdf(offer,order)) * Delay1 + cdf(offer,order) * Delay0\n",
    "                else:\n",
    "                    loss = loss + (1 - cdf(offer,order,i)) * Delay1 + cdf(offer,order,i) * Delay0\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "    \n",
    "allBits = [torch.tensor(bits, dtype=torch.int16) for bits in itertools.product([0, 1], repeat=n)]\n",
    "\n",
    "def test():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        costSharingLoss = 0\n",
    "        dpLoss = 0\n",
    "        nnLoss = 0\n",
    "        heuristicLoss=0\n",
    "        lenLoss= 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "                heuristicLoss+= heuristicDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        costSharingLoss /= lenLoss\n",
    "        dpLoss /= lenLoss\n",
    "        nnLoss /= lenLoss\n",
    "        heuristicLoss /= lenLoss\n",
    "        #print(lenLoss)\n",
    "        losslisttemp.append(nnLoss);\n",
    "        recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "        recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "        recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "        recordAndReport(\"heuristic\", runningLossHeuristic, heuristicLoss)\n",
    "        print(\"DP:\",dp[n, dpPrecision,0])\n",
    "        #for i in range(n, 0, -1):\n",
    "        #    print(\"Heuristic:\",i,5*(1-dp_H[i, i, dpPrecision]))\n",
    "        for i in range(n, 0, -1):\n",
    "            print(\n",
    "                    tpToPayments(\n",
    "                            torch.tensor([0 if ii >= i else 1 for ii in range(n)], dtype=torch.float32)\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "dpPrecision = 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def producedata(order):\n",
    "    global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "    dpPrecision = 100 \n",
    "    if(order==\"twopeak\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = np.random.normal(\n",
    "            loc=doublePeakLowMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=doublePeakLowMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samples2 = np.random.normal(\n",
    "            loc=doublePeakHighMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = np.random.normal(\n",
    "                        loc=doublePeakHighMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "    elif(order==\"normal\"):\n",
    "        print(\"loc\",normalloc, \"scale\",normalscale)\n",
    "        samples1 = np.random.normal(\n",
    "            loc=normalloc, scale=normalscale, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=normalloc, scale=normalscale\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"uniform\"):  \n",
    "        print(\"uniformlow\",uniformlow, \"uniformhigh\",uniformhigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        uniformlow, uniformhigh\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent1\"):\n",
    "        print(\"loc\",independentnormalloc1,\"scale\",independentnormalscale1)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent2\"):\n",
    "        print(\"loc\",independentnormalloc2, \"scale\",independentnormalscale2)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"cauchy\"):\n",
    "        print(\"cauchyloc\",cauchyloc, \"cauchyscale\",cauchyscalen)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"beta\"):\n",
    "        print(\"beta_a\",beta_a, \"beta_b\",beta_b)\n",
    "        print(\"kumaraswamy_a\",kumaraswamy_a, \"kumaraswamy_b\",kumaraswamy_b)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"arcsine\"):\n",
    "        print(\"betalow\",0.5, \"betahigh\",0.5)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"U-exponential\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = d81.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d81.rsample(torch.Size([1])).numpy()\n",
    "                    \n",
    "        samples2 = d82.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        \n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = d82.rsample(torch.Size([1])).numpy()\n",
    "        samples2 = 1.0 - samples2\n",
    "        samplesJoint = signals * samples1 - (signals - 1.0) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "        \n",
    "    plt.hist(samplesJoint,bins=500)\n",
    "    plt.show()\n",
    "    \n",
    "    tp_dataset = TensorDataset(tp_tensor[: int(trainSize * percentage_train_test)])\n",
    "    tp_dataset_testing = TensorDataset(tp_tensor[int(trainSize * (1.0-percentage_train_test)) :])\n",
    "    tp_dataloader = DataLoader(tp_dataset, batch_size=128, shuffle=True)\n",
    "    tp_dataloader_testing = DataLoader(tp_dataset_testing, batch_size=256, shuffle=False)\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "    decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for yes in range(n + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp[0, 0, yes] = 0\n",
    "            else:\n",
    "                dp[0, money, yes] = yes# + 1.0\n",
    "    for ppl in range(1,  n + 1):\n",
    "        for yes in range(n + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                minSoFar = 1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1 - cdf(offer,order)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order) * (1 + dp[ppl - 1, money, yes])\n",
    "                    else:\n",
    "                        res = (1 - cdf(offer,order,n-ppl)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order,n-ppl) * (1 + dp[ppl - 1, money, yes])\n",
    "                    if minSoFar > res.item():\n",
    "                        minSoFar = res.item()\n",
    "                        decision[ppl, money, yes] = offerIndex\n",
    "                dp[ppl, money, yes] = minSoFar\n",
    "    \n",
    "    print(\"dp\",dp[n, dpPrecision, 0])\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp_H = np.zeros([n + 1 , n + 1, dpPrecision + 1])\n",
    "    decision_H = np.zeros([n + 1 , n + 1, dpPrecision + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for i in range(1,n+1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp_H[i, 0, 0] = 1\n",
    "            else:\n",
    "                offer = money / dpPrecision\n",
    "                dp_H[i, 0, money] = 0#cdf(offer)# + 1.0\n",
    "    for i in range(1,n+1):\n",
    "        for ppl in range(1, i + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                maxSoFar = -1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1-cdf(offer,order)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    else:\n",
    "                        res = (1-cdf(offer,order,ppl-1)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    if maxSoFar < res.item():\n",
    "                        maxSoFar = res.item()\n",
    "                        decision_H[i, ppl, money] = offerIndex\n",
    "                dp_H[i, ppl, money] = maxSoFar\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uniformlow 0 uniformhigh 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAASQklEQVR4nO3de6ykd13H8ffHLhcRlNae1tqLW81aLVYCHhFvBK2VgoStCSSLihus2RgR0WikhURMTBOMRtEokg1U1kjaNIh2Fa02q1iNXNxCgbZr7Uq1Xbp0D+ItmKAtX/84D+T09OyeOfPM7fnN+5WczDy3me9v5pnP/OY3zzwnVYUkqS1fMu8CJEmTZ7hLUoMMd0lqkOEuSQ0y3CWpQbvmXQDAueeeW7t37553GZI0KHfeeeenq2plq2ULEe67d+/m6NGj8y5DkgYlyb+ebpnDMpLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGbRvuSW5McirJ3Vss+/kkleTcDfOuT3I8yX1JXjTpgiVJ2xul5/5O4OrNM5NcDFwFPLhh3uXAPuBZ3TZvTXLWRCqVJI1s23CvqjuAz2yx6DeAXwA2/iunvcDNVfW5qnoAOA48bxKFSpJGN9aYe5KXAZ+sqo9uWnQh8NCG6RPdPEnSDO34xGFJnga8Efj+rRZvMW/Lf9Ka5ABwAOCSSy7ZaRmSpDMYp+f+dcClwEeT/AtwEfDhJF/Fek/94g3rXgQ8vNWNVNXBqlqtqtWVlS3PWClJGtOOw72qPl5V51XV7qrazXqgP7eqPgUcBvYleUqSS4E9wIcmWrEkaVujHAp5E/B+4LIkJ5Jce7p1q+oe4BbgXuA24DVV9dikipUkjWbbMfeqeuU2y3dvmr4BuKFfWZKkPvyFqiQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl2bpl75i3hVoSRjuktQgw12SGmS4S1KDDHdJapDhLkkNMtzVHo9IkQx3SWqR4S5Jnd3XvXfeJUyM4S6pXUs8RGe4S1KDDHdJatC24Z7kxiSnkty9Yd6vJvnHJB9L8kdJnrlh2fVJjie5L8mLplW4pAFa4mGSWRul5/5O4OpN824Hvqmqvhn4J+B6gCSXA/uAZ3XbvDXJWROrVmpAS1/aaXFtG+5VdQfwmU3z/rKqHu0mPwBc1F3fC9xcVZ+rqgeA48DzJlivJGkEkxhz/zHgz7vrFwIPbVh2opv3BEkOJDma5Oja2toEypCkKRjoUFKvcE/yRuBR4F1fmLXFarXVtlV1sKpWq2p1ZWWlTxlLwY/yknZi17gbJtkPvBS4sqq+EOAngIs3rHYR8PD45UmSxjFWzz3J1cDrgZdV1f9sWHQY2JfkKUkuBfYAH+pfpmZhlE8HfoJ4vCsOXTH+xgP9uL9Rr/YvoSsOXTGz19Aoh0LeBLwfuCzJiSTXAr8NPAO4PcldSd4GUFX3ALcA9wK3Aa+pqsemVr00ppm+STUQ4hqebYdlquqVW8x+xxnWvwG4oU9RkqR+/IWqJmeUHuoC9WJPN6TgUMMmC/ScTUyLbdrEcG9c62PkrbdPjZjDm4nhLkkNMty1JXvEmiT3p9kz3OdsKcZ3l2B8U1o0Sx/um3sUSxG2YxjC42LvcIJ8Q56Yeb12lj7cJalFhntfm3o49h5HN8THahA12+uengE9tob7GIYwRKF+BhHi0hkY7pLUoCbDfVq9rqn15sb4qDfVTw9z+Og5jcd2kT9hTaK2rU5C1fd2z/Q8zPKkV0O3CPtek+G+SBbhSX6CAY0bLpTWH7dG3tSnZdTX8qK0yXCXpAYZ7nO0KO/wU9F6L3ejgbV1IT9N7sQCPt6L+FpuL9wncWjiAu48ZzKp8dtpG2SobLEv+FhtsuCvl0UM3lloL9wlSYb7EM27JzLtXuUk27ej21rwHmhLhrQPTc2U97cmwn1QH2H7WrAAOtNjP4gX2DKZ4r7T57me9et38/21+r+Dmwh3SdLjLXW4L1WPnwH2Phbtx12NmvZ+MavnZFLtGKXeIexn24Z7khuTnEpy94Z55yS5Pcn93eXZG5Zdn+R4kvuSvGhahS+8CRxl0XtnXbAhnKWyAI/9PANoUcNvcB2cHkbpub8TuHrTvOuAI1W1BzjSTZPkcmAf8Kxum7cmOWti1UqSRrJtuFfVHcBnNs3eCxzqrh8Crtkw/+aq+lxVPQAcB543oVrVxwL0JHdipJ7fuG3a4XZb9vbm+Xhudd+zrqfF/akx4465n19VJwG6y/O6+RcCD21Y70Q37wmSHEhyNMnRtbW1McvQF816fHpgL+5B8jEez4ze9BfdpL9QzRbzaqsVq+pgVa1W1erKysqEy5igRfhS73Q1TGJnbGyHPpPWx1tbaN+OXztz3H8X/fEeN9wfSXIBQHd5qpt/Arh4w3oXAQ+PX54kaRzjhvthYH93fT9w64b5+5I8JcmlwB7gQ/1KnJEl6sGOatxPIEP89eEyjskCX9zvZ9n+nTx/i947XmSjHAp5E/B+4LIkJ5JcC7wZuCrJ/cBV3TRVdQ9wC3AvcBvwmqp6bFrFj2pm52Ee4Q1iu3+GMCtDfdEsQwjP+7kZwmM878fotBaok7hruxWq6pWnWXTlada/AbihT1GSpH6W+heqs9LquSs0Qz17hNPojbe+z+60fYv2iWd5wn0SxzZrIS3ai2ory7A/Tep5WIbHahaWJ9wlaYk0E+4L9yvCORlCL3ahLOhpcAerldfcTtqxoG1uJtxHtfBjjxPYUZYyVBbAoh46upXW9pGhdmqm+TwsXbhL0jIw3JfMvP/rzbxuQ1o2hrvm4nGBvaBjlpM2zpkuPad/p5V2zJDhLkkNMtzVBnt2C6e1L213bM77pOE+pkXZcWdRx6K0VdrI72LOrNlw94mfD98INJZZ/HepCfxf4yFpNtwlaZkZ7hPScg+gdYvwaWMRatATDfl1bbifwVSf2KF8ATiUOvU4g3izcN+aKsNdkhq0nOFuj0EDtPu69w56mECztZzhruFZwH9WMStDrn2SBjHUtEAMd0lqkOEuSQ3qFe5JfjbJPUnuTnJTkqcmOSfJ7Unu7y7PnlSxmjO/q3gchwm0yMYO9yQXAj8NrFbVNwFnAfuA64AjVbUHONJNS5JmqO+wzC7gS5PsAp4GPAzsBQ51yw8B1/S8j4Xgl1qShmTscK+qTwK/BjwInAT+s6r+Eji/qk5265wEzttq+yQHkhxNcnRtbW3cMqSF4lCNFkWfYZmzWe+lXwp8NfBlSX5k1O2r6mBVrVbV6srKyrhlSJK20GdY5vuAB6pqrar+D3gP8B3AI0kuAOguT/UvU8vgikNX+KWtNCF9wv1B4PlJnpYkwJXAMeAwsL9bZz9wa78SJWmd332Nbte4G1bVB5O8G/gw8CjwEeAg8HTgliTXsv4G8IpJFCpJGt3Y4Q5QVW8C3rRp9udY78VLkubEX6iqN48QkRaP4S5JDTLcJalBhvuC8qgASX0Y7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhruk0/KQ3OEy3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUG9wj3JM5O8O8k/JjmW5NuTnJPk9iT3d5dnT6pYSdJo+vbcfxO4raq+AXg2cAy4DjhSVXuAI920JGmGxg73JF8OvAB4B0BV/W9V/QewFzjUrXYIuKZvkZKknenTc/9aYA34vSQfSfL2JF8GnF9VJwG6y/O22jjJgSRHkxxdW1vrUYYkabM+4b4LeC7wu1X1HOCz7GAIpqoOVtVqVa2urKz0KEOStFmfcD8BnKiqD3bT72Y97B9JcgFAd3mqX4mSpJ0aO9yr6lPAQ0ku62ZdCdwLHAb2d/P2A7f2qlCStGO7em7/WuBdSZ4MfAJ4NetvGLckuRZ4EHhFz/uQJO1Qr3CvqruA1S0WXdnndiVJ/fgLVUlqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNah3uCc5K8lHkvxpN31OktuT3N9dnt2/TEnSTkyi5/464NiG6euAI1W1BzjSTUuSZqhXuCe5CPgB4O0bZu8FDnXXDwHX9LkPSdLO9e25vwX4BeDzG+adX1UnAbrL87baMMmBJEeTHF1bW+tZhiRpo7HDPclLgVNVdec421fVwapararVlZWVccuQJG1hV49tvxN4WZKXAE8FvjzJHwCPJLmgqk4muQA4NYlCJUmjG7vnXlXXV9VFVbUb2Af8VVX9CHAY2N+tth+4tXeVkqQdmcZx7m8GrkpyP3BVNy1JmqE+wzJfVFXvA97XXf834MpJ3K4kaTz+QlWSGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0aO9yTXJzkr5McS3JPktd1889JcnuS+7vLsydXriRpFH167o8CP1dV3wg8H3hNksuB64AjVbUHONJNS5JmaOxwr6qTVfXh7vp/A8eAC4G9wKFutUPANX2LlCTtzETG3JPsBp4DfBA4v6pOwvobAHDeJO5DkjS63uGe5OnAHwI/U1X/tYPtDiQ5muTo2tpa3zIkSRv0CvckT2I92N9VVe/pZj+S5IJu+QXAqa22raqDVbVaVasrKyt9ypAkbdLnaJkA7wCOVdWvb1h0GNjfXd8P3Dp+eZKkcezqse13Aq8CPp7krm7eG4A3A7ckuRZ4EHhFvxIlSTs1drhX1d8BOc3iK8e9XUlSf/5CVZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBUwv3JFcnuS/J8STXTet+JElPNJVwT3IW8DvAi4HLgVcmuXwa9yVJeqJp9dyfBxyvqk9U1f8CNwN7p3RfkqRNUlWTv9Hk5cDVVfXj3fSrgG+rqp/asM4B4EA3eRlw35h3dy7w6R7lDpFtXg62eTn0afPXVNXKVgt2jV/PGWWLeY97F6mqg8DB3neUHK2q1b63MyS2eTnY5uUwrTZPa1jmBHDxhumLgIendF+SpE2mFe7/AOxJcmmSJwP7gMNTui9J0iZTGZapqkeT/BTwF8BZwI1Vdc807osJDO0MkG1eDrZ5OUylzVP5QlWSNF/+QlWSGmS4S1KDBhPu253OIOt+q1v+sSTPnUedkzRCm3+4a+vHkvx9kmfPo85JGvW0FUm+Nclj3W8qBm2UNid5YZK7ktyT5G9mXeOkjbBvf0WSP0ny0a7Nr55HnZOS5MYkp5LcfZrlk8+vqlr4P9a/lP1n4GuBJwMfBS7ftM5LgD9n/Rj75wMfnHfdM2jzdwBnd9dfvAxt3rDeXwF/Brx83nXP4Hl+JnAvcEk3fd68655Bm98A/Ep3fQX4DPDkedfeo80vAJ4L3H2a5RPPr6H03Ec5ncFe4Pdr3QeAZya5YNaFTtC2ba6qv6+qf+8mP8D67wmGbNTTVrwW+EPg1CyLm5JR2vxDwHuq6kGAqhp6u0dpcwHPSBLg6ayH+6OzLXNyquoO1ttwOhPPr6GE+4XAQxumT3TzdrrOkOy0Pdey/s4/ZNu2OcmFwA8Cb5thXdM0yvP89cDZSd6X5M4kPzqz6qZjlDb/NvCNrP/48ePA66rq87Mpby4mnl/TOv3ApG17OoMR1xmSkduT5HtYD/fvmmpF0zdKm98CvL6qHlvv1A3eKG3eBXwLcCXwpcD7k3ygqv5p2sVNyShtfhFwF/C9wNcBtyf526r6r2kXNycTz6+hhPsopzNo7ZQHI7UnyTcDbwdeXFX/NqPapmWUNq8CN3fBfi7wkiSPVtUfz6bEiRt13/50VX0W+GySO4BnA0MN91Ha/GrgzbU+IH08yQPANwAfmk2JMzfx/BrKsMwopzM4DPxo963z84H/rKqTsy50grZtc5JLgPcArxpwL26jbdtcVZdW1e6q2g28G/jJAQc7jLZv3wp8d5JdSZ4GfBtwbMZ1TtIobX6Q9U8qJDmf9TPHfmKmVc7WxPNrED33Os3pDJL8RLf8bawfOfES4DjwP6y/8w/WiG3+ReArgbd2PdlHa8Bn1BuxzU0Zpc1VdSzJbcDHgM8Db6+qLQ+pG4IRn+dfBt6Z5OOsD1m8vqoGeyrgJDcBLwTOTXICeBPwJJhefnn6AUlq0FCGZSRJO2C4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAb9P3A75lijHcNpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dp 1.7344958782196045\n",
      "Supervised Aim: uniform dp\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 0.008186\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 0.000162\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 0.000011\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 0.000000\n",
      "NN 1 : tensor(1.6838)\n",
      "CS 1 : 1.61472\n",
      "DP 1 : 1.74272\n",
      "heuristic 1 : 1.66756\n",
      "DP: 1.7344958782196045\n",
      "tensor([0.4600, 0.4200, 0.1200])\n",
      "tensor([0.5164, 0.4836, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 1.656122 testing loss: tensor(1.6815)\n",
      "Train Epoch: 1 [640/25000 (3%)]\tLoss: 1.601973 testing loss: tensor(1.6676)\n",
      "Train Epoch: 1 [1280/25000 (5%)]\tLoss: 1.491939 testing loss: tensor(1.6578)\n",
      "Train Epoch: 1 [1920/25000 (8%)]\tLoss: 1.608782 testing loss: tensor(1.6460)\n",
      "Train Epoch: 1 [2560/25000 (10%)]\tLoss: 1.671277 testing loss: tensor(1.6391)\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 1.636250 testing loss: tensor(1.6342)\n",
      "Train Epoch: 1 [3840/25000 (15%)]\tLoss: 1.602232 testing loss: tensor(1.6260)\n",
      "Train Epoch: 1 [4480/25000 (18%)]\tLoss: 1.618835 testing loss: tensor(1.6220)\n",
      "Train Epoch: 1 [5120/25000 (20%)]\tLoss: 1.531278 testing loss: tensor(1.6206)\n",
      "Train Epoch: 1 [5760/25000 (23%)]\tLoss: 1.640722 testing loss: tensor(1.6202)\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 1.578851 testing loss: tensor(1.6198)\n",
      "Train Epoch: 1 [7040/25000 (28%)]\tLoss: 1.508822 testing loss: tensor(1.6186)\n",
      "Train Epoch: 1 [7680/25000 (31%)]\tLoss: 1.655179 testing loss: tensor(1.6160)\n",
      "Train Epoch: 1 [8320/25000 (33%)]\tLoss: 1.712654 testing loss: tensor(1.6160)\n",
      "Train Epoch: 1 [8960/25000 (36%)]\tLoss: 1.622890 testing loss: tensor(1.6152)\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 1.675412 testing loss: tensor(1.6181)\n",
      "Train Epoch: 1 [10240/25000 (41%)]\tLoss: 1.635816 testing loss: tensor(1.6185)\n",
      "Train Epoch: 1 [10880/25000 (43%)]\tLoss: 1.691643 testing loss: tensor(1.6173)\n",
      "Train Epoch: 1 [11520/25000 (46%)]\tLoss: 1.624219 testing loss: tensor(1.6158)\n",
      "Train Epoch: 1 [12160/25000 (48%)]\tLoss: 1.635292 testing loss: tensor(1.6146)\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 1.575537 testing loss: tensor(1.6169)\n",
      "Train Epoch: 1 [13440/25000 (54%)]\tLoss: 1.599481 testing loss: tensor(1.6178)\n",
      "Train Epoch: 1 [14080/25000 (56%)]\tLoss: 1.687487 testing loss: tensor(1.6184)\n",
      "Train Epoch: 1 [14720/25000 (59%)]\tLoss: 1.643332 testing loss: tensor(1.6175)\n",
      "Train Epoch: 1 [15360/25000 (61%)]\tLoss: 1.594770 testing loss: tensor(1.6168)\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 1.655110 testing loss: tensor(1.6177)\n",
      "Train Epoch: 1 [16640/25000 (66%)]\tLoss: 1.609092 testing loss: tensor(1.6189)\n",
      "Train Epoch: 1 [17280/25000 (69%)]\tLoss: 1.614443 testing loss: tensor(1.6185)\n",
      "Train Epoch: 1 [17920/25000 (71%)]\tLoss: 1.638387 testing loss: tensor(1.6187)\n",
      "Train Epoch: 1 [18560/25000 (74%)]\tLoss: 1.572119 testing loss: tensor(1.6191)\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 1.695251 testing loss: tensor(1.6199)\n",
      "Train Epoch: 1 [19840/25000 (79%)]\tLoss: 1.600121 testing loss: tensor(1.6182)\n",
      "Train Epoch: 1 [20480/25000 (82%)]\tLoss: 1.595325 testing loss: tensor(1.6170)\n",
      "Train Epoch: 1 [21120/25000 (84%)]\tLoss: 1.704606 testing loss: tensor(1.6164)\n",
      "Train Epoch: 1 [21760/25000 (87%)]\tLoss: 1.514553 testing loss: tensor(1.6157)\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 1.616043 testing loss: tensor(1.6152)\n",
      "Train Epoch: 1 [23040/25000 (92%)]\tLoss: 1.710601 testing loss: tensor(1.6162)\n",
      "Train Epoch: 1 [23680/25000 (94%)]\tLoss: 1.611102 testing loss: tensor(1.6173)\n",
      "Train Epoch: 1 [24320/25000 (97%)]\tLoss: 1.574926 testing loss: tensor(1.6173)\n",
      "Train Epoch: 1 [7800/25000 (99%)]\tLoss: 1.722425 testing loss: tensor(1.6173)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.6173)\n",
      "CS 2 : 1.61472\n",
      "DP 2 : 1.74272\n",
      "heuristic 2 : 1.66756\n",
      "DP: 1.7344958782196045\n",
      "tensor([0.3307, 0.3378, 0.3315])\n",
      "tensor([0.4889, 0.5111, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: uniform random initializing\n",
      "do nothing\n",
      "NN 1 : tensor(1.6899)\n",
      "CS 1 : 1.61472\n",
      "DP 1 : 1.74272\n",
      "heuristic 1 : 1.66756\n",
      "DP: 1.7344958782196045\n",
      "tensor([0.4471, 0.3585, 0.1944])\n",
      "tensor([0.5509, 0.4491, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 1.762979 testing loss: tensor(1.6636)\n",
      "Train Epoch: 1 [640/25000 (3%)]\tLoss: 1.689464 testing loss: tensor(1.6173)\n",
      "Train Epoch: 1 [1280/25000 (5%)]\tLoss: 1.707110 testing loss: tensor(1.6201)\n",
      "Train Epoch: 1 [1920/25000 (8%)]\tLoss: 1.509619 testing loss: tensor(1.6237)\n",
      "Train Epoch: 1 [2560/25000 (10%)]\tLoss: 1.503511 testing loss: tensor(1.6190)\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 1.658159 testing loss: tensor(1.6158)\n",
      "Train Epoch: 1 [3840/25000 (15%)]\tLoss: 1.642919 testing loss: tensor(1.6171)\n",
      "Train Epoch: 1 [4480/25000 (18%)]\tLoss: 1.547376 testing loss: tensor(1.6148)\n",
      "Train Epoch: 1 [5120/25000 (20%)]\tLoss: 1.814636 testing loss: tensor(1.6175)\n",
      "Train Epoch: 1 [5760/25000 (23%)]\tLoss: 1.592752 testing loss: tensor(1.6215)\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 1.636635 testing loss: tensor(1.6216)\n",
      "Train Epoch: 1 [7040/25000 (28%)]\tLoss: 1.643264 testing loss: tensor(1.6224)\n",
      "Train Epoch: 1 [7680/25000 (31%)]\tLoss: 1.671215 testing loss: tensor(1.6152)\n",
      "Train Epoch: 1 [8320/25000 (33%)]\tLoss: 1.576165 testing loss: tensor(1.6178)\n",
      "Train Epoch: 1 [8960/25000 (36%)]\tLoss: 1.571881 testing loss: tensor(1.6147)\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 1.571092 testing loss: tensor(1.6191)\n",
      "Train Epoch: 1 [10240/25000 (41%)]\tLoss: 1.651926 testing loss: tensor(1.6150)\n",
      "Train Epoch: 1 [10880/25000 (43%)]\tLoss: 1.565063 testing loss: tensor(1.6223)\n",
      "Train Epoch: 1 [11520/25000 (46%)]\tLoss: 1.653747 testing loss: tensor(1.6185)\n",
      "Train Epoch: 1 [12160/25000 (48%)]\tLoss: 1.553341 testing loss: tensor(1.6166)\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 1.641624 testing loss: tensor(1.6192)\n",
      "Train Epoch: 1 [13440/25000 (54%)]\tLoss: 1.720544 testing loss: tensor(1.6183)\n",
      "Train Epoch: 1 [14080/25000 (56%)]\tLoss: 1.546994 testing loss: tensor(1.6169)\n",
      "Train Epoch: 1 [14720/25000 (59%)]\tLoss: 1.494057 testing loss: tensor(1.6149)\n",
      "Train Epoch: 1 [15360/25000 (61%)]\tLoss: 1.597974 testing loss: tensor(1.6192)\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 1.668311 testing loss: tensor(1.6189)\n",
      "Train Epoch: 1 [16640/25000 (66%)]\tLoss: 1.671897 testing loss: tensor(1.6201)\n",
      "Train Epoch: 1 [17280/25000 (69%)]\tLoss: 1.687655 testing loss: tensor(1.6214)\n",
      "Train Epoch: 1 [17920/25000 (71%)]\tLoss: 1.550046 testing loss: tensor(1.6202)\n",
      "Train Epoch: 1 [18560/25000 (74%)]\tLoss: 1.582805 testing loss: tensor(1.6146)\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 1.605881 testing loss: tensor(1.6142)\n",
      "Train Epoch: 1 [19840/25000 (79%)]\tLoss: 1.689078 testing loss: tensor(1.6197)\n",
      "Train Epoch: 1 [20480/25000 (82%)]\tLoss: 1.623595 testing loss: tensor(1.6256)\n",
      "Train Epoch: 1 [21120/25000 (84%)]\tLoss: 1.631661 testing loss: tensor(1.6212)\n",
      "Train Epoch: 1 [21760/25000 (87%)]\tLoss: 1.564313 testing loss: tensor(1.6158)\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 1.693214 testing loss: tensor(1.6202)\n",
      "Train Epoch: 1 [23040/25000 (92%)]\tLoss: 1.599243 testing loss: tensor(1.6198)\n",
      "Train Epoch: 1 [23680/25000 (94%)]\tLoss: 1.729778 testing loss: tensor(1.6160)\n",
      "Train Epoch: 1 [24320/25000 (97%)]\tLoss: 1.595943 testing loss: tensor(1.6170)\n",
      "Train Epoch: 1 [7800/25000 (99%)]\tLoss: 1.641754 testing loss: tensor(1.6216)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.6216)\n",
      "CS 2 : 1.61472\n",
      "DP 2 : 1.74272\n",
      "heuristic 2 : 1.66756\n",
      "DP: 1.7344958782196045\n",
      "tensor([0.3567, 0.3255, 0.3178])\n",
      "tensor([0.5090, 0.4910, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: uniform costsharing\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 0.007200\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 0.000006\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 0.000003\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 0.000000\n",
      "NN 1 : tensor(1.6147)\n",
      "CS 1 : 1.61472\n",
      "DP 1 : 1.74272\n",
      "heuristic 1 : 1.66756\n",
      "DP: 1.7344958782196045\n",
      "tensor([0.3333, 0.3333, 0.3333])\n",
      "tensor([0.5000, 0.5000, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 1.677084 testing loss: tensor(1.6141)\n",
      "Train Epoch: 1 [640/25000 (3%)]\tLoss: 1.443253 testing loss: tensor(1.6169)\n",
      "Train Epoch: 1 [1280/25000 (5%)]\tLoss: 1.528937 testing loss: tensor(1.6169)\n",
      "Train Epoch: 1 [1920/25000 (8%)]\tLoss: 1.570612 testing loss: tensor(1.6186)\n",
      "Train Epoch: 1 [2560/25000 (10%)]\tLoss: 1.691004 testing loss: tensor(1.6188)\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 1.672010 testing loss: tensor(1.6174)\n",
      "Train Epoch: 1 [3840/25000 (15%)]\tLoss: 1.687294 testing loss: tensor(1.6177)\n",
      "Train Epoch: 1 [4480/25000 (18%)]\tLoss: 1.653622 testing loss: tensor(1.6180)\n",
      "Train Epoch: 1 [5120/25000 (20%)]\tLoss: 1.565886 testing loss: tensor(1.6170)\n",
      "Train Epoch: 1 [5760/25000 (23%)]\tLoss: 1.534766 testing loss: tensor(1.6148)\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 1.604763 testing loss: tensor(1.6140)\n",
      "Train Epoch: 1 [7040/25000 (28%)]\tLoss: 1.599949 testing loss: tensor(1.6158)\n",
      "Train Epoch: 1 [7680/25000 (31%)]\tLoss: 1.720669 testing loss: tensor(1.6156)\n",
      "Train Epoch: 1 [8320/25000 (33%)]\tLoss: 1.607440 testing loss: tensor(1.6168)\n",
      "Train Epoch: 1 [8960/25000 (36%)]\tLoss: 1.569509 testing loss: tensor(1.6166)\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 1.520843 testing loss: tensor(1.6175)\n",
      "Train Epoch: 1 [10240/25000 (41%)]\tLoss: 1.560347 testing loss: tensor(1.6185)\n",
      "Train Epoch: 1 [10880/25000 (43%)]\tLoss: 1.579530 testing loss: tensor(1.6177)\n",
      "Train Epoch: 1 [11520/25000 (46%)]\tLoss: 1.701917 testing loss: tensor(1.6159)\n",
      "Train Epoch: 1 [12160/25000 (48%)]\tLoss: 1.763128 testing loss: tensor(1.6162)\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 1.566722 testing loss: tensor(1.6127)\n",
      "Train Epoch: 1 [13440/25000 (54%)]\tLoss: 1.629395 testing loss: tensor(1.6136)\n",
      "Train Epoch: 1 [14080/25000 (56%)]\tLoss: 1.674098 testing loss: tensor(1.6146)\n",
      "Train Epoch: 1 [14720/25000 (59%)]\tLoss: 1.590364 testing loss: tensor(1.6157)\n",
      "Train Epoch: 1 [15360/25000 (61%)]\tLoss: 1.537920 testing loss: tensor(1.6150)\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 1.633017 testing loss: tensor(1.6152)\n",
      "Train Epoch: 1 [16640/25000 (66%)]\tLoss: 1.647345 testing loss: tensor(1.6180)\n",
      "Train Epoch: 1 [17280/25000 (69%)]\tLoss: 1.626566 testing loss: tensor(1.6177)\n",
      "Train Epoch: 1 [17920/25000 (71%)]\tLoss: 1.545796 testing loss: tensor(1.6176)\n",
      "Train Epoch: 1 [18560/25000 (74%)]\tLoss: 1.626624 testing loss: tensor(1.6179)\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 1.529786 testing loss: tensor(1.6168)\n",
      "Train Epoch: 1 [19840/25000 (79%)]\tLoss: 1.552708 testing loss: tensor(1.6182)\n",
      "Train Epoch: 1 [20480/25000 (82%)]\tLoss: 1.679515 testing loss: tensor(1.6170)\n",
      "Train Epoch: 1 [21120/25000 (84%)]\tLoss: 1.616354 testing loss: tensor(1.6192)\n",
      "Train Epoch: 1 [21760/25000 (87%)]\tLoss: 1.535420 testing loss: tensor(1.6191)\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 1.586942 testing loss: tensor(1.6172)\n",
      "Train Epoch: 1 [23040/25000 (92%)]\tLoss: 1.659181 testing loss: tensor(1.6157)\n",
      "Train Epoch: 1 [23680/25000 (94%)]\tLoss: 1.512592 testing loss: tensor(1.6126)\n",
      "Train Epoch: 1 [24320/25000 (97%)]\tLoss: 1.558383 testing loss: tensor(1.6118)\n",
      "Train Epoch: 1 [7800/25000 (99%)]\tLoss: 1.450081 testing loss: tensor(1.6141)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.6141)\n",
      "CS 2 : 1.61472\n",
      "DP 2 : 1.74272\n",
      "heuristic 2 : 1.66756\n",
      "DP: 1.7344958782196045\n",
      "tensor([0.3378, 0.3303, 0.3319])\n",
      "tensor([0.5051, 0.4949, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: uniform heuristic\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 0.003495\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 0.000048\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 0.000003\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 0.000001\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 0.000000\n",
      "NN 1 : tensor(1.6157)\n",
      "CS 1 : 1.61472\n",
      "DP 1 : 1.74272\n",
      "heuristic 1 : 1.66756\n",
      "DP: 1.7344958782196045\n",
      "tensor([0.3300, 0.3300, 0.3400])\n",
      "tensor([0.5000, 0.5000, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 1.612057 testing loss: tensor(1.6164)\n",
      "Train Epoch: 1 [640/25000 (3%)]\tLoss: 1.637059 testing loss: tensor(1.6176)\n",
      "Train Epoch: 1 [1280/25000 (5%)]\tLoss: 1.570151 testing loss: tensor(1.6197)\n",
      "Train Epoch: 1 [1920/25000 (8%)]\tLoss: 1.599278 testing loss: tensor(1.6195)\n",
      "Train Epoch: 1 [2560/25000 (10%)]\tLoss: 1.670818 testing loss: tensor(1.6204)\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 1.635790 testing loss: tensor(1.6222)\n",
      "Train Epoch: 1 [3840/25000 (15%)]\tLoss: 1.657663 testing loss: tensor(1.6200)\n",
      "Train Epoch: 1 [4480/25000 (18%)]\tLoss: 1.539508 testing loss: tensor(1.6184)\n",
      "Train Epoch: 1 [5120/25000 (20%)]\tLoss: 1.730475 testing loss: tensor(1.6187)\n",
      "Train Epoch: 1 [5760/25000 (23%)]\tLoss: 1.643378 testing loss: tensor(1.6173)\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 1.700424 testing loss: tensor(1.6141)\n",
      "Train Epoch: 1 [7040/25000 (28%)]\tLoss: 1.672571 testing loss: tensor(1.6141)\n",
      "Train Epoch: 1 [7680/25000 (31%)]\tLoss: 1.520849 testing loss: tensor(1.6137)\n",
      "Train Epoch: 1 [8320/25000 (33%)]\tLoss: 1.640135 testing loss: tensor(1.6141)\n",
      "Train Epoch: 1 [8960/25000 (36%)]\tLoss: 1.614719 testing loss: tensor(1.6149)\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 1.520846 testing loss: tensor(1.6145)\n",
      "Train Epoch: 1 [10240/25000 (41%)]\tLoss: 1.799934 testing loss: tensor(1.6134)\n",
      "Train Epoch: 1 [10880/25000 (43%)]\tLoss: 1.584350 testing loss: tensor(1.6149)\n",
      "Train Epoch: 1 [11520/25000 (46%)]\tLoss: 1.563308 testing loss: tensor(1.6158)\n",
      "Train Epoch: 1 [12160/25000 (48%)]\tLoss: 1.517939 testing loss: tensor(1.6164)\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 1.580257 testing loss: tensor(1.6162)\n",
      "Train Epoch: 1 [13440/25000 (54%)]\tLoss: 1.701377 testing loss: tensor(1.6158)\n",
      "Train Epoch: 1 [14080/25000 (56%)]\tLoss: 1.579203 testing loss: tensor(1.6158)\n",
      "Train Epoch: 1 [14720/25000 (59%)]\tLoss: 1.553510 testing loss: tensor(1.6161)\n",
      "Train Epoch: 1 [15360/25000 (61%)]\tLoss: 1.608597 testing loss: tensor(1.6154)\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 1.589291 testing loss: tensor(1.6152)\n",
      "Train Epoch: 1 [16640/25000 (66%)]\tLoss: 1.623747 testing loss: tensor(1.6162)\n",
      "Train Epoch: 1 [17280/25000 (69%)]\tLoss: 1.659831 testing loss: tensor(1.6153)\n",
      "Train Epoch: 1 [17920/25000 (71%)]\tLoss: 1.690121 testing loss: tensor(1.6159)\n",
      "Train Epoch: 1 [18560/25000 (74%)]\tLoss: 1.589375 testing loss: tensor(1.6170)\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 1.728306 testing loss: tensor(1.6158)\n",
      "Train Epoch: 1 [19840/25000 (79%)]\tLoss: 1.581563 testing loss: tensor(1.6151)\n",
      "Train Epoch: 1 [20480/25000 (82%)]\tLoss: 1.645018 testing loss: tensor(1.6154)\n",
      "Train Epoch: 1 [21120/25000 (84%)]\tLoss: 1.612105 testing loss: tensor(1.6144)\n",
      "Train Epoch: 1 [21760/25000 (87%)]\tLoss: 1.562543 testing loss: tensor(1.6145)\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 1.591754 testing loss: tensor(1.6157)\n",
      "Train Epoch: 1 [23040/25000 (92%)]\tLoss: 1.653247 testing loss: tensor(1.6158)\n",
      "Train Epoch: 1 [23680/25000 (94%)]\tLoss: 1.630633 testing loss: tensor(1.6152)\n",
      "Train Epoch: 1 [24320/25000 (97%)]\tLoss: 1.604475 testing loss: tensor(1.6145)\n",
      "Train Epoch: 1 [7800/25000 (99%)]\tLoss: 1.450131 testing loss: tensor(1.6136)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.6136)\n",
      "CS 2 : 1.61472\n",
      "DP 2 : 1.74272\n",
      "heuristic 2 : 1.66756\n",
      "DP: 1.7344958782196045\n",
      "tensor([0.3366, 0.3295, 0.3339])\n",
      "tensor([0.5032, 0.4968, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.1)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "    \n",
    "for order in stage:\n",
    "    producedata(order)\n",
    "    #running\n",
    "    for order1 in order1name:\n",
    "        print(\"Supervised Aim:\",order,order1)\n",
    "        \n",
    "        # for mapping binary to payments before softmax\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(n, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, n),\n",
    "        )\n",
    "        model.apply(init_weights)\n",
    "        # optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr1)\n",
    "        \n",
    "        runningLossNN = []\n",
    "        runningLossCS = []\n",
    "        runningLossDP = []\n",
    "        runningLossHeuristic = []\n",
    "        #model=torch.load(\"save/pytorchNN=5dp1\");\n",
    "        #model.eval()\n",
    "        ##order1name=[\"costsharing\",\"dp\",\"heuristic\",\"random initializing\"]\n",
    "        for epoch in range(1, supervisionEpochs + 1):\n",
    "#             print(\"distributionRatio\",distributionRatio)\n",
    "            if(order1==\"costsharing\"):\n",
    "                supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "            elif(order1==\"dp\"):\n",
    "                supervisionTrain(epoch, dpSupervisionRule)\n",
    "            elif(order1==\"heuristic\"):\n",
    "                supervisionTrain(epoch, heuristicSupervisionRule)\n",
    "            elif(order1==\"random initializing\"):\n",
    "                print(\"do nothing\");\n",
    "\n",
    "        test()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr2)\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(epoch)\n",
    "            test()\n",
    "        losslistname.append(order+\" \"+order1);\n",
    "        losslist.append(losslisttemp);\n",
    "        losslisttemp=[];\n",
    "        savepath=\"save/pytorchNN=5all-beta\"+order+str(order1)\n",
    "        torch.save(model, savepath);\n",
    "        print(\"end\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colorlist=[\"#D2691E\",'#4169E1',\"#9ACD32\",\"#B22222\",\"#FF00FF\",\"#708090\"]\n",
    "for i in range(len(losslist)):\n",
    "    plt.plot(losslist[i], '.-', color=colorlist[i], alpha=0.8, label=losslistname[i])\n",
    "\n",
    "# 显示标签，如果不加这句，即使加了label='一些数字'的参数，最终还是不会显示标签\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('Trianing Times')\n",
    "plt.ylabel('Delay')\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('pytorchNN=3_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
