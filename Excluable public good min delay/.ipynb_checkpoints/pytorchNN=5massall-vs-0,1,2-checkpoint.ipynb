{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "from pylab import *\n",
    "from scipy.stats import beta\n",
    "import random \n",
    "n = 5\n",
    "epochs = 1\n",
    "supervisionEpochs = 1\n",
    "lr = 0.0001\n",
    "log_interval = 5\n",
    "trainSize = 40000#100000\n",
    "percentage_train_test= 0.75\n",
    "penaltyLambda = 10\n",
    "doublePeakHighMean = 0.85\n",
    "doublePeakLowMean = 0.15\n",
    "doublePeakStd = 0.1\n",
    "uniformlow=0\n",
    "uniformhigh=1.0\n",
    "normalloc = 0.2\n",
    "normalscale = 0.1\n",
    "\n",
    "cauchyloc = 1/n\n",
    "cauchyscalen = 0.004\n",
    "\n",
    "exponentialhigh = 15 #Symbol(\"b\", real=True)\n",
    "exponentiallow  = 15 #Symbol(\"a\", real=True)\n",
    "\n",
    "Sigmoidrate=0.00005\n",
    "Sigmoidbias=0.001\n",
    "\n",
    "\n",
    "\n",
    "independentnormalloc1=[(float(ii)+1)/(2*n+1) for ii in range(n,0,-1)]\n",
    "independentnormalscale1=[0.05 for ii in range(n)]\n",
    "\n",
    "independentnormalloc2=[(float(ii)+1)/(2*n+1) for ii in range(1,n+1,1)]\n",
    "independentnormalscale2=[0.05 for ii in range(n)]\n",
    "stage=[\"twopeak\"]\n",
    "# \"twopeak\",\"normal\",\"uniform\",\"independent1\",\"independent2\",\"cauchy\",\"U-exponential\"\n",
    "order1name=[\"random initializing\"]\n",
    "numberofpeople=['0','1','2']\n",
    "# \"costsharing\",\"dp\",\"heuristic\",\"random initializing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5000)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio1 = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase1 = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "d3 = D.normal.Normal(normalloc, normalscale)\n",
    "distributionRatio3 = d3.cdf(1) - d3.cdf(0)\n",
    "distributionBase3 = d3.cdf(0)\n",
    "\n",
    "d4 = D.uniform.Uniform(uniformlow,uniformhigh)\n",
    "distributionRatio4 = d4.cdf(1) - d4.cdf(0)\n",
    "distributionBase4 = d4.cdf(0)\n",
    "\n",
    "d5 = [D.normal.Normal(independentnormalloc1[ii], independentnormalscale1[ii]) for ii in range(n)]\n",
    "d6 = [D.normal.Normal(independentnormalloc2[ii], independentnormalscale2[ii]) for ii in range(n)]\n",
    "\n",
    "d7 = D.cauchy.Cauchy(cauchyloc,cauchyscalen)\n",
    "\n",
    "d81 = D.exponential.Exponential(exponentiallow)\n",
    "d82 = D.exponential.Exponential(exponentialhigh)\n",
    "\n",
    "#d8 = beta(betahigh,betalow)\n",
    "#d9 = D.beta.Beta(betahigh,betalow)\n",
    "def cdf(x,y, i=None):\n",
    "    if(y==\"twopeak\"):\n",
    "        return (d1.cdf(x) + d2.cdf(x) - distributionBase1) / 2 / distributionRatio1\n",
    "    elif(y==\"normal\"):\n",
    "        return (d3.cdf(x)-distributionBase3)/distributionRatio3;\n",
    "    elif(y==\"uniform\"):\n",
    "        return (d4.cdf(x)-distributionBase4)/distributionRatio4;\n",
    "    elif(y==\"independent1\"):\n",
    "        return d5[i].cdf(x);\n",
    "    elif(y==\"independent2\"):\n",
    "        return d6[i].cdf(x);\n",
    "    elif(y==\"cauchy\"):\n",
    "        return d7.cdf(x);\n",
    "#     elif(y==\"beta\"):\n",
    "#         return torch.tensor(d8.cdf(x));\n",
    "    elif(y==\"U-exponential\"):\n",
    "        return (d81.cdf(x) + (1.0 - d82.cdf(1.0-x)))  / 2 \n",
    "print(cdf(0.5,\"U-exponential\"))\n",
    "\n",
    "print(d81.cdf(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    if torch.sum(bits).item() == 0:\n",
    "        return torch.ones(n)\n",
    "    bits = bits.type(torch.float32)\n",
    "    negBits = torch.ones(n) - bits\n",
    "    payments = model(bits)\n",
    "    payments = payments - 1000 * negBits\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    payments = payments + negBits\n",
    "    return payments\n",
    "\n",
    "def tpToBits(tp, bits=torch.ones(n).type(torch.uint8)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = (tp >= payments).type(torch.uint8)\n",
    "    if torch.equal(newBits, bits):\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits(tp, newBits)#bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    return n - torch.sum(tpToBits(tp).type(torch.float32))\n",
    "\n",
    "def tpToBits0(tp, deep ,bits=torch.ones(n).type(torch.float32)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = torch.sigmoid((tp - payments + Sigmoidbias)/Sigmoidrate)\n",
    "    if torch.allclose(newBits, bits,rtol=0.01,atol=0.01) or deep>=n:\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits0(tp,deep+1 ,newBits)#bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "def tpToTotalDelay0(tp):\n",
    "    return n - torch.sum(tpToBits0(tp,0).type(torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    global  dp,decision\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money, yes]\n",
    "        offer = float(offerIndex) / dpPrecision\n",
    "        if tp[i] >= offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits[i] = 0\n",
    "            payments[i] = 1\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "#     print()\n",
    "#     print(tp)\n",
    "#     print(bits)\n",
    "#     print(payments)\n",
    "#     print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def heuristicSupervisionRule(tp):\n",
    "    global  dp_H,decision_H\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    tempold=-1;\n",
    "    for turns in range(n,0,-1):\n",
    "        money = dpPrecision\n",
    "        j=0\n",
    "        tempo=sum(bits)\n",
    "        #print(\"bits\",tempo)\n",
    "        for i in range(n):\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            \n",
    "            offerIndex = decision_H[tempo ,tempo - i, money]\n",
    "            offer = float(offerIndex) / dpPrecision\n",
    "            while(j<n):\n",
    "                if(bits[j]!=0):\n",
    "                    break;\n",
    "                j+=1;\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            if tp[j] >= offer:\n",
    "                #print(money,j,tp[j],offer)\n",
    "                money -= offerIndex\n",
    "                bits[j] = 1\n",
    "                payments[j] = offer\n",
    "            else:\n",
    "                bits[j] = 0;\n",
    "                payments[j] = 1.0;\n",
    "            j+=1;\n",
    "        #print(\"money\",money)\n",
    "        if(money==0 and tempold==tempo):\n",
    "            break;\n",
    "        tempold=tempo;\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    for k in range(n, -1, -1):\n",
    "        if k == 0:\n",
    "            break\n",
    "        bits = [1 if tp[ii] >= 1.0 / k else 0 for ii in range(n)]\n",
    "        if sum(bits) == k:\n",
    "            break\n",
    "    if k == 0:\n",
    "        payments = [1 for ii in range(n)]\n",
    "    else:\n",
    "        payments = [1.0 / k if bits[ii] == 1 else 1 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    return float(n - torch.sum(costSharingSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def dpDelay(tp):\n",
    "    return float(n - torch.sum(dpSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def heuristicDelay(tp):\n",
    "    return float(n - torch.sum(heuristicSupervisionRule(tp)[0]).item())\n",
    "\n",
    "\n",
    "# templire=0;\n",
    "# num=1\n",
    "# for i in range(num):\n",
    "#     temp=torch.tensor(samplesJoint[i])\n",
    "#     #print(temp)\n",
    "#     #print(temp);\n",
    "#     #print(heuristicSupervisionRule(temp))\n",
    "#     #print(dpSupervisionRule(temp))\n",
    "#     #print(dpDelay(temp))\n",
    "#     res=dpDelay(temp)\n",
    "#     templire+=res\n",
    "#     #print(\"delay\",res)\n",
    "#     #print()\n",
    "# print(templire/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordAndReport(name, source, loss):\n",
    "    source.append(loss)\n",
    "    realLength = len(source)\n",
    "    #print(f\"{name} ({realLength}): {loss}\")\n",
    "    print(name,realLength,\":\" ,loss)\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    \n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda /100000\n",
    "        \n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "losslist=[];\n",
    "losslistname=[];\n",
    "losslisttemp=[];\n",
    "def test_batch():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lenLoss= 0\n",
    "        nnLoss = 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "            for tp in tp_batch:\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        nnLoss/=lenLoss\n",
    "    return nnLoss\n",
    "\n",
    "\n",
    "def train0(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            delay0 = tpToTotalDelay0(tp)\n",
    "            loss = loss + delay0\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "\n",
    "def train1(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                delay1 = tpToTotalDelay(tp1)\n",
    "                delay0 = tpToTotalDelay(tp0)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                    loss = loss + (1 - cdf(offer,order)) * delay1 + cdf(offer,order) * delay0\n",
    "                else:\n",
    "                    loss = loss + (1 - cdf(offer,order,i)) * delay1 + cdf(offer,order,i) * delay0\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "    \n",
    "def train2(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                x1=random.randint(0,n-1);\n",
    "                x2=random.randint(0,n-1);\n",
    "                while(x2==x1):\n",
    "                    x2=random.randint(0,n-1);\n",
    "                    \n",
    "                tp11 = tp.clone()\n",
    "                tp11[x1] = 1\n",
    "                tp11[x2] = 1\n",
    "                tp10 = tp.clone()\n",
    "                tp10[x1] = 1\n",
    "                tp10[x2] = 0\n",
    "                tp01 = tp.clone()\n",
    "                tp01[x1] = 0\n",
    "                tp01[x2] = 1\n",
    "                tp00 = tp.clone()\n",
    "                tp00[x1] = 0\n",
    "                tp00[x2] = 0\n",
    "                \n",
    "                offer1 = tpToPayments(tp11)[x1]\n",
    "                offer2 = tpToPayments(tp11)[x2]\n",
    "                offer3 = tpToPayments(tp10)[x1]\n",
    "                offer4 = tpToPayments(tp01)[x2]\n",
    "                \n",
    "                #print(tp)\n",
    "                #print(offer1,offer2)\n",
    "                \n",
    "                delay11 = tpToTotalDelay(tp11)\n",
    "                delay01 = tpToTotalDelay(tp01)\n",
    "                delay10 = tpToTotalDelay(tp10)\n",
    "                delay00 = tpToTotalDelay(tp00)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "#                     print ((1.0-cdf(offer1,order)) * (1.0-cdf(offer2,order))+                                                   \n",
    "#                            (1.0-cdf(offer3,order)) * cdf(offer2,order)+\n",
    "#                            cdf(offer1,order) * (1.0-cdf(offer4,order))+\n",
    "#                            (cdf(offer3,order) * cdf(offer4,order) -\n",
    "#                                      (cdf(offer3,order)-cdf(offer1,order))*(cdf(offer4,order)-cdf(offer2,order)) )\n",
    "#                           )\n",
    "                    loss = loss +(      (1.0-cdf(offer1,order)) * (1.0-cdf(offer2,order)) * delay11 +\n",
    "                                  (1.0-cdf(offer3,order)) * cdf(offer2,order) * delay10 +\n",
    "                                  cdf(offer1,order) * (1.0-cdf(offer4,order)) * delay01 +\n",
    "                                    (cdf(offer3,order) * cdf(offer4,order) -\n",
    "                                     (cdf(offer3,order)-cdf(offer1,order))*(cdf(offer4,order)-cdf(offer2,order)) )* delay00 \n",
    "                                                       )\n",
    "                else:\n",
    "                    loss = loss +(      (1.0-cdf(offer1,order,x1)) * (1.0-cdf(offer2,order),x2) * delay11 +\n",
    "                                  (1.0-cdf(offer3,order,x1)) * cdf(offer2,order,x2) * delay10 +\n",
    "                                  cdf(offer1,order,x1) * (1.0-cdf(offer4,order,x2)) * delay01 +\n",
    "                                    (cdf(offer3,order,x1) * cdf(offer4,order,x2) -\n",
    "                                     (cdf(offer3,order,x1)-cdf(offer1,order,x1))*(cdf(offer4,order,x2)-cdf(offer2,order,x2)) )* delay00 \n",
    "                                                       )\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "    \n",
    "allBits = [torch.tensor(bits, dtype=torch.int16) for bits in itertools.product([0, 1], repeat=n)]\n",
    "\n",
    "def test():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        costSharingLoss = 0\n",
    "        dpLoss = 0\n",
    "        nnLoss = 0\n",
    "        heuristicLoss=0\n",
    "        lenLoss= 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "                heuristicLoss+= heuristicDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        costSharingLoss /= lenLoss\n",
    "        dpLoss /= lenLoss\n",
    "        nnLoss /= lenLoss\n",
    "        heuristicLoss /= lenLoss\n",
    "        #print(lenLoss)\n",
    "        losslisttemp.append(nnLoss);\n",
    "        recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "        recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "        recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "        recordAndReport(\"heuristic\", runningLossHeuristic, heuristicLoss)\n",
    "        print(\"DP:\",dp[n, dpPrecision,0])\n",
    "        #for i in range(n, 0, -1):\n",
    "        #    print(\"Heuristic:\",i,5*(1-dp_H[i, i, dpPrecision]))\n",
    "        for i in range(n, 0, -1):\n",
    "            print(\n",
    "                    tpToPayments(\n",
    "                            torch.tensor([0 if ii >= i else 1 for ii in range(n)], dtype=torch.float32)\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "dpPrecision = 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def producedata(order):\n",
    "    global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "    dpPrecision = 100 \n",
    "    if(order==\"twopeak\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = np.random.normal(\n",
    "            loc=doublePeakLowMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=doublePeakLowMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samples2 = np.random.normal(\n",
    "            loc=doublePeakHighMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = np.random.normal(\n",
    "                        loc=doublePeakHighMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "    elif(order==\"normal\"):\n",
    "        print(\"loc\",normalloc, \"scale\",normalscale)\n",
    "        samples1 = np.random.normal(\n",
    "            loc=normalloc, scale=normalscale, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=normalloc, scale=normalscale\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"uniform\"):  \n",
    "        print(\"uniformlow\",uniformlow, \"uniformhigh\",uniformhigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        uniformlow, uniformhigh\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent1\"):\n",
    "        print(\"loc\",independentnormalloc1,\"scale\",independentnormalscale1)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent2\"):\n",
    "        print(\"loc\",independentnormalloc2, \"scale\",independentnormalscale2)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"cauchy\"):\n",
    "        print(\"cauchyloc\",cauchyloc, \"cauchyscale\",cauchyscalen)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"beta\"):\n",
    "        print(\"betalow\",betalow, \"betahigh\",betahigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(betahigh,betalow,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(betahigh,betalow,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"U-exponential\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = d81.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d81.rsample(torch.Size([1])).numpy()\n",
    "                    \n",
    "        samples2 = d82.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        \n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = d82.rsample(torch.Size([1])).numpy()\n",
    "        samples2 = 1.0 - samples2\n",
    "        samplesJoint = signals * samples1 - (signals - 1.0) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        plt.hist(samplesJoint,bins=500)\n",
    "        plt.show()\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "        \n",
    "        \n",
    "    tp_dataset = TensorDataset(tp_tensor[: int(trainSize * percentage_train_test)])\n",
    "    tp_dataset_testing = TensorDataset(tp_tensor[int(trainSize * (1.0-percentage_train_test)) :])\n",
    "    tp_dataloader = DataLoader(tp_dataset, batch_size=128, shuffle=True)\n",
    "    tp_dataloader_testing = DataLoader(tp_dataset_testing, batch_size=256, shuffle=False)\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "    decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for yes in range(n + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp[0, 0, yes] = 0\n",
    "            else:\n",
    "                dp[0, money, yes] = yes# + 1.0\n",
    "    for ppl in range(1,  n + 1):\n",
    "        for yes in range(n + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                minSoFar = 1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1 - cdf(offer,order)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order) * (1 + dp[ppl - 1, money, yes])\n",
    "                    else:\n",
    "                        res = (1 - cdf(offer,order,n-ppl)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order,n-ppl) * (1 + dp[ppl - 1, money, yes])\n",
    "                    if minSoFar > res.item():\n",
    "                        minSoFar = res.item()\n",
    "                        decision[ppl, money, yes] = offerIndex\n",
    "                dp[ppl, money, yes] = minSoFar\n",
    "    \n",
    "    print(\"dp\",dp[n, dpPrecision, 0])\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp_H = np.zeros([n + 1 , n + 1, dpPrecision + 1])\n",
    "    decision_H = np.zeros([n + 1 , n + 1, dpPrecision + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for i in range(1,n+1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp_H[i, 0, 0] = 1\n",
    "            else:\n",
    "                offer = money / dpPrecision\n",
    "                dp_H[i, 0, money] = 0#cdf(offer)# + 1.0\n",
    "    for i in range(1,n+1):\n",
    "        for ppl in range(1, i + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                maxSoFar = -1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1-cdf(offer,order)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    else:\n",
    "                        res = (1-cdf(offer,order,ppl-1)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    if maxSoFar < res.item():\n",
    "                        maxSoFar = res.item()\n",
    "                        decision_H[i, ppl, money] = offerIndex\n",
    "                dp_H[i, ppl, money] = maxSoFar\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loc 0.15 scale 0.1\n",
      "loc 0.85 scale 0.1\n",
      "dp 1.7567987442016602\n",
      "Supervised Aim: twopeak costsharing\n",
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 0.000578\n",
      "Train Epoch: 1 [1280/30000 (4%)]\tLoss: 0.000072\n",
      "Train Epoch: 1 [2560/30000 (9%)]\tLoss: 0.000025\n",
      "Train Epoch: 1 [3840/30000 (13%)]\tLoss: 0.000003\n",
      "Train Epoch: 1 [5120/30000 (17%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [6400/30000 (21%)]\tLoss: 0.000001\n",
      "Train Epoch: 1 [7680/30000 (26%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [8960/30000 (30%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [10240/30000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [11520/30000 (38%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [12800/30000 (43%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [14080/30000 (47%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [15360/30000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [16640/30000 (55%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [17920/30000 (60%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [19200/30000 (64%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [20480/30000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [21760/30000 (72%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [23040/30000 (77%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [24320/30000 (81%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [25600/30000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [26880/30000 (89%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [28160/30000 (94%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [29440/30000 (98%)]\tLoss: 0.000000\n",
      "NN 1 : tensor(2.3607)\n",
      "CS 1 : 2.3606666666666665\n",
      "DP 1 : 1.7577\n",
      "heuristic 1 : 1.7141666666666666\n",
      "DP: 1.7567987442016602\n",
      "tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500, 1.0000])\n",
      "tensor([0.3333, 0.3333, 0.3333, 1.0000, 1.0000])\n",
      "tensor([0.5000, 0.5000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 2.226562 testing loss: tensor(2.3607)\n",
      "Train Epoch: 1 [1280/30000 (4%)]\tLoss: 2.234375 testing loss: tensor(2.3507)\n",
      "Train Epoch: 1 [2560/30000 (9%)]\tLoss: 2.523411 testing loss: tensor(2.3379)\n",
      "Train Epoch: 1 [3840/30000 (13%)]\tLoss: 2.257812 testing loss: tensor(2.2927)\n",
      "Train Epoch: 1 [5120/30000 (17%)]\tLoss: 2.149270 testing loss: tensor(2.2285)\n",
      "Train Epoch: 1 [6400/30000 (21%)]\tLoss: 2.125000 testing loss: tensor(2.1770)\n",
      "Train Epoch: 1 [7680/30000 (26%)]\tLoss: 2.046875 testing loss: tensor(2.1744)\n",
      "Train Epoch: 1 [8960/30000 (30%)]\tLoss: 2.148438 testing loss: tensor(2.1362)\n",
      "Train Epoch: 1 [10240/30000 (34%)]\tLoss: 1.992188 testing loss: tensor(2.0939)\n",
      "Train Epoch: 1 [11520/30000 (38%)]\tLoss: 2.031250 testing loss: tensor(2.0743)\n",
      "Train Epoch: 1 [12800/30000 (43%)]\tLoss: 1.921922 testing loss: tensor(2.0038)\n",
      "Train Epoch: 1 [14080/30000 (47%)]\tLoss: 1.867188 testing loss: tensor(1.9112)\n",
      "Train Epoch: 1 [15360/30000 (51%)]\tLoss: 1.546875 testing loss: tensor(1.8698)\n",
      "Train Epoch: 1 [16640/30000 (55%)]\tLoss: 1.938484 testing loss: tensor(1.8503)\n",
      "Train Epoch: 1 [17920/30000 (60%)]\tLoss: 2.110445 testing loss: tensor(1.8623)\n",
      "Train Epoch: 1 [19200/30000 (64%)]\tLoss: 1.945312 testing loss: tensor(2.0746)\n",
      "Train Epoch: 1 [20480/30000 (68%)]\tLoss: 2.062500 testing loss: tensor(2.1568)\n",
      "Train Epoch: 1 [21760/30000 (72%)]\tLoss: 2.257812 testing loss: tensor(2.1791)\n",
      "Train Epoch: 1 [23040/30000 (77%)]\tLoss: 2.234375 testing loss: tensor(2.1825)\n",
      "Train Epoch: 1 [24320/30000 (81%)]\tLoss: 2.195312 testing loss: tensor(2.1821)\n",
      "Train Epoch: 1 [25600/30000 (85%)]\tLoss: 2.187500 testing loss: tensor(2.1805)\n",
      "Train Epoch: 1 [26880/30000 (89%)]\tLoss: 2.187500 testing loss: tensor(2.1782)\n",
      "Train Epoch: 1 [28160/30000 (94%)]\tLoss: 2.234375 testing loss: tensor(2.1641)\n",
      "Train Epoch: 1 [29440/30000 (98%)]\tLoss: 2.203125 testing loss: tensor(2.1431)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(2.1305)\n",
      "CS 2 : 2.3606666666666665\n",
      "DP 2 : 1.7577\n",
      "heuristic 2 : 1.7141666666666666\n",
      "DP: 1.7567987442016602\n",
      "tensor([0.0474, 0.2453, 0.1268, 0.4023, 0.1782])\n",
      "tensor([0.0676, 0.3021, 0.1676, 0.4627, 1.0000])\n",
      "tensor([0.1056, 0.5768, 0.3176, 1.0000, 1.0000])\n",
      "tensor([0.2400, 0.7600, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 0.000382\n",
      "Train Epoch: 1 [1280/30000 (4%)]\tLoss: 0.000041\n",
      "Train Epoch: 1 [2560/30000 (9%)]\tLoss: 0.000009\n",
      "Train Epoch: 1 [3840/30000 (13%)]\tLoss: 0.000004\n",
      "Train Epoch: 1 [5120/30000 (17%)]\tLoss: 0.000001\n",
      "Train Epoch: 1 [6400/30000 (21%)]\tLoss: 0.000001\n",
      "Train Epoch: 1 [7680/30000 (26%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [8960/30000 (30%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [10240/30000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [11520/30000 (38%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [12800/30000 (43%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [14080/30000 (47%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [15360/30000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [16640/30000 (55%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [17920/30000 (60%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [19200/30000 (64%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [20480/30000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [21760/30000 (72%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [23040/30000 (77%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [24320/30000 (81%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [25600/30000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [26880/30000 (89%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [28160/30000 (94%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [29440/30000 (98%)]\tLoss: 0.000000\n",
      "NN 1 : tensor(2.3607)\n",
      "CS 1 : 2.3606666666666665\n",
      "DP 1 : 1.7577\n",
      "heuristic 1 : 1.7141666666666666\n",
      "DP: 1.7567987442016602\n",
      "tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500, 1.0000])\n",
      "tensor([0.3333, 0.3333, 0.3333, 1.0000, 1.0000])\n",
      "tensor([0.5000, 0.5000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 2.320702 testing loss: tensor(2.3601)\n",
      "Train Epoch: 1 [1280/30000 (4%)]\tLoss: 2.285772 testing loss: tensor(2.1129)\n",
      "Train Epoch: 1 [2560/30000 (9%)]\tLoss: 1.990245 testing loss: tensor(1.8838)\n",
      "Train Epoch: 1 [3840/30000 (13%)]\tLoss: 1.823575 testing loss: tensor(1.8361)\n",
      "Train Epoch: 1 [5120/30000 (17%)]\tLoss: 2.048330 testing loss: tensor(1.7737)\n",
      "Train Epoch: 1 [6400/30000 (21%)]\tLoss: 1.909012 testing loss: tensor(1.7455)\n",
      "Train Epoch: 1 [7680/30000 (26%)]\tLoss: 1.650902 testing loss: tensor(1.7286)\n",
      "Train Epoch: 1 [8960/30000 (30%)]\tLoss: 1.701398 testing loss: tensor(1.7149)\n",
      "Train Epoch: 1 [10240/30000 (34%)]\tLoss: 1.741709 testing loss: tensor(1.6984)\n",
      "Train Epoch: 1 [11520/30000 (38%)]\tLoss: 1.455546 testing loss: tensor(1.6900)\n",
      "Train Epoch: 1 [12800/30000 (43%)]\tLoss: 1.540153 testing loss: tensor(1.6827)\n",
      "Train Epoch: 1 [14080/30000 (47%)]\tLoss: 1.733022 testing loss: tensor(1.6776)\n",
      "Train Epoch: 1 [15360/30000 (51%)]\tLoss: 1.741115 testing loss: tensor(1.6703)\n",
      "Train Epoch: 1 [16640/30000 (55%)]\tLoss: 1.800810 testing loss: tensor(1.6619)\n",
      "Train Epoch: 1 [17920/30000 (60%)]\tLoss: 1.668065 testing loss: tensor(1.6540)\n",
      "Train Epoch: 1 [19200/30000 (64%)]\tLoss: 1.873256 testing loss: tensor(1.6462)\n",
      "Train Epoch: 1 [20480/30000 (68%)]\tLoss: 1.459437 testing loss: tensor(1.6436)\n",
      "Train Epoch: 1 [21760/30000 (72%)]\tLoss: 1.842109 testing loss: tensor(1.6429)\n",
      "Train Epoch: 1 [23040/30000 (77%)]\tLoss: 1.540430 testing loss: tensor(1.6415)\n",
      "Train Epoch: 1 [24320/30000 (81%)]\tLoss: 1.612044 testing loss: tensor(1.6419)\n",
      "Train Epoch: 1 [25600/30000 (85%)]\tLoss: 1.493507 testing loss: tensor(1.6396)\n",
      "Train Epoch: 1 [26880/30000 (89%)]\tLoss: 1.557402 testing loss: tensor(1.6398)\n",
      "Train Epoch: 1 [28160/30000 (94%)]\tLoss: 1.598540 testing loss: tensor(1.6396)\n",
      "Train Epoch: 1 [29440/30000 (98%)]\tLoss: 1.585729 testing loss: tensor(1.6391)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.6390)\n",
      "CS 2 : 2.3606666666666665\n",
      "DP 2 : 1.7577\n",
      "heuristic 2 : 1.7141666666666666\n",
      "DP: 1.7567987442016602\n",
      "tensor([0.0758, 0.0792, 0.0838, 0.0818, 0.6794])\n",
      "tensor([0.1020, 0.6934, 0.0982, 0.1063, 1.0000])\n",
      "tensor([0.1338, 0.7170, 0.1492, 1.0000, 1.0000])\n",
      "tensor([0.2483, 0.7517, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 0.000595\n",
      "Train Epoch: 1 [1280/30000 (4%)]\tLoss: 0.000070\n",
      "Train Epoch: 1 [2560/30000 (9%)]\tLoss: 0.000019\n",
      "Train Epoch: 1 [3840/30000 (13%)]\tLoss: 0.000005\n",
      "Train Epoch: 1 [5120/30000 (17%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [6400/30000 (21%)]\tLoss: 0.000001\n",
      "Train Epoch: 1 [7680/30000 (26%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [8960/30000 (30%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [10240/30000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [11520/30000 (38%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [12800/30000 (43%)]\tLoss: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [14080/30000 (47%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [15360/30000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [16640/30000 (55%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [17920/30000 (60%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [19200/30000 (64%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [20480/30000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [21760/30000 (72%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [23040/30000 (77%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [24320/30000 (81%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [25600/30000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [26880/30000 (89%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [28160/30000 (94%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [29440/30000 (98%)]\tLoss: 0.000000\n",
      "NN 1 : tensor(2.3607)\n",
      "CS 1 : 2.3606666666666665\n",
      "DP 1 : 1.7577\n",
      "heuristic 1 : 1.7141666666666666\n",
      "DP: 1.7567987442016602\n",
      "tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500, 1.0000])\n",
      "tensor([0.3333, 0.3333, 0.3333, 1.0000, 1.0000])\n",
      "tensor([0.5000, 0.5000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 2.351262 testing loss: tensor(2.3585)\n",
      "Train Epoch: 1 [1280/30000 (4%)]\tLoss: 1.885581 testing loss: tensor(1.8987)\n",
      "Train Epoch: 1 [2560/30000 (9%)]\tLoss: 1.844999 testing loss: tensor(1.8248)\n",
      "Train Epoch: 1 [3840/30000 (13%)]\tLoss: 1.715957 testing loss: tensor(1.7605)\n",
      "Train Epoch: 1 [5120/30000 (17%)]\tLoss: 1.882489 testing loss: tensor(1.7391)\n",
      "Train Epoch: 1 [6400/30000 (21%)]\tLoss: 1.763439 testing loss: tensor(1.7266)\n",
      "Train Epoch: 1 [7680/30000 (26%)]\tLoss: 1.871228 testing loss: tensor(1.7126)\n",
      "Train Epoch: 1 [8960/30000 (30%)]\tLoss: 1.550374 testing loss: tensor(1.7117)\n",
      "Train Epoch: 1 [10240/30000 (34%)]\tLoss: 1.578766 testing loss: tensor(1.6919)\n",
      "Train Epoch: 1 [11520/30000 (38%)]\tLoss: 1.609092 testing loss: tensor(1.6803)\n",
      "Train Epoch: 1 [12800/30000 (43%)]\tLoss: 1.646535 testing loss: tensor(1.6676)\n",
      "Train Epoch: 1 [14080/30000 (47%)]\tLoss: 1.684658 testing loss: tensor(1.6574)\n",
      "Train Epoch: 1 [15360/30000 (51%)]\tLoss: 1.535769 testing loss: tensor(1.6476)\n",
      "Train Epoch: 1 [16640/30000 (55%)]\tLoss: 1.534030 testing loss: tensor(1.6383)\n",
      "Train Epoch: 1 [17920/30000 (60%)]\tLoss: 1.579053 testing loss: tensor(1.6368)\n",
      "Train Epoch: 1 [19200/30000 (64%)]\tLoss: 1.686037 testing loss: tensor(1.6320)\n",
      "Train Epoch: 1 [20480/30000 (68%)]\tLoss: 1.688372 testing loss: tensor(1.6302)\n",
      "Train Epoch: 1 [21760/30000 (72%)]\tLoss: 1.650975 testing loss: tensor(1.6329)\n",
      "Train Epoch: 1 [23040/30000 (77%)]\tLoss: 1.523167 testing loss: tensor(1.6301)\n",
      "Train Epoch: 1 [24320/30000 (81%)]\tLoss: 1.593115 testing loss: tensor(1.6320)\n",
      "Train Epoch: 1 [25600/30000 (85%)]\tLoss: 1.577977 testing loss: tensor(1.6339)\n",
      "Train Epoch: 1 [26880/30000 (89%)]\tLoss: 1.562988 testing loss: tensor(1.6293)\n",
      "Train Epoch: 1 [28160/30000 (94%)]\tLoss: 1.601033 testing loss: tensor(1.6308)\n",
      "Train Epoch: 1 [29440/30000 (98%)]\tLoss: 1.764661 testing loss: tensor(1.6288)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.6306)\n",
      "CS 2 : 2.3606666666666665\n",
      "DP 2 : 1.7577\n",
      "heuristic 2 : 1.7141666666666666\n",
      "DP: 1.7567987442016602\n",
      "tensor([0.0791, 0.0774, 0.0777, 0.6822, 0.0836])\n",
      "tensor([0.1052, 0.1071, 0.0919, 0.6959, 1.0000])\n",
      "tensor([0.1459, 0.7071, 0.1470, 1.0000, 1.0000])\n",
      "tensor([0.2796, 0.7204, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "    \n",
    "for order in stage:\n",
    "    producedata(order)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "    #running\n",
    "    for order1 in order1name:\n",
    "        print(\"Supervised Aim:\",order,order1)\n",
    "        for trainingnumber in numberofpeople:\n",
    "            # for mapping binary to payments before softmax\n",
    "            model = nn.Sequential(\n",
    "                nn.Linear(n, 100),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(100, 100),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(100, 100),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(100, n),\n",
    "            )\n",
    "            model.apply(init_weights)\n",
    "            # optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "            runningLossNN = []\n",
    "            runningLossCS = []\n",
    "            runningLossDP = []\n",
    "            runningLossHeuristic = []\n",
    "            #model=torch.load(\"save/pytorchNN=5dp1\");\n",
    "            #model.eval()\n",
    "            ##order1name=[\"costsharing\",\"dp\",\"heuristic\",\"random initializing\"]\n",
    "            for epoch in range(1, supervisionEpochs + 1):\n",
    "        #             print(\"distributionRatio\",distributionRatio)\n",
    "                if(order1==\"costsharing\"):\n",
    "                    supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "                elif(order1==\"dp\"):\n",
    "                    supervisionTrain(epoch, dpSupervisionRule)\n",
    "                elif(order1==\"heuristic\"):\n",
    "                    supervisionTrain(epoch, heuristicSupervisionRule)\n",
    "                elif(order1==\"random initializing\"):\n",
    "                    print(\"do nothing\");\n",
    "\n",
    "            test()\n",
    "\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                if(trainingnumber=='0'):\n",
    "                    train0(epoch)\n",
    "                if(trainingnumber=='1'):\n",
    "                    train1(epoch)\n",
    "                if(trainingnumber=='2'):\n",
    "                    train2(epoch)\n",
    "                test()\n",
    "            losslistname.append(order+\" \"+order1+\" choose people:\"+trainingnumber);\n",
    "            losslist.append(losslisttemp);\n",
    "            losslisttemp=[];\n",
    "            savepath=\"save/pytorchNN=5all-phoenix\"+order+str(order1)+trainingnumber\n",
    "            torch.save(model, savepath);\n",
    "            print(\"end\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colorlist=[\"#D2691E\",'#4169E1',\"#9ACD32\",\"#B22222\",\"#FF00FF\",\"#708090\"]\n",
    "for i in range(len(losslist)):\n",
    "    plt.plot(losslist[i], '.-', color=colorlist[i], alpha=0.8, label=losslistname[i])\n",
    "\n",
    "# 显示标签，如果不加这句，即使加了label='一些数字'的参数，最终还是不会显示标签\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('trianing times')\n",
    "plt.ylabel('delay')\n",
    "plt.title(\"n=\"+str(n))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
