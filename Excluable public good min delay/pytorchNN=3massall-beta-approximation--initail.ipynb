{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3540388371733616\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from random import randint\n",
    "import random\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "from pylab import *\n",
    "from scipy.stats import beta\n",
    "import math\n",
    "n = 3\n",
    "epochs = 6\n",
    "supervisionEpochs = 10\n",
    "lr = 0.0005\n",
    "log_interval = 20\n",
    "trainSize = 60000#100000\n",
    "percentage_train_test= 0.25\n",
    "penaltyLambda = 10\n",
    "doublePeakHighMean = 0.9\n",
    "doublePeakLowMean = 0.1\n",
    "doublePeakStd = 0.1\n",
    "uniformlow=0\n",
    "uniformhigh=1.0\n",
    "normalloc = 0.2\n",
    "normalscale = 0.1\n",
    "\n",
    "cauchyloc = 1/n\n",
    "cauchyscalen = 0.004\n",
    "\n",
    "exponentialhigh = 15 #Symbol(\"b\", real=True)\n",
    "exponentiallow  = 15 #Symbol(\"a\", real=True)\n",
    "\n",
    "\n",
    "beta_a = 0.1\n",
    "beta_b  = 0.1\n",
    "kumaraswamy_a = beta_a \n",
    "kumaraswamy_b = (1.0+(beta_a-1.0)*math.pow( (beta_a+beta_b-2.0)/(beta_a-1.0), beta_a) )/beta_a \n",
    "print(kumaraswamy_b)\n",
    "\n",
    "independentnormalloc1=[(float(ii)+1)/(2*n+1) for ii in range(n,0,-1)]\n",
    "independentnormalscale1=[0.05 for ii in range(n)]\n",
    "\n",
    "independentnormalloc2=[(float(ii)+1)/(2*n+1) for ii in range(1,n+1,1)]\n",
    "independentnormalscale2=[0.05 for ii in range(n)]\n",
    "stage=[\"twopeak\"]\n",
    "# \"twopeak\",\"normal\",\"uniform\",\"independent1\",\"independent2\",\"cauchy\",\"beta\",\"U-exponential\",\"arcsine\"\n",
    "order1name=[\"random initializing1\",\"random initializing2\",\"random initializing3\"]\n",
    "# \"costsharing\",\"dp\",\"heuristic\",\"random initializing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio1 = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase1 = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "d3 = D.normal.Normal(normalloc, normalscale)\n",
    "distributionRatio3 = d3.cdf(1) - d3.cdf(0)\n",
    "distributionBase3 = d3.cdf(0)\n",
    "\n",
    "d4 = D.uniform.Uniform(uniformlow,uniformhigh)\n",
    "distributionRatio4 = d4.cdf(1) - d4.cdf(0)\n",
    "distributionBase4 = d4.cdf(0)\n",
    "\n",
    "d5 = [D.normal.Normal(independentnormalloc1[ii], independentnormalscale1[ii]) for ii in range(n)]\n",
    "d6 = [D.normal.Normal(independentnormalloc2[ii], independentnormalscale2[ii]) for ii in range(n)]\n",
    "\n",
    "d7 = D.cauchy.Cauchy(cauchyloc,cauchyscalen)\n",
    "\n",
    "d81 = D.exponential.Exponential(exponentiallow)\n",
    "d82 = D.exponential.Exponential(exponentialhigh)\n",
    "\n",
    "d9 = D.beta.Beta(beta_a,beta_b)\n",
    "#sample_d9=d9.rsample(torch.Size([100000]))\n",
    "\n",
    "#d9_sample=np.linspace(0.0001, 0.9999, 10000)\n",
    "#d9_pdf=torch.exp(d9.log_prob(torch.tensor(d9_sample,dtype=torch.float32)))\n",
    "#d9_delta=d9_sample[1]-d9_sample[0]\n",
    "#d9_sum_pdf=torch.sum(d9_pdf*d9_delta)\n",
    "\n",
    "d10 = D.beta.Beta(0.5,0.5)\n",
    "\n",
    "def cdf(x,y, i=None):\n",
    "    if(y==\"twopeak\"):\n",
    "        return (d1.cdf(x) + d2.cdf(x) - distributionBase1) / 2 / distributionRatio1\n",
    "    elif(y==\"normal\"):\n",
    "        return (d3.cdf(x)-distributionBase3)/distributionRatio3;\n",
    "    elif(y==\"uniform\"):\n",
    "        return (d4.cdf(x)-distributionBase4)/distributionRatio4;\n",
    "    elif(y==\"independent1\"):\n",
    "        return d5[i].cdf(x);\n",
    "    elif(y==\"independent2\"):\n",
    "        return d6[i].cdf(x);\n",
    "    elif(y==\"cauchy\"):\n",
    "        return d7.cdf(x);\n",
    "    elif(y==\"beta\"):\n",
    "#         sum_cdf=0.0;\n",
    "#         if(x<0.0001):\n",
    "#             x=0.00011;\n",
    "#         if(x>0.9999):\n",
    "#             x=0.99989;\n",
    "#         for i in range(len(d9_pdf)):\n",
    "#             if(d9_sample[i]<x):\n",
    "#                 sum_cdf+=d9_pdf[i]*d9_delta;\n",
    "#             else:\n",
    "#                 sum_cdf+=(d9_pdf[i]+d9_pdf[i-1])/ 2 *(x-d9_sample[i-1])\n",
    "#                 break;\n",
    "#         return sum_cdf/d9_sum_pdf\n",
    "#         cdf_v=torch.sum((sample_d9<(x)), dtype=torch.float32)/100000\n",
    "#         return cdf_v\n",
    "#    F(x|a,b)=1–(1–x^a)^b\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(x,kumaraswamy_a),kumaraswamy_b);\n",
    "        except:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(torch.tensor(x,dtype=torch.float32),kumaraswamy_a),kumaraswamy_b);\n",
    "    elif(y==\"arcsine\"):\n",
    "        #\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            res=2.0/math.pi * torch.asin(torch.sqrt(x))\n",
    "            #print(x)\n",
    "            return res# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "        except:\n",
    "            return 2.0/math.pi * torch.asin(torch.sqrt(torch.tensor(x,dtype=torch.float32)))# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "    elif(y==\"U-exponential\"):\n",
    "        return (d81.cdf(x) + (1.0 - d82.cdf(1.0-x)))  / 2 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"beta_a\",beta_a, \"beta_b\",beta_b)\n",
    "# print(\"kumaraswamy_a\",kumaraswamy_a, \"kumaraswamy_b\",kumaraswamy_b)\n",
    "# samples1 = np.random.uniform(\n",
    "#     uniformlow, uniformhigh, size=(trainSize, n)\n",
    "# )\n",
    "# for i in range(trainSize):\n",
    "#     for j in range(n):\n",
    "#         samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "#         while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "#             samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "\n",
    "# samplesJoint = samples1\n",
    "# tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "\n",
    "# dpPrecision=100\n",
    "# # howManyPpl left, money left, yes already\n",
    "# dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "# decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "# # ppl = 0 left\n",
    "# for yes in range(n + 1):\n",
    "#     for money in range(dpPrecision + 1):\n",
    "#         if money == 0:\n",
    "#             dp[0, 0, yes] = 0\n",
    "#         else:\n",
    "#             dp[0, money, yes] = yes#+ 1.0\n",
    "# order=\"beta\"\n",
    "# for ppl in range(1,  n + 1):\n",
    "#     for yes in range(n + 1):\n",
    "#         for money in range(dpPrecision + 1):\n",
    "#             minSoFar = 1000000\n",
    "#             for offerIndex in range(money + 1):\n",
    "#                 offer = float(offerIndex) / dpPrecision\n",
    "#                 if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "#                     res = (1 - cdf(offer,order)) * dp[\n",
    "#                     ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "#                     ] + cdf(offer,order) * (1.0 + dp[ppl - 1, money, yes])\n",
    "#                 else:\n",
    "#                     res = (1 - cdf(offer,order,n-ppl)) * dp[\n",
    "#                     ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "#                     ] + cdf(offer,order,n-ppl) * (1.0 + dp[ppl - 1, money, yes])\n",
    "#                 if minSoFar > res.item():\n",
    "#                     minSoFar = res.item()\n",
    "#                     decision[ppl, money, yes] = offerIndex\n",
    "#             dp[ppl, money, yes] = minSoFar\n",
    "\n",
    "# print(\"dp\",dp[n, dpPrecision, 0])\n",
    "\n",
    "# def plan_dp(temp):\n",
    "#     #print(temp)\n",
    "#     remain=dpPrecision\n",
    "#     yes=0;\n",
    "#     ans =0;\n",
    "#     o_list=[];\n",
    "#     remain_list=[];\n",
    "#     for ppl in range(n,0,-1):\n",
    "#         o=decision[ppl, remain, yes]\n",
    "#         #print(o,remain)\n",
    "#         o_list.append(o)\n",
    "#         remain_list.append(remain);\n",
    "#         if(o<temp[n-ppl]):\n",
    "#             remain-=int(o);\n",
    "#             yes+=1;\n",
    "#         elif (remain>0):\n",
    "#             ans+=1;\n",
    "#     if(remain<=0):\n",
    "#         return ans,o_list;\n",
    "#     else:\n",
    "#         return n,o_list;\n",
    "# ans_list=[];\n",
    "# for i in range(10000):\n",
    "#     temp=samplesJoint[i]*dpPrecision\n",
    "#     #print(temp)\n",
    "#     ans_list.append(plan_dp(temp)[0]);\n",
    "#     #print(\"\\n\",temp)\n",
    "#     #print(plan_dp(temp))\n",
    "# print(sum(ans_list)/len(ans_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    if torch.sum(bits).item() == 0:\n",
    "        return torch.ones(n)\n",
    "    bits     = bits.type(torch.float32)\n",
    "    negBits  = torch.ones(n) - bits\n",
    "    payments = model(bits)\n",
    "    payments = payments - 1000 * negBits\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    payments = payments + negBits\n",
    "    return payments\n",
    "\n",
    "def tpToBits(tp, bits=torch.ones(n).type(torch.uint8)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = (tp >= payments).type(torch.uint8)\n",
    "    if torch.equal(newBits, bits):\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits(tp, newBits)#bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    return n - torch.sum(tpToBits(tp).type(torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    global  dp,decision\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money, yes]\n",
    "        offer = float(offerIndex) / dpPrecision\n",
    "        if tp[i] >= offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits[i] = 0\n",
    "            payments[i] = 1\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "#     print()\n",
    "#     print(tp)\n",
    "#     print(bits)\n",
    "#     print(payments)\n",
    "#     print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def heuristicSupervisionRule(tp):\n",
    "    global  dp_H,decision_H\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    tempold=-1;\n",
    "    for turns in range(n,0,-1):\n",
    "        money = dpPrecision\n",
    "        j=0\n",
    "        tempo=sum(bits)\n",
    "        #print(\"bits\",tempo)\n",
    "        for i in range(n):\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            \n",
    "            offerIndex = decision_H[tempo ,tempo - i, money]\n",
    "            offer = float(offerIndex) / dpPrecision\n",
    "            while(j<n):\n",
    "                if(bits[j]!=0):\n",
    "                    break;\n",
    "                j+=1;\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            if tp[j] >= offer:\n",
    "                #print(money,j,tp[j],offer)\n",
    "                money -= offerIndex\n",
    "                bits[j] = 1\n",
    "                payments[j] = offer\n",
    "            else:\n",
    "                bits[j] = 0;\n",
    "                payments[j] = 1.0;\n",
    "            j+=1;\n",
    "        #print(\"money\",money)\n",
    "        if(money==0 and tempold==tempo):\n",
    "            break;\n",
    "        tempold=tempo;\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    for k in range(n, -1, -1):\n",
    "        if k == 0:\n",
    "            break\n",
    "        bits = [1 if tp[ii] >= 1.0 / k else 0 for ii in range(n)]\n",
    "        if sum(bits) == k:\n",
    "            break\n",
    "    if k == 0:\n",
    "        payments = [1 for ii in range(n)]\n",
    "    else:\n",
    "        payments = [1.0 / k if bits[ii] == 1 else 1 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    return float(n - torch.sum(costSharingSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def dpDelay(tp):\n",
    "    return float(n - torch.sum(dpSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def heuristicDelay(tp):\n",
    "    return float(n - torch.sum(heuristicSupervisionRule(tp)[0]).item())\n",
    "\n",
    "\n",
    "# templire=0;\n",
    "# num=1\n",
    "# for i in range(num):\n",
    "#     temp=torch.tensor(samplesJoint[i])\n",
    "#     #print(temp)\n",
    "#     #print(temp);\n",
    "#     #print(heuristicSupervisionRule(temp))\n",
    "#     #print(dpSupervisionRule(temp))\n",
    "#     #print(dpDelay(temp))\n",
    "#     res=dpDelay(temp)\n",
    "#     templire+=res\n",
    "#     #print(\"delay\",res)\n",
    "#     #print()\n",
    "# print(templire/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordAndReport(name, source, loss):\n",
    "    source.append(loss)\n",
    "    realLength = len(source)\n",
    "    #print(f\"{name} ({realLength}): {loss}\")\n",
    "    print(name,realLength,\":\" ,loss)\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    \n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda /100000\n",
    "        \n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "losslist=[];\n",
    "losslistname=[];\n",
    "losslisttemp=[];\n",
    "def test_batch():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lenLoss= 0\n",
    "        nnLoss = 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "            for tp in tp_batch:\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        nnLoss/=lenLoss\n",
    "    return nnLoss\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                delay1 = tpToTotalDelay(tp1)\n",
    "                delay0 = tpToTotalDelay(tp0)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                    loss = loss + (1 - cdf(offer,order)) * delay1 + cdf(offer,order) * delay0\n",
    "                else:\n",
    "                    loss = loss + (1 - cdf(offer,order,i)) * delay1 + cdf(offer,order,i) * delay0\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "    \n",
    "allBits = [torch.tensor(bits, dtype=torch.int16) for bits in itertools.product([0, 1], repeat=n)]\n",
    "\n",
    "def test():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        costSharingLoss = 0\n",
    "        dpLoss = 0\n",
    "        nnLoss = 0\n",
    "        heuristicLoss=0\n",
    "        lenLoss= 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "                heuristicLoss+= heuristicDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        costSharingLoss /= lenLoss\n",
    "        dpLoss /= lenLoss\n",
    "        nnLoss /= lenLoss\n",
    "        heuristicLoss /= lenLoss\n",
    "        #print(lenLoss)\n",
    "        losslisttemp.append(nnLoss);\n",
    "        recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "        recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "        recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "        recordAndReport(\"heuristic\", runningLossHeuristic, heuristicLoss)\n",
    "        print(\"DP:\",dp[n, dpPrecision,0])\n",
    "        #for i in range(n, 0, -1):\n",
    "        #    print(\"Heuristic:\",i,5*(1-dp_H[i, i, dpPrecision]))\n",
    "        for i in range(n, 0, -1):\n",
    "            print(\n",
    "                    tpToPayments(\n",
    "                            torch.tensor([0 if ii >= i else 1 for ii in range(n)], dtype=torch.float32)\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "dpPrecision = 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def producedata(order):\n",
    "    global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "    dpPrecision = 100 \n",
    "    if(order==\"twopeak\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = np.random.normal(\n",
    "            loc=doublePeakLowMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=doublePeakLowMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samples2 = np.random.normal(\n",
    "            loc=doublePeakHighMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = np.random.normal(\n",
    "                        loc=doublePeakHighMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "    elif(order==\"normal\"):\n",
    "        print(\"loc\",normalloc, \"scale\",normalscale)\n",
    "        samples1 = np.random.normal(\n",
    "            loc=normalloc, scale=normalscale, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=normalloc, scale=normalscale\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"uniform\"):  \n",
    "        print(\"uniformlow\",uniformlow, \"uniformhigh\",uniformhigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        uniformlow, uniformhigh\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent1\"):\n",
    "        print(\"loc\",independentnormalloc1,\"scale\",independentnormalscale1)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent2\"):\n",
    "        print(\"loc\",independentnormalloc2, \"scale\",independentnormalscale2)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"cauchy\"):\n",
    "        print(\"cauchyloc\",cauchyloc, \"cauchyscale\",cauchyscalen)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"beta\"):\n",
    "        print(\"beta_a\",beta_a, \"beta_b\",beta_b)\n",
    "        print(\"kumaraswamy_a\",kumaraswamy_a, \"kumaraswamy_b\",kumaraswamy_b)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"arcsine\"):\n",
    "        print(\"betalow\",0.5, \"betahigh\",0.5)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"U-exponential\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = d81.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d81.rsample(torch.Size([1])).numpy()\n",
    "                    \n",
    "        samples2 = d82.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        \n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = d82.rsample(torch.Size([1])).numpy()\n",
    "        samples2 = 1.0 - samples2\n",
    "        samplesJoint = signals * samples1 - (signals - 1.0) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "        \n",
    "    plt.hist(samplesJoint,bins=500)\n",
    "    plt.show()\n",
    "    \n",
    "    tp_dataset = TensorDataset(tp_tensor[: int(trainSize * percentage_train_test)])\n",
    "    tp_dataset_testing = TensorDataset(tp_tensor[int(trainSize * (1.0-percentage_train_test)) :])\n",
    "    tp_dataloader = DataLoader(tp_dataset, batch_size=128, shuffle=True)\n",
    "    tp_dataloader_testing = DataLoader(tp_dataset_testing, batch_size=256, shuffle=False)\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "    decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for yes in range(n + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp[0, 0, yes] = 0\n",
    "            else:\n",
    "                dp[0, money, yes] = yes# + 1.0\n",
    "    for ppl in range(1,  n + 1):\n",
    "        for yes in range(n + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                minSoFar = 1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1 - cdf(offer,order)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order) * (1 + dp[ppl - 1, money, yes])\n",
    "                    else:\n",
    "                        res = (1 - cdf(offer,order,n-ppl)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order,n-ppl) * (1 + dp[ppl - 1, money, yes])\n",
    "                    if minSoFar > res.item():\n",
    "                        minSoFar = res.item()\n",
    "                        decision[ppl, money, yes] = offerIndex\n",
    "                dp[ppl, money, yes] = minSoFar\n",
    "    \n",
    "    print(\"dp\",dp[n, dpPrecision, 0])\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp_H = np.zeros([n + 1 , n + 1, dpPrecision + 1])\n",
    "    decision_H = np.zeros([n + 1 , n + 1, dpPrecision + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for i in range(1,n+1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp_H[i, 0, 0] = 1\n",
    "            else:\n",
    "                offer = money / dpPrecision\n",
    "                dp_H[i, 0, money] = 0#cdf(offer)# + 1.0\n",
    "    for i in range(1,n+1):\n",
    "        for ppl in range(1, i + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                maxSoFar = -1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1-cdf(offer,order)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    else:\n",
    "                        res = (1-cdf(offer,order,ppl-1)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    if maxSoFar < res.item():\n",
    "                        maxSoFar = res.item()\n",
    "                        decision_H[i, ppl, money] = offerIndex\n",
    "                dp_H[i, ppl, money] = maxSoFar\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_a 0.1 beta_b 0.1\n",
      "kumaraswamy_a 0.1 kumaraswamy_b 0.3540388371733616\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dp 2.2055420875549316\n",
      "Supervised Aim: beta dp\n",
      "Train Epoch: 1 [0/15000 (0%)]\tLoss: 0.023199\n",
      "Train Epoch: 1 [2560/15000 (17%)]\tLoss: 0.003848\n",
      "Train Epoch: 1 [5120/15000 (34%)]\tLoss: 0.001179\n",
      "Train Epoch: 1 [7680/15000 (51%)]\tLoss: 0.000566\n",
      "Train Epoch: 1 [10240/15000 (68%)]\tLoss: 0.000230\n",
      "Train Epoch: 1 [12800/15000 (85%)]\tLoss: 0.000107\n",
      "Train Epoch: 2 [0/15000 (0%)]\tLoss: 0.000076\n",
      "Train Epoch: 2 [2560/15000 (17%)]\tLoss: 0.000076\n",
      "Train Epoch: 2 [5120/15000 (34%)]\tLoss: 0.000050\n",
      "Train Epoch: 2 [7680/15000 (51%)]\tLoss: 0.000036\n",
      "Train Epoch: 2 [10240/15000 (68%)]\tLoss: 0.000032\n",
      "Train Epoch: 2 [12800/15000 (85%)]\tLoss: 0.000019\n",
      "Train Epoch: 3 [0/15000 (0%)]\tLoss: 0.000019\n",
      "Train Epoch: 3 [2560/15000 (17%)]\tLoss: 0.000014\n",
      "Train Epoch: 3 [5120/15000 (34%)]\tLoss: 0.000012\n",
      "Train Epoch: 3 [7680/15000 (51%)]\tLoss: 0.000008\n",
      "Train Epoch: 3 [10240/15000 (68%)]\tLoss: 0.000007\n",
      "Train Epoch: 3 [12800/15000 (85%)]\tLoss: 0.000008\n",
      "Train Epoch: 4 [0/15000 (0%)]\tLoss: 0.000005\n",
      "Train Epoch: 4 [2560/15000 (17%)]\tLoss: 0.000005\n",
      "Train Epoch: 4 [5120/15000 (34%)]\tLoss: 0.000004\n",
      "Train Epoch: 4 [7680/15000 (51%)]\tLoss: 0.000004\n",
      "Train Epoch: 4 [10240/15000 (68%)]\tLoss: 0.000003\n",
      "Train Epoch: 4 [12800/15000 (85%)]\tLoss: 0.000003\n",
      "Train Epoch: 5 [0/15000 (0%)]\tLoss: 0.000003\n",
      "Train Epoch: 5 [2560/15000 (17%)]\tLoss: 0.000002\n",
      "Train Epoch: 5 [5120/15000 (34%)]\tLoss: 0.000002\n",
      "Train Epoch: 5 [7680/15000 (51%)]\tLoss: 0.000002\n",
      "Train Epoch: 5 [10240/15000 (68%)]\tLoss: 0.000002\n",
      "Train Epoch: 5 [12800/15000 (85%)]\tLoss: 0.000001\n",
      "Train Epoch: 6 [0/15000 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 6 [2560/15000 (17%)]\tLoss: 0.000001\n",
      "Train Epoch: 6 [5120/15000 (34%)]\tLoss: 0.000002\n",
      "Train Epoch: 6 [7680/15000 (51%)]\tLoss: 0.000001\n",
      "Train Epoch: 6 [10240/15000 (68%)]\tLoss: 0.000001\n",
      "Train Epoch: 6 [12800/15000 (85%)]\tLoss: 0.000002\n",
      "Train Epoch: 7 [0/15000 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 7 [2560/15000 (17%)]\tLoss: 0.000001\n",
      "Train Epoch: 7 [5120/15000 (34%)]\tLoss: 0.000006\n",
      "Train Epoch: 7 [7680/15000 (51%)]\tLoss: 0.000005\n",
      "Train Epoch: 7 [10240/15000 (68%)]\tLoss: 0.000001\n",
      "Train Epoch: 7 [12800/15000 (85%)]\tLoss: 0.000001\n",
      "Train Epoch: 8 [0/15000 (0%)]\tLoss: 0.000001\n",
      "Train Epoch: 8 [2560/15000 (17%)]\tLoss: 0.000001\n",
      "Train Epoch: 8 [5120/15000 (34%)]\tLoss: 0.000003\n",
      "Train Epoch: 8 [7680/15000 (51%)]\tLoss: 0.000002\n",
      "Train Epoch: 8 [10240/15000 (68%)]\tLoss: 0.000006\n",
      "Train Epoch: 8 [12800/15000 (85%)]\tLoss: 0.000006\n",
      "Train Epoch: 9 [0/15000 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 9 [2560/15000 (17%)]\tLoss: 0.000001\n",
      "Train Epoch: 9 [5120/15000 (34%)]\tLoss: 0.000001\n",
      "Train Epoch: 9 [7680/15000 (51%)]\tLoss: 0.000001\n",
      "Train Epoch: 9 [10240/15000 (68%)]\tLoss: 0.000001\n",
      "Train Epoch: 9 [12800/15000 (85%)]\tLoss: 0.000001\n",
      "Train Epoch: 10 [0/15000 (0%)]\tLoss: 0.000001\n",
      "Train Epoch: 10 [2560/15000 (17%)]\tLoss: 0.000001\n",
      "Train Epoch: 10 [5120/15000 (34%)]\tLoss: 0.000001\n",
      "Train Epoch: 10 [7680/15000 (51%)]\tLoss: 0.000001\n",
      "Train Epoch: 10 [10240/15000 (68%)]\tLoss: 0.000001\n",
      "Train Epoch: 10 [12800/15000 (85%)]\tLoss: 0.000001\n",
      "NN 1 : tensor(1.7586)\n",
      "CS 1 : 1.7839333333333334\n",
      "DP 1 : 1.7580666666666667\n",
      "heuristic 1 : 1.7560666666666667\n",
      "DP: 2.2055420875549316\n",
      "tensor([0.4988, 0.4988, 0.0025])\n",
      "tensor([0.5001, 0.4999, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 1 [0/15000 (0%)]\tLoss: 1.851133 testing loss: tensor(1.7553)\n",
      "Train Epoch: 1 [2560/15000 (17%)]\tLoss: 1.958935 testing loss: tensor(1.7499)\n",
      "Train Epoch: 1 [5120/15000 (34%)]\tLoss: 1.948893 testing loss: tensor(1.7426)\n",
      "Train Epoch: 1 [7680/15000 (51%)]\tLoss: 1.954785 testing loss: tensor(1.7290)\n",
      "Train Epoch: 1 [10240/15000 (68%)]\tLoss: 1.935464 testing loss: tensor(1.7305)\n",
      "Train Epoch: 1 [12800/15000 (85%)]\tLoss: 1.854394 testing loss: tensor(1.7129)\n",
      "penalty: 0.004109412431716919\n",
      "NN 2 : tensor(1.7171)\n",
      "CS 2 : 1.7839333333333334\n",
      "DP 2 : 1.7580666666666667\n",
      "heuristic 2 : 1.7560666666666667\n",
      "DP: 2.2055420875549316\n",
      "tensor([4.1884e-01, 5.8116e-01, 5.3269e-08])\n",
      "tensor([0.4214, 0.5786, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 2 [0/15000 (0%)]\tLoss: 1.887426 testing loss: tensor(1.7136)\n",
      "Train Epoch: 2 [2560/15000 (17%)]\tLoss: 1.969863 testing loss: tensor(1.7135)\n",
      "Train Epoch: 2 [5120/15000 (34%)]\tLoss: 1.870174 testing loss: tensor(1.7121)\n",
      "Train Epoch: 2 [7680/15000 (51%)]\tLoss: 1.700703 testing loss: tensor(1.7130)\n",
      "Train Epoch: 2 [10240/15000 (68%)]\tLoss: 1.944876 testing loss: tensor(1.7363)\n",
      "Train Epoch: 2 [12800/15000 (85%)]\tLoss: 1.781399 testing loss: tensor(1.7121)\n",
      "penalty: 0.001110672950744629\n",
      "NN 3 : tensor(1.7111)\n",
      "CS 3 : 1.7839333333333334\n",
      "DP 3 : 1.7580666666666667\n",
      "heuristic 3 : 1.7560666666666667\n",
      "DP: 2.2055420875549316\n",
      "tensor([5.0933e-01, 4.9067e-01, 6.0255e-08])\n",
      "tensor([0.5136, 0.4864, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 3 [0/15000 (0%)]\tLoss: 1.872293 testing loss: tensor(1.7113)\n",
      "Train Epoch: 3 [2560/15000 (17%)]\tLoss: 1.864159 testing loss: tensor(1.7130)\n",
      "Train Epoch: 3 [5120/15000 (34%)]\tLoss: 1.999060 testing loss: tensor(1.7130)\n",
      "Train Epoch: 3 [7680/15000 (51%)]\tLoss: 1.770209 testing loss: tensor(1.7119)\n",
      "Train Epoch: 3 [10240/15000 (68%)]\tLoss: 1.806972 testing loss: tensor(1.7141)\n",
      "Train Epoch: 3 [12800/15000 (85%)]\tLoss: 1.885996 testing loss: tensor(1.7145)\n",
      "penalty: 0.0014654994010925293\n",
      "NN 4 : tensor(1.7128)\n",
      "CS 4 : 1.7839333333333334\n",
      "DP 4 : 1.7580666666666667\n",
      "heuristic 4 : 1.7560666666666667\n",
      "DP: 2.2055420875549316\n",
      "tensor([4.9981e-01, 5.0019e-01, 7.7857e-08])\n",
      "tensor([0.5024, 0.4976, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 4 [0/15000 (0%)]\tLoss: 1.965125 testing loss: tensor(1.7124)\n",
      "Train Epoch: 4 [2560/15000 (17%)]\tLoss: 1.837762 testing loss: tensor(1.7143)\n",
      "Train Epoch: 4 [5120/15000 (34%)]\tLoss: 1.954530 testing loss: tensor(1.7148)\n",
      "Train Epoch: 4 [7680/15000 (51%)]\tLoss: 1.905614 testing loss: tensor(1.7172)\n",
      "Train Epoch: 4 [10240/15000 (68%)]\tLoss: 1.947685 testing loss: tensor(1.7167)\n",
      "Train Epoch: 4 [12800/15000 (85%)]\tLoss: 1.919983 testing loss: tensor(1.7142)\n",
      "penalty: 0.0028850436210632324\n",
      "NN 5 : tensor(1.7148)\n",
      "CS 5 : 1.7839333333333334\n",
      "DP 5 : 1.7580666666666667\n",
      "heuristic 5 : 1.7560666666666667\n",
      "DP: 2.2055420875549316\n",
      "tensor([4.9230e-01, 5.0770e-01, 4.1848e-08])\n",
      "tensor([0.4912, 0.5088, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 5 [0/15000 (0%)]\tLoss: 1.831339 testing loss: tensor(1.7152)\n",
      "Train Epoch: 5 [2560/15000 (17%)]\tLoss: 1.913467 testing loss: tensor(1.7118)\n",
      "Train Epoch: 5 [5120/15000 (34%)]\tLoss: 1.939348 testing loss: tensor(1.7127)\n",
      "Train Epoch: 5 [7680/15000 (51%)]\tLoss: 1.940356 testing loss: tensor(1.7106)\n",
      "Train Epoch: 5 [10240/15000 (68%)]\tLoss: 1.829733 testing loss: tensor(1.7141)\n",
      "Train Epoch: 5 [12800/15000 (85%)]\tLoss: 1.839294 testing loss: tensor(1.7129)\n",
      "penalty: 0.0008533000946044922\n",
      "NN 6 : tensor(1.7144)\n",
      "CS 6 : 1.7839333333333334\n",
      "DP 6 : 1.7580666666666667\n",
      "heuristic 6 : 1.7560666666666667\n",
      "DP: 2.2055420875549316\n",
      "tensor([4.6336e-01, 5.3664e-01, 5.0375e-08])\n",
      "tensor([0.4629, 0.5371, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 6 [0/15000 (0%)]\tLoss: 1.988562 testing loss: tensor(1.7132)\n",
      "Train Epoch: 6 [2560/15000 (17%)]\tLoss: 1.921402 testing loss: tensor(1.7115)\n",
      "Train Epoch: 6 [5120/15000 (34%)]\tLoss: 1.867226 testing loss: tensor(1.7124)\n",
      "Train Epoch: 6 [7680/15000 (51%)]\tLoss: 1.803809 testing loss: tensor(1.7141)\n",
      "Train Epoch: 6 [10240/15000 (68%)]\tLoss: 1.939749 testing loss: tensor(1.7111)\n",
      "Train Epoch: 6 [12800/15000 (85%)]\tLoss: 2.015837 testing loss: tensor(1.7125)\n",
      "penalty: 0.0013490021228790283\n",
      "NN 7 : tensor(1.7133)\n",
      "CS 7 : 1.7839333333333334\n",
      "DP 7 : 1.7580666666666667\n",
      "heuristic 7 : 1.7560666666666667\n",
      "DP: 2.2055420875549316\n",
      "tensor([5.7205e-01, 4.2795e-01, 5.5483e-08])\n",
      "tensor([0.5724, 0.4276, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: beta random initializing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "do nothing\n",
      "NN 1 : tensor(1.7832)\n",
      "CS 1 : 1.7839333333333334\n",
      "DP 1 : 1.7580666666666667\n",
      "heuristic 1 : 1.7560666666666667\n",
      "DP: 2.2055420875549316\n",
      "tensor([0.2822, 0.4284, 0.2894])\n",
      "tensor([0.4593, 0.5407, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 1 [0/15000 (0%)]\tLoss: 1.855579 testing loss: tensor(1.7835)\n",
      "Train Epoch: 1 [2560/15000 (17%)]\tLoss: 1.980838 testing loss: tensor(1.7805)\n",
      "Train Epoch: 1 [5120/15000 (34%)]\tLoss: 1.931752 testing loss: tensor(1.7817)\n",
      "Train Epoch: 1 [7680/15000 (51%)]\tLoss: 1.943973 testing loss: tensor(1.7829)\n",
      "Train Epoch: 1 [10240/15000 (68%)]\tLoss: 1.934012 testing loss: tensor(1.7825)\n",
      "Train Epoch: 1 [12800/15000 (85%)]\tLoss: 2.063055 testing loss: tensor(1.7824)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.7835)\n",
      "CS 2 : 1.7839333333333334\n",
      "DP 2 : 1.7580666666666667\n",
      "heuristic 2 : 1.7560666666666667\n",
      "DP: 2.2055420875549316\n",
      "tensor([0.2741, 0.3608, 0.3651])\n",
      "tensor([0.4802, 0.5198, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 2 [0/15000 (0%)]\tLoss: 1.966955 testing loss: tensor(1.7833)\n",
      "Train Epoch: 2 [2560/15000 (17%)]\tLoss: 1.829310 testing loss: tensor(1.7838)\n",
      "Train Epoch: 2 [5120/15000 (34%)]\tLoss: 1.933887 testing loss: tensor(1.7825)\n",
      "Train Epoch: 2 [7680/15000 (51%)]\tLoss: 1.937468 testing loss: tensor(1.7834)\n",
      "Train Epoch: 2 [10240/15000 (68%)]\tLoss: 1.886829 testing loss: tensor(1.7821)\n",
      "Train Epoch: 2 [12800/15000 (85%)]\tLoss: 1.936097 testing loss: tensor(1.7820)\n",
      "penalty: 0.0\n",
      "NN 3 : tensor(1.7807)\n",
      "CS 3 : 1.7839333333333334\n",
      "DP 3 : 1.7580666666666667\n",
      "heuristic 3 : 1.7560666666666667\n",
      "DP: 2.2055420875549316\n",
      "tensor([0.3239, 0.3806, 0.2955])\n",
      "tensor([0.5104, 0.4896, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 3 [0/15000 (0%)]\tLoss: 1.877249 testing loss: tensor(1.7799)\n",
      "Train Epoch: 3 [2560/15000 (17%)]\tLoss: 1.957270 testing loss: tensor(1.7817)\n",
      "Train Epoch: 3 [5120/15000 (34%)]\tLoss: 1.998880 testing loss: tensor(1.7833)\n",
      "Train Epoch: 3 [7680/15000 (51%)]\tLoss: 1.889276 testing loss: tensor(1.7833)\n",
      "Train Epoch: 3 [10240/15000 (68%)]\tLoss: 1.913806 testing loss: tensor(1.7815)\n",
      "Train Epoch: 3 [12800/15000 (85%)]\tLoss: 1.980672 testing loss: tensor(1.7815)\n",
      "penalty: 0.0\n",
      "NN 4 : tensor(1.7822)\n",
      "CS 4 : 1.7839333333333334\n",
      "DP 4 : 1.7580666666666667\n",
      "heuristic 4 : 1.7560666666666667\n",
      "DP: 2.2055420875549316\n",
      "tensor([0.5251, 0.3409, 0.1340])\n",
      "tensor([0.6273, 0.3727, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 4 [0/15000 (0%)]\tLoss: 2.038692 testing loss: tensor(1.7819)\n",
      "Train Epoch: 4 [2560/15000 (17%)]\tLoss: 2.037361 testing loss: tensor(1.7808)\n",
      "Train Epoch: 4 [5120/15000 (34%)]\tLoss: 1.943030 testing loss: tensor(1.7766)\n",
      "Train Epoch: 4 [7680/15000 (51%)]\tLoss: 1.800735 testing loss: tensor(1.7786)\n",
      "Train Epoch: 4 [10240/15000 (68%)]\tLoss: 1.901770 testing loss: tensor(1.7681)\n",
      "Train Epoch: 4 [12800/15000 (85%)]\tLoss: 1.921378 testing loss: tensor(1.7547)\n",
      "penalty: 0.0019587278366088867\n",
      "NN 5 : tensor(1.7442)\n",
      "CS 5 : 1.7839333333333334\n",
      "DP 5 : 1.7580666666666667\n",
      "heuristic 5 : 1.7560666666666667\n",
      "DP: 2.2055420875549316\n",
      "tensor([6.0382e-01, 3.9587e-01, 3.1513e-04])\n",
      "tensor([0.6131, 0.3869, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 5 [0/15000 (0%)]\tLoss: 2.051570 testing loss: tensor(1.7432)\n",
      "Train Epoch: 5 [2560/15000 (17%)]\tLoss: 1.813105 testing loss: tensor(1.7329)\n",
      "Train Epoch: 5 [5120/15000 (34%)]\tLoss: 1.953452 testing loss: tensor(1.7226)\n",
      "Train Epoch: 5 [7680/15000 (51%)]\tLoss: 1.798513 testing loss: tensor(1.7172)\n",
      "Train Epoch: 5 [10240/15000 (68%)]\tLoss: 1.926298 testing loss: tensor(1.7147)\n",
      "Train Epoch: 5 [12800/15000 (85%)]\tLoss: 1.936068 testing loss: tensor(1.7127)\n",
      "penalty: 0.003377258777618408\n",
      "NN 6 : tensor(1.7113)\n",
      "CS 6 : 1.7839333333333334\n",
      "DP 6 : 1.7580666666666667\n",
      "heuristic 6 : 1.7560666666666667\n",
      "DP: 2.2055420875549316\n",
      "tensor([4.9457e-01, 5.0543e-01, 4.1932e-08])\n",
      "tensor([0.5075, 0.4925, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 6 [0/15000 (0%)]\tLoss: 1.793417 testing loss: tensor(1.7120)\n",
      "Train Epoch: 6 [2560/15000 (17%)]\tLoss: 1.860719 testing loss: tensor(1.7142)\n",
      "Train Epoch: 6 [5120/15000 (34%)]\tLoss: 1.794422 testing loss: tensor(1.7125)\n",
      "Train Epoch: 6 [7680/15000 (51%)]\tLoss: 1.856292 testing loss: tensor(1.7111)\n",
      "Train Epoch: 6 [10240/15000 (68%)]\tLoss: 1.900093 testing loss: tensor(1.7129)\n",
      "Train Epoch: 6 [12800/15000 (85%)]\tLoss: 1.853526 testing loss: tensor(1.7157)\n",
      "penalty: 0.011703342199325562\n",
      "NN 7 : tensor(1.7170)\n",
      "CS 7 : 1.7839333333333334\n",
      "DP 7 : 1.7580666666666667\n",
      "heuristic 7 : 1.7560666666666667\n",
      "DP: 2.2055420875549316\n",
      "tensor([4.3972e-01, 5.6028e-01, 7.9983e-08])\n",
      "tensor([0.4456, 0.5544, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: beta costsharing\n",
      "Train Epoch: 1 [0/15000 (0%)]\tLoss: 0.003321\n",
      "Train Epoch: 1 [2560/15000 (17%)]\tLoss: 0.000136\n",
      "Train Epoch: 1 [5120/15000 (34%)]\tLoss: 0.000021\n",
      "Train Epoch: 1 [7680/15000 (51%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [0/15000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [2560/15000 (17%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [5120/15000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "NN 1 : tensor(1.7839)\n",
      "CS 1 : 1.7839333333333334\n",
      "DP 1 : 1.7580666666666667\n",
      "heuristic 1 : 1.7560666666666667\n",
      "DP: 2.2055420875549316\n",
      "tensor([0.3333, 0.3333, 0.3333])\n",
      "tensor([0.5000, 0.5000, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 1 [0/15000 (0%)]\tLoss: 1.788111 testing loss: tensor(1.7826)\n",
      "Train Epoch: 1 [2560/15000 (17%)]\tLoss: 1.839800 testing loss: tensor(1.7849)\n",
      "Train Epoch: 1 [5120/15000 (34%)]\tLoss: 1.976783 testing loss: tensor(1.7827)\n",
      "Train Epoch: 1 [7680/15000 (51%)]\tLoss: 1.971273 testing loss: tensor(1.7839)\n",
      "Train Epoch: 1 [10240/15000 (68%)]\tLoss: 1.992494 testing loss: tensor(1.7837)\n",
      "Train Epoch: 1 [12800/15000 (85%)]\tLoss: 1.885889 testing loss: tensor(1.7828)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.7807)\n",
      "CS 2 : 1.7839333333333334\n",
      "DP 2 : 1.7580666666666667\n",
      "heuristic 2 : 1.7560666666666667\n",
      "DP: 2.2055420875549316\n",
      "tensor([0.2846, 0.3102, 0.4052])\n",
      "tensor([0.4651, 0.5349, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 2 [0/15000 (0%)]\tLoss: 1.936601 testing loss: tensor(1.7819)\n",
      "Train Epoch: 2 [2560/15000 (17%)]\tLoss: 1.881069 testing loss: tensor(1.7847)\n",
      "Train Epoch: 2 [5120/15000 (34%)]\tLoss: 2.012171 testing loss: tensor(1.7826)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [7680/15000 (51%)]\tLoss: 1.844549 testing loss: tensor(1.7833)\n",
      "Train Epoch: 2 [10240/15000 (68%)]\tLoss: 1.738719 testing loss: tensor(1.7823)\n",
      "Train Epoch: 2 [12800/15000 (85%)]\tLoss: 1.889918 testing loss: tensor(1.7836)\n",
      "penalty: 0.0\n",
      "NN 3 : tensor(1.7832)\n",
      "CS 3 : 1.7839333333333334\n",
      "DP 3 : 1.7580666666666667\n",
      "heuristic 3 : 1.7560666666666667\n",
      "DP: 2.2055420875549316\n",
      "tensor([0.3700, 0.2918, 0.3382])\n",
      "tensor([0.5316, 0.4684, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 3 [0/15000 (0%)]\tLoss: 1.882000 testing loss: tensor(1.7837)\n",
      "Train Epoch: 3 [2560/15000 (17%)]\tLoss: 1.943518 testing loss: tensor(1.7811)\n",
      "Train Epoch: 3 [5120/15000 (34%)]\tLoss: 1.936265 testing loss: tensor(1.7832)\n",
      "Train Epoch: 3 [7680/15000 (51%)]\tLoss: 1.933128 testing loss: tensor(1.7833)\n",
      "Train Epoch: 3 [10240/15000 (68%)]\tLoss: 1.871297 testing loss: tensor(1.7827)\n",
      "Train Epoch: 3 [12800/15000 (85%)]\tLoss: 1.891334 testing loss: tensor(1.7817)\n",
      "penalty: 0.0\n",
      "NN 4 : tensor(1.7822)\n",
      "CS 4 : 1.7839333333333334\n",
      "DP 4 : 1.7580666666666667\n",
      "heuristic 4 : 1.7560666666666667\n",
      "DP: 2.2055420875549316\n",
      "tensor([0.3510, 0.3033, 0.3456])\n",
      "tensor([0.5354, 0.4646, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 4 [0/15000 (0%)]\tLoss: 1.996145 testing loss: tensor(1.7819)\n",
      "Train Epoch: 4 [2560/15000 (17%)]\tLoss: 1.841400 testing loss: tensor(1.7824)\n",
      "Train Epoch: 4 [5120/15000 (34%)]\tLoss: 1.993562 testing loss: tensor(1.7835)\n",
      "Train Epoch: 4 [7680/15000 (51%)]\tLoss: 1.849560 testing loss: tensor(1.7842)\n",
      "Train Epoch: 4 [10240/15000 (68%)]\tLoss: 1.947485 testing loss: tensor(1.7812)\n",
      "Train Epoch: 4 [12800/15000 (85%)]\tLoss: 1.958377 testing loss: tensor(1.7809)\n",
      "penalty: 0.0\n",
      "NN 5 : tensor(1.7813)\n",
      "CS 5 : 1.7839333333333334\n",
      "DP 5 : 1.7580666666666667\n",
      "heuristic 5 : 1.7560666666666667\n",
      "DP: 2.2055420875549316\n",
      "tensor([0.3356, 0.3235, 0.3409])\n",
      "tensor([0.5053, 0.4947, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 5 [0/15000 (0%)]\tLoss: 1.933237 testing loss: tensor(1.7813)\n",
      "Train Epoch: 5 [2560/15000 (17%)]\tLoss: 1.907763 testing loss: tensor(1.7821)\n",
      "Train Epoch: 5 [5120/15000 (34%)]\tLoss: 1.933324 testing loss: tensor(1.7829)\n",
      "Train Epoch: 5 [7680/15000 (51%)]\tLoss: 2.070232 testing loss: tensor(1.7810)\n",
      "Train Epoch: 5 [10240/15000 (68%)]\tLoss: 1.935952 testing loss: tensor(1.7818)\n",
      "Train Epoch: 5 [12800/15000 (85%)]\tLoss: 2.029149 testing loss: tensor(1.7823)\n",
      "penalty: 0.0\n",
      "NN 6 : tensor(1.7821)\n",
      "CS 6 : 1.7839333333333334\n",
      "DP 6 : 1.7580666666666667\n",
      "heuristic 6 : 1.7560666666666667\n",
      "DP: 2.2055420875549316\n",
      "tensor([0.3454, 0.3177, 0.3368])\n",
      "tensor([0.5272, 0.4728, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "Train Epoch: 6 [0/15000 (0%)]\tLoss: 1.839014 testing loss: tensor(1.7822)\n",
      "Train Epoch: 6 [2560/15000 (17%)]\tLoss: 1.894041 testing loss: tensor(1.7829)\n",
      "Train Epoch: 6 [5120/15000 (34%)]\tLoss: 1.837160 testing loss: tensor(1.7818)\n",
      "Train Epoch: 6 [7680/15000 (51%)]\tLoss: 1.903392 testing loss: tensor(1.7821)\n",
      "Train Epoch: 6 [10240/15000 (68%)]\tLoss: 1.871676 testing loss: tensor(1.7819)\n",
      "Train Epoch: 6 [12800/15000 (85%)]\tLoss: 1.981914 testing loss: tensor(1.7823)\n",
      "penalty: 0.0\n",
      "NN 7 : tensor(1.7813)\n",
      "CS 7 : 1.7839333333333334\n",
      "DP 7 : 1.7580666666666667\n",
      "heuristic 7 : 1.7560666666666667\n",
      "DP: 2.2055420875549316\n",
      "tensor([0.3477, 0.3131, 0.3392])\n",
      "tensor([0.5295, 0.4705, 1.0000])\n",
      "tensor([1., 1., 1.])\n",
      "end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "        \n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.1)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "        \n",
    "def init_weights_xavier_uniform(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.1)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "        \n",
    "def init_weights_xavier_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.1)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "        \n",
    "        \n",
    "def init_weights_kaiming_uniform(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.kaiming_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.1)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "        \n",
    "        \n",
    "def init_weights_kaiming_normal_(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.1)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "\n",
    "\n",
    "        \n",
    "for order in stage:\n",
    "    producedata(order)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "    #running\n",
    "    for order1 in order1name:\n",
    "        print(\"Supervised Aim:\",order,order1)\n",
    "        \n",
    "        # for mapping binary to payments before softmax\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(n, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, n),\n",
    "        )\n",
    "        model.apply(init_weights)\n",
    "        # optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        runningLossNN = []\n",
    "        runningLossCS = []\n",
    "        runningLossDP = []\n",
    "        runningLossHeuristic = []\n",
    "        #model=torch.load(\"save/pytorchNN=5dp1\");\n",
    "        #model.eval()\n",
    "        ##order1name=[\"costsharing\",\"dp\",\"heuristic\",\"random initializing\"]\n",
    "        for epoch in range(1, supervisionEpochs + 1):\n",
    "#             print(\"distributionRatio\",distributionRatio)\n",
    "            if(order1==\"costsharing\"):\n",
    "                supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "            elif(order1==\"dp\"):\n",
    "                supervisionTrain(epoch, dpSupervisionRule)\n",
    "            elif(order1==\"heuristic\"):\n",
    "                supervisionTrain(epoch, heuristicSupervisionRule)\n",
    "            elif(order1==\"random initializing\"):\n",
    "                print(\"do nothing\");\n",
    "\n",
    "        test()\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(epoch)\n",
    "            test()\n",
    "        losslistname.append(order+\" \"+order1);\n",
    "        losslist.append(losslisttemp);\n",
    "        losslisttemp=[];\n",
    "        savepath=\"save/pytorchNN=5all-beta\"+order+str(order1)\n",
    "        torch.save(model, savepath);\n",
    "        print(\"end\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colorlist=[\"#D2691E\",'#4169E1',\"#9ACD32\",\"#B22222\",\"#FF00FF\",\"#708090\"]\n",
    "for i in range(len(losslist)):\n",
    "    plt.plot(losslist[i], 'ro-', color=colorlist[i], alpha=0.8, label=losslistname[i])\n",
    "\n",
    "# 显示标签，如果不加这句，即使加了label='一些数字'的参数，最终还是不会显示标签\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('trianing times')\n",
    "plt.ylabel('delay')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
