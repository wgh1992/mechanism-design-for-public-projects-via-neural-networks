{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40058530825361593\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from random import randint\n",
    "import random\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "from pylab import *\n",
    "from scipy.stats import beta\n",
    "import math\n",
    "n = 5\n",
    "epochs = 1\n",
    "supervisionEpochs = 1\n",
    "lr1 = 0.01\n",
    "lr2 = 0.0005\n",
    "log_interval = 5\n",
    "trainSize = 50000#100000\n",
    "percentage_train_test= 0.5\n",
    "penaltyLambda = 10\n",
    "doublePeakHighMean = 0.6\n",
    "doublePeakLowMean = 0.2\n",
    "doublePeakStd = 0.1\n",
    "uniformlow=0\n",
    "uniformhigh=1.0\n",
    "normalloc = 0.2\n",
    "normalscale = 0.1\n",
    "\n",
    "cauchyloc = 1/n\n",
    "cauchyscalen = 0.004\n",
    "\n",
    "exponentialhigh = 15 #Symbol(\"b\", real=True)\n",
    "exponentiallow  = 15 #Symbol(\"a\", real=True)\n",
    "\n",
    "\n",
    "beta_a = 0.3\n",
    "beta_b  = 0.2\n",
    "kumaraswamy_a = beta_a \n",
    "kumaraswamy_b = (1.0+(beta_a-1.0)*math.pow( (beta_a+beta_b-2.0)/(beta_a-1.0), beta_a) )/beta_a \n",
    "print(kumaraswamy_b)\n",
    "\n",
    "\n",
    "independentnormalloc1=[(float(ii)+1)/(2*n+1) for ii in range(n,0,-1)]\n",
    "independentnormalscale1=[0.05 for ii in range(n)]\n",
    "\n",
    "independentnormalloc2=[(float(ii)+1)/(2*n+1) for ii in range(1,n+1,1)]\n",
    "independentnormalscale2=[0.05 for ii in range(n)]\n",
    "stage=[\"beta\"]\n",
    "# \"twopeak\",\"normal\",\"uniform\",\"independent1\",\"independent2\",\"cauchy\",\"beta\",\"U-exponential\",\"arcsine\"\n",
    "order1name=[\"dp\",\"random initializing\",\"costsharing\",\"heuristic\"]\n",
    "# \"costsharing\",\"dp\",\"heuristic\",\"random initializing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio1 = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase1 = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "d3 = D.normal.Normal(normalloc, normalscale)\n",
    "distributionRatio3 = d3.cdf(1) - d3.cdf(0)\n",
    "distributionBase3 = d3.cdf(0)\n",
    "\n",
    "d4 = D.uniform.Uniform(uniformlow,uniformhigh)\n",
    "distributionRatio4 = d4.cdf(1) - d4.cdf(0)\n",
    "distributionBase4 = d4.cdf(0)\n",
    "\n",
    "d5 = [D.normal.Normal(independentnormalloc1[ii], independentnormalscale1[ii]) for ii in range(n)]\n",
    "d6 = [D.normal.Normal(independentnormalloc2[ii], independentnormalscale2[ii]) for ii in range(n)]\n",
    "\n",
    "d7 = D.cauchy.Cauchy(cauchyloc,cauchyscalen)\n",
    "\n",
    "d81 = D.exponential.Exponential(exponentiallow)\n",
    "d82 = D.exponential.Exponential(exponentialhigh)\n",
    "\n",
    "d9 = D.beta.Beta(beta_a,beta_b)\n",
    "#sample_d9=d9.rsample(torch.Size([100000]))\n",
    "\n",
    "#d9_sample=np.linspace(0.0001, 0.9999, 10000)\n",
    "#d9_pdf=torch.exp(d9.log_prob(torch.tensor(d9_sample,dtype=torch.float32)))\n",
    "#d9_delta=d9_sample[1]-d9_sample[0]\n",
    "#d9_sum_pdf=torch.sum(d9_pdf*d9_delta)\n",
    "\n",
    "d10 = D.beta.Beta(0.5,0.5)\n",
    "\n",
    "def cdf(x,y, i=None):\n",
    "    if(y==\"twopeak\"):\n",
    "        return (d1.cdf(x) + d2.cdf(x) - distributionBase1) / 2 / distributionRatio1\n",
    "    elif(y==\"normal\"):\n",
    "        return (d3.cdf(x)-distributionBase3)/distributionRatio3;\n",
    "    elif(y==\"uniform\"):\n",
    "        return (d4.cdf(x)-distributionBase4)/distributionRatio4;\n",
    "    elif(y==\"independent1\"):\n",
    "        return d5[i].cdf(x);\n",
    "    elif(y==\"independent2\"):\n",
    "        return d6[i].cdf(x);\n",
    "    elif(y==\"cauchy\"):\n",
    "        return d7.cdf(x);\n",
    "    elif(y==\"beta\"):\n",
    "#         sum_cdf=0.0;\n",
    "#         if(x<0.0001):\n",
    "#             x=0.00011;\n",
    "#         if(x>0.9999):\n",
    "#             x=0.99989;\n",
    "#         for i in range(len(d9_pdf)):\n",
    "#             if(d9_sample[i]<x):\n",
    "#                 sum_cdf+=d9_pdf[i]*d9_delta;\n",
    "#             else:\n",
    "#                 sum_cdf+=(d9_pdf[i]+d9_pdf[i-1])/ 2 *(x-d9_sample[i-1])\n",
    "#                 break;\n",
    "#         return sum_cdf/d9_sum_pdf\n",
    "#         cdf_v=torch.sum((sample_d9<(x)), dtype=torch.float32)/100000\n",
    "#         return cdf_v\n",
    "#    F(x|a,b)=1–(1–x^a)^b\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(x,kumaraswamy_a),kumaraswamy_b);\n",
    "        except:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(torch.tensor(x,dtype=torch.float32),kumaraswamy_a),kumaraswamy_b);\n",
    "    elif(y==\"arcsine\"):\n",
    "        #\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            res=2.0/math.pi * torch.asin(torch.sqrt(x))\n",
    "            #print(x)\n",
    "            return res# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "        except:\n",
    "            return 2.0/math.pi * torch.asin(torch.sqrt(torch.tensor(x,dtype=torch.float32)))# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "    elif(y==\"U-exponential\"):\n",
    "        return (d81.cdf(x) + (1.0 - d82.cdf(1.0-x)))  / 2 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    if torch.sum(bits).item() == 0:\n",
    "        return torch.ones(n)\n",
    "    bits = bits.type(torch.float32)\n",
    "    negBits = torch.ones(n) - bits\n",
    "    payments = model(bits)\n",
    "    payments = payments - 1000 * negBits\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    payments = payments + negBits\n",
    "    return payments\n",
    "\n",
    "def tpToBits(tp, bits=torch.ones(n).type(torch.uint8)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = (tp >= payments).type(torch.uint8)\n",
    "    if torch.equal(newBits, bits):\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits(tp, newBits)#bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    return n - torch.sum(tpToBits(tp).type(torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    global  dp,decision\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money, yes]\n",
    "        offer = float(offerIndex) / dpPrecision\n",
    "        if tp[i] >= offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits[i] = 0\n",
    "            payments[i] = 1\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "#     print()\n",
    "#     print(tp)\n",
    "#     print(bits)\n",
    "#     print(payments)\n",
    "#     print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def heuristicSupervisionRule(tp):\n",
    "    global  dp_H,decision_H\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    tempold=-1;\n",
    "    for turns in range(n,0,-1):\n",
    "        money = dpPrecision\n",
    "        j=0\n",
    "        tempo=sum(bits)\n",
    "        #print(\"bits\",tempo)\n",
    "        for i in range(n):\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            \n",
    "            offerIndex = decision_H[tempo ,tempo - i, money]\n",
    "            offer = float(offerIndex) / dpPrecision\n",
    "            while(j<n):\n",
    "                if(bits[j]!=0):\n",
    "                    break;\n",
    "                j+=1;\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            if tp[j] >= offer:\n",
    "                #print(money,j,tp[j],offer)\n",
    "                money -= offerIndex\n",
    "                bits[j] = 1\n",
    "                payments[j] = offer\n",
    "            else:\n",
    "                bits[j] = 0;\n",
    "                payments[j] = 1.0;\n",
    "            j+=1;\n",
    "        #print(\"money\",money)\n",
    "        if(money==0 and tempold==tempo):\n",
    "            break;\n",
    "        tempold=tempo;\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    for k in range(n, -1, -1):\n",
    "        if k == 0:\n",
    "            break\n",
    "        bits = [1 if tp[ii] >= 1.0 / k else 0 for ii in range(n)]\n",
    "        if sum(bits) == k:\n",
    "            break\n",
    "    if k == 0:\n",
    "        payments = [1 for ii in range(n)]\n",
    "    else:\n",
    "        payments = [1.0 / k if bits[ii] == 1 else 1 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    return float(n - torch.sum(costSharingSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def dpDelay(tp):\n",
    "    return float(n - torch.sum(dpSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def heuristicDelay(tp):\n",
    "    return float(n - torch.sum(heuristicSupervisionRule(tp)[0]).item())\n",
    "\n",
    "\n",
    "# templire=0;\n",
    "# num=1\n",
    "# for i in range(num):\n",
    "#     temp=torch.tensor(samplesJoint[i])\n",
    "#     #print(temp)\n",
    "#     #print(temp);\n",
    "#     #print(heuristicSupervisionRule(temp))\n",
    "#     #print(dpSupervisionRule(temp))\n",
    "#     #print(dpDelay(temp))\n",
    "#     res=dpDelay(temp)\n",
    "#     templire+=res\n",
    "#     #print(\"Delay\",res)\n",
    "#     #print()\n",
    "# print(templire/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordAndReport(name, source, loss):\n",
    "    source.append(loss)\n",
    "    realLength = len(source)\n",
    "    #print(f\"{name} ({realLength}): {loss}\")\n",
    "    print(name,realLength,\":\" ,loss)\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    \n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda /100000\n",
    "        \n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % (log_interval*5) == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "losslist=[];\n",
    "losslistname=[];\n",
    "losslisttemp=[];\n",
    "def test_batch():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lenLoss= 0\n",
    "        nnLoss = 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "            for tp in tp_batch:\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        nnLoss/=lenLoss\n",
    "    return nnLoss\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                Delay1 = tpToTotalDelay(tp1)\n",
    "                Delay0 = tpToTotalDelay(tp0)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                    loss = loss + (1 - cdf(offer,order)) * Delay1 + cdf(offer,order) * Delay0\n",
    "                else:\n",
    "                    loss = loss + (1 - cdf(offer,order,i)) * Delay1 + cdf(offer,order,i) * Delay0\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "    \n",
    "allBits = [torch.tensor(bits, dtype=torch.int16) for bits in itertools.product([0, 1], repeat=n)]\n",
    "\n",
    "def test():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        costSharingLoss = 0\n",
    "        dpLoss = 0\n",
    "        nnLoss = 0\n",
    "        heuristicLoss=0\n",
    "        lenLoss= 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "                heuristicLoss+= heuristicDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        costSharingLoss /= lenLoss\n",
    "        dpLoss /= lenLoss\n",
    "        nnLoss /= lenLoss\n",
    "        heuristicLoss /= lenLoss\n",
    "        #print(lenLoss)\n",
    "        losslisttemp.append(nnLoss);\n",
    "        recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "        recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "        recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "        recordAndReport(\"heuristic\", runningLossHeuristic, heuristicLoss)\n",
    "        print(\"DP:\",dp[n, dpPrecision,0])\n",
    "        #for i in range(n, 0, -1):\n",
    "        #    print(\"Heuristic:\",i,5*(1-dp_H[i, i, dpPrecision]))\n",
    "        for i in range(n, 0, -1):\n",
    "            print(\n",
    "                    tpToPayments(\n",
    "                            torch.tensor([0 if ii >= i else 1 for ii in range(n)], dtype=torch.float32)\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "dpPrecision = 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def producedata(order):\n",
    "    global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "    dpPrecision = 100 \n",
    "    if(order==\"twopeak\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = np.random.normal(\n",
    "            loc=doublePeakLowMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=doublePeakLowMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samples2 = np.random.normal(\n",
    "            loc=doublePeakHighMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = np.random.normal(\n",
    "                        loc=doublePeakHighMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "    elif(order==\"normal\"):\n",
    "        print(\"loc\",normalloc, \"scale\",normalscale)\n",
    "        samples1 = np.random.normal(\n",
    "            loc=normalloc, scale=normalscale, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=normalloc, scale=normalscale\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"uniform\"):  \n",
    "        print(\"uniformlow\",uniformlow, \"uniformhigh\",uniformhigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        uniformlow, uniformhigh\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent1\"):\n",
    "        print(\"loc\",independentnormalloc1,\"scale\",independentnormalscale1)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent2\"):\n",
    "        print(\"loc\",independentnormalloc2, \"scale\",independentnormalscale2)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"cauchy\"):\n",
    "        print(\"cauchyloc\",cauchyloc, \"cauchyscale\",cauchyscalen)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"beta\"):\n",
    "        print(\"beta_a\",beta_a, \"beta_b\",beta_b)\n",
    "        print(\"kumaraswamy_a\",kumaraswamy_a, \"kumaraswamy_b\",kumaraswamy_b)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"arcsine\"):\n",
    "        print(\"betalow\",0.5, \"betahigh\",0.5)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"U-exponential\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = d81.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d81.rsample(torch.Size([1])).numpy()\n",
    "                    \n",
    "        samples2 = d82.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        \n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = d82.rsample(torch.Size([1])).numpy()\n",
    "        samples2 = 1.0 - samples2\n",
    "        samplesJoint = signals * samples1 - (signals - 1.0) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "        \n",
    "    plt.hist(samplesJoint,bins=500)\n",
    "    plt.show()\n",
    "    \n",
    "    tp_dataset = TensorDataset(tp_tensor[: int(trainSize * percentage_train_test)])\n",
    "    tp_dataset_testing = TensorDataset(tp_tensor[int(trainSize * (1.0-percentage_train_test)) :])\n",
    "    tp_dataloader = DataLoader(tp_dataset, batch_size=128, shuffle=True)\n",
    "    tp_dataloader_testing = DataLoader(tp_dataset_testing, batch_size=256, shuffle=False)\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "    decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for yes in range(n + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp[0, 0, yes] = 0\n",
    "            else:\n",
    "                dp[0, money, yes] = yes# + 1.0\n",
    "    for ppl in range(1,  n + 1):\n",
    "        for yes in range(n + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                minSoFar = 1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1 - cdf(offer,order)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order) * (1 + dp[ppl - 1, money, yes])\n",
    "                    else:\n",
    "                        res = (1 - cdf(offer,order,n-ppl)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order,n-ppl) * (1 + dp[ppl - 1, money, yes])\n",
    "                    if minSoFar > res.item():\n",
    "                        minSoFar = res.item()\n",
    "                        decision[ppl, money, yes] = offerIndex\n",
    "                dp[ppl, money, yes] = minSoFar\n",
    "    \n",
    "    print(\"dp\",dp[n, dpPrecision, 0])\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp_H = np.zeros([n + 1 , n + 1, dpPrecision + 1])\n",
    "    decision_H = np.zeros([n + 1 , n + 1, dpPrecision + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for i in range(1,n+1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp_H[i, 0, 0] = 1\n",
    "            else:\n",
    "                offer = money / dpPrecision\n",
    "                dp_H[i, 0, money] = 0#cdf(offer)# + 1.0\n",
    "    for i in range(1,n+1):\n",
    "        for ppl in range(1, i + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                maxSoFar = -1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1-cdf(offer,order)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    else:\n",
    "                        res = (1-cdf(offer,order,ppl-1)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    if maxSoFar < res.item():\n",
    "                        maxSoFar = res.item()\n",
    "                        decision_H[i, ppl, money] = offerIndex\n",
    "                dp_H[i, ppl, money] = maxSoFar\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_a 0.3 beta_b 0.2\n",
      "kumaraswamy_a 0.3 kumaraswamy_b 0.40058530825361593\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dp 1.866951823234558\n",
      "Supervised Aim: beta dp\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 0.055524\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 0.002198\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 0.000360\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 0.000077\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 0.000061\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 0.000022\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 0.000015\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 0.000008\n",
      "NN 1 : tensor(1.4188)\n",
      "CS 1 : 1.61196\n",
      "DP 1 : 1.3184\n",
      "heuristic 1 : 1.92332\n",
      "DP: 1.866951823234558\n",
      "tensor([4.8132e-01, 5.1451e-01, 4.1145e-03, 5.1307e-05, 4.4274e-08])\n",
      "tensor([4.7236e-01, 5.2242e-01, 5.1787e-03, 4.3766e-05, 1.0000e+00])\n",
      "tensor([0.4693, 0.5256, 0.0051, 1.0000, 1.0000])\n",
      "tensor([0.4851, 0.5149, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 1.398123 testing loss: tensor(1.4063)\n",
      "Train Epoch: 1 [640/25000 (3%)]\tLoss: 1.585016 testing loss: tensor(1.3988)\n",
      "Train Epoch: 1 [1280/25000 (5%)]\tLoss: 1.618448 testing loss: tensor(1.3982)\n",
      "Train Epoch: 1 [1920/25000 (8%)]\tLoss: 1.537212 testing loss: tensor(1.3991)\n",
      "Train Epoch: 1 [2560/25000 (10%)]\tLoss: 1.416514 testing loss: tensor(1.3962)\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 1.482865 testing loss: tensor(1.3974)\n",
      "Train Epoch: 1 [3840/25000 (15%)]\tLoss: 1.371222 testing loss: tensor(1.3949)\n",
      "Train Epoch: 1 [4480/25000 (18%)]\tLoss: 1.599236 testing loss: tensor(1.3959)\n",
      "Train Epoch: 1 [5120/25000 (20%)]\tLoss: 1.675365 testing loss: tensor(1.3944)\n",
      "Train Epoch: 1 [5760/25000 (23%)]\tLoss: 1.509376 testing loss: tensor(1.3895)\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 1.507604 testing loss: tensor(1.3877)\n",
      "Train Epoch: 1 [7040/25000 (28%)]\tLoss: 1.538613 testing loss: tensor(1.3858)\n",
      "Train Epoch: 1 [7680/25000 (31%)]\tLoss: 1.534307 testing loss: tensor(1.3838)\n",
      "Train Epoch: 1 [8320/25000 (33%)]\tLoss: 1.590363 testing loss: tensor(1.3794)\n",
      "Train Epoch: 1 [8960/25000 (36%)]\tLoss: 1.305454 testing loss: tensor(1.3765)\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 1.601719 testing loss: tensor(1.3736)\n",
      "Train Epoch: 1 [10240/25000 (41%)]\tLoss: 1.378629 testing loss: tensor(1.3730)\n",
      "Train Epoch: 1 [10880/25000 (43%)]\tLoss: 1.590552 testing loss: tensor(1.3720)\n",
      "Train Epoch: 1 [11520/25000 (46%)]\tLoss: 1.421239 testing loss: tensor(1.3699)\n",
      "Train Epoch: 1 [12160/25000 (48%)]\tLoss: 1.338967 testing loss: tensor(1.3660)\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 1.508262 testing loss: tensor(1.3670)\n",
      "Train Epoch: 1 [13440/25000 (54%)]\tLoss: 1.422595 testing loss: tensor(1.3644)\n",
      "Train Epoch: 1 [14080/25000 (56%)]\tLoss: 1.377054 testing loss: tensor(1.3646)\n",
      "Train Epoch: 1 [14720/25000 (59%)]\tLoss: 1.546746 testing loss: tensor(1.3606)\n",
      "Train Epoch: 1 [15360/25000 (61%)]\tLoss: 1.415047 testing loss: tensor(1.3599)\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 1.355250 testing loss: tensor(1.3598)\n",
      "Train Epoch: 1 [16640/25000 (66%)]\tLoss: 1.667641 testing loss: tensor(1.3585)\n",
      "Train Epoch: 1 [17280/25000 (69%)]\tLoss: 1.412127 testing loss: tensor(1.3597)\n",
      "Train Epoch: 1 [17920/25000 (71%)]\tLoss: 1.343336 testing loss: tensor(1.3602)\n",
      "Train Epoch: 1 [18560/25000 (74%)]\tLoss: 1.500677 testing loss: tensor(1.3552)\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 1.563351 testing loss: tensor(1.3578)\n",
      "Train Epoch: 1 [19840/25000 (79%)]\tLoss: 1.500040 testing loss: tensor(1.3553)\n",
      "Train Epoch: 1 [20480/25000 (82%)]\tLoss: 1.476115 testing loss: tensor(1.3525)\n",
      "Train Epoch: 1 [21120/25000 (84%)]\tLoss: 1.584941 testing loss: tensor(1.3523)\n",
      "Train Epoch: 1 [21760/25000 (87%)]\tLoss: 1.386599 testing loss: tensor(1.3514)\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 1.468252 testing loss: tensor(1.3509)\n",
      "Train Epoch: 1 [23040/25000 (92%)]\tLoss: 1.504974 testing loss: tensor(1.3505)\n",
      "Train Epoch: 1 [23680/25000 (94%)]\tLoss: 1.478775 testing loss: tensor(1.3506)\n",
      "Train Epoch: 1 [24320/25000 (97%)]\tLoss: 1.383849 testing loss: tensor(1.3521)\n",
      "Train Epoch: 1 [7800/25000 (99%)]\tLoss: 1.273810 testing loss: tensor(1.3532)\n",
      "penalty: 0.042401932179927826\n",
      "NN 2 : tensor(1.3532)\n",
      "CS 2 : 1.61196\n",
      "DP 2 : 1.3184\n",
      "heuristic 2 : 1.92332\n",
      "DP: 1.866951823234558\n",
      "tensor([4.8877e-01, 5.1105e-01, 1.7311e-04, 3.7418e-08, 7.0744e-13])\n",
      "tensor([4.8845e-01, 5.1129e-01, 2.6277e-04, 6.8884e-08, 1.0000e+00])\n",
      "tensor([4.8925e-01, 5.1034e-01, 4.1852e-04, 1.0000e+00, 1.0000e+00])\n",
      "tensor([0.4883, 0.5117, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: beta random initializing\n",
      "do nothing\n",
      "NN 1 : tensor(1.6158)\n",
      "CS 1 : 1.61196\n",
      "DP 1 : 1.3184\n",
      "heuristic 1 : 1.92332\n",
      "DP: 1.866951823234558\n",
      "tensor([0.1314, 0.3380, 0.1744, 0.1294, 0.2269])\n",
      "tensor([0.1802, 0.4897, 0.2098, 0.1203, 1.0000])\n",
      "tensor([0.2120, 0.5199, 0.2681, 1.0000, 1.0000])\n",
      "tensor([0.3021, 0.6979, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 1.784175 testing loss: tensor(1.6141)\n",
      "Train Epoch: 1 [640/25000 (3%)]\tLoss: 1.563367 testing loss: tensor(1.6151)\n",
      "Train Epoch: 1 [1280/25000 (5%)]\tLoss: 1.720476 testing loss: tensor(1.6129)\n",
      "Train Epoch: 1 [1920/25000 (8%)]\tLoss: 1.689628 testing loss: tensor(1.6136)\n",
      "Train Epoch: 1 [2560/25000 (10%)]\tLoss: 1.815601 testing loss: tensor(1.6126)\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 1.679621 testing loss: tensor(1.6106)\n",
      "Train Epoch: 1 [3840/25000 (15%)]\tLoss: 1.848693 testing loss: tensor(1.6096)\n",
      "Train Epoch: 1 [4480/25000 (18%)]\tLoss: 1.551413 testing loss: tensor(1.6107)\n",
      "Train Epoch: 1 [5120/25000 (20%)]\tLoss: 1.656099 testing loss: tensor(1.6098)\n",
      "Train Epoch: 1 [5760/25000 (23%)]\tLoss: 1.626789 testing loss: tensor(1.6093)\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 1.625925 testing loss: tensor(1.6086)\n",
      "Train Epoch: 1 [7040/25000 (28%)]\tLoss: 1.672500 testing loss: tensor(1.6068)\n",
      "Train Epoch: 1 [7680/25000 (31%)]\tLoss: 1.605233 testing loss: tensor(1.6062)\n",
      "Train Epoch: 1 [8320/25000 (33%)]\tLoss: 1.745412 testing loss: tensor(1.6038)\n",
      "Train Epoch: 1 [8960/25000 (36%)]\tLoss: 1.603493 testing loss: tensor(1.6036)\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 1.816842 testing loss: tensor(1.5993)\n",
      "Train Epoch: 1 [10240/25000 (41%)]\tLoss: 1.582213 testing loss: tensor(1.5966)\n",
      "Train Epoch: 1 [10880/25000 (43%)]\tLoss: 1.544108 testing loss: tensor(1.5898)\n",
      "Train Epoch: 1 [11520/25000 (46%)]\tLoss: 1.717211 testing loss: tensor(1.5848)\n",
      "Train Epoch: 1 [12160/25000 (48%)]\tLoss: 1.581175 testing loss: tensor(1.5738)\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 1.751657 testing loss: tensor(1.5698)\n",
      "Train Epoch: 1 [13440/25000 (54%)]\tLoss: 1.649858 testing loss: tensor(1.5620)\n",
      "Train Epoch: 1 [14080/25000 (56%)]\tLoss: 1.544172 testing loss: tensor(1.5554)\n",
      "Train Epoch: 1 [14720/25000 (59%)]\tLoss: 1.610176 testing loss: tensor(1.5466)\n",
      "Train Epoch: 1 [15360/25000 (61%)]\tLoss: 1.620876 testing loss: tensor(1.5417)\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 1.621814 testing loss: tensor(1.5334)\n",
      "Train Epoch: 1 [16640/25000 (66%)]\tLoss: 1.574200 testing loss: tensor(1.5319)\n",
      "Train Epoch: 1 [17280/25000 (69%)]\tLoss: 1.583482 testing loss: tensor(1.5269)\n",
      "Train Epoch: 1 [17920/25000 (71%)]\tLoss: 1.556854 testing loss: tensor(1.5249)\n",
      "Train Epoch: 1 [18560/25000 (74%)]\tLoss: 1.725059 testing loss: tensor(1.5186)\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 1.675788 testing loss: tensor(1.5125)\n",
      "Train Epoch: 1 [19840/25000 (79%)]\tLoss: 1.645892 testing loss: tensor(1.5099)\n",
      "Train Epoch: 1 [20480/25000 (82%)]\tLoss: 1.414762 testing loss: tensor(1.5066)\n",
      "Train Epoch: 1 [21120/25000 (84%)]\tLoss: 1.735325 testing loss: tensor(1.5056)\n",
      "Train Epoch: 1 [21760/25000 (87%)]\tLoss: 1.520902 testing loss: tensor(1.5026)\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 1.503692 testing loss: tensor(1.5004)\n",
      "Train Epoch: 1 [23040/25000 (92%)]\tLoss: 1.547515 testing loss: tensor(1.4975)\n",
      "Train Epoch: 1 [23680/25000 (94%)]\tLoss: 1.590304 testing loss: tensor(1.4991)\n",
      "Train Epoch: 1 [24320/25000 (97%)]\tLoss: 1.563597 testing loss: tensor(1.4954)\n",
      "Train Epoch: 1 [7800/25000 (99%)]\tLoss: 1.758168 testing loss: tensor(1.4935)\n",
      "penalty: 0.0304298996925354\n",
      "NN 2 : tensor(1.4935)\n",
      "CS 2 : 1.61196\n",
      "DP 2 : 1.3184\n",
      "heuristic 2 : 1.92332\n",
      "DP: 1.866951823234558\n",
      "tensor([1.8605e-01, 2.8964e-01, 3.0852e-01, 5.3713e-08, 2.1578e-01])\n",
      "tensor([2.8427e-01, 3.4494e-01, 3.7078e-01, 1.1904e-06, 1.0000e+00])\n",
      "tensor([0.2823, 0.3453, 0.3724, 1.0000, 1.0000])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4669, 0.5331, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: beta costsharing\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 0.009506\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 0.000033\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 0.000003\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 0.000001\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 0.000000\n",
      "NN 1 : tensor(1.6123)\n",
      "CS 1 : 1.61196\n",
      "DP 1 : 1.3184\n",
      "heuristic 1 : 1.92332\n",
      "DP: 1.866951823234558\n",
      "tensor([0.2000, 0.1999, 0.2001, 0.2003, 0.1997])\n",
      "tensor([0.2502, 0.2502, 0.2498, 0.2498, 1.0000])\n",
      "tensor([0.3330, 0.3338, 0.3331, 1.0000, 1.0000])\n",
      "tensor([0.5004, 0.4996, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 1.667571 testing loss: tensor(1.6126)\n",
      "Train Epoch: 1 [640/25000 (3%)]\tLoss: 1.771527 testing loss: tensor(1.6128)\n",
      "Train Epoch: 1 [1280/25000 (5%)]\tLoss: 1.663891 testing loss: tensor(1.6138)\n",
      "Train Epoch: 1 [1920/25000 (8%)]\tLoss: 1.743703 testing loss: tensor(1.6140)\n",
      "Train Epoch: 1 [2560/25000 (10%)]\tLoss: 1.669825 testing loss: tensor(1.6134)\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 1.545055 testing loss: tensor(1.6129)\n",
      "Train Epoch: 1 [3840/25000 (15%)]\tLoss: 1.764974 testing loss: tensor(1.6128)\n",
      "Train Epoch: 1 [4480/25000 (18%)]\tLoss: 1.716521 testing loss: tensor(1.6128)\n",
      "Train Epoch: 1 [5120/25000 (20%)]\tLoss: 1.726662 testing loss: tensor(1.6124)\n",
      "Train Epoch: 1 [5760/25000 (23%)]\tLoss: 1.714586 testing loss: tensor(1.6135)\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 1.642545 testing loss: tensor(1.6135)\n",
      "Train Epoch: 1 [7040/25000 (28%)]\tLoss: 1.710019 testing loss: tensor(1.6137)\n",
      "Train Epoch: 1 [7680/25000 (31%)]\tLoss: 1.718527 testing loss: tensor(1.6136)\n",
      "Train Epoch: 1 [8320/25000 (33%)]\tLoss: 1.792999 testing loss: tensor(1.6129)\n",
      "Train Epoch: 1 [8960/25000 (36%)]\tLoss: 1.730476 testing loss: tensor(1.6126)\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 1.798143 testing loss: tensor(1.6130)\n",
      "Train Epoch: 1 [10240/25000 (41%)]\tLoss: 1.718614 testing loss: tensor(1.6126)\n",
      "Train Epoch: 1 [10880/25000 (43%)]\tLoss: 1.675231 testing loss: tensor(1.6124)\n",
      "Train Epoch: 1 [11520/25000 (46%)]\tLoss: 1.679063 testing loss: tensor(1.6122)\n",
      "Train Epoch: 1 [12160/25000 (48%)]\tLoss: 1.752752 testing loss: tensor(1.6120)\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 1.949328 testing loss: tensor(1.6124)\n",
      "Train Epoch: 1 [13440/25000 (54%)]\tLoss: 1.742967 testing loss: tensor(1.6129)\n",
      "Train Epoch: 1 [14080/25000 (56%)]\tLoss: 1.573674 testing loss: tensor(1.6132)\n",
      "Train Epoch: 1 [14720/25000 (59%)]\tLoss: 1.790642 testing loss: tensor(1.6130)\n",
      "Train Epoch: 1 [15360/25000 (61%)]\tLoss: 1.797971 testing loss: tensor(1.6132)\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 1.854200 testing loss: tensor(1.6133)\n",
      "Train Epoch: 1 [16640/25000 (66%)]\tLoss: 1.685996 testing loss: tensor(1.6132)\n",
      "Train Epoch: 1 [17280/25000 (69%)]\tLoss: 1.566705 testing loss: tensor(1.6129)\n",
      "Train Epoch: 1 [17920/25000 (71%)]\tLoss: 1.711430 testing loss: tensor(1.6127)\n",
      "Train Epoch: 1 [18560/25000 (74%)]\tLoss: 1.616290 testing loss: tensor(1.6129)\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 1.917516 testing loss: tensor(1.6139)\n",
      "Train Epoch: 1 [19840/25000 (79%)]\tLoss: 1.577908 testing loss: tensor(1.6137)\n",
      "Train Epoch: 1 [20480/25000 (82%)]\tLoss: 1.624372 testing loss: tensor(1.6136)\n",
      "Train Epoch: 1 [21120/25000 (84%)]\tLoss: 1.568932 testing loss: tensor(1.6132)\n",
      "Train Epoch: 1 [21760/25000 (87%)]\tLoss: 1.692976 testing loss: tensor(1.6130)\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 1.684196 testing loss: tensor(1.6133)\n",
      "Train Epoch: 1 [23040/25000 (92%)]\tLoss: 1.723145 testing loss: tensor(1.6130)\n",
      "Train Epoch: 1 [23680/25000 (94%)]\tLoss: 1.934289 testing loss: tensor(1.6131)\n",
      "Train Epoch: 1 [24320/25000 (97%)]\tLoss: 1.676329 testing loss: tensor(1.6130)\n",
      "Train Epoch: 1 [7800/25000 (99%)]\tLoss: 1.374653 testing loss: tensor(1.6132)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.6132)\n",
      "CS 2 : 1.61196\n",
      "DP 2 : 1.3184\n",
      "heuristic 2 : 1.92332\n",
      "DP: 1.866951823234558\n",
      "tensor([0.1946, 0.2060, 0.2087, 0.1966, 0.1942])\n",
      "tensor([0.2401, 0.2574, 0.2577, 0.2448, 1.0000])\n",
      "tensor([0.3165, 0.3403, 0.3432, 1.0000, 1.0000])\n",
      "tensor([0.4827, 0.5173, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: beta heuristic\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 0.027975\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 0.000888\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 0.000105\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 0.000190\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 0.000058\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 0.000328\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 0.000062\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 0.000120\n",
      "NN 1 : tensor(1.4508)\n",
      "CS 1 : 1.61196\n",
      "DP 1 : 1.3184\n",
      "heuristic 1 : 1.92332\n",
      "DP: 1.866951823234558\n",
      "tensor([4.1158e-05, 3.3600e-04, 4.8217e-03, 4.9793e-01, 4.9687e-01])\n",
      "tensor([8.1391e-04, 7.2284e-03, 4.9039e-01, 5.0157e-01, 1.0000e+00])\n",
      "tensor([0.0047, 0.4990, 0.4963, 1.0000, 1.0000])\n",
      "tensor([0.4927, 0.5073, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 1.644908 testing loss: tensor(1.4453)\n",
      "Train Epoch: 1 [640/25000 (3%)]\tLoss: 1.257221 testing loss: tensor(1.4388)\n",
      "Train Epoch: 1 [1280/25000 (5%)]\tLoss: 1.467129 testing loss: tensor(1.4289)\n",
      "Train Epoch: 1 [1920/25000 (8%)]\tLoss: 1.715454 testing loss: tensor(1.4229)\n",
      "Train Epoch: 1 [2560/25000 (10%)]\tLoss: 1.646086 testing loss: tensor(1.4138)\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 1.412722 testing loss: tensor(1.4050)\n",
      "Train Epoch: 1 [3840/25000 (15%)]\tLoss: 1.559913 testing loss: tensor(1.3985)\n",
      "Train Epoch: 1 [4480/25000 (18%)]\tLoss: 1.576105 testing loss: tensor(1.3939)\n",
      "Train Epoch: 1 [5120/25000 (20%)]\tLoss: 1.471478 testing loss: tensor(1.3894)\n",
      "Train Epoch: 1 [5760/25000 (23%)]\tLoss: 1.483963 testing loss: tensor(1.3846)\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 1.636995 testing loss: tensor(1.3808)\n",
      "Train Epoch: 1 [7040/25000 (28%)]\tLoss: 1.737231 testing loss: tensor(1.3786)\n",
      "Train Epoch: 1 [7680/25000 (31%)]\tLoss: 1.678347 testing loss: tensor(1.3787)\n",
      "Train Epoch: 1 [8320/25000 (33%)]\tLoss: 1.412997 testing loss: tensor(1.3744)\n",
      "Train Epoch: 1 [8960/25000 (36%)]\tLoss: 1.544436 testing loss: tensor(1.3734)\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 1.468482 testing loss: tensor(1.3680)\n",
      "Train Epoch: 1 [10240/25000 (41%)]\tLoss: 1.408868 testing loss: tensor(1.3667)\n",
      "Train Epoch: 1 [10880/25000 (43%)]\tLoss: 1.468286 testing loss: tensor(1.3634)\n",
      "Train Epoch: 1 [11520/25000 (46%)]\tLoss: 1.545680 testing loss: tensor(1.3608)\n",
      "Train Epoch: 1 [12160/25000 (48%)]\tLoss: 1.435932 testing loss: tensor(1.3573)\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 1.393567 testing loss: tensor(1.3555)\n",
      "Train Epoch: 1 [13440/25000 (54%)]\tLoss: 1.435374 testing loss: tensor(1.3522)\n",
      "Train Epoch: 1 [14080/25000 (56%)]\tLoss: 1.563843 testing loss: tensor(1.3524)\n",
      "Train Epoch: 1 [14720/25000 (59%)]\tLoss: 1.483320 testing loss: tensor(1.3505)\n",
      "Train Epoch: 1 [15360/25000 (61%)]\tLoss: 1.638204 testing loss: tensor(1.3496)\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 1.588371 testing loss: tensor(1.3495)\n",
      "Train Epoch: 1 [16640/25000 (66%)]\tLoss: 1.462695 testing loss: tensor(1.3484)\n",
      "Train Epoch: 1 [17280/25000 (69%)]\tLoss: 1.487488 testing loss: tensor(1.3463)\n",
      "Train Epoch: 1 [17920/25000 (71%)]\tLoss: 1.423415 testing loss: tensor(1.3445)\n",
      "Train Epoch: 1 [18560/25000 (74%)]\tLoss: 1.325015 testing loss: tensor(1.3452)\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 1.544001 testing loss: tensor(1.3442)\n",
      "Train Epoch: 1 [19840/25000 (79%)]\tLoss: 1.373294 testing loss: tensor(1.3424)\n",
      "Train Epoch: 1 [20480/25000 (82%)]\tLoss: 1.529157 testing loss: tensor(1.3414)\n",
      "Train Epoch: 1 [21120/25000 (84%)]\tLoss: 1.507778 testing loss: tensor(1.3379)\n",
      "Train Epoch: 1 [21760/25000 (87%)]\tLoss: 1.638863 testing loss: tensor(1.3372)\n",
      "Train Epoch: 1 [22400/25000 (89%)]\tLoss: 1.466461 testing loss: tensor(1.3390)\n",
      "Train Epoch: 1 [23040/25000 (92%)]\tLoss: 1.259917 testing loss: tensor(1.3373)\n",
      "Train Epoch: 1 [23680/25000 (94%)]\tLoss: 1.422663 testing loss: tensor(1.3366)\n",
      "Train Epoch: 1 [24320/25000 (97%)]\tLoss: 1.516906 testing loss: tensor(1.3342)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [7800/25000 (99%)]\tLoss: 1.310047 testing loss: tensor(1.3351)\n",
      "penalty: 0.06135911867022514\n",
      "NN 2 : tensor(1.3351)\n",
      "CS 2 : 1.61196\n",
      "DP 2 : 1.3184\n",
      "heuristic 2 : 1.92332\n",
      "DP: 1.866951823234558\n",
      "tensor([1.8832e-11, 1.9158e-08, 4.1877e-05, 5.0297e-01, 4.9699e-01])\n",
      "tensor([8.0519e-08, 4.4790e-05, 4.9200e-01, 5.0795e-01, 1.0000e+00])\n",
      "tensor([4.2864e-05, 4.7673e-01, 5.2323e-01, 1.0000e+00, 1.0000e+00])\n",
      "tensor([0.4322, 0.5678, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.1)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "    \n",
    "for order in stage:\n",
    "    producedata(order)\n",
    "    #running\n",
    "    for order1 in order1name:\n",
    "        print(\"Supervised Aim:\",order,order1)\n",
    "        \n",
    "        # for mapping binary to payments before softmax\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(n, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, n),\n",
    "        )\n",
    "        model.apply(init_weights)\n",
    "        # optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr1)\n",
    "        \n",
    "        runningLossNN = []\n",
    "        runningLossCS = []\n",
    "        runningLossDP = []\n",
    "        runningLossHeuristic = []\n",
    "        #model=torch.load(\"save/pytorchNN=5dp1\");\n",
    "        #model.eval()\n",
    "        ##order1name=[\"costsharing\",\"dp\",\"heuristic\",\"random initializing\"]\n",
    "        for epoch in range(1, supervisionEpochs + 1):\n",
    "#             print(\"distributionRatio\",distributionRatio)\n",
    "            if(order1==\"costsharing\"):\n",
    "                supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "            elif(order1==\"dp\"):\n",
    "                supervisionTrain(epoch, dpSupervisionRule)\n",
    "            elif(order1==\"heuristic\"):\n",
    "                supervisionTrain(epoch, heuristicSupervisionRule)\n",
    "            elif(order1==\"random initializing\"):\n",
    "                print(\"do nothing\");\n",
    "\n",
    "        test()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr2)\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(epoch)\n",
    "            test()\n",
    "        losslistname.append(order+\" \"+order1);\n",
    "        losslist.append(losslisttemp);\n",
    "        losslisttemp=[];\n",
    "        savepath=\"save/pytorchNN=5all-beta\"+order+str(order1)\n",
    "        torch.save(model, savepath);\n",
    "        print(\"end\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colorlist=[\"#D2691E\",'#4169E1',\"#9ACD32\",\"#B22222\",\"#FF00FF\",\"#708090\"]\n",
    "for i in range(len(losslist)):\n",
    "    plt.plot(losslist[i], '.-', color=colorlist[i], alpha=0.8, label=losslistname[i])\n",
    "\n",
    "# 显示标签，如果不加这句，即使加了label='一些数字'的参数，最终还是不会显示标签\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('Trianing Times')\n",
    "plt.ylabel('Delay')\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('pytorchNN=5_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
