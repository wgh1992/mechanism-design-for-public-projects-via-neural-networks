{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40058530825361593\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from random import randint\n",
    "import random\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "from pylab import *\n",
    "from scipy.stats import beta\n",
    "import math\n",
    "n = 6\n",
    "epochs = 1\n",
    "supervisionEpochs = 1\n",
    "lr1 = 0.01\n",
    "lr2 = 0.0002\n",
    "log_interval = 5\n",
    "trainSize = 40000#100000\n",
    "percentage_train_test= 0.5\n",
    "penaltyLambda = 50\n",
    "doublePeakHighMean = 0.9\n",
    "doublePeakLowMean = 0.1\n",
    "doublePeakStd = 0.04\n",
    "uniformlow=0\n",
    "uniformhigh=1.0\n",
    "normalloc = 0.2\n",
    "normalscale = 0.1\n",
    "\n",
    "cauchyloc = 1/n\n",
    "cauchyscalen = 0.004\n",
    "\n",
    "exponentialhigh = 15 #Symbol(\"b\", real=True)\n",
    "exponentiallow  = 15 #Symbol(\"a\", real=True)\n",
    "\n",
    "\n",
    "beta_a = 0.3\n",
    "beta_b  = 0.2\n",
    "kumaraswamy_a = beta_a \n",
    "kumaraswamy_b = (1.0+(beta_a-1.0)*math.pow( (beta_a+beta_b-2.0)/(beta_a-1.0), beta_a) )/beta_a \n",
    "print(kumaraswamy_b)\n",
    "\n",
    "\n",
    "independentnormalloc1=[(float(ii)+1)/(2*n+1) for ii in range(n,0,-1)]\n",
    "independentnormalscale1=[0.05 for ii in range(n)]\n",
    "\n",
    "independentnormalloc2=[(float(ii)+1)/(2*n+1) for ii in range(1,n+1,1)]\n",
    "independentnormalscale2=[0.05 for ii in range(n)]\n",
    "stage=[\"twopeak\"]\n",
    "# \"twopeak\",\"normal\",\"uniform\",\"independent1\",\"independent2\",\"cauchy\",\"beta\",\"U-exponential\",\"arcsine\"\n",
    "order1name=[\"dp\",\"random initializing\",\"costsharing\",\"heuristic\"]\n",
    "# \"costsharing\",\"dp\",\"heuristic\",\"random initializing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio1 = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase1 = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "d3 = D.normal.Normal(normalloc, normalscale)\n",
    "distributionRatio3 = d3.cdf(1) - d3.cdf(0)\n",
    "distributionBase3 = d3.cdf(0)\n",
    "\n",
    "d4 = D.uniform.Uniform(uniformlow,uniformhigh)\n",
    "distributionRatio4 = d4.cdf(1) - d4.cdf(0)\n",
    "distributionBase4 = d4.cdf(0)\n",
    "\n",
    "d5 = [D.normal.Normal(independentnormalloc1[ii], independentnormalscale1[ii]) for ii in range(n)]\n",
    "d6 = [D.normal.Normal(independentnormalloc2[ii], independentnormalscale2[ii]) for ii in range(n)]\n",
    "\n",
    "d7 = D.cauchy.Cauchy(cauchyloc,cauchyscalen)\n",
    "\n",
    "d81 = D.exponential.Exponential(exponentiallow)\n",
    "d82 = D.exponential.Exponential(exponentialhigh)\n",
    "\n",
    "d9 = D.beta.Beta(beta_a,beta_b)\n",
    "#sample_d9=d9.rsample(torch.Size([100000]))\n",
    "\n",
    "#d9_sample=np.linspace(0.0001, 0.9999, 10000)\n",
    "#d9_pdf=torch.exp(d9.log_prob(torch.tensor(d9_sample,dtype=torch.float32)))\n",
    "#d9_delta=d9_sample[1]-d9_sample[0]\n",
    "#d9_sum_pdf=torch.sum(d9_pdf*d9_delta)\n",
    "\n",
    "d10 = D.beta.Beta(0.5,0.5)\n",
    "\n",
    "def cdf(x,y, i=None):\n",
    "    if(y==\"twopeak\"):\n",
    "        return (d1.cdf(x) + d2.cdf(x) - distributionBase1) / 2 / distributionRatio1\n",
    "    elif(y==\"normal\"):\n",
    "        return (d3.cdf(x)-distributionBase3)/distributionRatio3;\n",
    "    elif(y==\"uniform\"):\n",
    "        return (d4.cdf(x)-distributionBase4)/distributionRatio4;\n",
    "    elif(y==\"independent1\"):\n",
    "        return d5[i].cdf(x);\n",
    "    elif(y==\"independent2\"):\n",
    "        return d6[i].cdf(x);\n",
    "    elif(y==\"cauchy\"):\n",
    "        return d7.cdf(x);\n",
    "    elif(y==\"beta\"):\n",
    "#         sum_cdf=0.0;\n",
    "#         if(x<0.0001):\n",
    "#             x=0.00011;\n",
    "#         if(x>0.9999):\n",
    "#             x=0.99989;\n",
    "#         for i in range(len(d9_pdf)):\n",
    "#             if(d9_sample[i]<x):\n",
    "#                 sum_cdf+=d9_pdf[i]*d9_delta;\n",
    "#             else:\n",
    "#                 sum_cdf+=(d9_pdf[i]+d9_pdf[i-1])/ 2 *(x-d9_sample[i-1])\n",
    "#                 break;\n",
    "#         return sum_cdf/d9_sum_pdf\n",
    "#         cdf_v=torch.sum((sample_d9<(x)), dtype=torch.float32)/100000\n",
    "#         return cdf_v\n",
    "#    F(x|a,b)=1–(1–x^a)^b\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(x,kumaraswamy_a),kumaraswamy_b);\n",
    "        except:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(torch.tensor(x,dtype=torch.float32),kumaraswamy_a),kumaraswamy_b);\n",
    "    elif(y==\"arcsine\"):\n",
    "        #\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            res=2.0/math.pi * torch.asin(torch.sqrt(x))\n",
    "            #print(x)\n",
    "            return res# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "        except:\n",
    "            return 2.0/math.pi * torch.asin(torch.sqrt(torch.tensor(x,dtype=torch.float32)))# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "    elif(y==\"U-exponential\"):\n",
    "        return (d81.cdf(x) + (1.0 - d82.cdf(1.0-x)))  / 2 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    if torch.sum(bits).item() == 0:\n",
    "        return torch.ones(n)\n",
    "    bits = bits.type(torch.float32)\n",
    "    negBits = torch.ones(n) - bits\n",
    "    payments = model(bits)\n",
    "    payments = payments - 1000 * negBits\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    payments = payments + negBits\n",
    "    return payments\n",
    "\n",
    "def tpToBits(tp, bits=torch.ones(n).type(torch.uint8)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = (tp >= payments).type(torch.uint8)\n",
    "    if torch.equal(newBits, bits):\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits(tp, newBits)#bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    return n - torch.sum(tpToBits(tp).type(torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    global  dp,decision\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money, yes]\n",
    "        offer = float(offerIndex) / dpPrecision\n",
    "        if tp[i] >= offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits[i] = 0\n",
    "            payments[i] = 1\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "#     print()\n",
    "#     print(tp)\n",
    "#     print(bits)\n",
    "#     print(payments)\n",
    "#     print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def heuristicSupervisionRule(tp):\n",
    "    global  dp_H,decision_H\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0.0 for ii in range(n)]\n",
    "    tempold=-1;\n",
    "    for turns in range(n,0,-1):\n",
    "        money = dpPrecision\n",
    "        j=0\n",
    "        tempo=sum(bits)\n",
    "        #print(\"bits\",tempo)\n",
    "        for i in range(n):\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            \n",
    "            offerIndex = decision_H[tempo ,tempo - i, money]\n",
    "            offer = float(offerIndex) / dpPrecision\n",
    "            while(j<n):\n",
    "                if(bits[j]!=0):\n",
    "                    break;\n",
    "                j+=1;\n",
    "            if(j>=n):\n",
    "                break;\n",
    "            if tp[j] >= offer:\n",
    "                #print(money,j,tp[j],offer)\n",
    "                money -= offerIndex\n",
    "                bits[j] = 1\n",
    "                payments[j] = offer\n",
    "            else:\n",
    "                bits[j] = 0;\n",
    "                payments[j] = 1.0;\n",
    "            j+=1;\n",
    "        #print(\"money\",money)\n",
    "        if(money==0 and tempold==tempo):\n",
    "            break;\n",
    "        tempold=tempo;\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1.0 for ii in range(n)]\n",
    "\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    for k in range(n, -1, -1):\n",
    "        if k == 0:\n",
    "            break\n",
    "        bits = [1 if tp[ii] >= 1.0 / k else 0 for ii in range(n)]\n",
    "        if sum(bits) == k:\n",
    "            break\n",
    "    if k == 0:\n",
    "        payments = [1 for ii in range(n)]\n",
    "    else:\n",
    "        payments = [1.0 / k if bits[ii] == 1 else 1 for ii in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    return float(n - torch.sum(costSharingSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def dpDelay(tp):\n",
    "    return float(n - torch.sum(dpSupervisionRule(tp)[0]).item())\n",
    "\n",
    "def heuristicDelay(tp):\n",
    "    return float(n - torch.sum(heuristicSupervisionRule(tp)[0]).item())\n",
    "\n",
    "\n",
    "# templire=0;\n",
    "# num=1\n",
    "# for i in range(num):\n",
    "#     temp=torch.tensor(samplesJoint[i])\n",
    "#     #print(temp)\n",
    "#     #print(temp);\n",
    "#     #print(heuristicSupervisionRule(temp))\n",
    "#     #print(dpSupervisionRule(temp))\n",
    "#     #print(dpDelay(temp))\n",
    "#     res=dpDelay(temp)\n",
    "#     templire+=res\n",
    "#     #print(\"Delay\",res)\n",
    "#     #print()\n",
    "# print(templire/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordAndReport(name, source, loss):\n",
    "    source.append(loss)\n",
    "    realLength = len(source)\n",
    "    #print(f\"{name} ({realLength}): {loss}\")\n",
    "    print(name,realLength,\":\" ,loss)\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    \n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda /100000\n",
    "        \n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % (log_interval*5) == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "losslist=[];\n",
    "losslistname=[];\n",
    "losslisttemp=[];\n",
    "def test_batch():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lenLoss= 0\n",
    "        nnLoss = 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "            for tp in tp_batch:\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        nnLoss/=lenLoss\n",
    "    return nnLoss\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + torch.sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                Delay1 = tpToTotalDelay(tp1)\n",
    "                Delay0 = tpToTotalDelay(tp0)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                    loss = loss + (1 - cdf(offer,order)) * Delay1 + cdf(offer,order) * Delay0\n",
    "                else:\n",
    "                    loss = loss + (1 - cdf(offer,order,i)) * Delay1 + cdf(offer,order,i) * Delay0\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    print(\"penalty:\",penalty.item())\n",
    "    \n",
    "allBits = [torch.tensor(bits, dtype=torch.int16) for bits in itertools.product([0, 1], repeat=n)]\n",
    "\n",
    "def test():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        costSharingLoss = 0\n",
    "        dpLoss = 0\n",
    "        nnLoss = 0\n",
    "        heuristicLoss=0\n",
    "        lenLoss= 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "                heuristicLoss+= heuristicDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        costSharingLoss /= lenLoss\n",
    "        dpLoss /= lenLoss\n",
    "        nnLoss /= lenLoss\n",
    "        heuristicLoss /= lenLoss\n",
    "        #print(lenLoss)\n",
    "        losslisttemp.append(nnLoss);\n",
    "        recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "        recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "        recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "        recordAndReport(\"heuristic\", runningLossHeuristic, heuristicLoss)\n",
    "        print(\"DP:\",dp[n, dpPrecision,0])\n",
    "        #for i in range(n, 0, -1):\n",
    "        #    print(\"Heuristic:\",i,5*(1-dp_H[i, i, dpPrecision]))\n",
    "        for i in range(n, 0, -1):\n",
    "            print(\n",
    "                    tpToPayments(\n",
    "                            torch.tensor([0 if ii >= i else 1 for ii in range(n)], dtype=torch.float32)\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "dpPrecision = 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def producedata(order):\n",
    "    global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "    dpPrecision = 100 \n",
    "    if(order==\"twopeak\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = np.random.normal(\n",
    "            loc=doublePeakLowMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=doublePeakLowMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samples2 = np.random.normal(\n",
    "            loc=doublePeakHighMean, scale=doublePeakStd, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = np.random.normal(\n",
    "                        loc=doublePeakHighMean, scale=doublePeakStd\n",
    "                    )\n",
    "        samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "    elif(order==\"normal\"):\n",
    "        print(\"loc\",normalloc, \"scale\",normalscale)\n",
    "        samples1 = np.random.normal(\n",
    "            loc=normalloc, scale=normalscale, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        loc=normalloc, scale=normalscale\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"uniform\"):  \n",
    "        print(\"uniformlow\",uniformlow, \"uniformhigh\",uniformhigh)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        uniformlow, uniformhigh\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent1\"):\n",
    "        print(\"loc\",independentnormalloc1,\"scale\",independentnormalscale1)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc1[j], independentnormalscale1[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"independent2\"):\n",
    "        print(\"loc\",independentnormalloc2, \"scale\",independentnormalscale2)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = np.random.normal(\n",
    "                        independentnormalloc2[j], independentnormalscale2[j]\n",
    "                    )\n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"cauchy\"):\n",
    "        print(\"cauchyloc\",cauchyloc, \"cauchyscale\",cauchyscalen)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d7.rsample(torch.Size([1]))\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"beta\"):\n",
    "        print(\"beta_a\",beta_a, \"beta_b\",beta_b)\n",
    "        print(\"kumaraswamy_a\",kumaraswamy_a, \"kumaraswamy_b\",kumaraswamy_b)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(beta_a,beta_b,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"arcsine\"):\n",
    "        print(\"betalow\",0.5, \"betahigh\",0.5)\n",
    "        samples1 = np.random.uniform(\n",
    "            uniformlow, uniformhigh, size=(trainSize, n)\n",
    "        )\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = beta.rvs(0.5,0.5,  size = 1)\n",
    "                    \n",
    "        samplesJoint = samples1\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "    elif(order==\"U-exponential\"):\n",
    "        print(\"loc\",doublePeakLowMean, \"scale\",doublePeakStd)\n",
    "        print(\"loc\",doublePeakHighMean, \"scale\",doublePeakStd)\n",
    "        signals = np.random.randint(2, size=(trainSize, n))\n",
    "        samples1 = d81.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "                    samples1[i, j] = d81.rsample(torch.Size([1])).numpy()\n",
    "                    \n",
    "        samples2 = d82.rsample(torch.Size([trainSize, n])).numpy()\n",
    "        \n",
    "        for i in range(trainSize):\n",
    "            for j in range(n):\n",
    "                while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "                    samples2[i, j] = d82.rsample(torch.Size([1])).numpy()\n",
    "        samples2 = 1.0 - samples2\n",
    "        samplesJoint = signals * samples1 - (signals - 1.0) * samples2\n",
    "        tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "\n",
    "        # tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "        \n",
    "    plt.hist(samplesJoint,bins=500)\n",
    "    plt.show()\n",
    "    \n",
    "    tp_dataset = TensorDataset(tp_tensor[: int(trainSize * percentage_train_test)])\n",
    "    tp_dataset_testing = TensorDataset(tp_tensor[int(trainSize * (1.0-percentage_train_test)) :])\n",
    "    tp_dataloader = DataLoader(tp_dataset, batch_size=128, shuffle=True)\n",
    "    tp_dataloader_testing = DataLoader(tp_dataset_testing, batch_size=256, shuffle=False)\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "    decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for yes in range(n + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp[0, 0, yes] = 0\n",
    "            else:\n",
    "                dp[0, money, yes] = yes# + 1.0\n",
    "    for ppl in range(1,  n + 1):\n",
    "        for yes in range(n + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                minSoFar = 1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1 - cdf(offer,order)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order) * (1 + dp[ppl - 1, money, yes])\n",
    "                    else:\n",
    "                        res = (1 - cdf(offer,order,n-ppl)) * dp[\n",
    "                        ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                        ] + cdf(offer,order,n-ppl) * (1 + dp[ppl - 1, money, yes])\n",
    "                    if minSoFar > res.item():\n",
    "                        minSoFar = res.item()\n",
    "                        decision[ppl, money, yes] = offerIndex\n",
    "                dp[ppl, money, yes] = minSoFar\n",
    "    \n",
    "    print(\"dp\",dp[n, dpPrecision, 0])\n",
    "    \n",
    "    # howManyPpl left, money left, yes already\n",
    "    dp_H = np.zeros([n + 1 , n + 1, dpPrecision + 1])\n",
    "    decision_H = np.zeros([n + 1 , n + 1, dpPrecision + 1], dtype=np.uint8)\n",
    "    # ppl = 0 left\n",
    "    for i in range(1,n+1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            if money == 0:\n",
    "                dp_H[i, 0, 0] = 1\n",
    "            else:\n",
    "                offer = money / dpPrecision\n",
    "                dp_H[i, 0, money] = 0#cdf(offer)# + 1.0\n",
    "    for i in range(1,n+1):\n",
    "        for ppl in range(1, i + 1):\n",
    "            for money in range(dpPrecision + 1):\n",
    "                maxSoFar = -1000000\n",
    "                for offerIndex in range(money + 1):\n",
    "                    offer = float(offerIndex) / dpPrecision\n",
    "                    if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                        res = (1-cdf(offer,order)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    else:\n",
    "                        res = (1-cdf(offer,order,ppl-1)) * dp_H[\n",
    "                         i, ppl - 1, money - offerIndex\n",
    "                        ]\n",
    "                    if maxSoFar < res.item():\n",
    "                        maxSoFar = res.item()\n",
    "                        decision_H[i, ppl, money] = offerIndex\n",
    "                dp_H[i, ppl, money] = maxSoFar\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loc 0.1 scale 0.04\n",
      "loc 0.9 scale 0.04\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dp 1.3416093587875366\n",
      "Supervised Aim: twopeak dp\n",
      "Train Epoch: 1 [0/20000 (0%)]\tLoss: 0.063852\n",
      "Train Epoch: 1 [3200/20000 (16%)]\tLoss: 0.005036\n",
      "Train Epoch: 1 [6400/20000 (32%)]\tLoss: 0.003975\n",
      "Train Epoch: 1 [9600/20000 (48%)]\tLoss: 0.004032\n",
      "Train Epoch: 1 [12800/20000 (64%)]\tLoss: 0.003681\n",
      "Train Epoch: 1 [16000/20000 (80%)]\tLoss: 0.003705\n",
      "Train Epoch: 1 [19200/20000 (96%)]\tLoss: 0.004380\n",
      "NN 1 : tensor(3.3017)\n",
      "CS 1 : 3.0761\n",
      "DP 1 : 1.34155\n",
      "heuristic 1 : 1.3555\n",
      "DP: 1.3416093587875366\n",
      "tensor([1.0000e+00, 5.2082e-07, 6.7503e-08, 6.3999e-08, 5.1053e-14, 2.0832e-13])\n",
      "tensor([1.0000e+00, 5.7260e-07, 7.5128e-08, 7.0091e-08, 7.9502e-14, 1.0000e+00])\n",
      "tensor([1.0000e+00, 1.5224e-06, 1.8843e-07, 1.6948e-07, 1.0000e+00, 1.0000e+00])\n",
      "tensor([9.9999e-01, 9.0549e-06, 1.6646e-06, 1.0000e+00, 1.0000e+00, 1.0000e+00])\n",
      "tensor([9.9996e-01, 4.1963e-05, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00])\n",
      "tensor([1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/20000 (0%)]\tLoss: 3.446543 testing loss: tensor(3.2757)\n",
      "Train Epoch: 1 [640/20000 (3%)]\tLoss: 3.237888 testing loss: tensor(3.2750)\n",
      "Train Epoch: 1 [1280/20000 (6%)]\tLoss: 3.246300 testing loss: tensor(3.2666)\n",
      "Train Epoch: 1 [1920/20000 (10%)]\tLoss: 3.123867 testing loss: tensor(3.2724)\n",
      "Train Epoch: 1 [2560/20000 (13%)]\tLoss: 2.982543 testing loss: tensor(3.2615)\n",
      "Train Epoch: 1 [3200/20000 (16%)]\tLoss: 2.929330 testing loss: tensor(3.2515)\n",
      "Train Epoch: 1 [3840/20000 (19%)]\tLoss: 3.602572 testing loss: tensor(3.2543)\n",
      "Train Epoch: 1 [4480/20000 (22%)]\tLoss: 3.101979 testing loss: tensor(3.2517)\n",
      "Train Epoch: 1 [5120/20000 (25%)]\tLoss: 3.269607 testing loss: tensor(3.2499)\n",
      "Train Epoch: 1 [5760/20000 (29%)]\tLoss: 3.214288 testing loss: tensor(3.2479)\n",
      "Train Epoch: 1 [6400/20000 (32%)]\tLoss: 3.317140 testing loss: tensor(3.2501)\n",
      "Train Epoch: 1 [7040/20000 (35%)]\tLoss: 3.498604 testing loss: tensor(3.2513)\n",
      "Train Epoch: 1 [7680/20000 (38%)]\tLoss: 3.252392 testing loss: tensor(3.2561)\n",
      "Train Epoch: 1 [8320/20000 (41%)]\tLoss: 3.223713 testing loss: tensor(3.2518)\n",
      "Train Epoch: 1 [8960/20000 (45%)]\tLoss: 3.350309 testing loss: tensor(3.2497)\n",
      "Train Epoch: 1 [9600/20000 (48%)]\tLoss: 3.367410 testing loss: tensor(3.2473)\n",
      "Train Epoch: 1 [10240/20000 (51%)]\tLoss: 3.344187 testing loss: tensor(3.2524)\n",
      "Train Epoch: 1 [10880/20000 (54%)]\tLoss: 3.124129 testing loss: tensor(3.2508)\n",
      "Train Epoch: 1 [11520/20000 (57%)]\tLoss: 3.291560 testing loss: tensor(3.2490)\n",
      "Train Epoch: 1 [12160/20000 (61%)]\tLoss: 3.137988 testing loss: tensor(3.2486)\n",
      "Train Epoch: 1 [12800/20000 (64%)]\tLoss: 3.662768 testing loss: tensor(3.2499)\n",
      "Train Epoch: 1 [13440/20000 (67%)]\tLoss: 3.138378 testing loss: tensor(3.2470)\n",
      "Train Epoch: 1 [14080/20000 (70%)]\tLoss: 3.340911 testing loss: tensor(3.2511)\n",
      "Train Epoch: 1 [14720/20000 (73%)]\tLoss: 3.189919 testing loss: tensor(3.2513)\n",
      "Train Epoch: 1 [15360/20000 (76%)]\tLoss: 3.493876 testing loss: tensor(3.2517)\n",
      "Train Epoch: 1 [16000/20000 (80%)]\tLoss: 3.269020 testing loss: tensor(3.2512)\n",
      "Train Epoch: 1 [16640/20000 (83%)]\tLoss: 3.440024 testing loss: tensor(3.2474)\n",
      "Train Epoch: 1 [17280/20000 (86%)]\tLoss: 3.401904 testing loss: tensor(3.2487)\n",
      "Train Epoch: 1 [17920/20000 (89%)]\tLoss: 3.190200 testing loss: tensor(3.2462)\n",
      "Train Epoch: 1 [18560/20000 (92%)]\tLoss: 3.194083 testing loss: tensor(3.2497)\n",
      "Train Epoch: 1 [19200/20000 (96%)]\tLoss: 3.254578 testing loss: tensor(3.2500)\n",
      "Train Epoch: 1 [19840/20000 (99%)]\tLoss: 3.389177 testing loss: tensor(3.2513)\n",
      "penalty: 0.004185906611382961\n",
      "NN 2 : tensor(3.2462)\n",
      "CS 2 : 3.0761\n",
      "DP 2 : 1.34155\n",
      "heuristic 2 : 1.3555\n",
      "DP: 1.3416093587875366\n",
      "tensor([1.0000e+00, 3.8649e-08, 4.6114e-09, 4.8984e-09, 9.6402e-15, 1.8552e-15])\n",
      "tensor([1.0000e+00, 4.1348e-08, 4.9141e-09, 5.1768e-09, 1.3567e-14, 1.0000e+00])\n",
      "tensor([1.0000e+00, 1.2465e-07, 1.4275e-08, 1.4795e-08, 1.0000e+00, 1.0000e+00])\n",
      "tensor([1.0000e+00, 8.3583e-07, 1.7115e-07, 1.0000e+00, 1.0000e+00, 1.0000e+00])\n",
      "tensor([1.0000e+00, 3.1488e-06, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00])\n",
      "tensor([1., 1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: twopeak random initializing\n",
      "do nothing\n",
      "NN 1 : tensor(2.8790)\n",
      "CS 1 : 3.0761\n",
      "DP 1 : 1.34155\n",
      "heuristic 1 : 1.3555\n",
      "DP: 1.3416093587875366\n",
      "tensor([0.0732, 0.2215, 0.3327, 0.1630, 0.1090, 0.1007])\n",
      "tensor([0.0850, 0.2293, 0.3674, 0.1848, 0.1336, 1.0000])\n",
      "tensor([0.0952, 0.2514, 0.4240, 0.2294, 1.0000, 1.0000])\n",
      "tensor([0.1235, 0.3665, 0.5100, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.2937, 0.7063, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/20000 (0%)]\tLoss: 2.892895 testing loss: tensor(2.8532)\n",
      "Train Epoch: 1 [640/20000 (3%)]\tLoss: 2.689361 testing loss: tensor(2.7095)\n",
      "Train Epoch: 1 [1280/20000 (6%)]\tLoss: 2.596080 testing loss: tensor(2.5533)\n",
      "Train Epoch: 1 [1920/20000 (10%)]\tLoss: 2.359863 testing loss: tensor(2.3809)\n",
      "Train Epoch: 1 [2560/20000 (13%)]\tLoss: 2.381045 testing loss: tensor(2.1963)\n",
      "Train Epoch: 1 [3200/20000 (16%)]\tLoss: 2.038054 testing loss: tensor(2.0284)\n",
      "Train Epoch: 1 [3840/20000 (19%)]\tLoss: 1.998360 testing loss: tensor(1.9015)\n",
      "Train Epoch: 1 [4480/20000 (22%)]\tLoss: 2.132273 testing loss: tensor(1.8540)\n",
      "Train Epoch: 1 [5120/20000 (25%)]\tLoss: 1.819900 testing loss: tensor(1.8257)\n",
      "Train Epoch: 1 [5760/20000 (29%)]\tLoss: 1.610994 testing loss: tensor(1.7730)\n",
      "Train Epoch: 1 [6400/20000 (32%)]\tLoss: 1.859016 testing loss: tensor(1.7242)\n",
      "Train Epoch: 1 [7040/20000 (35%)]\tLoss: 1.865558 testing loss: tensor(1.6988)\n",
      "Train Epoch: 1 [7680/20000 (38%)]\tLoss: 1.670465 testing loss: tensor(1.6925)\n",
      "Train Epoch: 1 [8320/20000 (41%)]\tLoss: 1.331492 testing loss: tensor(1.6904)\n",
      "Train Epoch: 1 [8960/20000 (45%)]\tLoss: 1.614745 testing loss: tensor(1.6895)\n",
      "Train Epoch: 1 [9600/20000 (48%)]\tLoss: 1.585588 testing loss: tensor(1.6890)\n",
      "Train Epoch: 1 [10240/20000 (51%)]\tLoss: 1.782799 testing loss: tensor(1.6891)\n",
      "Train Epoch: 1 [10880/20000 (54%)]\tLoss: 2.144529 testing loss: tensor(1.6885)\n",
      "Train Epoch: 1 [11520/20000 (57%)]\tLoss: 1.630150 testing loss: tensor(1.6885)\n",
      "Train Epoch: 1 [12160/20000 (61%)]\tLoss: 1.621510 testing loss: tensor(1.6881)\n",
      "Train Epoch: 1 [12800/20000 (64%)]\tLoss: 1.689452 testing loss: tensor(1.6879)\n",
      "Train Epoch: 1 [13440/20000 (67%)]\tLoss: 1.529769 testing loss: tensor(1.6885)\n",
      "Train Epoch: 1 [14080/20000 (70%)]\tLoss: 1.697447 testing loss: tensor(1.6866)\n",
      "Train Epoch: 1 [14720/20000 (73%)]\tLoss: 1.695097 testing loss: tensor(1.6867)\n",
      "Train Epoch: 1 [15360/20000 (76%)]\tLoss: 1.508470 testing loss: tensor(1.6872)\n",
      "Train Epoch: 1 [16000/20000 (80%)]\tLoss: 1.676105 testing loss: tensor(1.6861)\n",
      "Train Epoch: 1 [16640/20000 (83%)]\tLoss: 1.789079 testing loss: tensor(1.6876)\n",
      "Train Epoch: 1 [17280/20000 (86%)]\tLoss: 1.346060 testing loss: tensor(1.6858)\n",
      "Train Epoch: 1 [17920/20000 (89%)]\tLoss: 1.661883 testing loss: tensor(1.6856)\n",
      "Train Epoch: 1 [18560/20000 (92%)]\tLoss: 1.804578 testing loss: tensor(1.6860)\n",
      "Train Epoch: 1 [19200/20000 (96%)]\tLoss: 1.768505 testing loss: tensor(1.6866)\n",
      "Train Epoch: 1 [19840/20000 (99%)]\tLoss: 1.770984 testing loss: tensor(1.6858)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(1.6850)\n",
      "CS 2 : 3.0761\n",
      "DP 2 : 1.34155\n",
      "heuristic 2 : 1.3555\n",
      "DP: 1.3416093587875366\n",
      "tensor([0.0207, 0.4467, 0.4738, 0.0200, 0.0183, 0.0205])\n",
      "tensor([0.0230, 0.4484, 0.4780, 0.0263, 0.0243, 1.0000])\n",
      "tensor([0.0266, 0.4555, 0.4846, 0.0334, 1.0000, 1.0000])\n",
      "tensor([0.0340, 0.4593, 0.5066, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.0867, 0.9133, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: twopeak costsharing\n",
      "Train Epoch: 1 [0/20000 (0%)]\tLoss: 0.000556\n",
      "Train Epoch: 1 [3200/20000 (16%)]\tLoss: 0.000014\n",
      "Train Epoch: 1 [6400/20000 (32%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [9600/20000 (48%)]\tLoss: 0.000001\n",
      "Train Epoch: 1 [12800/20000 (64%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [16000/20000 (80%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [19200/20000 (96%)]\tLoss: 0.000000\n",
      "NN 1 : tensor(3.0763)\n",
      "CS 1 : 3.0761\n",
      "DP 1 : 1.34155\n",
      "heuristic 1 : 1.3555\n",
      "DP: 1.3416093587875366\n",
      "tensor([0.1669, 0.1671, 0.1670, 0.1666, 0.1659, 0.1666])\n",
      "tensor([0.2003, 0.1996, 0.2005, 0.1992, 0.2004, 1.0000])\n",
      "tensor([0.2497, 0.2502, 0.2501, 0.2500, 1.0000, 1.0000])\n",
      "tensor([0.3326, 0.3336, 0.3338, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.4998, 0.5002, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/20000 (0%)]\tLoss: 3.156489 testing loss: tensor(3.0762)\n",
      "Train Epoch: 1 [640/20000 (3%)]\tLoss: 3.088676 testing loss: tensor(3.0763)\n",
      "Train Epoch: 1 [1280/20000 (6%)]\tLoss: 3.043118 testing loss: tensor(3.0762)\n",
      "Train Epoch: 1 [1920/20000 (10%)]\tLoss: 3.170860 testing loss: tensor(3.0761)\n",
      "Train Epoch: 1 [2560/20000 (13%)]\tLoss: 3.057433 testing loss: tensor(3.0760)\n",
      "Train Epoch: 1 [3200/20000 (16%)]\tLoss: 3.049029 testing loss: tensor(3.0759)\n",
      "Train Epoch: 1 [3840/20000 (19%)]\tLoss: 3.188121 testing loss: tensor(3.0759)\n",
      "Train Epoch: 1 [4480/20000 (22%)]\tLoss: 3.069427 testing loss: tensor(3.0756)\n",
      "Train Epoch: 1 [5120/20000 (25%)]\tLoss: 3.409716 testing loss: tensor(3.0752)\n",
      "Train Epoch: 1 [5760/20000 (29%)]\tLoss: 2.965507 testing loss: tensor(3.0749)\n",
      "Train Epoch: 1 [6400/20000 (32%)]\tLoss: 2.982524 testing loss: tensor(3.0748)\n",
      "Train Epoch: 1 [7040/20000 (35%)]\tLoss: 3.026610 testing loss: tensor(3.0740)\n",
      "Train Epoch: 1 [7680/20000 (38%)]\tLoss: 3.045529 testing loss: tensor(3.0733)\n",
      "Train Epoch: 1 [8320/20000 (41%)]\tLoss: 3.288254 testing loss: tensor(3.0721)\n",
      "Train Epoch: 1 [8960/20000 (45%)]\tLoss: 3.108313 testing loss: tensor(3.0713)\n",
      "Train Epoch: 1 [9600/20000 (48%)]\tLoss: 3.102829 testing loss: tensor(3.0704)\n",
      "Train Epoch: 1 [10240/20000 (51%)]\tLoss: 3.020366 testing loss: tensor(3.0681)\n",
      "Train Epoch: 1 [10880/20000 (54%)]\tLoss: 2.959203 testing loss: tensor(3.0654)\n",
      "Train Epoch: 1 [11520/20000 (57%)]\tLoss: 3.111656 testing loss: tensor(3.0622)\n",
      "Train Epoch: 1 [12160/20000 (61%)]\tLoss: 2.885300 testing loss: tensor(3.0566)\n",
      "Train Epoch: 1 [12800/20000 (64%)]\tLoss: 2.990832 testing loss: tensor(3.0491)\n",
      "Train Epoch: 1 [13440/20000 (67%)]\tLoss: 3.005654 testing loss: tensor(3.0371)\n",
      "Train Epoch: 1 [14080/20000 (70%)]\tLoss: 2.889449 testing loss: tensor(3.0221)\n",
      "Train Epoch: 1 [14720/20000 (73%)]\tLoss: 2.975729 testing loss: tensor(3.0004)\n",
      "Train Epoch: 1 [15360/20000 (76%)]\tLoss: 2.844735 testing loss: tensor(2.9662)\n",
      "Train Epoch: 1 [16000/20000 (80%)]\tLoss: 3.006380 testing loss: tensor(2.9197)\n",
      "Train Epoch: 1 [16640/20000 (83%)]\tLoss: 2.934866 testing loss: tensor(2.8561)\n",
      "Train Epoch: 1 [17280/20000 (86%)]\tLoss: 2.842783 testing loss: tensor(2.7736)\n",
      "Train Epoch: 1 [17920/20000 (89%)]\tLoss: 2.711156 testing loss: tensor(2.6727)\n",
      "Train Epoch: 1 [18560/20000 (92%)]\tLoss: 2.603648 testing loss: tensor(2.5533)\n",
      "Train Epoch: 1 [19200/20000 (96%)]\tLoss: 2.573071 testing loss: tensor(2.4290)\n",
      "Train Epoch: 1 [19840/20000 (99%)]\tLoss: 2.259720 testing loss: tensor(2.2952)\n",
      "penalty: 0.0\n",
      "NN 2 : tensor(2.2680)\n",
      "CS 2 : 3.0761\n",
      "DP 2 : 1.34155\n",
      "heuristic 2 : 1.3555\n",
      "DP: 1.3416093587875366\n",
      "tensor([0.0311, 0.1384, 0.0296, 0.0514, 0.2470, 0.5025])\n",
      "tensor([0.0695, 0.2733, 0.0678, 0.1112, 0.4782, 1.0000])\n",
      "tensor([0.1425, 0.4913, 0.1425, 0.2237, 1.0000, 1.0000])\n",
      "tensor([0.1942, 0.6110, 0.1947, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.2555, 0.7445, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n",
      "Supervised Aim: twopeak heuristic\n",
      "Train Epoch: 1 [0/20000 (0%)]\tLoss: 0.073121\n",
      "Train Epoch: 1 [3200/20000 (16%)]\tLoss: 0.013827\n",
      "Train Epoch: 1 [6400/20000 (32%)]\tLoss: 0.025710\n",
      "Train Epoch: 1 [9600/20000 (48%)]\tLoss: 0.019236\n",
      "Train Epoch: 1 [12800/20000 (64%)]\tLoss: 0.011096\n",
      "Train Epoch: 1 [16000/20000 (80%)]\tLoss: 0.013733\n",
      "Train Epoch: 1 [19200/20000 (96%)]\tLoss: 0.003623\n",
      "NN 1 : tensor(2.8205)\n",
      "CS 1 : 3.0761\n",
      "DP 1 : 1.34155\n",
      "heuristic 1 : 1.3555\n",
      "DP: 1.3416093587875366\n",
      "tensor([6.3543e-04, 1.2365e-03, 3.1777e-03, 9.7273e-01, 7.6924e-03, 1.4528e-02])\n",
      "tensor([0.0118, 0.0123, 0.0088, 0.4056, 0.5615, 1.0000])\n",
      "tensor([0.0081, 0.0118, 0.0148, 0.9652, 1.0000, 1.0000])\n",
      "tensor([0.0249, 0.0502, 0.9249, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.2906, 0.7094, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1.])\n",
      "Train Epoch: 1 [0/20000 (0%)]\tLoss: 3.990070 testing loss: tensor(2.7821)\n",
      "Train Epoch: 1 [640/20000 (3%)]\tLoss: 3.558776 testing loss: tensor(2.6532)\n",
      "Train Epoch: 1 [1280/20000 (6%)]\tLoss: 3.754148 testing loss: tensor(2.5353)\n",
      "Train Epoch: 1 [1920/20000 (10%)]\tLoss: 3.398924 testing loss: tensor(2.3874)\n",
      "Train Epoch: 1 [2560/20000 (13%)]\tLoss: 3.191415 testing loss: tensor(2.1876)\n",
      "Train Epoch: 1 [3200/20000 (16%)]\tLoss: 2.916662 testing loss: tensor(1.8540)\n",
      "Train Epoch: 1 [3840/20000 (19%)]\tLoss: 2.508434 testing loss: tensor(1.5750)\n",
      "Train Epoch: 1 [4480/20000 (22%)]\tLoss: 2.281044 testing loss: tensor(1.5546)\n",
      "Train Epoch: 1 [5120/20000 (25%)]\tLoss: 2.307401 testing loss: tensor(1.6185)\n",
      "Train Epoch: 1 [5760/20000 (29%)]\tLoss: 2.285154 testing loss: tensor(1.6784)\n",
      "Train Epoch: 1 [6400/20000 (32%)]\tLoss: 2.347867 testing loss: tensor(1.7185)\n",
      "Train Epoch: 1 [7040/20000 (35%)]\tLoss: 2.595584 testing loss: tensor(1.7515)\n",
      "Train Epoch: 1 [7680/20000 (38%)]\tLoss: 2.490882 testing loss: tensor(1.7854)\n",
      "Train Epoch: 1 [8320/20000 (41%)]\tLoss: 2.529362 testing loss: tensor(1.8144)\n",
      "Train Epoch: 1 [8960/20000 (45%)]\tLoss: 2.283023 testing loss: tensor(1.8554)\n",
      "Train Epoch: 1 [9600/20000 (48%)]\tLoss: 2.415047 testing loss: tensor(1.8958)\n",
      "Train Epoch: 1 [10240/20000 (51%)]\tLoss: 2.182988 testing loss: tensor(1.9346)\n",
      "Train Epoch: 1 [10880/20000 (54%)]\tLoss: 2.233705 testing loss: tensor(1.9797)\n",
      "Train Epoch: 1 [11520/20000 (57%)]\tLoss: 2.314751 testing loss: tensor(2.0332)\n",
      "Train Epoch: 1 [12160/20000 (61%)]\tLoss: 2.586998 testing loss: tensor(2.0814)\n",
      "Train Epoch: 1 [12800/20000 (64%)]\tLoss: 2.322909 testing loss: tensor(2.1309)\n",
      "Train Epoch: 1 [13440/20000 (67%)]\tLoss: 2.360797 testing loss: tensor(2.1628)\n",
      "Train Epoch: 1 [14080/20000 (70%)]\tLoss: 2.300625 testing loss: tensor(2.1795)\n",
      "Train Epoch: 1 [14720/20000 (73%)]\tLoss: 2.554450 testing loss: tensor(2.1820)\n",
      "Train Epoch: 1 [15360/20000 (76%)]\tLoss: 2.388072 testing loss: tensor(2.1725)\n",
      "Train Epoch: 1 [16000/20000 (80%)]\tLoss: 2.496592 testing loss: tensor(2.1471)\n",
      "Train Epoch: 1 [16640/20000 (83%)]\tLoss: 2.185082 testing loss: tensor(2.1071)\n",
      "Train Epoch: 1 [17280/20000 (86%)]\tLoss: 2.241075 testing loss: tensor(2.0286)\n",
      "Train Epoch: 1 [17920/20000 (89%)]\tLoss: 2.182664 testing loss: tensor(1.9311)\n",
      "Train Epoch: 1 [18560/20000 (92%)]\tLoss: 1.643844 testing loss: tensor(1.8226)\n",
      "Train Epoch: 1 [19200/20000 (96%)]\tLoss: 1.825139 testing loss: tensor(1.7220)\n",
      "Train Epoch: 1 [19840/20000 (99%)]\tLoss: 1.702357 testing loss: tensor(1.6437)\n",
      "penalty: 0.9372671246528625\n",
      "NN 2 : tensor(1.6275)\n",
      "CS 2 : 3.0761\n",
      "DP 2 : 1.34155\n",
      "heuristic 2 : 1.3555\n",
      "DP: 1.3416093587875366\n",
      "tensor([0.0168, 0.0386, 0.0686, 0.0660, 0.0538, 0.7562])\n",
      "tensor([0.0284, 0.0468, 0.0503, 0.0689, 0.8057, 1.0000])\n",
      "tensor([0.1261, 0.1722, 0.3090, 0.3928, 1.0000, 1.0000])\n",
      "tensor([0.1043, 0.1428, 0.7529, 1.0000, 1.0000, 1.0000])\n",
      "tensor([0.3128, 0.6872, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1., 1.])\n",
      "end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.1)\n",
    "        #m.bias.data.fill_(0.1)\n",
    "    \n",
    "for order in stage:\n",
    "    producedata(order)\n",
    "    #running\n",
    "    for order1 in order1name:\n",
    "        print(\"Supervised Aim:\",order,order1)\n",
    "        \n",
    "        # for mapping binary to payments before softmax\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(n, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, n),\n",
    "        )\n",
    "        model.apply(init_weights)\n",
    "        # optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr1)\n",
    "        \n",
    "        runningLossNN = []\n",
    "        runningLossCS = []\n",
    "        runningLossDP = []\n",
    "        runningLossHeuristic = []\n",
    "        #model=torch.load(\"save/pytorchNN=5dp1\");\n",
    "        #model.eval()\n",
    "        ##order1name=[\"costsharing\",\"dp\",\"heuristic\",\"random initializing\"]\n",
    "        for epoch in range(1, supervisionEpochs + 1):\n",
    "#             print(\"distributionRatio\",distributionRatio)\n",
    "            if(order1==\"costsharing\"):\n",
    "                supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "            elif(order1==\"dp\"):\n",
    "                supervisionTrain(epoch, dpSupervisionRule)\n",
    "            elif(order1==\"heuristic\"):\n",
    "                supervisionTrain(epoch, heuristicSupervisionRule)\n",
    "            elif(order1==\"random initializing\"):\n",
    "                print(\"do nothing\");\n",
    "\n",
    "        test()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr2)\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(epoch)\n",
    "            test()\n",
    "        losslistname.append(order+\" \"+order1);\n",
    "        losslist.append(losslisttemp);\n",
    "        losslisttemp=[];\n",
    "        savepath=\"save/pytorchNN=5all-beta\"+order+str(order1)\n",
    "        torch.save(model, savepath);\n",
    "        print(\"end\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colorlist=[\"#D2691E\",'#4169E1',\"#9ACD32\",\"#B22222\",\"#FF00FF\",\"#708090\"]\n",
    "for i in range(len(losslist)):\n",
    "    plt.plot(losslist[i], '.-', color=colorlist[i], alpha=0.8, label=losslistname[i])\n",
    "\n",
    "# 显示标签，如果不加这句，即使加了label='一些数字'的参数，最终还是不会显示标签\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('Trianing Times')\n",
    "plt.ylabel('Delay')\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('pytorchNN=5_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
