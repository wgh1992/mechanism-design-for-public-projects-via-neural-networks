{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0228)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "n = 5\n",
    "epochs = 3\n",
    "supervisionEpochs = 5\n",
    "lr = 0.001\n",
    "log_interval = 20\n",
    "trainSize = 10000\n",
    "penaltyLambda = 10\n",
    "doublePeakHighMean = 0.8\n",
    "doublePeakLowMean = 0.2\n",
    "doublePeakStd = 0.1\n",
    "\n",
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "\n",
    "def cdf(x, i=None):\n",
    "    return (d1.cdf(x) + d2.cdf(x) - distributionBase) / 2 / distributionRatio\n",
    "\n",
    "# def cdf(x, i=None):\n",
    "#     if x < 0.1:\n",
    "#         return 0\n",
    "#     if x <= 0.2:\n",
    "#         return 0.5 * (x - 0.1) / 0.1\n",
    "#     if x < 0.8:\n",
    "#         return 0.5\n",
    "#     if x < 0.9:\n",
    "#         return 0.5 + 0.5 * (x - 0.8) / 0.1\n",
    "#     return 1\n",
    "\n",
    "\n",
    "print(distributionBase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals = np.random.randint(2, size=(trainSize, n))\n",
    "# samples1 = np.random.uniform(low=0.1, high=0.2, size=(trainSize, n))\n",
    "# samples2 = np.random.uniform(low=0.8, high=0.9, size=(trainSize, n))\n",
    "samples1 = np.random.normal(\n",
    "    loc=doublePeakLowMean, scale=doublePeakStd, size=(trainSize, n)\n",
    ")\n",
    "for i in range(trainSize):\n",
    "    for j in range(n):\n",
    "        while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "            samples1[i, j] = np.random.normal(\n",
    "                loc=doublePeakLowMean, scale=doublePeakStd\n",
    "            )\n",
    "samples2 = np.random.normal(\n",
    "    loc=doublePeakHighMean, scale=doublePeakStd, size=(trainSize, n)\n",
    ")\n",
    "for i in range(trainSize):\n",
    "    for j in range(n):\n",
    "        while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "            samples2[i, j] = np.random.normal(\n",
    "                loc=doublePeakHighMean, scale=doublePeakStd\n",
    "            )\n",
    "samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "# tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "tp_dataset = TensorDataset(tp_tensor[: round(trainSize * 0.5)])\n",
    "tp_dataset_testing = TensorDataset(tp_tensor[round(trainSize * 0.5) :])\n",
    "tp_dataloader = DataLoader(tp_dataset, batch_size=64, shuffle=True)\n",
    "tp_dataloader_testing = DataLoader(tp_dataset_testing, batch_size=128, shuffle=False)\n",
    "\n",
    "\n",
    "# for mapping binary to payments before softmax\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(n, 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, n),\n",
    ")\n",
    "\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    if sum(bits).item() == 0:\n",
    "        return torch.ones(n)\n",
    "    bits = bits.type(torch.float32)\n",
    "    negBits = torch.ones(n) - bits\n",
    "    payments = model(bits)\n",
    "    payments = payments - 1000 * negBits\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    payments = payments + negBits\n",
    "    return payments\n",
    "\n",
    "\n",
    "def tpToBits(tp, deep , bits=torch.ones(n).type(torch.float32)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = torch.sigmoid((tp-payments+0.0001)/0.000001);\n",
    "    \n",
    "    if torch.allclose(newBits, bits) or deep >= n:\n",
    "        return bits\n",
    "    else:\n",
    "        return tpToBits(tp,deep+1, newBits)\n",
    "\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp,0))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    return n - sum(tpToBits(tp,0).type(torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "tensor(0.5000)\n"
     ]
    }
   ],
   "source": [
    "print(1_000_000)\n",
    "print(cdf(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpPrecision = 100\n",
    "# howManyPpl left, money left, yes already\n",
    "dp = np.zeros([n + 1, dpPrecision + 1, n + 1])\n",
    "decision = np.zeros([n + 1, dpPrecision + 1, n + 1], dtype=np.uint8)\n",
    "# ppl = 0 left\n",
    "for yes in range(n + 1):\n",
    "    for money in range(dpPrecision + 1):\n",
    "        if money == 0:\n",
    "            dp[0, 0, yes] = 0\n",
    "        else:\n",
    "            dp[0, money, yes] = yes# + 1.0\n",
    "for ppl in range(1, n + 1):\n",
    "    for yes in range(n + 1):\n",
    "        for money in range(dpPrecision + 1):\n",
    "            minSoFar = 1_000_000\n",
    "            for offerIndex in range(money + 1):\n",
    "                offer = offerIndex / dpPrecision\n",
    "                res = (1 - cdf(offer)) * dp[\n",
    "                    ppl - 1, money - offerIndex, min(yes + 1, n)\n",
    "                ] + cdf(offer) * (1 + dp[ppl - 1, money, yes])\n",
    "                if minSoFar > res:\n",
    "                    minSoFar = res\n",
    "                    decision[ppl, money, yes] = offerIndex\n",
    "            dp[ppl, money, yes] = minSoFar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6029672622680664\n"
     ]
    }
   ],
   "source": [
    "print(dp[n, dpPrecision, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.17277808 0.60050321 0.19842925 0.15407684 0.09864626]\n",
      " [0.77984277 0.83124041 0.24105947 0.93007951 0.06622317]\n",
      " [0.26075575 0.25323873 0.81273585 0.7889999  0.8519511 ]\n",
      " ...\n",
      " [0.7347689  0.27999154 0.28087902 0.41997666 0.13640263]\n",
      " [0.19700437 0.2649288  0.43073066 0.26059002 0.20466843]\n",
      " [0.60098487 0.15776561 0.20621417 0.48105998 0.7203915 ]]\n"
     ]
    }
   ],
   "source": [
    "print(samplesJoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_dp(temp):\n",
    "    #print(temp)\n",
    "    remain=dpPrecision\n",
    "    yes=0;\n",
    "    ans =0;\n",
    "    o_list=[];\n",
    "    remain_list=[];\n",
    "    for ppl in range(n,0,-1):\n",
    "        o=decision[ppl, remain, yes]\n",
    "        #print(o,remain)\n",
    "        o_list.append(o)\n",
    "        remain_list.append(remain);\n",
    "        if(o<temp[n-ppl]):\n",
    "            remain-=int(o);\n",
    "            yes+=1;\n",
    "        elif (remain>0):\n",
    "            ans+=1;\n",
    "    if(remain<=0):\n",
    "        return ans,o_list;\n",
    "    else:\n",
    "        return n,o_list;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5916\n"
     ]
    }
   ],
   "source": [
    "ans_list=[];\n",
    "for i in range(10000):\n",
    "    temp=samplesJoint[i]*dpPrecision\n",
    "    #print(temp)\n",
    "    ans_list.append(plan_dp(temp)[0]);\n",
    "    #print(\"\\n\",temp)\n",
    "    #print(plan_dp(temp))\n",
    "print(sum(ans_list)/len(ans_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [0] * n\n",
    "    payments = [0] * n\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money, yes]\n",
    "        offer = offerIndex / dpPrecision\n",
    "        if tp[i] > offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits[i] = 0\n",
    "            payments[i] = 1\n",
    "    if money > 0:\n",
    "        bits = [0] * n\n",
    "        payments = [1] * n\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def dpDelay(tp):\n",
    "    bits, payments = dpSupervisionRule(tp)\n",
    "    totalPayment = torch.dot(bits.type(torch.float32), payments).item()\n",
    "    if totalPayment < 0.99:\n",
    "        return n\n",
    "    else:\n",
    "        return n - sum(bits).item()\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    for k in range(n, -1, -1):\n",
    "        if k == 0:\n",
    "            break\n",
    "        bits = [1 if tp[i] >= 1 / k else 0 for i in range(n)]\n",
    "        if sum(bits) == k:\n",
    "            break\n",
    "    if k == 0:\n",
    "        payments = [1] * n\n",
    "    else:\n",
    "        payments = [1 / k if bits[i] == 1 else 1 for i in range(n)]\n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    return n - sum(costSharingSupervisionRule(tp)[0]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "allBits = [torch.tensor(bits) for bits in itertools.product([0, 1], repeat=n)]\n",
    "\n",
    "runningLossNN = []\n",
    "runningLossCS = []\n",
    "runningLossDP = []\n",
    "\n",
    "\n",
    "def recordAndReport(name, source, loss, n=100):\n",
    "    source.append(loss)\n",
    "    realLength = min(n, len(source))\n",
    "    avgLoss = sum(source[-n:]) / realLength\n",
    "    print(f\"{name} ({realLength}): {avgLoss}\")\n",
    "\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        loss = 0\n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            # print()\n",
    "            # print(\"supervision\")\n",
    "            # print(tp)\n",
    "            # print(bits)\n",
    "            # print()\n",
    "            # print(payments)\n",
    "            # print(bitsToPayments(bits))\n",
    "            # print()\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        for bitsMoreOnes in allBits:\n",
    "            for i in range(n):\n",
    "                if bitsMoreOnes[i] == 1:\n",
    "                    bitsLessOnes = bitsMoreOnes.clone()\n",
    "                    bitsLessOnes[i] = 0\n",
    "                    penalty = penalty + sum(\n",
    "                        torch.relu(\n",
    "                            bitsToPayments(bitsMoreOnes) - bitsToPayments(bitsLessOnes)\n",
    "                        )\n",
    "                    )\n",
    "        loss = penalty * penaltyLambda\n",
    "        # costSharingLoss = 0\n",
    "        # dpLoss = 0\n",
    "        for tp in tp_batch:\n",
    "            # costSharingLoss += costSharingDelay(tp)\n",
    "            # dpLoss += dpDelay(tp)\n",
    "            # print()\n",
    "            # print(\"---\")\n",
    "            # print(tp)\n",
    "            # print(costSharingSupervisionRule(tp))\n",
    "            # print(dpSupervisionRule(tp))\n",
    "            # print(costSharingDelay(tp), dpDelay(tp))\n",
    "            # print()\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                delay1 = tpToTotalDelay(tp1)\n",
    "                delay0 = tpToTotalDelay(tp0)\n",
    "                loss = loss + (1 - cdf(offer)) * delay1 + cdf(offer) * delay0\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        # costSharingLoss /= len(tp_batch)\n",
    "        # dpLoss /= len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "            # recordAndReport(\"NN\", runningLossNN, loss.item())\n",
    "            # recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "            # recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "            # print(dp[n, dpPrecision, 0])\n",
    "            # print(penalty.item())\n",
    "            # print(distributionRatio)\n",
    "            # for i in range(n, 0, -1):\n",
    "            #     print(\n",
    "            #         tpToPayments(\n",
    "            #             torch.tensor([1] * i + [0] * (n - i), dtype=torch.float32)\n",
    "            #         )\n",
    "            #     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9772)\n",
      "Train Epoch: 1 [0/5000 (0%)]\tLoss: 0.000088\n",
      "Train Epoch: 1 [1280/5000 (25%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [2560/5000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [3840/5000 (76%)]\tLoss: 0.000000\n",
      "tensor(0.9772)\n",
      "Train Epoch: 2 [0/5000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [1280/5000 (25%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [2560/5000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [3840/5000 (76%)]\tLoss: 0.000000\n",
      "tensor(0.9772)\n",
      "Train Epoch: 3 [0/5000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [1280/5000 (25%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [2560/5000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [3840/5000 (76%)]\tLoss: 0.000000\n",
      "tensor(0.9772)\n",
      "Train Epoch: 4 [0/5000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [1280/5000 (25%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [2560/5000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [3840/5000 (76%)]\tLoss: 0.000000\n",
      "tensor(0.9772)\n",
      "Train Epoch: 5 [0/5000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [1280/5000 (25%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [2560/5000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [3840/5000 (76%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/5000 (0%)]\tLoss: 2.130418\n",
      "Train Epoch: 1 [1280/5000 (25%)]\tLoss: 1.532712\n",
      "Train Epoch: 1 [2560/5000 (51%)]\tLoss: 1.539047\n",
      "Train Epoch: 1 [3840/5000 (76%)]\tLoss: 1.341744\n",
      "NN (1): 1.3515625\n",
      "CS (1): 2.0390625\n",
      "DP (1): 1.515625\n",
      "1.6029672622680664\n",
      "tensor([0.1049, 0.5974, 0.1028, 0.0982, 0.0967])\n",
      "tensor([0.1298, 0.6223, 0.1261, 0.1218, 1.0000])\n",
      "tensor([0.1707, 0.6533, 0.1759, 1.0000, 1.0000])\n",
      "tensor([0.2529, 0.7471, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "NN (2): 1.53125\n",
      "CS (2): 2.1328125\n",
      "DP (2): 1.55078125\n",
      "1.6029672622680664\n",
      "tensor([0.1049, 0.5974, 0.1028, 0.0982, 0.0967])\n",
      "tensor([0.1298, 0.6223, 0.1261, 0.1218, 1.0000])\n",
      "tensor([0.1707, 0.6533, 0.1759, 1.0000, 1.0000])\n",
      "tensor([0.2529, 0.7471, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 2 [0/5000 (0%)]\tLoss: 1.591315\n",
      "Train Epoch: 2 [1280/5000 (25%)]\tLoss: 1.336270\n",
      "Train Epoch: 2 [2560/5000 (51%)]\tLoss: 1.369722\n",
      "Train Epoch: 2 [3840/5000 (76%)]\tLoss: 1.472546\n",
      "NN (3): 1.5182291269302368\n",
      "CS (3): 2.1015625\n",
      "DP (3): 1.5390625\n",
      "1.6029672622680664\n",
      "tensor([0.0503, 0.6186, 0.1052, 0.0611, 0.1648])\n",
      "tensor([0.0790, 0.6387, 0.1661, 0.1162, 1.0000])\n",
      "tensor([0.1111, 0.6863, 0.2026, 1.0000, 1.0000])\n",
      "tensor([0.2093, 0.7907, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "NN (4): 1.6328125\n",
      "CS (4): 2.1328125\n",
      "DP (4): 1.55078125\n",
      "1.6029672622680664\n",
      "tensor([0.0503, 0.6186, 0.1052, 0.0611, 0.1648])\n",
      "tensor([0.0790, 0.6387, 0.1661, 0.1162, 1.0000])\n",
      "tensor([0.1111, 0.6863, 0.2026, 1.0000, 1.0000])\n",
      "tensor([0.2093, 0.7907, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "Train Epoch: 3 [0/5000 (0%)]\tLoss: 1.847847\n",
      "Train Epoch: 3 [1280/5000 (25%)]\tLoss: 1.755886\n",
      "Train Epoch: 3 [2560/5000 (51%)]\tLoss: 1.597862\n",
      "Train Epoch: 3 [3840/5000 (76%)]\tLoss: 1.537247\n",
      "NN (5): 1.571874976158142\n",
      "CS (5): 2.1140625\n",
      "DP (5): 1.54375\n",
      "1.6029672622680664\n",
      "tensor([0.1021, 0.5850, 0.1011, 0.0946, 0.1171])\n",
      "tensor([0.1445, 0.6165, 0.1300, 0.1089, 1.0000])\n",
      "tensor([0.1849, 0.6607, 0.1544, 1.0000, 1.0000])\n",
      "tensor([0.2763, 0.7237, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "NN (6): 1.6106771230697632\n",
      "CS (6): 2.1328125\n",
      "DP (6): 1.55078125\n",
      "1.6029672622680664\n",
      "tensor([0.1021, 0.5850, 0.1011, 0.0946, 0.1171])\n",
      "tensor([0.1445, 0.6165, 0.1300, 0.1089, 1.0000])\n",
      "tensor([0.1849, 0.6607, 0.1544, 1.0000, 1.0000])\n",
      "tensor([0.2763, 0.7237, 1.0000, 1.0000, 1.0000])\n",
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (tp_batch,) in enumerate(tp_dataloader_testing):\n",
    "            costSharingLoss = 0\n",
    "            dpLoss = 0\n",
    "            nnLoss = 0\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            costSharingLoss /= len(tp_batch)\n",
    "            dpLoss /= len(tp_batch)\n",
    "            nnLoss /= len(tp_batch)\n",
    "            if batch_idx % log_interval == 0:\n",
    "                recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "                recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "                recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "                print(dp[n, dpPrecision, 0])\n",
    "                for i in range(n, 0, -1):\n",
    "                    print(\n",
    "                        tpToPayments(\n",
    "                            torch.tensor([1] * i + [0] * (n - i), dtype=torch.float32)\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "\n",
    "for epoch in range(1, supervisionEpochs + 1):\n",
    "    print(distributionRatio)\n",
    "    supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "    # supervisionTrain(epoch, dpSupervisionRule)\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
