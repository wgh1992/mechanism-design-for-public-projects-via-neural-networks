{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3540388371733616\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from random import randint\n",
    "import random\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "from pylab import *\n",
    "from scipy.stats import beta\n",
    "import math\n",
    "n = 3\n",
    "epochs = 1\n",
    "supervisionEpochs = 1\n",
    "lr = 0.0005\n",
    "log_interval = 20\n",
    "trainSize = 60000#100000\n",
    "percentage_train_test= 0.25\n",
    "penaltyLambda = 10\n",
    "doublePeakHighMean = 0.9\n",
    "doublePeakLowMean = 0.1\n",
    "doublePeakStd = 0.1\n",
    "uniformlow=0\n",
    "uniformhigh=1.0\n",
    "normalloc = 0.2\n",
    "normalscale = 0.1\n",
    "\n",
    "cauchyloc = 1/n\n",
    "cauchyscalen = 0.004\n",
    "\n",
    "exponentialhigh = 15 #Symbol(\"b\", real=True)\n",
    "exponentiallow  = 15 #Symbol(\"a\", real=True)\n",
    "\n",
    "\n",
    "beta_a = 0.1\n",
    "beta_b  = 0.1\n",
    "kumaraswamy_a = beta_a \n",
    "kumaraswamy_b = (1.0+(beta_a-1.0)*math.pow( (beta_a+beta_b-2.0)/(beta_a-1.0), beta_a) )/beta_a \n",
    "print(kumaraswamy_b)\n",
    "\n",
    "independentnormalloc1=[(float(ii)+1)/(2*n+1) for ii in range(n,0,-1)]\n",
    "independentnormalscale1=[0.05 for ii in range(n)]\n",
    "\n",
    "independentnormalloc2=[(float(ii)+1)/(2*n+1) for ii in range(1,n+1,1)]\n",
    "independentnormalscale2=[0.05 for ii in range(n)]\n",
    "stage=[\"twopeak\"]\n",
    "order=\"twopeak\"\n",
    "# \"twopeak\",\"normal\",\"uniform\",\"independent1\",\"independent2\",\"cauchy\",\"beta\",\"U-exponential\",\"arcsine\"\n",
    "order1name=[\"costsharing\",\"random initializing\",\"dp\"]\n",
    "#order1name=[\"random initializing1\",\"random initializing2\",\"random initializing3\"]\n",
    "# \"costsharing\",\"dp\",\"heuristic\",\"random initializing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio1 = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase1 = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "d3 = D.normal.Normal(normalloc, normalscale)\n",
    "distributionRatio3 = d3.cdf(1) - d3.cdf(0)\n",
    "distributionBase3 = d3.cdf(0)\n",
    "\n",
    "d4 = D.uniform.Uniform(uniformlow,uniformhigh)\n",
    "distributionRatio4 = d4.cdf(1) - d4.cdf(0)\n",
    "distributionBase4 = d4.cdf(0)\n",
    "\n",
    "d5 = [D.normal.Normal(independentnormalloc1[ii], independentnormalscale1[ii]) for ii in range(n)]\n",
    "d6 = [D.normal.Normal(independentnormalloc2[ii], independentnormalscale2[ii]) for ii in range(n)]\n",
    "\n",
    "d7 = D.cauchy.Cauchy(cauchyloc,cauchyscalen)\n",
    "\n",
    "d81 = D.exponential.Exponential(exponentiallow)\n",
    "d82 = D.exponential.Exponential(exponentialhigh)\n",
    "\n",
    "d9 = D.beta.Beta(beta_a,beta_b)\n",
    "#sample_d9=d9.rsample(torch.Size([100000]))\n",
    "\n",
    "#d9_sample=np.linspace(0.0001, 0.9999, 10000)\n",
    "#d9_pdf=torch.exp(d9.log_prob(torch.tensor(d9_sample,dtype=torch.float32)))\n",
    "#d9_delta=d9_sample[1]-d9_sample[0]\n",
    "#d9_sum_pdf=torch.sum(d9_pdf*d9_delta)\n",
    "\n",
    "d10 = D.beta.Beta(0.5,0.5)\n",
    "\n",
    "def cdf(x,y, i=None):\n",
    "    if(y==\"twopeak\"):\n",
    "        return (d1.cdf(x) + d2.cdf(x) - distributionBase1) / 2 / distributionRatio1\n",
    "    elif(y==\"normal\"):\n",
    "        return (d3.cdf(x)-distributionBase3)/distributionRatio3;\n",
    "    elif(y==\"uniform\"):\n",
    "        return (d4.cdf(x)-distributionBase4)/distributionRatio4;\n",
    "    elif(y==\"independent1\"):\n",
    "        return d5[i].cdf(x);\n",
    "    elif(y==\"independent2\"):\n",
    "        return d6[i].cdf(x);\n",
    "    elif(y==\"cauchy\"):\n",
    "        return d7.cdf(x);\n",
    "    elif(y==\"beta\"):\n",
    "#         sum_cdf=0.0;\n",
    "#         if(x<0.0001):\n",
    "#             x=0.00011;\n",
    "#         if(x>0.9999):\n",
    "#             x=0.99989;\n",
    "#         for i in range(len(d9_pdf)):\n",
    "#             if(d9_sample[i]<x):\n",
    "#                 sum_cdf+=d9_pdf[i]*d9_delta;\n",
    "#             else:\n",
    "#                 sum_cdf+=(d9_pdf[i]+d9_pdf[i-1])/ 2 *(x-d9_sample[i-1])\n",
    "#                 break;\n",
    "#         return sum_cdf/d9_sum_pdf\n",
    "#         cdf_v=torch.sum((sample_d9<(x)), dtype=torch.float32)/100000\n",
    "#         return cdf_v\n",
    "#    F(x|a,b)=1–(1–x^a)^b\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(x,kumaraswamy_a),kumaraswamy_b);\n",
    "        except:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(torch.tensor(x,dtype=torch.float32),kumaraswamy_a),kumaraswamy_b);\n",
    "    elif(y==\"arcsine\"):\n",
    "        #\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            res=2.0/math.pi * torch.asin(torch.sqrt(x))\n",
    "            #print(x)\n",
    "            return res# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "        except:\n",
    "            return 2.0/math.pi * torch.asin(torch.sqrt(torch.tensor(x,dtype=torch.float32)))# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "    elif(y==\"U-exponential\"):\n",
    "        return (d81.cdf(x) + (1.0 - d82.cdf(1.0-x)))  / 2 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2585742771625519\n",
      "2.2242771685123444\n"
     ]
    }
   ],
   "source": [
    "dpPrecision = 100\n",
    "# howManyPpl left, money left, yes already\n",
    "dp = np.zeros([n + 1, dpPrecision + 1])\n",
    "decision = np.zeros([n + 1, dpPrecision + 1], dtype=np.uint8)\n",
    "# ppl = 0 left\n",
    "for money in range(dpPrecision + 1):\n",
    "    if money == 0:\n",
    "        dp[0, 0] = 1\n",
    "    else:\n",
    "        offer = money / dpPrecision\n",
    "        dp[0, money] = 0#cdf(offer)# + 1.0\n",
    "for ppl in range(1, n + 1):\n",
    "    for money in range(dpPrecision + 1):\n",
    "        maxSoFar = -1_000_000\n",
    "        for offerIndex in range(money + 1):\n",
    "            offer = offerIndex / dpPrecision\n",
    "            res = (1-cdf(offer,order)) * dp[\n",
    "                 ppl - 1, money - offerIndex\n",
    "                ]\n",
    "            if maxSoFar < res:\n",
    "                maxSoFar = res\n",
    "                decision[ppl, money] = offerIndex\n",
    "        dp[ppl, money] = maxSoFar\n",
    "print(dp[n, dpPrecision])\n",
    "print(n*(1-dp[n, dpPrecision]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "signals = np.random.randint(2, size=(trainSize, n))\n",
    "# samples1 = np.random.uniform(low=0.1, high=0.2, size=(trainSize, n))\n",
    "# samples2 = np.random.uniform(low=0.8, high=0.9, size=(trainSize, n))\n",
    "samples1 = np.random.normal(\n",
    "    loc=doublePeakLowMean, scale=doublePeakStd, size=(trainSize, n)\n",
    ")\n",
    "for i in range(trainSize):\n",
    "    for j in range(n):\n",
    "        while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "            samples1[i, j] = np.random.normal(\n",
    "                loc=doublePeakLowMean, scale=doublePeakStd\n",
    "            )\n",
    "samples2 = np.random.normal(\n",
    "    loc=doublePeakHighMean, scale=doublePeakStd, size=(trainSize, n)\n",
    ")\n",
    "for i in range(trainSize):\n",
    "    for j in range(n):\n",
    "        while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "            samples2[i, j] = np.random.normal(\n",
    "                loc=doublePeakHighMean, scale=doublePeakStd\n",
    "            )\n",
    "samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "# tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "tp_dataset = TensorDataset(tp_tensor[: int(trainSize * percentage_train_test)])\n",
    "tp_dataset_testing = TensorDataset(tp_tensor[int(trainSize * (1.0-percentage_train_test)) :])\n",
    "tp_dataloader = DataLoader(tp_dataset, batch_size=128, shuffle=True)\n",
    "tp_dataloader_testing = DataLoader(tp_dataset_testing, batch_size=256, shuffle=False)\n",
    "plt.hist(samplesJoint,bins=500)\n",
    "plt.show()\n",
    "runningLossNN = []\n",
    "runningLossCS = []\n",
    "runningLossDP = []\n",
    "# for mapping binary to payments before softmax\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(n, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, n),\n",
    ")\n",
    "\n",
    "model.apply(init_weights)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    bits = bits.type(torch.float32)\n",
    "    payments = model(torch.ones(n))\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    return payments\n",
    "\n",
    "\n",
    "def tpToBits(tp, bits=torch.ones(n).type(torch.uint8)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = (tp >= payments).type(torch.uint8)\n",
    "    \n",
    "    if torch.equal(newBits, bits):\n",
    "        return bits\n",
    "    else:\n",
    "        return bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    if torch.sum(tpToBits(tp).type(torch.float32))==n:\n",
    "        return 0\n",
    "    else:\n",
    "        return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_dp(temp,debug):\n",
    "    if(debug==1):\n",
    "        print(temp)\n",
    "    remain=dpPrecision\n",
    "    ans =0;\n",
    "    o_list=[];\n",
    "    remain_list=[];\n",
    "    for ppl in range(n,0,-1):\n",
    "        o=decision[ppl, remain]\n",
    "        if(debug==1):\n",
    "            print(o,remain)\n",
    "        o_list.append(o)\n",
    "        remain_list.append(remain);\n",
    "        if(o<temp[n-ppl]):\n",
    "            remain-=int(o);\n",
    "        elif (remain>0):\n",
    "            ans=n;\n",
    "    if(remain<=1):\n",
    "        return ans,o_list;\n",
    "    else:\n",
    "        return n,o_list;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[94.20015963 80.90979548  8.29852422]\n",
      "79 100\n",
      "10 21\n",
      "11 11\n",
      "(3, [79, 10, 11])\n",
      "[88.68253324 10.53975971 99.73108268]\n",
      "79 100\n",
      "10 21\n",
      "11 11\n",
      "(0, [79, 10, 11])\n",
      "[ 7.32902682  4.33073842 91.21819931]\n",
      "79 100\n",
      "47 100\n",
      "0 100\n",
      "(3, [79, 47, 0])\n",
      "[95.45390608 13.03992994 87.25995751]\n",
      "79 100\n",
      "10 21\n",
      "11 11\n",
      "(0, [79, 10, 11])\n",
      "[96.26853058  4.90471606 66.99564523]\n",
      "79 100\n",
      "10 21\n",
      "21 21\n",
      "(3, [79, 10, 21])\n",
      "2.2239880059970014\n"
     ]
    }
   ],
   "source": [
    "ans_list=[];\n",
    "for i in range(5):\n",
    "    temp=samplesJoint[i]*dpPrecision\n",
    "    #print(temp)\n",
    "    tempres=plan_dp(temp,1)\n",
    "    ans_list.append(tempres[0]);\n",
    "    print(tempres)\n",
    "    #print(\"\\n\",temp)\n",
    "    #print(plan_dp(temp,1))\n",
    "for i in range(10000):\n",
    "    temp=samplesJoint[i]*dpPrecision\n",
    "    #print(temp)\n",
    "    ans_list.append(plan_dp(temp,0)[0]);\n",
    "    #print(\"\\n\",temp)\n",
    "    #print(plan_dp(temp))\n",
    "print(sum(ans_list)/len(ans_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0 for ii in range(n)]\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money]\n",
    "        offer = offerIndex / dpPrecision\n",
    "        if tp[i] >= offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits = [0 for ii in range(n)]\n",
    "            payments = [1 for ii in range(n)]\n",
    "            money=1\n",
    "            #bits[i] = 0\n",
    "            #payments[i] = 0#1\n",
    "            break\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1 for ii in range(n)]\n",
    "\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    #for k in range(n, -1, -1):\n",
    "    k=n;\n",
    "    bits = [1 if tp[ii] >= 1 / k else 0 for ii in range(n)]\n",
    "    payments = [1 / k  for ii in range(n)]\n",
    "        \n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    if torch.sum(costSharingSupervisionRule(tp)[0]).item() == n:\n",
    "        return 0\n",
    "    else:\n",
    "        return n;\n",
    "\n",
    "def dpDelay(tp):\n",
    "    if torch.sum(dpSupervisionRule(tp)[0]).item() == n:\n",
    "        return 0\n",
    "    else:\n",
    "        return n;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.6847e-01, 7.5347e-05, 9.8409e-01])\n",
      "3\n",
      "(tensor([0, 0, 1], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "1\n",
      "\n",
      "\n",
      "tensor([0.9978, 0.9872, 0.1631])\n",
      "3\n",
      "(tensor([1, 1, 0], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "2\n",
      "\n",
      "\n",
      "tensor([0.0317, 0.1240, 0.8726])\n",
      "3\n",
      "(tensor([0, 0, 1], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "1\n",
      "\n",
      "\n",
      "tensor([0.1971, 0.9260, 0.0434])\n",
      "3\n",
      "(tensor([0, 1, 0], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "1\n",
      "\n",
      "\n",
      "tensor([0.8662, 0.8711, 0.8143])\n",
      "0\n",
      "(tensor([1, 1, 1], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "3\n",
      "\n",
      "\n",
      "tensor([0.8512, 0.1408, 0.8403])\n",
      "3\n",
      "(tensor([1, 0, 1], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "2\n",
      "\n",
      "\n",
      "tensor([0.0736, 0.8423, 0.0553])\n",
      "3\n",
      "(tensor([0, 1, 0], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "1\n",
      "\n",
      "\n",
      "tensor([0.8835, 0.9760, 0.0087])\n",
      "3\n",
      "(tensor([1, 1, 0], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "2\n",
      "\n",
      "\n",
      "tensor([0.9089, 0.8796, 0.1120])\n",
      "3\n",
      "(tensor([1, 1, 0], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "2\n",
      "\n",
      "\n",
      "tensor([0.0273, 0.7009, 0.0881])\n",
      "3\n",
      "(tensor([0, 1, 0], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "1\n",
      "\n",
      "\n",
      "tensor([0.1140, 0.0012, 0.1207])\n",
      "3\n",
      "(tensor([0, 0, 0], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "0\n",
      "\n",
      "\n",
      "tensor([0.9654, 0.9265, 0.1879])\n",
      "3\n",
      "(tensor([1, 1, 0], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "2\n",
      "\n",
      "\n",
      "tensor([0.8553, 0.0211, 0.2951])\n",
      "3\n",
      "(tensor([1, 0, 0], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "1\n",
      "\n",
      "\n",
      "tensor([0.2097, 0.3389, 0.0683])\n",
      "3\n",
      "(tensor([0, 1, 0], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "1\n",
      "\n",
      "\n",
      "tensor([0.1120, 0.1975, 0.7838])\n",
      "3\n",
      "(tensor([0, 0, 1], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "1\n",
      "\n",
      "\n",
      "tensor([0.0288, 0.1489, 0.9368])\n",
      "3\n",
      "(tensor([0, 0, 1], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "1\n",
      "\n",
      "\n",
      "tensor([0.9597, 0.1825, 0.2894])\n",
      "3\n",
      "(tensor([1, 0, 0], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "1\n",
      "\n",
      "\n",
      "tensor([0.1109, 0.0034, 0.8786])\n",
      "3\n",
      "(tensor([0, 0, 1], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "1\n",
      "\n",
      "\n",
      "tensor([0.9403, 0.0769, 0.7460])\n",
      "3\n",
      "(tensor([1, 0, 1], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "2\n",
      "\n",
      "\n",
      "tensor([0.6212, 0.1048, 0.8593])\n",
      "3\n",
      "(tensor([1, 0, 1], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "2\n",
      "\n",
      "\n",
      "tensor([0.1275, 0.0807, 0.1176])\n",
      "3\n",
      "(tensor([0, 0, 0], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "0\n",
      "\n",
      "\n",
      "tensor([0.0836, 0.7893, 0.9386])\n",
      "3\n",
      "(tensor([0, 1, 1], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "2\n",
      "\n",
      "\n",
      "tensor([0.1833, 0.1409, 0.8684])\n",
      "3\n",
      "(tensor([0, 0, 1], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "1\n",
      "\n",
      "\n",
      "tensor([0.1271, 0.9139, 0.9575])\n",
      "3\n",
      "(tensor([0, 1, 1], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "2\n",
      "\n",
      "\n",
      "tensor([0.1048, 0.1937, 0.2862])\n",
      "3\n",
      "(tensor([0, 0, 0], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "0\n",
      "\n",
      "\n",
      "tensor([0.1579, 0.8910, 0.9328])\n",
      "3\n",
      "(tensor([0, 1, 1], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "2\n",
      "\n",
      "\n",
      "tensor([0.1716, 0.9527, 0.9818])\n",
      "3\n",
      "(tensor([0, 1, 1], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "2\n",
      "\n",
      "\n",
      "tensor([0.7837, 0.8520, 0.1347])\n",
      "3\n",
      "(tensor([1, 1, 0], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "2\n",
      "\n",
      "\n",
      "tensor([0.2904, 0.8852, 0.0460])\n",
      "3\n",
      "(tensor([0, 1, 0], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "1\n",
      "\n",
      "\n",
      "tensor([0.0798, 0.2910, 0.0608])\n",
      "3\n",
      "(tensor([0, 0, 0], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "0\n",
      "\n",
      "\n",
      "tensor([0.1021, 0.9258, 0.6683])\n",
      "3\n",
      "(tensor([0, 1, 1], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "2\n",
      "\n",
      "\n",
      "tensor([0.9639, 0.2036, 0.8875])\n",
      "3\n",
      "(tensor([1, 0, 1], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "2\n",
      "\n",
      "\n",
      "tensor([0.1582, 0.0698, 0.8187])\n",
      "3\n",
      "(tensor([0, 0, 1], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "1\n",
      "\n",
      "\n",
      "tensor([0.0802, 0.1676, 0.0920])\n",
      "3\n",
      "(tensor([0, 0, 0], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "0\n",
      "\n",
      "\n",
      "tensor([0.8169, 0.1010, 0.8291])\n",
      "3\n",
      "(tensor([1, 0, 1], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "2\n",
      "\n",
      "\n",
      "tensor([0.7946, 0.8225, 0.1387])\n",
      "3\n",
      "(tensor([1, 1, 0], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "2\n",
      "\n",
      "\n",
      "tensor([0.9413, 0.0532, 0.0944])\n",
      "3\n",
      "(tensor([1, 0, 0], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "1\n",
      "\n",
      "\n",
      "tensor([0.8186, 0.7379, 0.1287])\n",
      "3\n",
      "(tensor([1, 1, 0], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "2\n",
      "\n",
      "\n",
      "tensor([0.2968, 0.1526, 0.9437])\n",
      "3\n",
      "(tensor([0, 0, 1], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "1\n",
      "\n",
      "\n",
      "tensor([0.8223, 0.8678, 0.6373])\n",
      "0\n",
      "(tensor([1, 1, 1], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "3\n",
      "\n",
      "\n",
      "tensor([0.6174, 0.0734, 0.1831])\n",
      "3\n",
      "(tensor([1, 0, 0], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "1\n",
      "\n",
      "\n",
      "tensor([0.9005, 0.7655, 0.2701])\n",
      "3\n",
      "(tensor([1, 1, 0], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "2\n",
      "\n",
      "\n",
      "tensor([0.0635, 0.9572, 0.1662])\n",
      "3\n",
      "(tensor([0, 1, 0], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "1\n",
      "\n",
      "\n",
      "tensor([0.9138, 0.9616, 0.1203])\n",
      "3\n",
      "(tensor([1, 1, 0], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "2\n",
      "\n",
      "\n",
      "tensor([0.0548, 0.8359, 0.2360])\n",
      "3\n",
      "(tensor([0, 1, 0], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "1\n",
      "\n",
      "\n",
      "tensor([0.8228, 0.0225, 0.0146])\n",
      "3\n",
      "(tensor([1, 0, 0], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "1\n",
      "\n",
      "\n",
      "tensor([0.8727, 0.7722, 0.8021])\n",
      "0\n",
      "(tensor([1, 1, 1], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "3\n",
      "\n",
      "\n",
      "tensor([0.7830, 0.8791, 0.7610])\n",
      "0\n",
      "(tensor([1, 1, 1], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "3\n",
      "\n",
      "\n",
      "tensor([0.0813, 0.7887, 0.7169])\n",
      "3\n",
      "(tensor([0, 1, 1], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "2\n",
      "\n",
      "\n",
      "tensor([0.1313, 0.0786, 0.2344])\n",
      "3\n",
      "(tensor([0, 0, 0], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "0\n",
      "\n",
      "\n",
      "tensor([0.0744, 0.2403, 0.0972])\n",
      "3\n",
      "(tensor([0, 0, 0], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "0\n",
      "\n",
      "\n",
      "tensor([0.7852, 0.1771, 0.8951])\n",
      "3\n",
      "(tensor([1, 0, 1], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "2\n",
      "\n",
      "\n",
      "tensor([0.6921, 0.0777, 0.0085])\n",
      "3\n",
      "(tensor([1, 0, 0], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "1\n",
      "\n",
      "\n",
      "tensor([0.9179, 0.1700, 0.2859])\n",
      "3\n",
      "(tensor([1, 0, 0], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "1\n",
      "\n",
      "\n",
      "tensor([0.8446, 0.1586, 0.1200])\n",
      "3\n",
      "(tensor([1, 0, 0], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "1\n",
      "\n",
      "\n",
      "tensor([0.9015, 0.0978, 0.7346])\n",
      "3\n",
      "(tensor([1, 0, 1], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "2\n",
      "\n",
      "\n",
      "tensor([0.0485, 0.9871, 0.8487])\n",
      "3\n",
      "(tensor([0, 1, 1], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "2\n",
      "\n",
      "\n",
      "tensor([0.0724, 0.7832, 0.9424])\n",
      "3\n",
      "(tensor([0, 1, 1], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "2\n",
      "\n",
      "\n",
      "tensor([0.8247, 0.2645, 0.1631])\n",
      "3\n",
      "(tensor([1, 0, 0], dtype=torch.uint8), tensor([0.3333, 0.3333, 0.3333]))\n",
      "1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (tp_batch,) in enumerate(tp_dataloader_testing):\n",
    "    print(tp_batch[0])\n",
    "    print(costSharingDelay(tp_batch[0]))\n",
    "    print(costSharingSupervisionRule(tp_batch[0]))\n",
    "    print(torch.sum(costSharingSupervisionRule(tp_batch[0])[0]).item())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0, 0, 0]), tensor([0, 0, 1]), tensor([0, 1, 0]), tensor([0, 1, 1]), tensor([1, 0, 0]), tensor([1, 0, 1]), tensor([1, 1, 0]), tensor([1, 1, 1])]\n",
      "\n",
      "tensor([0.9044, 0.1242, 0.9773])\n",
      "tensor(0.3495, grad_fn=<SelectBackward>)\n",
      "tensor([0.3495, 0.2747, 0.3758], grad_fn=<SoftmaxBackward>)\n",
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "allBits = [torch.tensor(bits) for bits in itertools.product([0, 1], repeat=n)]\n",
    "print(allBits)\n",
    "\n",
    "for batch_idx, (tp_batch,) in enumerate(tp_dataloader_testing):\n",
    "    penalty = 0\n",
    "    loss = penalty * penaltyLambda\n",
    "    for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                delay1 = tpToTotalDelay(tp1)\n",
    "                delay0 = tpToTotalDelay(tp0)\n",
    "                #loss = loss + (1 - cdf(offer)) * delay1 + cdf(offer) * delay0\n",
    "                loss = loss + (1 - cdf(offer,order)) * delay1 + cdf(offer,order) * delay0\n",
    "    print()\n",
    "    print(tp)\n",
    "    tp1 = tp.clone()\n",
    "    tp1[0] = 1\n",
    "    tp0 = tp.clone()\n",
    "    tp0[0] = 0\n",
    "    offer = tpToPayments(tp1)[0]\n",
    "    print(offer)\n",
    "    print(tpToPayments(tp1))\n",
    "    print(delay1)\n",
    "    print(delay0)\n",
    "    break\n",
    "#print(loss)\n",
    "#print(penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordAndReport(name, source, loss):\n",
    "    source.append(loss)\n",
    "    realLength = len(source)\n",
    "    #print(f\"{name} ({realLength}): {loss}\")\n",
    "    print(name,realLength,\":\" ,loss)\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        loss = penalty * penaltyLambda /100000\n",
    "        \n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            #print(\"bits\",bitsToPayments(bits))\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "losslist=[];\n",
    "losslistname=[];\n",
    "losslisttemp=[];\n",
    "def test_batch():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lenLoss= 0\n",
    "        nnLoss = 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "            for tp in tp_batch:\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        nnLoss/=lenLoss\n",
    "    return nnLoss\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                delay1 = tpToTotalDelay(tp1)\n",
    "                delay0 = tpToTotalDelay(tp0)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                    loss = loss + (1 - cdf(offer,order)) * delay1 + cdf(offer,order) * delay0\n",
    "                else:\n",
    "                    loss = loss + (1 - cdf(offer,order,i)) * delay1 + cdf(offer,order,i) * delay0\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    \n",
    "allBits = [torch.tensor(bits, dtype=torch.int16) for bits in itertools.product([0, 1], repeat=n)]\n",
    "\n",
    "def test():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        costSharingLoss = 0\n",
    "        dpLoss = 0\n",
    "        nnLoss = 0\n",
    "        lenLoss= 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        costSharingLoss /= lenLoss\n",
    "        dpLoss /= lenLoss\n",
    "        nnLoss /= lenLoss\n",
    "        #print(lenLoss)\n",
    "        losslisttemp.append(nnLoss);\n",
    "        recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "        recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "        recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "        print(\"DP:\",n*(1-dp[n, dpPrecision]))\n",
    "        #for i in range(n, 0, -1):\n",
    "        #    print(\"Heuristic:\",i,5*(1-dp_H[i, i, dpPrecision]))\n",
    "        for i in range(n, 0, -1):\n",
    "            print(\n",
    "                    tpToPayments(\n",
    "                            torch.tensor([0 if ii >= i else 1 for ii in range(n)], dtype=torch.float32)\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "dpPrecision = 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8413)\n",
      "Train Epoch: 1 [0/15000 (0%)]\tLoss: 0.001837\n",
      "Train Epoch: 1 [2560/15000 (17%)]\tLoss: 0.000044\n",
      "Train Epoch: 1 [5120/15000 (34%)]\tLoss: 0.000012\n",
      "Train Epoch: 1 [7680/15000 (51%)]\tLoss: 0.000001\n",
      "Train Epoch: 1 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "NN 1 : 2.6066\n",
      "CS 1 : 2.6066\n",
      "DP 1 : 2.2132\n",
      "DP: 2.2242771685123444\n",
      "tensor([0.3333, 0.3333, 0.3334])\n",
      "tensor([0.3333, 0.3333, 0.3334])\n",
      "tensor([0.3333, 0.3333, 0.3334])\n",
      "Train Epoch: 1 [0/15000 (0%)]\tLoss: 2.545542 testing loss: 2.6078\n",
      "Train Epoch: 1 [2560/15000 (17%)]\tLoss: 2.217364 testing loss: 2.2732\n",
      "Train Epoch: 1 [5120/15000 (34%)]\tLoss: 2.262953 testing loss: 2.2194\n",
      "Train Epoch: 1 [7680/15000 (51%)]\tLoss: 2.227471 testing loss: 2.2166\n",
      "Train Epoch: 1 [10240/15000 (68%)]\tLoss: 2.246026 testing loss: 2.2212\n",
      "Train Epoch: 1 [12800/15000 (85%)]\tLoss: 2.223267 testing loss: 2.2156\n",
      "NN 2 : 2.215\n",
      "CS 2 : 2.6066\n",
      "DP 2 : 2.2132\n",
      "DP: 2.2242771685123444\n",
      "tensor([0.7956, 0.1023, 0.1021])\n",
      "tensor([0.7956, 0.1023, 0.1021])\n",
      "tensor([0.7956, 0.1023, 0.1021])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'order1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-5162d3bce20a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mlosslistname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0morder1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mlosslist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosslisttemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mlosslisttemp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'order1' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, supervisionEpochs + 1):\n",
    "    print(distributionRatio1)\n",
    "    supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "    #supervisionTrain(epoch, dpSupervisionRule)\n",
    "test()\n",
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    train(epoch)\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'order1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-da3a47955f66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlosslistname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0morder1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlosslist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosslisttemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlosslisttemp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'order1' is not defined"
     ]
    }
   ],
   "source": [
    "order1=\"\"\n",
    "losslistname.append(order+\" \"+order1);\n",
    "losslist.append(losslisttemp);\n",
    "losslisttemp=[];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorlist=[\"#D2691E\",'#4169E1',\"#9ACD32\",\"#B22222\",\"#FF00FF\",\"#708090\"]\n",
    "for i in range(len(losslist)):\n",
    "    plt.plot(losslist[i], 'ro-', color=colorlist[i], alpha=0.8, label=losslistname[i])\n",
    "\n",
    "# 显示标签，如果不加这句，即使加了label='一些数字'的参数，最终还是不会显示标签\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('trianing times')\n",
    "plt.ylabel('delay')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
