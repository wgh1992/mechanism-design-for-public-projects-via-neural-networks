{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3540388371733616\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from random import randint\n",
    "import random\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "global  samplesJoint,tp_dataloader,tp_dataloader_testing,dp,decision,dp_H,decision_H\n",
    "from pylab import *\n",
    "from scipy.stats import beta\n",
    "import math\n",
    "n = 5\n",
    "epochs = 1\n",
    "supervisionEpochs = 1\n",
    "lr = 0.0005\n",
    "log_interval = 20\n",
    "trainSize = 60000#100000\n",
    "percentage_train_test= 0.25\n",
    "penaltyLambda = 10\n",
    "doublePeakHighMean = 0.9\n",
    "doublePeakLowMean = 0.1\n",
    "doublePeakStd = 0.1\n",
    "uniformlow=0\n",
    "uniformhigh=1.0\n",
    "normalloc = 0.2\n",
    "normalscale = 0.1\n",
    "\n",
    "cauchyloc = 1/n\n",
    "cauchyscalen = 0.004\n",
    "\n",
    "exponentialhigh = 15 #Symbol(\"b\", real=True)\n",
    "exponentiallow  = 15 #Symbol(\"a\", real=True)\n",
    "\n",
    "\n",
    "beta_a = 0.1\n",
    "beta_b  = 0.1\n",
    "kumaraswamy_a = beta_a \n",
    "kumaraswamy_b = (1.0+(beta_a-1.0)*math.pow( (beta_a+beta_b-2.0)/(beta_a-1.0), beta_a) )/beta_a \n",
    "print(kumaraswamy_b)\n",
    "\n",
    "independentnormalloc1=[(float(ii)+1)/(2*n+1) for ii in range(n,0,-1)]\n",
    "independentnormalscale1=[0.05 for ii in range(n)]\n",
    "\n",
    "independentnormalloc2=[(float(ii)+1)/(2*n+1) for ii in range(1,n+1,1)]\n",
    "independentnormalscale2=[0.05 for ii in range(n)]\n",
    "stage=[\"twopeak\"]\n",
    "order=\"twopeak\"\n",
    "# \"twopeak\",\"normal\",\"uniform\",\"independent1\",\"independent2\",\"cauchy\",\"beta\",\"U-exponential\",\"arcsine\"\n",
    "order1name=[\"costsharing\",\"random initializing\",\"dp\"]\n",
    "#order1name=[\"random initializing1\",\"random initializing2\",\"random initializing3\"]\n",
    "# \"costsharing\",\"dp\",\"heuristic\",\"random initializing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = D.normal.Normal(doublePeakLowMean, doublePeakStd)\n",
    "d2 = D.normal.Normal(doublePeakHighMean, doublePeakStd)\n",
    "distributionRatio1 = (d1.cdf(1) + d2.cdf(1) - d1.cdf(0) - d2.cdf(0)) / 2\n",
    "distributionBase1 = d1.cdf(0) + d2.cdf(0)\n",
    "\n",
    "d3 = D.normal.Normal(normalloc, normalscale)\n",
    "distributionRatio3 = d3.cdf(1) - d3.cdf(0)\n",
    "distributionBase3 = d3.cdf(0)\n",
    "\n",
    "d4 = D.uniform.Uniform(uniformlow,uniformhigh)\n",
    "distributionRatio4 = d4.cdf(1) - d4.cdf(0)\n",
    "distributionBase4 = d4.cdf(0)\n",
    "\n",
    "d5 = [D.normal.Normal(independentnormalloc1[ii], independentnormalscale1[ii]) for ii in range(n)]\n",
    "d6 = [D.normal.Normal(independentnormalloc2[ii], independentnormalscale2[ii]) for ii in range(n)]\n",
    "\n",
    "d7 = D.cauchy.Cauchy(cauchyloc,cauchyscalen)\n",
    "\n",
    "d81 = D.exponential.Exponential(exponentiallow)\n",
    "d82 = D.exponential.Exponential(exponentialhigh)\n",
    "\n",
    "d9 = D.beta.Beta(beta_a,beta_b)\n",
    "#sample_d9=d9.rsample(torch.Size([100000]))\n",
    "\n",
    "#d9_sample=np.linspace(0.0001, 0.9999, 10000)\n",
    "#d9_pdf=torch.exp(d9.log_prob(torch.tensor(d9_sample,dtype=torch.float32)))\n",
    "#d9_delta=d9_sample[1]-d9_sample[0]\n",
    "#d9_sum_pdf=torch.sum(d9_pdf*d9_delta)\n",
    "\n",
    "d10 = D.beta.Beta(0.5,0.5)\n",
    "\n",
    "def cdf(x,y, i=None):\n",
    "    if(y==\"twopeak\"):\n",
    "        return (d1.cdf(x) + d2.cdf(x) - distributionBase1) / 2 / distributionRatio1\n",
    "    elif(y==\"normal\"):\n",
    "        return (d3.cdf(x)-distributionBase3)/distributionRatio3;\n",
    "    elif(y==\"uniform\"):\n",
    "        return (d4.cdf(x)-distributionBase4)/distributionRatio4;\n",
    "    elif(y==\"independent1\"):\n",
    "        return d5[i].cdf(x);\n",
    "    elif(y==\"independent2\"):\n",
    "        return d6[i].cdf(x);\n",
    "    elif(y==\"cauchy\"):\n",
    "        return d7.cdf(x);\n",
    "    elif(y==\"beta\"):\n",
    "#         sum_cdf=0.0;\n",
    "#         if(x<0.0001):\n",
    "#             x=0.00011;\n",
    "#         if(x>0.9999):\n",
    "#             x=0.99989;\n",
    "#         for i in range(len(d9_pdf)):\n",
    "#             if(d9_sample[i]<x):\n",
    "#                 sum_cdf+=d9_pdf[i]*d9_delta;\n",
    "#             else:\n",
    "#                 sum_cdf+=(d9_pdf[i]+d9_pdf[i-1])/ 2 *(x-d9_sample[i-1])\n",
    "#                 break;\n",
    "#         return sum_cdf/d9_sum_pdf\n",
    "#         cdf_v=torch.sum((sample_d9<(x)), dtype=torch.float32)/100000\n",
    "#         return cdf_v\n",
    "#    F(x|a,b)=1–(1–x^a)^b\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(x,kumaraswamy_a),kumaraswamy_b);\n",
    "        except:\n",
    "            return 1.0-torch.pow(1.0-torch.pow(torch.tensor(x,dtype=torch.float32),kumaraswamy_a),kumaraswamy_b);\n",
    "    elif(y==\"arcsine\"):\n",
    "        #\n",
    "        if(x<0.0000001):\n",
    "            x=0.0000001\n",
    "        elif(x >0.9999999):\n",
    "            x=0.9999999\n",
    "        try:\n",
    "            res=2.0/math.pi * torch.asin(torch.sqrt(x))\n",
    "            #print(x)\n",
    "            return res# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "        except:\n",
    "            return 2.0/math.pi * torch.asin(torch.sqrt(torch.tensor(x,dtype=torch.float32)))# + 0.0001*1.0/(\n",
    "            #math.pi * torch.sqrt(torch.tensor(x)*torch.tensor(1.0-x)))\n",
    "    elif(y==\"U-exponential\"):\n",
    "        return (d81.cdf(x) + (1.0 - d82.cdf(1.0-x)))  / 2 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28367453813552856\n",
      "3.581627309322357\n"
     ]
    }
   ],
   "source": [
    "dpPrecision = 100\n",
    "# howManyPpl left, money left, yes already\n",
    "dp = np.zeros([n + 1, dpPrecision + 1])\n",
    "decision = np.zeros([n + 1, dpPrecision + 1], dtype=np.uint8)\n",
    "# ppl = 0 left\n",
    "for money in range(dpPrecision + 1):\n",
    "    if money == 0:\n",
    "        dp[0, 0] = 1\n",
    "    else:\n",
    "        offer = money / dpPrecision\n",
    "        dp[0, money] = 0#cdf(offer)# + 1.0\n",
    "for ppl in range(1, n + 1):\n",
    "    for money in range(dpPrecision + 1):\n",
    "        maxSoFar = -1_000_000\n",
    "        for offerIndex in range(money + 1):\n",
    "            offer = offerIndex / dpPrecision\n",
    "            res = (1-cdf(offer,order)) * dp[\n",
    "                 ppl - 1, money - offerIndex\n",
    "                ]\n",
    "            if maxSoFar < res:\n",
    "                maxSoFar = res\n",
    "                decision[ppl, money] = offerIndex\n",
    "        dp[ppl, money] = maxSoFar\n",
    "print(dp[n, dpPrecision])\n",
    "print(n*(1-dp[n, dpPrecision]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "signals = np.random.randint(2, size=(trainSize, n))\n",
    "# samples1 = np.random.uniform(low=0.1, high=0.2, size=(trainSize, n))\n",
    "# samples2 = np.random.uniform(low=0.8, high=0.9, size=(trainSize, n))\n",
    "samples1 = np.random.normal(\n",
    "    loc=doublePeakLowMean, scale=doublePeakStd, size=(trainSize, n)\n",
    ")\n",
    "for i in range(trainSize):\n",
    "    for j in range(n):\n",
    "        while samples1[i, j] < 0 or samples1[i, j] > 1:\n",
    "            samples1[i, j] = np.random.normal(\n",
    "                loc=doublePeakLowMean, scale=doublePeakStd\n",
    "            )\n",
    "samples2 = np.random.normal(\n",
    "    loc=doublePeakHighMean, scale=doublePeakStd, size=(trainSize, n)\n",
    ")\n",
    "for i in range(trainSize):\n",
    "    for j in range(n):\n",
    "        while samples2[i, j] < 0 or samples2[i, j] > 1:\n",
    "            samples2[i, j] = np.random.normal(\n",
    "                loc=doublePeakHighMean, scale=doublePeakStd\n",
    "            )\n",
    "samplesJoint = signals * samples1 - (signals - 1) * samples2\n",
    "tp_tensor = torch.tensor(samplesJoint, dtype=torch.float32)\n",
    "# tp_tensor = torch.tensor(np.random.rand(10000, n), dtype=torch.float32)\n",
    "tp_dataset = TensorDataset(tp_tensor[: int(trainSize * percentage_train_test)])\n",
    "tp_dataset_testing = TensorDataset(tp_tensor[int(trainSize * (1.0-percentage_train_test)) :])\n",
    "tp_dataloader = DataLoader(tp_dataset, batch_size=128, shuffle=True)\n",
    "tp_dataloader_testing = DataLoader(tp_dataset_testing, batch_size=256, shuffle=False)\n",
    "plt.hist(samplesJoint,bins=500)\n",
    "plt.show()\n",
    "runningLossNN = []\n",
    "runningLossCS = []\n",
    "runningLossDP = []\n",
    "# for mapping binary to payments before softmax\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(n, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, n),\n",
    ")\n",
    "\n",
    "model.apply(init_weights)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitsToPayments(bits):\n",
    "    bits = bits.type(torch.float32)\n",
    "    payments = model(torch.ones(n))\n",
    "    payments = torch.softmax(payments, 0)\n",
    "    return payments\n",
    "\n",
    "\n",
    "def tpToBits(tp, bits=torch.ones(n).type(torch.uint8)):\n",
    "    payments = bitsToPayments(bits)\n",
    "    newBits = (tp >= payments).type(torch.uint8)\n",
    "    \n",
    "    if torch.equal(newBits, bits):\n",
    "        return bits\n",
    "    else:\n",
    "        return bits-bits#tpToBits(tp, newBits)\n",
    "\n",
    "\n",
    "def tpToPayments(tp):\n",
    "    return bitsToPayments(tpToBits(tp))\n",
    "\n",
    "\n",
    "def tpToTotalDelay(tp):\n",
    "    if torch.sum(tpToBits(tp).type(torch.float32))==n:\n",
    "        return 0\n",
    "    else:\n",
    "        return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_dp(temp,debug):\n",
    "    if(debug==1):\n",
    "        print(temp)\n",
    "    remain=dpPrecision\n",
    "    ans =0;\n",
    "    o_list=[];\n",
    "    remain_list=[];\n",
    "    for ppl in range(n,0,-1):\n",
    "        o=decision[ppl, remain]\n",
    "        if(debug==1):\n",
    "            print(o,remain)\n",
    "        o_list.append(o)\n",
    "        remain_list.append(remain);\n",
    "        if(o<temp[n-ppl]):\n",
    "            remain-=int(o);\n",
    "        elif (remain>0):\n",
    "            ans=n;\n",
    "    if(remain<=1):\n",
    "        return ans,o_list;\n",
    "    else:\n",
    "        return n,o_list;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.07327407  3.55773461 98.64847479 12.34379118  8.82231971]\n",
      "5 100\n",
      "8 100\n",
      "79 100\n",
      "10 21\n",
      "11 11\n",
      "(5, [5, 8, 79, 10, 11])\n",
      "[ 7.10167689 79.33565798  3.48885382 62.40640129 94.01266983]\n",
      "5 100\n",
      "6 95\n",
      "6 89\n",
      "10 89\n",
      "79 79\n",
      "(5, [5, 6, 6, 10, 79])\n",
      "[87.94855781 25.07534359  8.22919637 21.77453514 79.19202246]\n",
      "5 100\n",
      "6 95\n",
      "6 89\n",
      "6 83\n",
      "77 77\n",
      "(0, [5, 6, 6, 6, 77])\n",
      "[98.70857311 90.36541931 11.01988101 77.19655177 16.25423961]\n",
      "5 100\n",
      "6 95\n",
      "6 89\n",
      "6 83\n",
      "77 77\n",
      "(5, [5, 6, 6, 6, 77])\n",
      "[20.87346774  3.62305364 77.29017809 84.27153745 10.93234057]\n",
      "5 100\n",
      "6 95\n",
      "9 95\n",
      "8 86\n",
      "78 78\n",
      "(5, [5, 6, 9, 8, 78])\n",
      "3.5742128935532236\n"
     ]
    }
   ],
   "source": [
    "ans_list=[];\n",
    "for i in range(5):\n",
    "    temp=samplesJoint[i]*dpPrecision\n",
    "    #print(temp)\n",
    "    tempres=plan_dp(temp,1)\n",
    "    ans_list.append(tempres[0]);\n",
    "    print(tempres)\n",
    "    #print(\"\\n\",temp)\n",
    "    #print(plan_dp(temp,1))\n",
    "for i in range(10000):\n",
    "    temp=samplesJoint[i]*dpPrecision\n",
    "    #print(temp)\n",
    "    ans_list.append(plan_dp(temp,0)[0]);\n",
    "    #print(\"\\n\",temp)\n",
    "    #print(plan_dp(temp))\n",
    "print(sum(ans_list)/len(ans_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    bits = [1 for ii in range(n)]\n",
    "    payments = [0 for ii in range(n)]\n",
    "    money = dpPrecision\n",
    "    yes = 0\n",
    "    for i in range(n):\n",
    "        offerIndex = decision[n - i, money]\n",
    "        offer = offerIndex / dpPrecision\n",
    "        if tp[i] >= offer:\n",
    "            money -= offerIndex\n",
    "            yes += 1\n",
    "            bits[i] = 1\n",
    "            payments[i] = offer\n",
    "        else:\n",
    "            bits = [0 for ii in range(n)]\n",
    "            payments = [1 for ii in range(n)]\n",
    "            money=1\n",
    "            #bits[i] = 0\n",
    "            #payments[i] = 0#1\n",
    "            break\n",
    "    if money > 0:\n",
    "        bits = [0 for ii in range(n)]\n",
    "        payments = [1 for ii in range(n)]\n",
    "\n",
    "    bits = torch.tensor(bits, dtype=torch.float32)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    # print()\n",
    "    # print(tp)\n",
    "    # print(bits)\n",
    "    # print(payments)\n",
    "    # print()\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def costSharingSupervisionRule(tp):\n",
    "    tp = list(tp.numpy())\n",
    "    #for k in range(n, -1, -1):\n",
    "    k=n;\n",
    "    bits = [1 if tp[ii] >= 1 / k else 0 for ii in range(n)]\n",
    "    payments = [1 / k  for ii in range(n)]\n",
    "        \n",
    "    bits = torch.tensor(bits, dtype=torch.uint8)\n",
    "    payments = torch.tensor(payments, dtype=torch.float32)\n",
    "    return (bits, payments)\n",
    "\n",
    "\n",
    "def costSharingDelay(tp):\n",
    "    if torch.sum(costSharingSupervisionRule(tp)[0]).item() == n:\n",
    "        return 0\n",
    "    else:\n",
    "        return n;\n",
    "\n",
    "def dpDelay(tp):\n",
    "    if torch.sum(dpSupervisionRule(tp)[0]).item() == n:\n",
    "        return 0\n",
    "    else:\n",
    "        return n;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8628, 0.7967, 0.1664, 0.6795, 0.7700])\n",
      "5\n",
      "(tensor([1, 1, 0, 1, 1], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "4\n",
      "\n",
      "\n",
      "tensor([0.9741, 0.0798, 0.8488, 0.1446, 0.8869])\n",
      "5\n",
      "(tensor([1, 0, 1, 0, 1], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "3\n",
      "\n",
      "\n",
      "tensor([0.2082, 0.9383, 0.9009, 0.1500, 0.0217])\n",
      "5\n",
      "(tensor([1, 1, 1, 0, 0], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "3\n",
      "\n",
      "\n",
      "tensor([0.2569, 0.8435, 0.0941, 0.7633, 0.1178])\n",
      "5\n",
      "(tensor([1, 1, 0, 1, 0], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "3\n",
      "\n",
      "\n",
      "tensor([0.9127, 0.0427, 0.0564, 0.9048, 0.1343])\n",
      "5\n",
      "(tensor([1, 0, 0, 1, 0], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "2\n",
      "\n",
      "\n",
      "tensor([0.1639, 0.8913, 0.0566, 0.8944, 0.7195])\n",
      "5\n",
      "(tensor([0, 1, 0, 1, 1], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "3\n",
      "\n",
      "\n",
      "tensor([0.2431, 0.7831, 0.0100, 0.8139, 0.8602])\n",
      "5\n",
      "(tensor([1, 1, 0, 1, 1], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "4\n",
      "\n",
      "\n",
      "tensor([0.9156, 0.1429, 0.0948, 0.1366, 0.2388])\n",
      "5\n",
      "(tensor([1, 0, 0, 0, 1], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "2\n",
      "\n",
      "\n",
      "tensor([0.1505, 0.1637, 0.0952, 0.1391, 0.1039])\n",
      "5\n",
      "(tensor([0, 0, 0, 0, 0], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "0\n",
      "\n",
      "\n",
      "tensor([0.9968, 0.9423, 0.2264, 0.8769, 0.8420])\n",
      "0\n",
      "(tensor([1, 1, 1, 1, 1], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "5\n",
      "\n",
      "\n",
      "tensor([0.1712, 0.2382, 0.0995, 0.9085, 0.7479])\n",
      "5\n",
      "(tensor([0, 1, 0, 1, 1], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "3\n",
      "\n",
      "\n",
      "tensor([0.8726, 0.8873, 0.0545, 0.3298, 0.8384])\n",
      "5\n",
      "(tensor([1, 1, 0, 1, 1], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "4\n",
      "\n",
      "\n",
      "tensor([0.9290, 0.9836, 0.8165, 0.9374, 0.7266])\n",
      "0\n",
      "(tensor([1, 1, 1, 1, 1], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "5\n",
      "\n",
      "\n",
      "tensor([0.9713, 0.0756, 0.1178, 0.0630, 0.0357])\n",
      "5\n",
      "(tensor([1, 0, 0, 0, 0], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "1\n",
      "\n",
      "\n",
      "tensor([0.1986, 0.9132, 0.1961, 0.8438, 0.9343])\n",
      "5\n",
      "(tensor([0, 1, 0, 1, 1], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "3\n",
      "\n",
      "\n",
      "tensor([0.8514, 0.8116, 0.0902, 0.3166, 0.1696])\n",
      "5\n",
      "(tensor([1, 1, 0, 1, 0], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "3\n",
      "\n",
      "\n",
      "tensor([0.0102, 0.8074, 0.9252, 0.8940, 0.0646])\n",
      "5\n",
      "(tensor([0, 1, 1, 1, 0], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "3\n",
      "\n",
      "\n",
      "tensor([7.5685e-02, 1.4307e-01, 9.9580e-01, 1.3543e-02, 3.2732e-04])\n",
      "5\n",
      "(tensor([0, 0, 1, 0, 0], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "1\n",
      "\n",
      "\n",
      "tensor([0.8642, 0.9865, 0.7111, 0.8586, 0.3387])\n",
      "0\n",
      "(tensor([1, 1, 1, 1, 1], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "5\n",
      "\n",
      "\n",
      "tensor([0.0290, 0.1570, 0.9665, 0.9297, 0.9506])\n",
      "5\n",
      "(tensor([0, 0, 1, 1, 1], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "3\n",
      "\n",
      "\n",
      "tensor([0.0297, 0.0743, 0.9852, 0.0201, 0.0093])\n",
      "5\n",
      "(tensor([0, 0, 1, 0, 0], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "1\n",
      "\n",
      "\n",
      "tensor([0.8305, 0.0474, 0.8914, 0.8547, 0.8419])\n",
      "5\n",
      "(tensor([1, 0, 1, 1, 1], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "4\n",
      "\n",
      "\n",
      "tensor([0.7428, 0.8067, 0.8739, 0.0979, 0.2353])\n",
      "5\n",
      "(tensor([1, 1, 1, 0, 1], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "4\n",
      "\n",
      "\n",
      "tensor([0.8118, 0.9300, 0.9275, 0.2380, 0.1189])\n",
      "5\n",
      "(tensor([1, 1, 1, 1, 0], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "4\n",
      "\n",
      "\n",
      "tensor([0.0912, 0.0964, 0.1570, 0.1002, 0.0294])\n",
      "5\n",
      "(tensor([0, 0, 0, 0, 0], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "0\n",
      "\n",
      "\n",
      "tensor([0.0925, 0.7744, 0.2100, 0.7739, 0.6929])\n",
      "5\n",
      "(tensor([0, 1, 1, 1, 1], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "4\n",
      "\n",
      "\n",
      "tensor([0.0493, 0.1289, 0.0276, 0.7458, 0.1646])\n",
      "5\n",
      "(tensor([0, 0, 0, 1, 0], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "1\n",
      "\n",
      "\n",
      "tensor([0.1186, 0.9031, 0.8973, 0.1528, 0.2060])\n",
      "5\n",
      "(tensor([0, 1, 1, 0, 1], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "3\n",
      "\n",
      "\n",
      "tensor([0.1683, 0.1866, 0.0323, 0.1853, 0.1382])\n",
      "5\n",
      "(tensor([0, 0, 0, 0, 0], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "0\n",
      "\n",
      "\n",
      "tensor([0.8890, 0.8213, 0.8072, 0.8770, 0.8110])\n",
      "0\n",
      "(tensor([1, 1, 1, 1, 1], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "5\n",
      "\n",
      "\n",
      "tensor([0.1181, 0.0350, 0.0664, 0.1437, 0.9144])\n",
      "5\n",
      "(tensor([0, 0, 0, 0, 1], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "1\n",
      "\n",
      "\n",
      "tensor([0.9464, 0.9735, 0.8899, 0.8819, 0.2155])\n",
      "0\n",
      "(tensor([1, 1, 1, 1, 1], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "5\n",
      "\n",
      "\n",
      "tensor([0.8685, 0.6474, 0.7520, 0.9740, 0.0466])\n",
      "5\n",
      "(tensor([1, 1, 1, 1, 0], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "4\n",
      "\n",
      "\n",
      "tensor([0.8421, 0.7399, 0.9863, 0.8709, 0.8731])\n",
      "0\n",
      "(tensor([1, 1, 1, 1, 1], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "5\n",
      "\n",
      "\n",
      "tensor([0.8029, 0.8578, 0.9468, 0.8240, 0.0608])\n",
      "5\n",
      "(tensor([1, 1, 1, 1, 0], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "4\n",
      "\n",
      "\n",
      "tensor([0.8517, 0.7914, 0.9741, 0.0817, 0.2564])\n",
      "5\n",
      "(tensor([1, 1, 1, 0, 1], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "4\n",
      "\n",
      "\n",
      "tensor([0.8052, 0.8766, 0.5899, 0.0542, 0.0796])\n",
      "5\n",
      "(tensor([1, 1, 1, 0, 0], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "3\n",
      "\n",
      "\n",
      "tensor([0.1648, 0.1578, 0.7868, 0.8347, 0.2003])\n",
      "5\n",
      "(tensor([0, 0, 1, 1, 1], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "3\n",
      "\n",
      "\n",
      "tensor([0.1861, 0.8537, 0.8653, 0.9609, 0.1092])\n",
      "5\n",
      "(tensor([0, 1, 1, 1, 0], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "3\n",
      "\n",
      "\n",
      "tensor([0.1777, 0.1182, 0.0477, 0.0645, 0.9167])\n",
      "5\n",
      "(tensor([0, 0, 0, 0, 1], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "1\n",
      "\n",
      "\n",
      "tensor([0.8197, 0.0634, 0.0681, 0.7217, 0.8871])\n",
      "5\n",
      "(tensor([1, 0, 0, 1, 1], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "3\n",
      "\n",
      "\n",
      "tensor([0.9688, 0.1019, 0.2940, 0.1515, 0.0461])\n",
      "5\n",
      "(tensor([1, 0, 1, 0, 0], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "2\n",
      "\n",
      "\n",
      "tensor([0.0675, 0.1084, 0.9961, 0.0804, 0.1466])\n",
      "5\n",
      "(tensor([0, 0, 1, 0, 0], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "1\n",
      "\n",
      "\n",
      "tensor([0.0069, 0.0384, 0.0726, 0.1049, 0.2714])\n",
      "5\n",
      "(tensor([0, 0, 0, 0, 1], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "1\n",
      "\n",
      "\n",
      "tensor([0.2292, 0.0665, 0.7592, 0.1655, 0.9571])\n",
      "5\n",
      "(tensor([1, 0, 1, 0, 1], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "3\n",
      "\n",
      "\n",
      "tensor([0.1216, 0.8947, 0.7819, 0.1657, 0.1177])\n",
      "5\n",
      "(tensor([0, 1, 1, 0, 0], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "2\n",
      "\n",
      "\n",
      "tensor([0.8858, 0.0393, 0.2067, 0.1987, 0.7810])\n",
      "5\n",
      "(tensor([1, 0, 1, 0, 1], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "3\n",
      "\n",
      "\n",
      "tensor([0.1461, 0.8682, 0.9800, 0.8455, 0.9406])\n",
      "5\n",
      "(tensor([0, 1, 1, 1, 1], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "4\n",
      "\n",
      "\n",
      "tensor([0.0970, 0.7023, 0.7299, 0.9196, 0.0935])\n",
      "5\n",
      "(tensor([0, 1, 1, 1, 0], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "3\n",
      "\n",
      "\n",
      "tensor([0.9399, 0.0404, 0.8688, 0.2973, 0.0455])\n",
      "5\n",
      "(tensor([1, 0, 1, 1, 0], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "3\n",
      "\n",
      "\n",
      "tensor([0.8237, 0.9377, 0.7446, 0.2157, 0.8553])\n",
      "0\n",
      "(tensor([1, 1, 1, 1, 1], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "5\n",
      "\n",
      "\n",
      "tensor([0.7179, 0.8430, 0.8755, 0.8658, 0.0773])\n",
      "5\n",
      "(tensor([1, 1, 1, 1, 0], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "4\n",
      "\n",
      "\n",
      "tensor([0.0337, 0.9634, 0.1927, 0.9206, 0.1605])\n",
      "5\n",
      "(tensor([0, 1, 0, 1, 0], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "2\n",
      "\n",
      "\n",
      "tensor([0.6933, 0.9663, 0.8566, 0.8736, 0.9435])\n",
      "0\n",
      "(tensor([1, 1, 1, 1, 1], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "5\n",
      "\n",
      "\n",
      "tensor([0.9329, 0.7912, 0.0675, 0.2052, 0.8369])\n",
      "5\n",
      "(tensor([1, 1, 0, 1, 1], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "4\n",
      "\n",
      "\n",
      "tensor([0.0576, 0.8833, 0.8829, 0.7688, 0.0762])\n",
      "5\n",
      "(tensor([0, 1, 1, 1, 0], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "3\n",
      "\n",
      "\n",
      "tensor([0.9077, 0.8307, 0.0297, 0.7877, 0.1853])\n",
      "5\n",
      "(tensor([1, 1, 0, 1, 0], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "3\n",
      "\n",
      "\n",
      "tensor([0.7646, 0.1672, 0.8833, 0.7612, 0.0644])\n",
      "5\n",
      "(tensor([1, 0, 1, 1, 0], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "3\n",
      "\n",
      "\n",
      "tensor([0.8573, 0.0763, 0.9983, 0.9340, 0.2030])\n",
      "5\n",
      "(tensor([1, 0, 1, 1, 1], dtype=torch.uint8), tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]))\n",
      "4\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (tp_batch,) in enumerate(tp_dataloader_testing):\n",
    "    print(tp_batch[0])\n",
    "    print(costSharingDelay(tp_batch[0]))\n",
    "    print(costSharingSupervisionRule(tp_batch[0]))\n",
    "    print(torch.sum(costSharingSupervisionRule(tp_batch[0])[0]).item())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 1]), tensor([0, 0, 0, 1, 0]), tensor([0, 0, 0, 1, 1]), tensor([0, 0, 1, 0, 0]), tensor([0, 0, 1, 0, 1]), tensor([0, 0, 1, 1, 0]), tensor([0, 0, 1, 1, 1]), tensor([0, 1, 0, 0, 0]), tensor([0, 1, 0, 0, 1]), tensor([0, 1, 0, 1, 0]), tensor([0, 1, 0, 1, 1]), tensor([0, 1, 1, 0, 0]), tensor([0, 1, 1, 0, 1]), tensor([0, 1, 1, 1, 0]), tensor([0, 1, 1, 1, 1]), tensor([1, 0, 0, 0, 0]), tensor([1, 0, 0, 0, 1]), tensor([1, 0, 0, 1, 0]), tensor([1, 0, 0, 1, 1]), tensor([1, 0, 1, 0, 0]), tensor([1, 0, 1, 0, 1]), tensor([1, 0, 1, 1, 0]), tensor([1, 0, 1, 1, 1]), tensor([1, 1, 0, 0, 0]), tensor([1, 1, 0, 0, 1]), tensor([1, 1, 0, 1, 0]), tensor([1, 1, 0, 1, 1]), tensor([1, 1, 1, 0, 0]), tensor([1, 1, 1, 0, 1]), tensor([1, 1, 1, 1, 0]), tensor([1, 1, 1, 1, 1])]\n",
      "\n",
      "tensor([0.9068, 0.8811, 0.8624, 0.0336, 0.0558])\n",
      "tensor(0.2036, grad_fn=<SelectBackward>)\n",
      "tensor([0.2036, 0.1768, 0.1865, 0.2182, 0.2149], grad_fn=<SoftmaxBackward>)\n",
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "allBits = [torch.tensor(bits) for bits in itertools.product([0, 1], repeat=n)]\n",
    "print(allBits)\n",
    "\n",
    "for batch_idx, (tp_batch,) in enumerate(tp_dataloader_testing):\n",
    "    penalty = 0\n",
    "    loss = penalty * penaltyLambda\n",
    "    for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                delay1 = tpToTotalDelay(tp1)\n",
    "                delay0 = tpToTotalDelay(tp0)\n",
    "                #loss = loss + (1 - cdf(offer)) * delay1 + cdf(offer) * delay0\n",
    "                loss = loss + (1 - cdf(offer,order)) * delay1 + cdf(offer,order) * delay0\n",
    "    print()\n",
    "    print(tp)\n",
    "    tp1 = tp.clone()\n",
    "    tp1[0] = 1\n",
    "    tp0 = tp.clone()\n",
    "    tp0[0] = 0\n",
    "    offer = tpToPayments(tp1)[0]\n",
    "    print(offer)\n",
    "    print(tpToPayments(tp1))\n",
    "    print(delay1)\n",
    "    print(delay0)\n",
    "    break\n",
    "#print(loss)\n",
    "#print(penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordAndReport(name, source, loss):\n",
    "    source.append(loss)\n",
    "    realLength = len(source)\n",
    "    #print(f\"{name} ({realLength}): {loss}\")\n",
    "    print(name,realLength,\":\" ,loss)\n",
    "\n",
    "def supervisionTrain(epoch, supervisionRule):\n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        loss = penalty * penaltyLambda /100000\n",
    "        \n",
    "        for tp in tp_batch:\n",
    "            bits, payments = supervisionRule(tp)\n",
    "            #print(\"bits\",bitsToPayments(bits))\n",
    "            loss = loss + F.mse_loss(bitsToPayments(bits), payments)\n",
    "\n",
    "        loss = loss / len(tp_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "losslist=[];\n",
    "losslistname=[];\n",
    "losslisttemp=[];\n",
    "def test_batch():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lenLoss= 0\n",
    "        nnLoss = 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "            for tp in tp_batch:\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        nnLoss/=lenLoss\n",
    "    return nnLoss\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    global losslisttemp; \n",
    "    model.train()\n",
    "    for batch_idx, (tp_batch,) in enumerate(tp_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        penalty = 0\n",
    "        loss = penalty * penaltyLambda\n",
    "        for tp in tp_batch:\n",
    "            for i in range(n):\n",
    "                tp1 = tp.clone()\n",
    "                tp1[i] = 1\n",
    "                tp0 = tp.clone()\n",
    "                tp0[i] = 0\n",
    "                offer = tpToPayments(tp1)[i]\n",
    "                delay1 = tpToTotalDelay(tp1)\n",
    "                delay0 = tpToTotalDelay(tp0)\n",
    "                if(order!=\"independent1\" and order!=\"independent2\"):\n",
    "                    loss = loss + (1 - cdf(offer,order)) * delay1 + cdf(offer,order) * delay0\n",
    "                else:\n",
    "                    loss = loss + (1 - cdf(offer,order,i)) * delay1 + cdf(offer,order,i) * delay0\n",
    "\n",
    "        loss = loss / len(tp_batch) / n\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            losstemp=test_batch();\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(tp_batch),\n",
    "                    len(tp_dataloader.dataset),\n",
    "                    100.0 * batch_idx / len(tp_dataloader),\n",
    "                    loss.item(),\n",
    "                ),\"testing loss:\",losstemp\n",
    "            )\n",
    "            losslisttemp.append(losstemp);\n",
    "            \n",
    "    \n",
    "allBits = [torch.tensor(bits, dtype=torch.int16) for bits in itertools.product([0, 1], repeat=n)]\n",
    "\n",
    "def test():\n",
    "    global losslisttemp; \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        costSharingLoss = 0\n",
    "        dpLoss = 0\n",
    "        nnLoss = 0\n",
    "        lenLoss= 0\n",
    "        for (tp_batch,) in tp_dataloader_testing:\n",
    "\n",
    "            for tp in tp_batch:\n",
    "                costSharingLoss += costSharingDelay(tp)\n",
    "                dpLoss += dpDelay(tp)\n",
    "                nnLoss += tpToTotalDelay(tp)\n",
    "            lenLoss+=len(tp_batch)\n",
    "        costSharingLoss /= lenLoss\n",
    "        dpLoss /= lenLoss\n",
    "        nnLoss /= lenLoss\n",
    "        #print(lenLoss)\n",
    "        losslisttemp.append(nnLoss);\n",
    "        recordAndReport(\"NN\", runningLossNN, nnLoss)\n",
    "        recordAndReport(\"CS\", runningLossCS, costSharingLoss)\n",
    "        recordAndReport(\"DP\", runningLossDP, dpLoss)\n",
    "        print(\"DP:\",n*(1-dp[n, dpPrecision]))\n",
    "        #for i in range(n, 0, -1):\n",
    "        #    print(\"Heuristic:\",i,5*(1-dp_H[i, i, dpPrecision]))\n",
    "        for i in range(n, 0, -1):\n",
    "            print(\n",
    "                    tpToPayments(\n",
    "                            torch.tensor([0 if ii >= i else 1 for ii in range(n)], dtype=torch.float32)\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "dpPrecision = 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8413)\n",
      "Train Epoch: 1 [0/15000 (0%)]\tLoss: 0.000258\n",
      "Train Epoch: 1 [2560/15000 (17%)]\tLoss: 0.000013\n",
      "Train Epoch: 1 [5120/15000 (34%)]\tLoss: 0.000001\n",
      "Train Epoch: 1 [7680/15000 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [10240/15000 (68%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [12800/15000 (85%)]\tLoss: 0.000000\n",
      "NN 1 : 4.6273333333333335\n",
      "CS 1 : 4.6273333333333335\n",
      "DP 1 : 3.574\n",
      "DP: 3.581627309322357\n",
      "tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])\n",
      "tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])\n",
      "tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])\n",
      "tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])\n",
      "tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])\n",
      "Train Epoch: 1 [0/15000 (0%)]\tLoss: 4.623933 testing loss: 4.621666666666667\n",
      "Train Epoch: 1 [2560/15000 (17%)]\tLoss: 3.584737 testing loss: 3.599666666666667\n",
      "Train Epoch: 1 [5120/15000 (34%)]\tLoss: 3.544688 testing loss: 3.5813333333333333\n",
      "Train Epoch: 1 [7680/15000 (51%)]\tLoss: 3.653796 testing loss: 3.564\n",
      "Train Epoch: 1 [10240/15000 (68%)]\tLoss: 3.502817 testing loss: 3.565\n",
      "Train Epoch: 1 [12800/15000 (85%)]\tLoss: 3.643383 testing loss: 3.5646666666666667\n",
      "NN 2 : 3.562333333333333\n",
      "CS 2 : 4.6273333333333335\n",
      "DP 2 : 3.574\n",
      "DP: 3.581627309322357\n",
      "tensor([0.0583, 0.0569, 0.0566, 0.0578, 0.7704])\n",
      "tensor([0.0583, 0.0569, 0.0566, 0.0578, 0.7704])\n",
      "tensor([0.0583, 0.0569, 0.0566, 0.0578, 0.7704])\n",
      "tensor([0.0583, 0.0569, 0.0566, 0.0578, 0.7704])\n",
      "tensor([0.0583, 0.0569, 0.0566, 0.0578, 0.7704])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, supervisionEpochs + 1):\n",
    "    print(distributionRatio1)\n",
    "    supervisionTrain(epoch, costSharingSupervisionRule)\n",
    "    #supervisionTrain(epoch, dpSupervisionRule)\n",
    "test()\n",
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    train(epoch)\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "order1=\"\"\n",
    "losslistname.append(order+\" \"+order1);\n",
    "losslist.append(losslisttemp);\n",
    "losslisttemp=[];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colorlist=[\"#D2691E\",'#4169E1',\"#9ACD32\",\"#B22222\",\"#FF00FF\",\"#708090\"]\n",
    "for i in range(len(losslist)):\n",
    "    plt.plot(losslist[i], 'ro-', color=colorlist[i], alpha=0.8, label=losslistname[i])\n",
    "\n",
    "# 显示标签，如果不加这句，即使加了label='一些数字'的参数，最终还是不会显示标签\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('trianing times')\n",
    "plt.ylabel('delay')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
